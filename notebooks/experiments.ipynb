{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Experimenting rec approaches",
   "id": "bd82e23af48d488a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-06T22:17:37.257340Z",
     "start_time": "2025-08-06T22:17:34.732806Z"
    }
   },
   "source": [
    "import duckdb\n",
    "import torch\n",
    "import polars as pl\n",
    "import torchrec"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:17:38.897186Z",
     "start_time": "2025-08-06T22:17:38.895278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TRAIN_CUT_TIMESTAMP = '2017-01-01'  # Fetch reviews up to this day FOR TRAINING\n",
    "VAL_CUT_TIMESTAMP = '2018-01-01'  # Fetch reviews up to this day FOR VALIDATION"
   ],
   "id": "2d59a3e437a09288",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:17:39.896034Z",
     "start_time": "2025-08-06T22:17:39.838442Z"
    }
   },
   "cell_type": "code",
   "source": "duckdb_conn = duckdb.connect('../data/steam.duckdb', read_only=True)",
   "id": "23f962531e19e5c2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train dataset",
   "id": "28daba5c0ce680c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:17:42.431439Z",
     "start_time": "2025-08-06T22:17:41.343046Z"
    }
   },
   "cell_type": "code",
   "source": "train_games_df = duckdb_conn.sql(f\"SELECT * FROM game_features WHERE DATE(game_review_day) <= '{TRAIN_CUT_TIMESTAMP}'\").pl()",
   "id": "eda219fc8ac7c8e8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:18:17.232630Z",
     "start_time": "2025-08-06T22:17:43.821452Z"
    }
   },
   "cell_type": "code",
   "source": "train_reviews_df = duckdb_conn.sql(f\"SELECT * FROM fact_reviews WHERE DATE(timestamp_created) <= '{TRAIN_CUT_TIMESTAMP}'\").pl()",
   "id": "f55f1b1cd7a4a74d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c01a258d1f34a4c8642e9a069f59b88"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:18:20.878728Z",
     "start_time": "2025-08-06T22:18:20.854219Z"
    }
   },
   "cell_type": "code",
   "source": "len(train_reviews_df)",
   "id": "24bb7c62c16dd005",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3548588"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:18:23.307629Z",
     "start_time": "2025-08-06T22:18:23.221344Z"
    }
   },
   "cell_type": "code",
   "source": "dim_games_df = duckdb_conn.sql(f\"SELECT * FROM dim_games WHERE DATE(game_prerelease_date) <= '{TRAIN_CUT_TIMESTAMP}'\").pl()",
   "id": "8f0c85b0978b54c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:18:25.230799Z",
     "start_time": "2025-08-06T22:18:24.919884Z"
    }
   },
   "cell_type": "code",
   "source": "dim_users_df = duckdb_conn.sql(f\"SELECT * FROM dim_users WHERE DATE(first_review_timestamp) <= '{TRAIN_CUT_TIMESTAMP}'\").pl()",
   "id": "2222724047001173",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:18:27.016324Z",
     "start_time": "2025-08-06T22:18:26.725574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get users and games that are new during validation period\n",
    "new_users = duckdb_conn.sql(f\"SELECT * FROM dim_users WHERE DATE(first_review_timestamp) > '{TRAIN_CUT_TIMESTAMP}' \"\n",
    "                            f\"AND DATE(first_review_timestamp) <= '{VAL_CUT_TIMESTAMP}'\").pl()\n",
    "new_games = duckdb_conn.sql(f\"SELECT * FROM dim_games WHERE DATE(game_prerelease_date) > '{TRAIN_CUT_TIMESTAMP}' \"\n",
    "                            f\"AND DATE(game_prerelease_date) <= '{VAL_CUT_TIMESTAMP}'\").pl()"
   ],
   "id": "b5604ae4d1d84915",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:18:28.334628Z",
     "start_time": "2025-08-06T22:18:28.293802Z"
    }
   },
   "cell_type": "code",
   "source": "new_games.sort(\"game_index\")",
   "id": "a4269b04b3be4af8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (5_823, 14)\n",
       "┌────────────┬─────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ game_index ┆ game_id ┆ game_name ┆ game_is_f ┆ … ┆ game_prer ┆ game_shor ┆ game_revi ┆ game_revi │\n",
       "│ ---        ┆ ---     ┆ ---       ┆ ree       ┆   ┆ elease_da ┆ t_descrip ┆ ew_score  ┆ ew_score_ │\n",
       "│ i64        ┆ i64     ┆ str       ┆ ---       ┆   ┆ te        ┆ tion      ┆ ---       ┆ descripti │\n",
       "│            ┆         ┆           ┆ bool      ┆   ┆ ---       ┆ ---       ┆ i64       ┆ on        │\n",
       "│            ┆         ┆           ┆           ┆   ┆ datetime[ ┆ str       ┆           ┆ ---       │\n",
       "│            ┆         ┆           ┆           ┆   ┆ μs, Ameri ┆           ┆           ┆ str       │\n",
       "│            ┆         ┆           ┆           ┆   ┆ ca/Lima]  ┆           ┆           ┆           │\n",
       "╞════════════╪═════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 9098       ┆ 562240  ┆ The       ┆ false     ┆ … ┆ 2017-01-0 ┆ The       ┆ 5         ┆ Mixed     │\n",
       "│            ┆         ┆ Spirit    ┆           ┆   ┆ 2         ┆ Spirit    ┆           ┆           │\n",
       "│            ┆         ┆ Underneat ┆           ┆   ┆ 00:00:00  ┆ Underneat ┆           ┆           │\n",
       "│            ┆         ┆ h         ┆           ┆   ┆ -05       ┆ h is a    ┆           ┆           │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆ fir…      ┆           ┆           │\n",
       "│ 9099       ┆ 540900  ┆ Nuclear   ┆ false     ┆ … ┆ 2017-01-0 ┆ Nuclear   ┆ 6         ┆ Mostly    │\n",
       "│            ┆         ┆ Contingen ┆           ┆   ┆ 2         ┆ Contingen ┆           ┆ Positive  │\n",
       "│            ┆         ┆ cy        ┆           ┆   ┆ 00:00:00  ┆ cy is a   ┆           ┆           │\n",
       "│            ┆         ┆           ┆           ┆   ┆ -05       ┆ top d…    ┆           ┆           │\n",
       "│ 9100       ┆ 560930  ┆ Marimba   ┆ false     ┆ … ┆ 2017-01-0 ┆ Marimba   ┆ 0         ┆ 1 user    │\n",
       "│            ┆         ┆ VR        ┆           ┆   ┆ 2         ┆ VR is a   ┆           ┆ reviews   │\n",
       "│            ┆         ┆           ┆           ┆   ┆ 00:00:00  ┆ great way ┆           ┆           │\n",
       "│            ┆         ┆           ┆           ┆   ┆ -05       ┆ for …     ┆           ┆           │\n",
       "│ 9101       ┆ 520080  ┆ Your Star ┆ false     ┆ … ┆ 2017-01-0 ┆ In the    ┆ 0         ┆ 4 user    │\n",
       "│            ┆         ┆           ┆           ┆   ┆ 2         ┆ game, you ┆           ┆ reviews   │\n",
       "│            ┆         ┆           ┆           ┆   ┆ 00:00:00  ┆ control a ┆           ┆           │\n",
       "│            ┆         ┆           ┆           ┆   ┆ -05       ┆ you…      ┆           ┆           │\n",
       "│ 9102       ┆ 571550  ┆ Hawks     ┆ false     ┆ … ┆ 2017-01-0 ┆ Hawk and  ┆ 0         ┆ 4 user    │\n",
       "│            ┆         ┆ Tactical  ┆           ┆   ┆ 2         ┆ his       ┆           ┆ reviews   │\n",
       "│            ┆         ┆           ┆           ┆   ┆ 00:00:00  ┆ friends   ┆           ┆           │\n",
       "│            ┆         ┆           ┆           ┆   ┆ -05       ┆ have      ┆           ┆           │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆ swor…     ┆           ┆           │\n",
       "│ …          ┆ …       ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 14916      ┆ 766700  ┆ PearsAndG ┆ false     ┆ … ┆ 2018-01-0 ┆ A boy who ┆ 5         ┆ Mixed     │\n",
       "│            ┆         ┆ rayWitch  ┆           ┆   ┆ 1         ┆ likes &qu ┆           ┆           │\n",
       "│            ┆         ┆           ┆           ┆   ┆ 00:00:00  ┆ ot;Apples ┆           ┆           │\n",
       "│            ┆         ┆           ┆           ┆   ┆ -05       ┆ &q…       ┆           ┆           │\n",
       "│ 14917      ┆ 723020  ┆ 3571 The  ┆ false     ┆ … ┆ 2018-01-0 ┆ 3571 The  ┆ 0         ┆ 5 user    │\n",
       "│            ┆         ┆ Game      ┆           ┆   ┆ 1         ┆ Game is a ┆           ┆ reviews   │\n",
       "│            ┆         ┆           ┆           ┆   ┆ 00:00:00  ┆ 3D        ┆           ┆           │\n",
       "│            ┆         ┆           ┆           ┆   ┆ -05       ┆ procedur… ┆           ┆           │\n",
       "│ 14918      ┆ 524350  ┆ TileDynas ┆ false     ┆ … ┆ 2018-01-0 ┆ Looking   ┆ 0         ┆ 2 user    │\n",
       "│            ┆         ┆ ty FPS    ┆           ┆   ┆ 1         ┆ to bring  ┆           ┆ reviews   │\n",
       "│            ┆         ┆ Arena     ┆           ┆   ┆ 00:00:00  ┆ the old   ┆           ┆           │\n",
       "│            ┆         ┆           ┆           ┆   ┆ -05       ┆ arcad…    ┆           ┆           │\n",
       "│ 14919      ┆ 770520  ┆ Boomer    ┆ false     ┆ … ┆ 2018-01-0 ┆ Boomer    ┆ 0         ┆ No        │\n",
       "│            ┆         ┆ Rampage   ┆           ┆   ┆ 1         ┆ Rampage   ┆           ┆ reviews   │\n",
       "│            ┆         ┆           ┆           ┆   ┆ 00:00:00  ┆ is a      ┆           ┆           │\n",
       "│            ┆         ┆           ┆           ┆   ┆ -05       ┆ puzzle    ┆           ┆           │\n",
       "│            ┆         ┆           ┆           ┆   ┆           ┆ gam…      ┆           ┆           │\n",
       "│ 14920      ┆ 470310  ┆ TROUBLESH ┆ false     ┆ … ┆ 2018-01-0 ┆ TROUBLESH ┆ 8         ┆ Very      │\n",
       "│            ┆         ┆ OOTER:    ┆           ┆   ┆ 1         ┆ OOTER:    ┆           ┆ Positive  │\n",
       "│            ┆         ┆ Abandoned ┆           ┆   ┆ 04:46:45  ┆ Abandoned ┆           ┆           │\n",
       "│            ┆         ┆ Chil…     ┆           ┆   ┆ -05       ┆ Chil…     ┆           ┆           │\n",
       "└────────────┴─────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5_823, 14)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>game_index</th><th>game_id</th><th>game_name</th><th>game_is_free</th><th>game_developers</th><th>game_publishers</th><th>game_categories</th><th>game_genres</th><th>game_steam_release_date</th><th>game_release_date</th><th>game_prerelease_date</th><th>game_short_description</th><th>game_review_score</th><th>game_review_score_description</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>bool</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>datetime[μs]</td><td>datetime[μs, America/Lima]</td><td>datetime[μs, America/Lima]</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>9098</td><td>562240</td><td>&quot;The Spirit Underneath&quot;</td><td>false</td><td>[&quot;Acuze Interactives&quot;]</td><td>[&quot;Displacement Studios&quot;]</td><td>[&quot;Single-player&quot;, &quot;Steam Trading Cards&quot;, … &quot;Family Sharing&quot;]</td><td>[&quot;Violent&quot;, &quot;Adventure&quot;, &quot;Indie&quot;]</td><td>2017-01-02 00:00:00</td><td>2017-01-02 00:00:00 -05</td><td>2017-01-02 00:00:00 -05</td><td>&quot;The Spirit Underneath is a fir…</td><td>5</td><td>&quot;Mixed&quot;</td></tr><tr><td>9099</td><td>540900</td><td>&quot;Nuclear Contingency&quot;</td><td>false</td><td>[&quot;Amaterasu Software&quot;]</td><td>[&quot;Amaterasu Software&quot;]</td><td>[&quot;Single-player&quot;, &quot;Steam Achievements&quot;, … &quot;Family Sharing&quot;]</td><td>[&quot;Action&quot;, &quot;Adventure&quot;, &quot;Indie&quot;]</td><td>2017-01-02 00:00:00</td><td>2017-01-02 00:00:00 -05</td><td>2017-01-02 00:00:00 -05</td><td>&quot;Nuclear Contingency is a top d…</td><td>6</td><td>&quot;Mostly Positive&quot;</td></tr><tr><td>9100</td><td>560930</td><td>&quot;Marimba VR&quot;</td><td>false</td><td>[&quot;Ruby Games&quot;]</td><td>[&quot;Ruby Games&quot;]</td><td>[&quot;Single-player&quot;, &quot;Tracked Motion Controller&quot;, … &quot;Family Sharing&quot;]</td><td>[&quot;Casual&quot;, &quot;Indie&quot;, &quot;Simulation&quot;]</td><td>2017-01-02 00:00:00</td><td>2017-01-02 00:00:00 -05</td><td>2017-01-02 00:00:00 -05</td><td>&quot;Marimba VR is a great way for …</td><td>0</td><td>&quot;1 user reviews&quot;</td></tr><tr><td>9101</td><td>520080</td><td>&quot;Your Star&quot;</td><td>false</td><td>[&quot;natahem&quot;]</td><td>[&quot;natahem&quot;]</td><td>[&quot;Single-player&quot;, &quot;Steam Achievements&quot;, … &quot;Family Sharing&quot;]</td><td>[&quot;Adventure&quot;, &quot;Indie&quot;, &quot;RPG&quot;]</td><td>2017-01-02 00:00:00</td><td>2017-01-02 00:00:00 -05</td><td>2017-01-02 00:00:00 -05</td><td>&quot;In the game, you control a you…</td><td>0</td><td>&quot;4 user reviews&quot;</td></tr><tr><td>9102</td><td>571550</td><td>&quot;Hawks Tactical&quot;</td><td>false</td><td>[&quot;LYS&quot;]</td><td>[&quot;Indie&quot;]</td><td>[&quot;Single-player&quot;, &quot;Family Sharing&quot;]</td><td>[&quot;Adventure&quot;, &quot;Indie&quot;, … &quot;Strategy&quot;]</td><td>2017-01-02 00:00:00</td><td>2017-01-02 00:00:00 -05</td><td>2017-01-02 00:00:00 -05</td><td>&quot;Hawk and his friends have swor…</td><td>0</td><td>&quot;4 user reviews&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>14916</td><td>766700</td><td>&quot;PearsAndGrayWitch&quot;</td><td>false</td><td>[&quot;YAOYICHEN&quot;]</td><td>[&quot;YAOYICHEN&quot;]</td><td>[&quot;Single-player&quot;, &quot;Family Sharing&quot;]</td><td>[&quot;Action&quot;, &quot;Adventure&quot;, … &quot;RPG&quot;]</td><td>2018-01-01 00:00:00</td><td>2018-01-01 00:00:00 -05</td><td>2018-01-01 00:00:00 -05</td><td>&quot;A boy who likes &amp;quot;Apples&amp;q…</td><td>5</td><td>&quot;Mixed&quot;</td></tr><tr><td>14917</td><td>723020</td><td>&quot;3571 The Game&quot;</td><td>false</td><td>[&quot;FRAPPA STUDIO&quot;]</td><td>[&quot;FRAPPA STUDIO&quot;]</td><td>[&quot;Single-player&quot;, &quot;Steam Leaderboards&quot;, &quot;Family Sharing&quot;]</td><td>[&quot;Action&quot;, &quot;Adventure&quot;, … &quot;Early Access&quot;]</td><td>2018-01-01 00:00:00</td><td>2018-01-01 00:00:00 -05</td><td>2018-01-01 00:00:00 -05</td><td>&quot;3571 The Game is a 3D procedur…</td><td>0</td><td>&quot;5 user reviews&quot;</td></tr><tr><td>14918</td><td>524350</td><td>&quot;TileDynasty FPS Arena&quot;</td><td>false</td><td>[&quot;Purpl3Grape&quot;]</td><td>[&quot;Purpl3Grape&quot;]</td><td>[&quot;Single-player&quot;, &quot;Multi-player&quot;, … &quot;Family Sharing&quot;]</td><td>[&quot;Action&quot;, &quot;Indie&quot;, &quot;Early Access&quot;]</td><td>2018-01-01 00:00:00</td><td>2018-01-01 00:00:00 -05</td><td>2018-01-01 00:00:00 -05</td><td>&quot;Looking to bring the old arcad…</td><td>0</td><td>&quot;2 user reviews&quot;</td></tr><tr><td>14919</td><td>770520</td><td>&quot;Boomer Rampage&quot;</td><td>false</td><td>[&quot;Lunarhellgames&quot;]</td><td>[&quot;Lunarhellgames&quot;]</td><td>[&quot;Single-player&quot;, &quot;Steam Achievements&quot;, … &quot;Family Sharing&quot;]</td><td>[&quot;Action&quot;, &quot;Casual&quot;, &quot;Indie&quot;]</td><td>2018-01-01 00:00:00</td><td>2018-01-01 00:00:00 -05</td><td>2018-01-01 00:00:00 -05</td><td>&quot;Boomer Rampage is a puzzle gam…</td><td>0</td><td>&quot;No reviews&quot;</td></tr><tr><td>14920</td><td>470310</td><td>&quot;TROUBLESHOOTER: Abandoned Chil…</td><td>false</td><td>[&quot;Dandylion&quot;]</td><td>[&quot;Dandylion&quot;]</td><td>[&quot;Single-player&quot;, &quot;Steam Achievements&quot;, … &quot;Family Sharing&quot;]</td><td>[&quot;Casual&quot;, &quot;Indie&quot;, … &quot;Strategy&quot;]</td><td>2020-04-23 00:00:00</td><td>2020-04-23 00:00:00 -05</td><td>2018-01-01 04:46:45 -05</td><td>&quot;TROUBLESHOOTER: Abandoned Chil…</td><td>8</td><td>&quot;Very Positive&quot;</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:18:41.615718Z",
     "start_time": "2025-08-06T22:18:40.431160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This may include completely new users and games on the validation data\n",
    "val_reviews_df = duckdb_conn.sql(f\"SELECT * FROM fact_reviews WHERE DATE(timestamp_created) > '{TRAIN_CUT_TIMESTAMP}' \"\n",
    "                                 f\"AND DATE(timestamp_created) <= '{VAL_CUT_TIMESTAMP}'\").pl()"
   ],
   "id": "499c0bd6c800e2cb",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:19:04.958607Z",
     "start_time": "2025-08-06T22:19:04.955475Z"
    }
   },
   "cell_type": "code",
   "source": "len(val_reviews_df)",
   "id": "7f7ef398a89db54f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1407008"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:19:07.457528Z",
     "start_time": "2025-08-06T22:19:07.377065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make sure all games and users on validation data are also present in training\n",
    "val_reviews_df = val_reviews_df.filter((~pl.col(\"game_index\").is_in(new_games[\"game_index\"].implode())) & (~pl.col(\"user_index\").is_in(new_users[\"user_index\"].implode())))"
   ],
   "id": "c56e3a169967ee1d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:19:12.102165Z",
     "start_time": "2025-08-06T22:19:12.099405Z"
    }
   },
   "cell_type": "code",
   "source": "len(val_reviews_df)",
   "id": "89a5b1a47ec85d08",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475615"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:19:14.117992Z",
     "start_time": "2025-08-06T22:19:14.107626Z"
    }
   },
   "cell_type": "code",
   "source": "entity_count = duckdb_conn.sql(f\"SELECT * FROM mart_entities_cumulative WHERE review_day = '{TRAIN_CUT_TIMESTAMP}'\").pl()",
   "id": "3a287562555e86f4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:19:16.693238Z",
     "start_time": "2025-08-06T22:19:15.679649Z"
    }
   },
   "cell_type": "code",
   "source": "duckdb_conn.close()",
   "id": "d31555a8c621aa05",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:19:17.576539Z",
     "start_time": "2025-08-06T22:19:17.567205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_users = int(entity_count[\"cumulative_user_count\"].first())\n",
    "num_games = int(entity_count[\"cumulative_game_count\"].first())"
   ],
   "id": "4af7632d43cbae1f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:20:00.521222Z",
     "start_time": "2025-08-06T22:20:00.517649Z"
    }
   },
   "cell_type": "code",
   "source": "num_games, num_users",
   "id": "a0ad2ed78dd0fe5f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9098, 1840563)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example using torchrec EmbeddingCollections",
   "id": "1f8d32851f413675"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T00:40:55.501757Z",
     "start_time": "2025-08-06T00:40:53.910375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ec = torchrec.EmbeddingCollection(\n",
    "    device=torch.device(\"cpu\"),\n",
    "    tables=[\n",
    "        torchrec.EmbeddingConfig(name=\"user_index\", embedding_dim=128, num_embeddings=num_users + 1),\n",
    "        torchrec.EmbeddingConfig(name=\"game_index\", embedding_dim=128, num_embeddings=num_games + 1),\n",
    "    ]\n",
    ")"
   ],
   "id": "abf479fdf615dad0",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T00:40:55.514588Z",
     "start_time": "2025-08-06T00:40:55.510219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = train_reviews_df.sample(10)\n",
    "sample"
   ],
   "id": "1a7b804cf25f1e9f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (10, 11)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬──────────┬───────────┬───────────┬───────────┐\n",
       "│ review_id ┆ user_inde ┆ game_inde ┆ review    ┆ … ┆ votes_up ┆ comment_c ┆ timestamp ┆ scrape_da │\n",
       "│ ---       ┆ x         ┆ x         ┆ ---       ┆   ┆ ---      ┆ ount      ┆ _created  ┆ te        │\n",
       "│ i64       ┆ ---       ┆ ---       ┆ str       ┆   ┆ i64      ┆ ---       ┆ ---       ┆ ---       │\n",
       "│           ┆ i64       ┆ i64       ┆           ┆   ┆          ┆ i64       ┆ datetime[ ┆ date      │\n",
       "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆ μs, Ameri ┆           │\n",
       "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆ ca/Lima]  ┆           │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2145352   ┆ 26162     ┆ 1315      ┆ Bizarrely ┆ … ┆ 1        ┆ 0         ┆ 2013-01-3 ┆ 2025-06-1 │\n",
       "│           ┆           ┆           ┆ addictive ┆   ┆          ┆           ┆ 0         ┆ 2         │\n",
       "│           ┆           ┆           ┆ . Good    ┆   ┆          ┆           ┆ 07:43:16  ┆           │\n",
       "│           ┆           ┆           ┆ fun.…     ┆   ┆          ┆           ┆ -05       ┆           │\n",
       "│ 19859019  ┆ 214812    ┆ 125       ┆ Classic   ┆ … ┆ 0        ┆ 0         ┆ 2015-12-2 ┆ 2025-06-1 │\n",
       "│           ┆           ┆           ┆ game.     ┆   ┆          ┆           ┆ 2         ┆ 2         │\n",
       "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆ 21:11:23  ┆           │\n",
       "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆ -05       ┆           │\n",
       "│ 21742139  ┆ 1344824   ┆ 5762      ┆ I've had  ┆ … ┆ 0        ┆ 0         ┆ 2016-03-1 ┆ 2025-06-1 │\n",
       "│           ┆           ┆           ┆ more fun  ┆   ┆          ┆           ┆ 0         ┆ 2         │\n",
       "│           ┆           ┆           ┆ than      ┆   ┆          ┆           ┆ 01:10:24  ┆           │\n",
       "│           ┆           ┆           ┆ expecte…  ┆   ┆          ┆           ┆ -05       ┆           │\n",
       "│ 8179044   ┆ 215832    ┆ 1087      ┆ This is   ┆ … ┆ 0        ┆ 0         ┆ 2013-12-2 ┆ 2025-06-1 │\n",
       "│           ┆           ┆           ┆ the best  ┆   ┆          ┆           ┆ 1         ┆ 2         │\n",
       "│           ┆           ┆           ┆ game of   ┆   ┆          ┆           ┆ 16:26:46  ┆           │\n",
       "│           ┆           ┆           ┆ all t…    ┆   ┆          ┆           ┆ -05       ┆           │\n",
       "│ 28772518  ┆ 1828608   ┆ 1602      ┆ I was one ┆ … ┆ 2        ┆ 0         ┆ 2016-12-2 ┆ 2025-06-1 │\n",
       "│           ┆           ┆           ┆ of those  ┆   ┆          ┆           ┆ 8         ┆ 2         │\n",
       "│           ┆           ┆           ┆ people    ┆   ┆          ┆           ┆ 19:46:25  ┆           │\n",
       "│           ┆           ┆           ┆ who …     ┆   ┆          ┆           ┆ -05       ┆           │\n",
       "│ 19831456  ┆ 843359    ┆ 1494      ┆ One of    ┆ … ┆ 0        ┆ 0         ┆ 2015-12-2 ┆ 2025-06-1 │\n",
       "│           ┆           ┆           ┆ those     ┆   ┆          ┆           ┆ 1         ┆ 2         │\n",
       "│           ┆           ┆           ┆ true      ┆   ┆          ┆           ┆ 16:21:36  ┆           │\n",
       "│           ┆           ┆           ┆ gems,     ┆   ┆          ┆           ┆ -05       ┆           │\n",
       "│           ┆           ┆           ┆ must b…   ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 24568957  ┆ 1537652   ┆ 6914      ┆ If Dark   ┆ … ┆ 31       ┆ 1         ┆ 2016-07-2 ┆ 2025-06-1 │\n",
       "│           ┆           ┆           ┆ Souls and ┆   ┆          ┆           ┆ 8         ┆ 2         │\n",
       "│           ┆           ┆           ┆ Geometry  ┆   ┆          ┆           ┆ 00:19:51  ┆           │\n",
       "│           ┆           ┆           ┆ Das…      ┆   ┆          ┆           ┆ -05       ┆           │\n",
       "│ 13598925  ┆ 739702    ┆ 2688      ┆ ;-( <3    ┆ … ┆ 0        ┆ 0         ┆ 2014-12-2 ┆ 2025-06-1 │\n",
       "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆ 5         ┆ 2         │\n",
       "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆ 17:46:32  ┆           │\n",
       "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆ -05       ┆           │\n",
       "│ 24908430  ┆ 493167    ┆ 1475      ┆ I can now ┆ … ┆ 0        ┆ 0         ┆ 2016-08-1 ┆ 2025-06-1 │\n",
       "│           ┆           ┆           ┆ sort of   ┆   ┆          ┆           ┆ 4         ┆ 2         │\n",
       "│           ┆           ┆           ┆ fly a     ┆   ┆          ┆           ┆ 05:38:19  ┆           │\n",
       "│           ┆           ┆           ┆ helico…   ┆   ┆          ┆           ┆ -05       ┆           │\n",
       "│ 15880616  ┆ 405523    ┆ 898       ┆ Just as   ┆ … ┆ 0        ┆ 0         ┆ 2015-05-1 ┆ 2025-06-1 │\n",
       "│           ┆           ┆           ┆ good as   ┆   ┆          ┆           ┆ 7         ┆ 2         │\n",
       "│           ┆           ┆           ┆ the name  ┆   ┆          ┆           ┆ 20:54:19  ┆           │\n",
       "│           ┆           ┆           ┆ says,…    ┆   ┆          ┆           ┆ -05       ┆           │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴──────────┴───────────┴───────────┴───────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>review_id</th><th>user_index</th><th>game_index</th><th>review</th><th>written_during_early_access</th><th>voted_up</th><th>weighted_vote_score</th><th>votes_up</th><th>comment_count</th><th>timestamp_created</th><th>scrape_date</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>bool</td><td>bool</td><td>f64</td><td>i64</td><td>i64</td><td>datetime[μs, America/Lima]</td><td>date</td></tr></thead><tbody><tr><td>2145352</td><td>26162</td><td>1315</td><td>&quot;Bizarrely addictive. Good fun.…</td><td>false</td><td>true</td><td>0.42988</td><td>1</td><td>0</td><td>2013-01-30 07:43:16 -05</td><td>2025-06-12</td></tr><tr><td>19859019</td><td>214812</td><td>125</td><td>&quot;Classic game.&quot;</td><td>false</td><td>true</td><td>0.5</td><td>0</td><td>0</td><td>2015-12-22 21:11:23 -05</td><td>2025-06-12</td></tr><tr><td>21742139</td><td>1344824</td><td>5762</td><td>&quot;I&#x27;ve had more fun than expecte…</td><td>false</td><td>true</td><td>0.479452</td><td>0</td><td>0</td><td>2016-03-10 01:10:24 -05</td><td>2025-06-12</td></tr><tr><td>8179044</td><td>215832</td><td>1087</td><td>&quot;This is the best game of all t…</td><td>false</td><td>true</td><td>0.5</td><td>0</td><td>0</td><td>2013-12-21 16:26:46 -05</td><td>2025-06-12</td></tr><tr><td>28772518</td><td>1828608</td><td>1602</td><td>&quot;I was one of those people who …</td><td>false</td><td>true</td><td>0.5029</td><td>2</td><td>0</td><td>2016-12-28 19:46:25 -05</td><td>2025-06-12</td></tr><tr><td>19831456</td><td>843359</td><td>1494</td><td>&quot;One of those true gems, must b…</td><td>false</td><td>true</td><td>0.5</td><td>0</td><td>0</td><td>2015-12-21 16:21:36 -05</td><td>2025-06-12</td></tr><tr><td>24568957</td><td>1537652</td><td>6914</td><td>&quot;If Dark Souls and Geometry Das…</td><td>false</td><td>true</td><td>0.482958</td><td>31</td><td>1</td><td>2016-07-28 00:19:51 -05</td><td>2025-06-12</td></tr><tr><td>13598925</td><td>739702</td><td>2688</td><td>&quot;;-( &lt;3&quot;</td><td>false</td><td>true</td><td>0.5</td><td>0</td><td>0</td><td>2014-12-25 17:46:32 -05</td><td>2025-06-12</td></tr><tr><td>24908430</td><td>493167</td><td>1475</td><td>&quot;I can now sort of fly a helico…</td><td>false</td><td>true</td><td>0.5</td><td>0</td><td>0</td><td>2016-08-14 05:38:19 -05</td><td>2025-06-12</td></tr><tr><td>15880616</td><td>405523</td><td>898</td><td>&quot;Just as good as the name says,…</td><td>false</td><td>true</td><td>0.5</td><td>0</td><td>0</td><td>2015-05-17 20:54:19 -05</td><td>2025-06-12</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T00:40:55.581212Z",
     "start_time": "2025-08-06T00:40:55.579064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = torchrec.KeyedJaggedTensor.from_jt_dict(\n",
    "    {\n",
    "        \"user_index\": torchrec.JaggedTensor(values=torch.tensor(sample[\"user_index\"]), lengths=torch.ones(10, dtype=torch.int32)),\n",
    "        \"game_index\": torchrec.JaggedTensor(values=torch.tensor(sample[\"game_index\"]), lengths=torch.ones(10, dtype=torch.int32)),\n",
    "    }\n",
    ")"
   ],
   "id": "282b7a1604ea6558",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T00:40:55.647079Z",
     "start_time": "2025-08-06T00:40:55.644236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = ec(features)\n",
    "print(out[\"user_index\"].values().shape)"
   ],
   "id": "562e9cc591d2cc2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pytorch Dataset",
   "id": "a1b24327b472070b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:41:41.221466Z",
     "start_time": "2025-08-06T22:41:41.213491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReviewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pl.DataFrame):\n",
    "        self.reviews = df\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews.row(idx, named=True)\n",
    "        return {\n",
    "            \"user_id\": torch.tensor(review[\"user_index\"]),\n",
    "            \"game_id\": torch.tensor(review[\"game_index\"]),\n",
    "            \"voted_up\": torch.tensor(review[\"voted_up\"], dtype=torch.float)\n",
    "        }\n"
   ],
   "id": "a89b31bb2360dd2b",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T00:40:55.826737Z",
     "start_time": "2025-08-06T00:40:55.783089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = ReviewsDataset(train_reviews_df.filter(pl.col(\"voted_up\") == 1))\n",
    "val_dataset = ReviewsDataset(val_reviews_df.filter(pl.col(\"voted_up\") == 1))"
   ],
   "id": "9bb2200803dbb230",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T00:40:55.856998Z",
     "start_time": "2025-08-06T00:40:55.854616Z"
    }
   },
   "cell_type": "code",
   "source": "len(train_dataset), len(val_dataset)",
   "id": "2fccca5cf69cce4c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3042296, 375249)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simple PyTorch model",
   "id": "4bfc5f95810dce4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:41:46.939143Z",
     "start_time": "2025-08-06T22:41:46.930870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleRetrievalModel(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_users: int = num_users, num_games: int = num_games):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.user_emb = torch.nn.Embedding(num_users + 1, embedding_dim)\n",
    "        self.game_emb = torch.nn.Embedding(num_games + 1, embedding_dim)\n",
    "    def forward(self, batch: dict[str, torch.Tensor]):\n",
    "        users = batch[\"user_id\"]\n",
    "        games = batch[\"game_id\"]\n",
    "        users_embedding = self.user_emb(users)\n",
    "        games_embedding = self.game_emb(games)\n",
    "        logits = torch.matmul(users_embedding, games_embedding.t())\n",
    "        labels = torch.arange(len(users), device=users.device)\n",
    "        return torch.nn.functional.cross_entropy(logits, labels)"
   ],
   "id": "8454c938a45aab03",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:41:48.913783Z",
     "start_time": "2025-08-06T22:41:48.902667Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "4c9ace019fa7d5b0",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:41:51.419124Z",
     "start_time": "2025-08-06T22:41:49.910981Z"
    }
   },
   "cell_type": "code",
   "source": "model = SimpleRetrievalModel(embedding_dim=128).to(device)",
   "id": "ac22ddf946a1e7be",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T00:40:57.320540Z",
     "start_time": "2025-08-06T00:40:57.318733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True)"
   ],
   "id": "6548f6b60e5b17f1",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Loop",
   "id": "20c0e2ba5461652b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:41:54.815564Z",
     "start_time": "2025-08-06T22:41:54.807816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model: torch.nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    train_loader: torch.utils.data.DataLoader,\n",
    "                    val_loader: torch.utils.data.DataLoader,\n",
    "                    device: torch.device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        loss = model(batch)\n",
    "        print(f\"Processing batch {batch_idx + 1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            loss = model(batch)\n",
    "            print(f\"Processing batch {batch_idx + 1}/{len(train_loader)} - Val_Loss: {loss.item():.4f}\")\n",
    "            val_loss += loss.item()\n",
    "    return total_loss / len(train_loader), val_loss / len(val_loader)"
   ],
   "id": "141d9c4e9f713cae",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:41:57.474564Z",
     "start_time": "2025-08-06T22:41:57.471381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model: torch.nn.Module,\n",
    "          train_loader: torch.utils.data.DataLoader,\n",
    "          val_loader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          epochs: int,\n",
    "          device: torch.device,\n",
    "          scheduler=None):\n",
    "    train_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, val_loss = train_one_epoch(model, optimizer, train_loader, val_loader, device)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Train loss: {train_loss:.4f} - Val loss: {val_loss:.4f}\")\n",
    "    return train_loss"
   ],
   "id": "bfc813036b280178",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T02:16:30.187570Z",
     "start_time": "2025-08-06T01:18:07.292013Z"
    }
   },
   "cell_type": "code",
   "source": "train_loss = train(model, train_dataloader, val_dataloader, torch.optim.Adam(model.parameters(), lr=0.00001), epochs=5, device=torch.device(\"cuda\"))",
   "id": "53bae8e40bc948a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/11884 - Loss: 30.7446\n",
      "Processing batch 2/11884 - Loss: 30.1278\n",
      "Processing batch 3/11884 - Loss: 30.7613\n",
      "Processing batch 4/11884 - Loss: 30.9727\n",
      "Processing batch 5/11884 - Loss: 28.8415\n",
      "Processing batch 6/11884 - Loss: 29.3898\n",
      "Processing batch 7/11884 - Loss: 30.7560\n",
      "Processing batch 8/11884 - Loss: 32.0356\n",
      "Processing batch 9/11884 - Loss: 29.4798\n",
      "Processing batch 10/11884 - Loss: 31.8106\n",
      "Processing batch 11/11884 - Loss: 32.1769\n",
      "Processing batch 12/11884 - Loss: 31.0654\n",
      "Processing batch 13/11884 - Loss: 29.3575\n",
      "Processing batch 14/11884 - Loss: 31.7076\n",
      "Processing batch 15/11884 - Loss: 30.1346\n",
      "Processing batch 16/11884 - Loss: 29.9977\n",
      "Processing batch 17/11884 - Loss: 29.4831\n",
      "Processing batch 18/11884 - Loss: 30.6306\n",
      "Processing batch 19/11884 - Loss: 30.8012\n",
      "Processing batch 20/11884 - Loss: 31.2032\n",
      "Processing batch 21/11884 - Loss: 29.8524\n",
      "Processing batch 22/11884 - Loss: 30.6861\n",
      "Processing batch 23/11884 - Loss: 31.5160\n",
      "Processing batch 24/11884 - Loss: 31.1373\n",
      "Processing batch 25/11884 - Loss: 31.1136\n",
      "Processing batch 26/11884 - Loss: 30.3303\n",
      "Processing batch 27/11884 - Loss: 30.0807\n",
      "Processing batch 28/11884 - Loss: 30.4865\n",
      "Processing batch 29/11884 - Loss: 30.7843\n",
      "Processing batch 30/11884 - Loss: 30.6493\n",
      "Processing batch 31/11884 - Loss: 30.7045\n",
      "Processing batch 32/11884 - Loss: 31.0376\n",
      "Processing batch 33/11884 - Loss: 30.1700\n",
      "Processing batch 34/11884 - Loss: 29.8112\n",
      "Processing batch 35/11884 - Loss: 30.8848\n",
      "Processing batch 36/11884 - Loss: 30.6823\n",
      "Processing batch 37/11884 - Loss: 31.0241\n",
      "Processing batch 38/11884 - Loss: 31.7014\n",
      "Processing batch 39/11884 - Loss: 31.0545\n",
      "Processing batch 40/11884 - Loss: 30.9240\n",
      "Processing batch 41/11884 - Loss: 30.9291\n",
      "Processing batch 42/11884 - Loss: 30.0168\n",
      "Processing batch 43/11884 - Loss: 30.4160\n",
      "Processing batch 44/11884 - Loss: 29.9602\n",
      "Processing batch 45/11884 - Loss: 31.3270\n",
      "Processing batch 46/11884 - Loss: 30.3126\n",
      "Processing batch 47/11884 - Loss: 31.1038\n",
      "Processing batch 48/11884 - Loss: 29.4775\n",
      "Processing batch 49/11884 - Loss: 29.9381\n",
      "Processing batch 50/11884 - Loss: 31.2070\n",
      "Processing batch 51/11884 - Loss: 29.8199\n",
      "Processing batch 52/11884 - Loss: 31.4285\n",
      "Processing batch 53/11884 - Loss: 30.4855\n",
      "Processing batch 54/11884 - Loss: 30.1092\n",
      "Processing batch 55/11884 - Loss: 30.6588\n",
      "Processing batch 56/11884 - Loss: 30.5206\n",
      "Processing batch 57/11884 - Loss: 30.6860\n",
      "Processing batch 58/11884 - Loss: 29.8183\n",
      "Processing batch 59/11884 - Loss: 30.9748\n",
      "Processing batch 60/11884 - Loss: 30.5193\n",
      "Processing batch 61/11884 - Loss: 30.1038\n",
      "Processing batch 62/11884 - Loss: 30.8758\n",
      "Processing batch 63/11884 - Loss: 30.9302\n",
      "Processing batch 64/11884 - Loss: 30.5613\n",
      "Processing batch 65/11884 - Loss: 30.7681\n",
      "Processing batch 66/11884 - Loss: 30.4713\n",
      "Processing batch 67/11884 - Loss: 30.6611\n",
      "Processing batch 68/11884 - Loss: 30.9008\n",
      "Processing batch 69/11884 - Loss: 31.3449\n",
      "Processing batch 70/11884 - Loss: 31.7714\n",
      "Processing batch 71/11884 - Loss: 31.0461\n",
      "Processing batch 72/11884 - Loss: 30.5113\n",
      "Processing batch 73/11884 - Loss: 30.1501\n",
      "Processing batch 74/11884 - Loss: 30.4956\n",
      "Processing batch 75/11884 - Loss: 30.2796\n",
      "Processing batch 76/11884 - Loss: 30.8716\n",
      "Processing batch 77/11884 - Loss: 30.6004\n",
      "Processing batch 78/11884 - Loss: 30.8831\n",
      "Processing batch 79/11884 - Loss: 30.3648\n",
      "Processing batch 80/11884 - Loss: 31.7139\n",
      "Processing batch 81/11884 - Loss: 31.0197\n",
      "Processing batch 82/11884 - Loss: 30.0519\n",
      "Processing batch 83/11884 - Loss: 33.2247\n",
      "Processing batch 84/11884 - Loss: 30.6698\n",
      "Processing batch 85/11884 - Loss: 30.8956\n",
      "Processing batch 86/11884 - Loss: 31.0358\n",
      "Processing batch 87/11884 - Loss: 32.0826\n",
      "Processing batch 88/11884 - Loss: 30.3343\n",
      "Processing batch 89/11884 - Loss: 30.3987\n",
      "Processing batch 90/11884 - Loss: 30.2755\n",
      "Processing batch 91/11884 - Loss: 31.8795\n",
      "Processing batch 92/11884 - Loss: 31.9443\n",
      "Processing batch 93/11884 - Loss: 29.8111\n",
      "Processing batch 94/11884 - Loss: 29.4039\n",
      "Processing batch 95/11884 - Loss: 30.9666\n",
      "Processing batch 96/11884 - Loss: 30.1692\n",
      "Processing batch 97/11884 - Loss: 30.3432\n",
      "Processing batch 98/11884 - Loss: 31.2635\n",
      "Processing batch 99/11884 - Loss: 30.6258\n",
      "Processing batch 100/11884 - Loss: 29.0779\n",
      "Processing batch 101/11884 - Loss: 30.6673\n",
      "Processing batch 102/11884 - Loss: 30.0844\n",
      "Processing batch 103/11884 - Loss: 30.7351\n",
      "Processing batch 104/11884 - Loss: 29.8287\n",
      "Processing batch 105/11884 - Loss: 30.8547\n",
      "Processing batch 106/11884 - Loss: 31.0441\n",
      "Processing batch 107/11884 - Loss: 29.7198\n",
      "Processing batch 108/11884 - Loss: 29.9237\n",
      "Processing batch 109/11884 - Loss: 30.0948\n",
      "Processing batch 110/11884 - Loss: 31.4999\n",
      "Processing batch 111/11884 - Loss: 31.2282\n",
      "Processing batch 112/11884 - Loss: 30.6890\n",
      "Processing batch 113/11884 - Loss: 30.7435\n",
      "Processing batch 114/11884 - Loss: 32.0294\n",
      "Processing batch 115/11884 - Loss: 30.6682\n",
      "Processing batch 116/11884 - Loss: 30.7911\n",
      "Processing batch 117/11884 - Loss: 30.5893\n",
      "Processing batch 118/11884 - Loss: 30.4078\n",
      "Processing batch 119/11884 - Loss: 31.0295\n",
      "Processing batch 120/11884 - Loss: 31.1185\n",
      "Processing batch 121/11884 - Loss: 31.5398\n",
      "Processing batch 122/11884 - Loss: 30.4714\n",
      "Processing batch 123/11884 - Loss: 31.0040\n",
      "Processing batch 124/11884 - Loss: 29.9301\n",
      "Processing batch 125/11884 - Loss: 30.7860\n",
      "Processing batch 126/11884 - Loss: 29.7241\n",
      "Processing batch 127/11884 - Loss: 30.1301\n",
      "Processing batch 128/11884 - Loss: 30.3975\n",
      "Processing batch 129/11884 - Loss: 31.7351\n",
      "Processing batch 130/11884 - Loss: 32.5306\n",
      "Processing batch 131/11884 - Loss: 32.1848\n",
      "Processing batch 132/11884 - Loss: 31.1404\n",
      "Processing batch 133/11884 - Loss: 30.9690\n",
      "Processing batch 134/11884 - Loss: 29.9868\n",
      "Processing batch 135/11884 - Loss: 31.1640\n",
      "Processing batch 136/11884 - Loss: 31.0890\n",
      "Processing batch 137/11884 - Loss: 30.0418\n",
      "Processing batch 138/11884 - Loss: 30.5523\n",
      "Processing batch 139/11884 - Loss: 31.4512\n",
      "Processing batch 140/11884 - Loss: 30.6343\n",
      "Processing batch 141/11884 - Loss: 28.8670\n",
      "Processing batch 142/11884 - Loss: 29.3645\n",
      "Processing batch 143/11884 - Loss: 31.2995\n",
      "Processing batch 144/11884 - Loss: 31.4760\n",
      "Processing batch 145/11884 - Loss: 29.9755\n",
      "Processing batch 146/11884 - Loss: 31.1885\n",
      "Processing batch 147/11884 - Loss: 29.7075\n",
      "Processing batch 148/11884 - Loss: 30.9985\n",
      "Processing batch 149/11884 - Loss: 30.5547\n",
      "Processing batch 150/11884 - Loss: 30.6204\n",
      "Processing batch 151/11884 - Loss: 30.5602\n",
      "Processing batch 152/11884 - Loss: 32.3000\n",
      "Processing batch 153/11884 - Loss: 30.3403\n",
      "Processing batch 154/11884 - Loss: 30.5855\n",
      "Processing batch 155/11884 - Loss: 29.1292\n",
      "Processing batch 156/11884 - Loss: 30.8294\n",
      "Processing batch 157/11884 - Loss: 29.7552\n",
      "Processing batch 158/11884 - Loss: 30.6396\n",
      "Processing batch 159/11884 - Loss: 31.3153\n",
      "Processing batch 160/11884 - Loss: 30.8084\n",
      "Processing batch 161/11884 - Loss: 29.7896\n",
      "Processing batch 162/11884 - Loss: 30.2199\n",
      "Processing batch 163/11884 - Loss: 30.5046\n",
      "Processing batch 164/11884 - Loss: 29.4482\n",
      "Processing batch 165/11884 - Loss: 30.1987\n",
      "Processing batch 166/11884 - Loss: 30.9479\n",
      "Processing batch 167/11884 - Loss: 30.5005\n",
      "Processing batch 168/11884 - Loss: 30.5834\n",
      "Processing batch 169/11884 - Loss: 30.1660\n",
      "Processing batch 170/11884 - Loss: 29.8580\n",
      "Processing batch 171/11884 - Loss: 30.6244\n",
      "Processing batch 172/11884 - Loss: 29.8802\n",
      "Processing batch 173/11884 - Loss: 30.8358\n",
      "Processing batch 174/11884 - Loss: 30.5730\n",
      "Processing batch 175/11884 - Loss: 29.8757\n",
      "Processing batch 176/11884 - Loss: 30.8275\n",
      "Processing batch 177/11884 - Loss: 30.8095\n",
      "Processing batch 178/11884 - Loss: 31.4231\n",
      "Processing batch 179/11884 - Loss: 30.8029\n",
      "Processing batch 180/11884 - Loss: 30.8958\n",
      "Processing batch 181/11884 - Loss: 30.1864\n",
      "Processing batch 182/11884 - Loss: 30.6461\n",
      "Processing batch 183/11884 - Loss: 31.2709\n",
      "Processing batch 184/11884 - Loss: 30.5919\n",
      "Processing batch 185/11884 - Loss: 31.6769\n",
      "Processing batch 186/11884 - Loss: 29.0253\n",
      "Processing batch 187/11884 - Loss: 31.8809\n",
      "Processing batch 188/11884 - Loss: 29.4400\n",
      "Processing batch 189/11884 - Loss: 31.1793\n",
      "Processing batch 190/11884 - Loss: 31.7618\n",
      "Processing batch 191/11884 - Loss: 30.4965\n",
      "Processing batch 192/11884 - Loss: 29.7628\n",
      "Processing batch 193/11884 - Loss: 30.7438\n",
      "Processing batch 194/11884 - Loss: 30.0659\n",
      "Processing batch 195/11884 - Loss: 30.2419\n",
      "Processing batch 196/11884 - Loss: 30.6382\n",
      "Processing batch 197/11884 - Loss: 30.4636\n",
      "Processing batch 198/11884 - Loss: 31.3602\n",
      "Processing batch 199/11884 - Loss: 32.1935\n",
      "Processing batch 200/11884 - Loss: 31.3061\n",
      "Processing batch 201/11884 - Loss: 30.2486\n",
      "Processing batch 202/11884 - Loss: 30.9969\n",
      "Processing batch 203/11884 - Loss: 31.3264\n",
      "Processing batch 204/11884 - Loss: 30.9550\n",
      "Processing batch 205/11884 - Loss: 31.9429\n",
      "Processing batch 206/11884 - Loss: 30.9527\n",
      "Processing batch 207/11884 - Loss: 31.1935\n",
      "Processing batch 208/11884 - Loss: 30.4967\n",
      "Processing batch 209/11884 - Loss: 31.3000\n",
      "Processing batch 210/11884 - Loss: 31.7876\n",
      "Processing batch 211/11884 - Loss: 30.0660\n",
      "Processing batch 212/11884 - Loss: 31.4466\n",
      "Processing batch 213/11884 - Loss: 30.6320\n",
      "Processing batch 214/11884 - Loss: 31.3838\n",
      "Processing batch 215/11884 - Loss: 30.7054\n",
      "Processing batch 216/11884 - Loss: 31.0615\n",
      "Processing batch 217/11884 - Loss: 30.6476\n",
      "Processing batch 218/11884 - Loss: 30.6792\n",
      "Processing batch 219/11884 - Loss: 30.2235\n",
      "Processing batch 220/11884 - Loss: 30.5606\n",
      "Processing batch 221/11884 - Loss: 31.4413\n",
      "Processing batch 222/11884 - Loss: 28.7022\n",
      "Processing batch 223/11884 - Loss: 30.7144\n",
      "Processing batch 224/11884 - Loss: 29.8821\n",
      "Processing batch 225/11884 - Loss: 30.0146\n",
      "Processing batch 226/11884 - Loss: 31.9726\n",
      "Processing batch 227/11884 - Loss: 31.2110\n",
      "Processing batch 228/11884 - Loss: 30.5210\n",
      "Processing batch 229/11884 - Loss: 30.0292\n",
      "Processing batch 230/11884 - Loss: 32.3815\n",
      "Processing batch 231/11884 - Loss: 30.2937\n",
      "Processing batch 232/11884 - Loss: 30.0870\n",
      "Processing batch 233/11884 - Loss: 30.5154\n",
      "Processing batch 234/11884 - Loss: 29.7821\n",
      "Processing batch 235/11884 - Loss: 30.3886\n",
      "Processing batch 236/11884 - Loss: 30.2908\n",
      "Processing batch 237/11884 - Loss: 29.6066\n",
      "Processing batch 238/11884 - Loss: 30.9434\n",
      "Processing batch 239/11884 - Loss: 32.5110\n",
      "Processing batch 240/11884 - Loss: 30.1758\n",
      "Processing batch 241/11884 - Loss: 31.2398\n",
      "Processing batch 242/11884 - Loss: 31.3657\n",
      "Processing batch 243/11884 - Loss: 32.0202\n",
      "Processing batch 244/11884 - Loss: 29.4942\n",
      "Processing batch 245/11884 - Loss: 31.8684\n",
      "Processing batch 246/11884 - Loss: 31.0663\n",
      "Processing batch 247/11884 - Loss: 31.1741\n",
      "Processing batch 248/11884 - Loss: 29.6888\n",
      "Processing batch 249/11884 - Loss: 30.5541\n",
      "Processing batch 250/11884 - Loss: 30.7893\n",
      "Processing batch 251/11884 - Loss: 29.9818\n",
      "Processing batch 252/11884 - Loss: 31.2897\n",
      "Processing batch 253/11884 - Loss: 31.3033\n",
      "Processing batch 254/11884 - Loss: 31.2440\n",
      "Processing batch 255/11884 - Loss: 30.9280\n",
      "Processing batch 256/11884 - Loss: 29.1902\n",
      "Processing batch 257/11884 - Loss: 30.9402\n",
      "Processing batch 258/11884 - Loss: 32.6339\n",
      "Processing batch 259/11884 - Loss: 31.3229\n",
      "Processing batch 260/11884 - Loss: 29.4418\n",
      "Processing batch 261/11884 - Loss: 30.1245\n",
      "Processing batch 262/11884 - Loss: 32.2766\n",
      "Processing batch 263/11884 - Loss: 30.1897\n",
      "Processing batch 264/11884 - Loss: 30.7946\n",
      "Processing batch 265/11884 - Loss: 31.8052\n",
      "Processing batch 266/11884 - Loss: 31.1203\n",
      "Processing batch 267/11884 - Loss: 31.0417\n",
      "Processing batch 268/11884 - Loss: 31.0887\n",
      "Processing batch 269/11884 - Loss: 29.6230\n",
      "Processing batch 270/11884 - Loss: 30.1118\n",
      "Processing batch 271/11884 - Loss: 30.4345\n",
      "Processing batch 272/11884 - Loss: 30.8419\n",
      "Processing batch 273/11884 - Loss: 31.1787\n",
      "Processing batch 274/11884 - Loss: 30.6907\n",
      "Processing batch 275/11884 - Loss: 30.2626\n",
      "Processing batch 276/11884 - Loss: 28.9046\n",
      "Processing batch 277/11884 - Loss: 29.7069\n",
      "Processing batch 278/11884 - Loss: 30.4202\n",
      "Processing batch 279/11884 - Loss: 30.2236\n",
      "Processing batch 280/11884 - Loss: 30.7282\n",
      "Processing batch 281/11884 - Loss: 29.5565\n",
      "Processing batch 282/11884 - Loss: 30.2818\n",
      "Processing batch 283/11884 - Loss: 31.4806\n",
      "Processing batch 284/11884 - Loss: 30.5733\n",
      "Processing batch 285/11884 - Loss: 30.0019\n",
      "Processing batch 286/11884 - Loss: 31.2057\n",
      "Processing batch 287/11884 - Loss: 30.9584\n",
      "Processing batch 288/11884 - Loss: 31.9876\n",
      "Processing batch 289/11884 - Loss: 30.7340\n",
      "Processing batch 290/11884 - Loss: 31.7118\n",
      "Processing batch 291/11884 - Loss: 30.1820\n",
      "Processing batch 292/11884 - Loss: 30.9977\n",
      "Processing batch 293/11884 - Loss: 31.6112\n",
      "Processing batch 294/11884 - Loss: 29.9746\n",
      "Processing batch 295/11884 - Loss: 29.5191\n",
      "Processing batch 296/11884 - Loss: 32.3625\n",
      "Processing batch 297/11884 - Loss: 29.7295\n",
      "Processing batch 298/11884 - Loss: 30.1123\n",
      "Processing batch 299/11884 - Loss: 29.7006\n",
      "Processing batch 300/11884 - Loss: 30.1242\n",
      "Processing batch 301/11884 - Loss: 30.3846\n",
      "Processing batch 302/11884 - Loss: 30.6416\n",
      "Processing batch 303/11884 - Loss: 31.2586\n",
      "Processing batch 304/11884 - Loss: 30.1383\n",
      "Processing batch 305/11884 - Loss: 30.6336\n",
      "Processing batch 306/11884 - Loss: 30.7563\n",
      "Processing batch 307/11884 - Loss: 30.5339\n",
      "Processing batch 308/11884 - Loss: 30.4374\n",
      "Processing batch 309/11884 - Loss: 31.1492\n",
      "Processing batch 310/11884 - Loss: 30.7803\n",
      "Processing batch 311/11884 - Loss: 29.8764\n",
      "Processing batch 312/11884 - Loss: 29.5587\n",
      "Processing batch 313/11884 - Loss: 31.2508\n",
      "Processing batch 314/11884 - Loss: 30.5953\n",
      "Processing batch 315/11884 - Loss: 31.6560\n",
      "Processing batch 316/11884 - Loss: 30.2795\n",
      "Processing batch 317/11884 - Loss: 31.2042\n",
      "Processing batch 318/11884 - Loss: 30.9160\n",
      "Processing batch 319/11884 - Loss: 31.3628\n",
      "Processing batch 320/11884 - Loss: 29.9773\n",
      "Processing batch 321/11884 - Loss: 31.6935\n",
      "Processing batch 322/11884 - Loss: 30.6169\n",
      "Processing batch 323/11884 - Loss: 30.5676\n",
      "Processing batch 324/11884 - Loss: 30.4538\n",
      "Processing batch 325/11884 - Loss: 32.1701\n",
      "Processing batch 326/11884 - Loss: 31.2827\n",
      "Processing batch 327/11884 - Loss: 30.8517\n",
      "Processing batch 328/11884 - Loss: 31.2407\n",
      "Processing batch 329/11884 - Loss: 30.4812\n",
      "Processing batch 330/11884 - Loss: 29.1989\n",
      "Processing batch 331/11884 - Loss: 31.3527\n",
      "Processing batch 332/11884 - Loss: 30.1184\n",
      "Processing batch 333/11884 - Loss: 30.6351\n",
      "Processing batch 334/11884 - Loss: 30.5294\n",
      "Processing batch 335/11884 - Loss: 31.9683\n",
      "Processing batch 336/11884 - Loss: 30.9449\n",
      "Processing batch 337/11884 - Loss: 30.5875\n",
      "Processing batch 338/11884 - Loss: 31.5873\n",
      "Processing batch 339/11884 - Loss: 30.1592\n",
      "Processing batch 340/11884 - Loss: 31.8096\n",
      "Processing batch 341/11884 - Loss: 30.1160\n",
      "Processing batch 342/11884 - Loss: 30.1204\n",
      "Processing batch 343/11884 - Loss: 31.3347\n",
      "Processing batch 344/11884 - Loss: 31.8415\n",
      "Processing batch 345/11884 - Loss: 31.5180\n",
      "Processing batch 346/11884 - Loss: 30.6655\n",
      "Processing batch 347/11884 - Loss: 31.3614\n",
      "Processing batch 348/11884 - Loss: 31.1314\n",
      "Processing batch 349/11884 - Loss: 31.7620\n",
      "Processing batch 350/11884 - Loss: 30.6644\n",
      "Processing batch 351/11884 - Loss: 32.5537\n",
      "Processing batch 352/11884 - Loss: 30.7858\n",
      "Processing batch 353/11884 - Loss: 31.6632\n",
      "Processing batch 354/11884 - Loss: 30.1328\n",
      "Processing batch 355/11884 - Loss: 30.0582\n",
      "Processing batch 356/11884 - Loss: 31.5081\n",
      "Processing batch 357/11884 - Loss: 30.8288\n",
      "Processing batch 358/11884 - Loss: 30.4626\n",
      "Processing batch 359/11884 - Loss: 31.0986\n",
      "Processing batch 360/11884 - Loss: 31.2739\n",
      "Processing batch 361/11884 - Loss: 30.4868\n",
      "Processing batch 362/11884 - Loss: 31.1033\n",
      "Processing batch 363/11884 - Loss: 31.8994\n",
      "Processing batch 364/11884 - Loss: 31.0665\n",
      "Processing batch 365/11884 - Loss: 29.8722\n",
      "Processing batch 366/11884 - Loss: 31.9959\n",
      "Processing batch 367/11884 - Loss: 29.7233\n",
      "Processing batch 368/11884 - Loss: 30.0986\n",
      "Processing batch 369/11884 - Loss: 31.7700\n",
      "Processing batch 370/11884 - Loss: 30.1468\n",
      "Processing batch 371/11884 - Loss: 31.1757\n",
      "Processing batch 372/11884 - Loss: 32.5211\n",
      "Processing batch 373/11884 - Loss: 29.6394\n",
      "Processing batch 374/11884 - Loss: 29.9258\n",
      "Processing batch 375/11884 - Loss: 30.7780\n",
      "Processing batch 376/11884 - Loss: 29.6334\n",
      "Processing batch 377/11884 - Loss: 30.0098\n",
      "Processing batch 378/11884 - Loss: 31.1340\n",
      "Processing batch 379/11884 - Loss: 30.0145\n",
      "Processing batch 380/11884 - Loss: 30.1946\n",
      "Processing batch 381/11884 - Loss: 30.1056\n",
      "Processing batch 382/11884 - Loss: 29.1459\n",
      "Processing batch 383/11884 - Loss: 30.0107\n",
      "Processing batch 384/11884 - Loss: 29.9072\n",
      "Processing batch 385/11884 - Loss: 31.4852\n",
      "Processing batch 386/11884 - Loss: 29.6268\n",
      "Processing batch 387/11884 - Loss: 30.0555\n",
      "Processing batch 388/11884 - Loss: 31.1669\n",
      "Processing batch 389/11884 - Loss: 31.2779\n",
      "Processing batch 390/11884 - Loss: 30.7188\n",
      "Processing batch 391/11884 - Loss: 29.1278\n",
      "Processing batch 392/11884 - Loss: 30.8803\n",
      "Processing batch 393/11884 - Loss: 30.8932\n",
      "Processing batch 394/11884 - Loss: 30.0250\n",
      "Processing batch 395/11884 - Loss: 31.0339\n",
      "Processing batch 396/11884 - Loss: 30.3479\n",
      "Processing batch 397/11884 - Loss: 30.1665\n",
      "Processing batch 398/11884 - Loss: 31.9656\n",
      "Processing batch 399/11884 - Loss: 30.7726\n",
      "Processing batch 400/11884 - Loss: 30.8234\n",
      "Processing batch 401/11884 - Loss: 29.5807\n",
      "Processing batch 402/11884 - Loss: 31.7868\n",
      "Processing batch 403/11884 - Loss: 30.9271\n",
      "Processing batch 404/11884 - Loss: 30.8786\n",
      "Processing batch 405/11884 - Loss: 31.9197\n",
      "Processing batch 406/11884 - Loss: 29.0663\n",
      "Processing batch 407/11884 - Loss: 31.4911\n",
      "Processing batch 408/11884 - Loss: 31.8781\n",
      "Processing batch 409/11884 - Loss: 30.5698\n",
      "Processing batch 410/11884 - Loss: 30.6724\n",
      "Processing batch 411/11884 - Loss: 29.9472\n",
      "Processing batch 412/11884 - Loss: 30.9590\n",
      "Processing batch 413/11884 - Loss: 29.6776\n",
      "Processing batch 414/11884 - Loss: 30.3255\n",
      "Processing batch 415/11884 - Loss: 30.4030\n",
      "Processing batch 416/11884 - Loss: 30.3948\n",
      "Processing batch 417/11884 - Loss: 31.2469\n",
      "Processing batch 418/11884 - Loss: 30.6037\n",
      "Processing batch 419/11884 - Loss: 30.0747\n",
      "Processing batch 420/11884 - Loss: 30.0455\n",
      "Processing batch 421/11884 - Loss: 29.9960\n",
      "Processing batch 422/11884 - Loss: 29.5312\n",
      "Processing batch 423/11884 - Loss: 30.4821\n",
      "Processing batch 424/11884 - Loss: 30.9264\n",
      "Processing batch 425/11884 - Loss: 31.0104\n",
      "Processing batch 426/11884 - Loss: 30.9345\n",
      "Processing batch 427/11884 - Loss: 30.7351\n",
      "Processing batch 428/11884 - Loss: 31.6524\n",
      "Processing batch 429/11884 - Loss: 29.4251\n",
      "Processing batch 430/11884 - Loss: 30.9420\n",
      "Processing batch 431/11884 - Loss: 31.2976\n",
      "Processing batch 432/11884 - Loss: 31.0857\n",
      "Processing batch 433/11884 - Loss: 31.7881\n",
      "Processing batch 434/11884 - Loss: 29.0952\n",
      "Processing batch 435/11884 - Loss: 29.2744\n",
      "Processing batch 436/11884 - Loss: 30.4050\n",
      "Processing batch 437/11884 - Loss: 30.6757\n",
      "Processing batch 438/11884 - Loss: 31.7027\n",
      "Processing batch 439/11884 - Loss: 31.3155\n",
      "Processing batch 440/11884 - Loss: 30.1556\n",
      "Processing batch 441/11884 - Loss: 29.4926\n",
      "Processing batch 442/11884 - Loss: 33.3038\n",
      "Processing batch 443/11884 - Loss: 30.7542\n",
      "Processing batch 444/11884 - Loss: 30.6507\n",
      "Processing batch 445/11884 - Loss: 31.4729\n",
      "Processing batch 446/11884 - Loss: 29.8020\n",
      "Processing batch 447/11884 - Loss: 31.0452\n",
      "Processing batch 448/11884 - Loss: 31.7989\n",
      "Processing batch 449/11884 - Loss: 31.7905\n",
      "Processing batch 450/11884 - Loss: 30.2535\n",
      "Processing batch 451/11884 - Loss: 29.8698\n",
      "Processing batch 452/11884 - Loss: 30.4814\n",
      "Processing batch 453/11884 - Loss: 30.5560\n",
      "Processing batch 454/11884 - Loss: 29.5149\n",
      "Processing batch 455/11884 - Loss: 31.1225\n",
      "Processing batch 456/11884 - Loss: 30.6062\n",
      "Processing batch 457/11884 - Loss: 30.7854\n",
      "Processing batch 458/11884 - Loss: 31.4997\n",
      "Processing batch 459/11884 - Loss: 30.8346\n",
      "Processing batch 460/11884 - Loss: 29.8509\n",
      "Processing batch 461/11884 - Loss: 31.0403\n",
      "Processing batch 462/11884 - Loss: 30.5352\n",
      "Processing batch 463/11884 - Loss: 31.2515\n",
      "Processing batch 464/11884 - Loss: 29.8055\n",
      "Processing batch 465/11884 - Loss: 30.5139\n",
      "Processing batch 466/11884 - Loss: 29.8688\n",
      "Processing batch 467/11884 - Loss: 30.4925\n",
      "Processing batch 468/11884 - Loss: 30.2632\n",
      "Processing batch 469/11884 - Loss: 30.5921\n",
      "Processing batch 470/11884 - Loss: 30.5859\n",
      "Processing batch 471/11884 - Loss: 30.7889\n",
      "Processing batch 472/11884 - Loss: 30.3862\n",
      "Processing batch 473/11884 - Loss: 30.5596\n",
      "Processing batch 474/11884 - Loss: 30.8759\n",
      "Processing batch 475/11884 - Loss: 30.9616\n",
      "Processing batch 476/11884 - Loss: 29.6596\n",
      "Processing batch 477/11884 - Loss: 30.5687\n",
      "Processing batch 478/11884 - Loss: 31.0999\n",
      "Processing batch 479/11884 - Loss: 30.9216\n",
      "Processing batch 480/11884 - Loss: 31.0365\n",
      "Processing batch 481/11884 - Loss: 31.2688\n",
      "Processing batch 482/11884 - Loss: 30.4432\n",
      "Processing batch 483/11884 - Loss: 30.6274\n",
      "Processing batch 484/11884 - Loss: 30.9876\n",
      "Processing batch 485/11884 - Loss: 31.5209\n",
      "Processing batch 486/11884 - Loss: 31.2535\n",
      "Processing batch 487/11884 - Loss: 31.5193\n",
      "Processing batch 488/11884 - Loss: 31.0905\n",
      "Processing batch 489/11884 - Loss: 30.0801\n",
      "Processing batch 490/11884 - Loss: 30.5528\n",
      "Processing batch 491/11884 - Loss: 29.9213\n",
      "Processing batch 492/11884 - Loss: 30.0638\n",
      "Processing batch 493/11884 - Loss: 30.4335\n",
      "Processing batch 494/11884 - Loss: 29.8989\n",
      "Processing batch 495/11884 - Loss: 31.2863\n",
      "Processing batch 496/11884 - Loss: 29.3935\n",
      "Processing batch 497/11884 - Loss: 30.5993\n",
      "Processing batch 498/11884 - Loss: 31.8379\n",
      "Processing batch 499/11884 - Loss: 29.6035\n",
      "Processing batch 500/11884 - Loss: 30.4688\n",
      "Processing batch 501/11884 - Loss: 30.9316\n",
      "Processing batch 502/11884 - Loss: 29.1383\n",
      "Processing batch 503/11884 - Loss: 31.7270\n",
      "Processing batch 504/11884 - Loss: 30.2977\n",
      "Processing batch 505/11884 - Loss: 30.5798\n",
      "Processing batch 506/11884 - Loss: 29.2137\n",
      "Processing batch 507/11884 - Loss: 30.5836\n",
      "Processing batch 508/11884 - Loss: 30.5103\n",
      "Processing batch 509/11884 - Loss: 32.4425\n",
      "Processing batch 510/11884 - Loss: 31.4519\n",
      "Processing batch 511/11884 - Loss: 30.5440\n",
      "Processing batch 512/11884 - Loss: 30.8810\n",
      "Processing batch 513/11884 - Loss: 30.4128\n",
      "Processing batch 514/11884 - Loss: 30.2092\n",
      "Processing batch 515/11884 - Loss: 30.0152\n",
      "Processing batch 516/11884 - Loss: 30.6960\n",
      "Processing batch 517/11884 - Loss: 30.4520\n",
      "Processing batch 518/11884 - Loss: 31.0226\n",
      "Processing batch 519/11884 - Loss: 30.9290\n",
      "Processing batch 520/11884 - Loss: 30.8414\n",
      "Processing batch 521/11884 - Loss: 31.7823\n",
      "Processing batch 522/11884 - Loss: 30.3448\n",
      "Processing batch 523/11884 - Loss: 31.2601\n",
      "Processing batch 524/11884 - Loss: 30.4686\n",
      "Processing batch 525/11884 - Loss: 29.1225\n",
      "Processing batch 526/11884 - Loss: 30.8124\n",
      "Processing batch 527/11884 - Loss: 30.1831\n",
      "Processing batch 528/11884 - Loss: 29.5232\n",
      "Processing batch 529/11884 - Loss: 29.6150\n",
      "Processing batch 530/11884 - Loss: 31.8179\n",
      "Processing batch 531/11884 - Loss: 31.2684\n",
      "Processing batch 532/11884 - Loss: 31.2383\n",
      "Processing batch 533/11884 - Loss: 31.0521\n",
      "Processing batch 534/11884 - Loss: 30.4822\n",
      "Processing batch 535/11884 - Loss: 31.1450\n",
      "Processing batch 536/11884 - Loss: 30.9595\n",
      "Processing batch 537/11884 - Loss: 31.9273\n",
      "Processing batch 538/11884 - Loss: 31.4030\n",
      "Processing batch 539/11884 - Loss: 31.1645\n",
      "Processing batch 540/11884 - Loss: 31.6913\n",
      "Processing batch 541/11884 - Loss: 29.3734\n",
      "Processing batch 542/11884 - Loss: 31.4655\n",
      "Processing batch 543/11884 - Loss: 29.9555\n",
      "Processing batch 544/11884 - Loss: 29.5556\n",
      "Processing batch 545/11884 - Loss: 31.2711\n",
      "Processing batch 546/11884 - Loss: 30.8440\n",
      "Processing batch 547/11884 - Loss: 29.9657\n",
      "Processing batch 548/11884 - Loss: 31.2180\n",
      "Processing batch 549/11884 - Loss: 30.7083\n",
      "Processing batch 550/11884 - Loss: 31.6986\n",
      "Processing batch 551/11884 - Loss: 31.2824\n",
      "Processing batch 552/11884 - Loss: 30.6853\n",
      "Processing batch 553/11884 - Loss: 30.5969\n",
      "Processing batch 554/11884 - Loss: 30.1122\n",
      "Processing batch 555/11884 - Loss: 29.5180\n",
      "Processing batch 556/11884 - Loss: 31.2391\n",
      "Processing batch 557/11884 - Loss: 29.8336\n",
      "Processing batch 558/11884 - Loss: 31.7393\n",
      "Processing batch 559/11884 - Loss: 31.8033\n",
      "Processing batch 560/11884 - Loss: 29.3843\n",
      "Processing batch 561/11884 - Loss: 29.9689\n",
      "Processing batch 562/11884 - Loss: 30.8453\n",
      "Processing batch 563/11884 - Loss: 32.1187\n",
      "Processing batch 564/11884 - Loss: 29.1836\n",
      "Processing batch 565/11884 - Loss: 31.4813\n",
      "Processing batch 566/11884 - Loss: 30.8960\n",
      "Processing batch 567/11884 - Loss: 30.5055\n",
      "Processing batch 568/11884 - Loss: 30.5226\n",
      "Processing batch 569/11884 - Loss: 31.0947\n",
      "Processing batch 570/11884 - Loss: 31.0167\n",
      "Processing batch 571/11884 - Loss: 30.0142\n",
      "Processing batch 572/11884 - Loss: 31.3592\n",
      "Processing batch 573/11884 - Loss: 30.1472\n",
      "Processing batch 574/11884 - Loss: 31.2807\n",
      "Processing batch 575/11884 - Loss: 31.0606\n",
      "Processing batch 576/11884 - Loss: 30.9560\n",
      "Processing batch 577/11884 - Loss: 30.1576\n",
      "Processing batch 578/11884 - Loss: 31.6000\n",
      "Processing batch 579/11884 - Loss: 31.8816\n",
      "Processing batch 580/11884 - Loss: 30.1566\n",
      "Processing batch 581/11884 - Loss: 30.6960\n",
      "Processing batch 582/11884 - Loss: 31.1202\n",
      "Processing batch 583/11884 - Loss: 30.1809\n",
      "Processing batch 584/11884 - Loss: 29.7172\n",
      "Processing batch 585/11884 - Loss: 30.0677\n",
      "Processing batch 586/11884 - Loss: 31.3528\n",
      "Processing batch 587/11884 - Loss: 29.9648\n",
      "Processing batch 588/11884 - Loss: 29.2231\n",
      "Processing batch 589/11884 - Loss: 30.6354\n",
      "Processing batch 590/11884 - Loss: 30.0408\n",
      "Processing batch 591/11884 - Loss: 31.7507\n",
      "Processing batch 592/11884 - Loss: 31.2438\n",
      "Processing batch 593/11884 - Loss: 31.0188\n",
      "Processing batch 594/11884 - Loss: 31.4727\n",
      "Processing batch 595/11884 - Loss: 30.9116\n",
      "Processing batch 596/11884 - Loss: 29.4944\n",
      "Processing batch 597/11884 - Loss: 30.7352\n",
      "Processing batch 598/11884 - Loss: 31.5118\n",
      "Processing batch 599/11884 - Loss: 31.5448\n",
      "Processing batch 600/11884 - Loss: 29.8314\n",
      "Processing batch 601/11884 - Loss: 29.3128\n",
      "Processing batch 602/11884 - Loss: 29.4107\n",
      "Processing batch 603/11884 - Loss: 30.1475\n",
      "Processing batch 604/11884 - Loss: 31.4969\n",
      "Processing batch 605/11884 - Loss: 30.9593\n",
      "Processing batch 606/11884 - Loss: 30.8115\n",
      "Processing batch 607/11884 - Loss: 30.9570\n",
      "Processing batch 608/11884 - Loss: 31.4006\n",
      "Processing batch 609/11884 - Loss: 30.2947\n",
      "Processing batch 610/11884 - Loss: 31.5523\n",
      "Processing batch 611/11884 - Loss: 30.3281\n",
      "Processing batch 612/11884 - Loss: 32.0490\n",
      "Processing batch 613/11884 - Loss: 31.3596\n",
      "Processing batch 614/11884 - Loss: 31.8366\n",
      "Processing batch 615/11884 - Loss: 30.9227\n",
      "Processing batch 616/11884 - Loss: 30.8094\n",
      "Processing batch 617/11884 - Loss: 30.8309\n",
      "Processing batch 618/11884 - Loss: 30.7456\n",
      "Processing batch 619/11884 - Loss: 31.5368\n",
      "Processing batch 620/11884 - Loss: 30.5628\n",
      "Processing batch 621/11884 - Loss: 30.8749\n",
      "Processing batch 622/11884 - Loss: 31.3045\n",
      "Processing batch 623/11884 - Loss: 32.0569\n",
      "Processing batch 624/11884 - Loss: 30.9397\n",
      "Processing batch 625/11884 - Loss: 31.8042\n",
      "Processing batch 626/11884 - Loss: 29.9667\n",
      "Processing batch 627/11884 - Loss: 30.7216\n",
      "Processing batch 628/11884 - Loss: 30.3160\n",
      "Processing batch 629/11884 - Loss: 29.6181\n",
      "Processing batch 630/11884 - Loss: 29.4430\n",
      "Processing batch 631/11884 - Loss: 30.7685\n",
      "Processing batch 632/11884 - Loss: 30.5987\n",
      "Processing batch 633/11884 - Loss: 30.9282\n",
      "Processing batch 634/11884 - Loss: 31.8320\n",
      "Processing batch 635/11884 - Loss: 30.1906\n",
      "Processing batch 636/11884 - Loss: 30.4042\n",
      "Processing batch 637/11884 - Loss: 31.8457\n",
      "Processing batch 638/11884 - Loss: 30.8389\n",
      "Processing batch 639/11884 - Loss: 30.6877\n",
      "Processing batch 640/11884 - Loss: 29.7313\n",
      "Processing batch 641/11884 - Loss: 29.7420\n",
      "Processing batch 642/11884 - Loss: 29.0174\n",
      "Processing batch 643/11884 - Loss: 31.0964\n",
      "Processing batch 644/11884 - Loss: 30.7995\n",
      "Processing batch 645/11884 - Loss: 29.9098\n",
      "Processing batch 646/11884 - Loss: 30.9180\n",
      "Processing batch 647/11884 - Loss: 31.5561\n",
      "Processing batch 648/11884 - Loss: 30.4472\n",
      "Processing batch 649/11884 - Loss: 30.6912\n",
      "Processing batch 650/11884 - Loss: 30.8343\n",
      "Processing batch 651/11884 - Loss: 30.9218\n",
      "Processing batch 652/11884 - Loss: 30.3225\n",
      "Processing batch 653/11884 - Loss: 30.9499\n",
      "Processing batch 654/11884 - Loss: 31.4178\n",
      "Processing batch 655/11884 - Loss: 30.0250\n",
      "Processing batch 656/11884 - Loss: 31.7000\n",
      "Processing batch 657/11884 - Loss: 29.7089\n",
      "Processing batch 658/11884 - Loss: 31.2354\n",
      "Processing batch 659/11884 - Loss: 29.5565\n",
      "Processing batch 660/11884 - Loss: 30.9783\n",
      "Processing batch 661/11884 - Loss: 32.4221\n",
      "Processing batch 662/11884 - Loss: 30.3749\n",
      "Processing batch 663/11884 - Loss: 29.5385\n",
      "Processing batch 664/11884 - Loss: 31.2201\n",
      "Processing batch 665/11884 - Loss: 30.4745\n",
      "Processing batch 666/11884 - Loss: 30.5484\n",
      "Processing batch 667/11884 - Loss: 30.0759\n",
      "Processing batch 668/11884 - Loss: 29.7324\n",
      "Processing batch 669/11884 - Loss: 31.2763\n",
      "Processing batch 670/11884 - Loss: 29.8471\n",
      "Processing batch 671/11884 - Loss: 29.9241\n",
      "Processing batch 672/11884 - Loss: 30.5542\n",
      "Processing batch 673/11884 - Loss: 31.0563\n",
      "Processing batch 674/11884 - Loss: 31.9298\n",
      "Processing batch 675/11884 - Loss: 30.5329\n",
      "Processing batch 676/11884 - Loss: 33.2966\n",
      "Processing batch 677/11884 - Loss: 30.1548\n",
      "Processing batch 678/11884 - Loss: 31.7084\n",
      "Processing batch 679/11884 - Loss: 31.0187\n",
      "Processing batch 680/11884 - Loss: 29.8519\n",
      "Processing batch 681/11884 - Loss: 30.6481\n",
      "Processing batch 682/11884 - Loss: 30.2083\n",
      "Processing batch 683/11884 - Loss: 30.3469\n",
      "Processing batch 684/11884 - Loss: 29.9220\n",
      "Processing batch 685/11884 - Loss: 30.8895\n",
      "Processing batch 686/11884 - Loss: 29.4305\n",
      "Processing batch 687/11884 - Loss: 31.0928\n",
      "Processing batch 688/11884 - Loss: 31.5696\n",
      "Processing batch 689/11884 - Loss: 29.7232\n",
      "Processing batch 690/11884 - Loss: 31.5137\n",
      "Processing batch 691/11884 - Loss: 31.2880\n",
      "Processing batch 692/11884 - Loss: 31.2422\n",
      "Processing batch 693/11884 - Loss: 29.5156\n",
      "Processing batch 694/11884 - Loss: 31.8715\n",
      "Processing batch 695/11884 - Loss: 30.0376\n",
      "Processing batch 696/11884 - Loss: 30.3333\n",
      "Processing batch 697/11884 - Loss: 29.7007\n",
      "Processing batch 698/11884 - Loss: 30.5129\n",
      "Processing batch 699/11884 - Loss: 30.6226\n",
      "Processing batch 700/11884 - Loss: 30.9626\n",
      "Processing batch 701/11884 - Loss: 30.5521\n",
      "Processing batch 702/11884 - Loss: 30.4202\n",
      "Processing batch 703/11884 - Loss: 31.7429\n",
      "Processing batch 704/11884 - Loss: 30.7701\n",
      "Processing batch 705/11884 - Loss: 30.7815\n",
      "Processing batch 706/11884 - Loss: 30.5656\n",
      "Processing batch 707/11884 - Loss: 29.3718\n",
      "Processing batch 708/11884 - Loss: 31.3592\n",
      "Processing batch 709/11884 - Loss: 30.6670\n",
      "Processing batch 710/11884 - Loss: 31.5295\n",
      "Processing batch 711/11884 - Loss: 29.6233\n",
      "Processing batch 712/11884 - Loss: 29.8017\n",
      "Processing batch 713/11884 - Loss: 30.1284\n",
      "Processing batch 714/11884 - Loss: 31.3633\n",
      "Processing batch 715/11884 - Loss: 29.7353\n",
      "Processing batch 716/11884 - Loss: 31.0621\n",
      "Processing batch 717/11884 - Loss: 30.7051\n",
      "Processing batch 718/11884 - Loss: 30.1264\n",
      "Processing batch 719/11884 - Loss: 30.1255\n",
      "Processing batch 720/11884 - Loss: 31.8066\n",
      "Processing batch 721/11884 - Loss: 30.0861\n",
      "Processing batch 722/11884 - Loss: 29.9026\n",
      "Processing batch 723/11884 - Loss: 31.0629\n",
      "Processing batch 724/11884 - Loss: 30.4066\n",
      "Processing batch 725/11884 - Loss: 31.0450\n",
      "Processing batch 726/11884 - Loss: 30.4264\n",
      "Processing batch 727/11884 - Loss: 32.9296\n",
      "Processing batch 728/11884 - Loss: 30.5129\n",
      "Processing batch 729/11884 - Loss: 30.2718\n",
      "Processing batch 730/11884 - Loss: 31.2020\n",
      "Processing batch 731/11884 - Loss: 31.2244\n",
      "Processing batch 732/11884 - Loss: 30.9203\n",
      "Processing batch 733/11884 - Loss: 29.6596\n",
      "Processing batch 734/11884 - Loss: 31.3923\n",
      "Processing batch 735/11884 - Loss: 30.7776\n",
      "Processing batch 736/11884 - Loss: 29.1422\n",
      "Processing batch 737/11884 - Loss: 29.4743\n",
      "Processing batch 738/11884 - Loss: 31.6755\n",
      "Processing batch 739/11884 - Loss: 31.0560\n",
      "Processing batch 740/11884 - Loss: 30.4453\n",
      "Processing batch 741/11884 - Loss: 30.1899\n",
      "Processing batch 742/11884 - Loss: 30.9971\n",
      "Processing batch 743/11884 - Loss: 30.0229\n",
      "Processing batch 744/11884 - Loss: 30.2771\n",
      "Processing batch 745/11884 - Loss: 29.8580\n",
      "Processing batch 746/11884 - Loss: 30.1382\n",
      "Processing batch 747/11884 - Loss: 30.3268\n",
      "Processing batch 748/11884 - Loss: 30.7728\n",
      "Processing batch 749/11884 - Loss: 31.0248\n",
      "Processing batch 750/11884 - Loss: 29.6043\n",
      "Processing batch 751/11884 - Loss: 30.4229\n",
      "Processing batch 752/11884 - Loss: 30.9178\n",
      "Processing batch 753/11884 - Loss: 30.3653\n",
      "Processing batch 754/11884 - Loss: 31.1919\n",
      "Processing batch 755/11884 - Loss: 29.6649\n",
      "Processing batch 756/11884 - Loss: 30.9610\n",
      "Processing batch 757/11884 - Loss: 30.5319\n",
      "Processing batch 758/11884 - Loss: 29.8230\n",
      "Processing batch 759/11884 - Loss: 30.4662\n",
      "Processing batch 760/11884 - Loss: 30.4827\n",
      "Processing batch 761/11884 - Loss: 31.5989\n",
      "Processing batch 762/11884 - Loss: 30.7266\n",
      "Processing batch 763/11884 - Loss: 30.0429\n",
      "Processing batch 764/11884 - Loss: 29.5700\n",
      "Processing batch 765/11884 - Loss: 30.2041\n",
      "Processing batch 766/11884 - Loss: 31.4855\n",
      "Processing batch 767/11884 - Loss: 30.5840\n",
      "Processing batch 768/11884 - Loss: 31.1305\n",
      "Processing batch 769/11884 - Loss: 29.9636\n",
      "Processing batch 770/11884 - Loss: 31.6469\n",
      "Processing batch 771/11884 - Loss: 30.2023\n",
      "Processing batch 772/11884 - Loss: 30.6123\n",
      "Processing batch 773/11884 - Loss: 30.1330\n",
      "Processing batch 774/11884 - Loss: 31.3037\n",
      "Processing batch 775/11884 - Loss: 29.7401\n",
      "Processing batch 776/11884 - Loss: 30.4414\n",
      "Processing batch 777/11884 - Loss: 31.2315\n",
      "Processing batch 778/11884 - Loss: 31.1236\n",
      "Processing batch 779/11884 - Loss: 31.0599\n",
      "Processing batch 780/11884 - Loss: 30.9824\n",
      "Processing batch 781/11884 - Loss: 30.1378\n",
      "Processing batch 782/11884 - Loss: 30.6555\n",
      "Processing batch 783/11884 - Loss: 32.4548\n",
      "Processing batch 784/11884 - Loss: 31.9692\n",
      "Processing batch 785/11884 - Loss: 30.6664\n",
      "Processing batch 786/11884 - Loss: 31.0052\n",
      "Processing batch 787/11884 - Loss: 28.8002\n",
      "Processing batch 788/11884 - Loss: 30.1487\n",
      "Processing batch 789/11884 - Loss: 30.8640\n",
      "Processing batch 790/11884 - Loss: 31.4999\n",
      "Processing batch 791/11884 - Loss: 30.3699\n",
      "Processing batch 792/11884 - Loss: 30.8784\n",
      "Processing batch 793/11884 - Loss: 30.5524\n",
      "Processing batch 794/11884 - Loss: 30.2927\n",
      "Processing batch 795/11884 - Loss: 29.8109\n",
      "Processing batch 796/11884 - Loss: 31.2224\n",
      "Processing batch 797/11884 - Loss: 31.3042\n",
      "Processing batch 798/11884 - Loss: 30.7663\n",
      "Processing batch 799/11884 - Loss: 31.3875\n",
      "Processing batch 800/11884 - Loss: 30.8200\n",
      "Processing batch 801/11884 - Loss: 29.4402\n",
      "Processing batch 802/11884 - Loss: 30.1383\n",
      "Processing batch 803/11884 - Loss: 31.3388\n",
      "Processing batch 804/11884 - Loss: 30.3570\n",
      "Processing batch 805/11884 - Loss: 30.1830\n",
      "Processing batch 806/11884 - Loss: 30.6160\n",
      "Processing batch 807/11884 - Loss: 31.8608\n",
      "Processing batch 808/11884 - Loss: 30.0032\n",
      "Processing batch 809/11884 - Loss: 31.4834\n",
      "Processing batch 810/11884 - Loss: 30.6500\n",
      "Processing batch 811/11884 - Loss: 30.1711\n",
      "Processing batch 812/11884 - Loss: 29.1367\n",
      "Processing batch 813/11884 - Loss: 30.2481\n",
      "Processing batch 814/11884 - Loss: 31.7504\n",
      "Processing batch 815/11884 - Loss: 31.0181\n",
      "Processing batch 816/11884 - Loss: 31.5772\n",
      "Processing batch 817/11884 - Loss: 31.0094\n",
      "Processing batch 818/11884 - Loss: 30.9609\n",
      "Processing batch 819/11884 - Loss: 31.2695\n",
      "Processing batch 820/11884 - Loss: 31.2716\n",
      "Processing batch 821/11884 - Loss: 31.1668\n",
      "Processing batch 822/11884 - Loss: 30.0522\n",
      "Processing batch 823/11884 - Loss: 31.5531\n",
      "Processing batch 824/11884 - Loss: 29.1732\n",
      "Processing batch 825/11884 - Loss: 29.7640\n",
      "Processing batch 826/11884 - Loss: 30.0119\n",
      "Processing batch 827/11884 - Loss: 31.4616\n",
      "Processing batch 828/11884 - Loss: 31.1432\n",
      "Processing batch 829/11884 - Loss: 31.1666\n",
      "Processing batch 830/11884 - Loss: 31.8269\n",
      "Processing batch 831/11884 - Loss: 31.7854\n",
      "Processing batch 832/11884 - Loss: 29.5941\n",
      "Processing batch 833/11884 - Loss: 31.1521\n",
      "Processing batch 834/11884 - Loss: 29.8637\n",
      "Processing batch 835/11884 - Loss: 29.9540\n",
      "Processing batch 836/11884 - Loss: 30.9780\n",
      "Processing batch 837/11884 - Loss: 29.8774\n",
      "Processing batch 838/11884 - Loss: 30.3454\n",
      "Processing batch 839/11884 - Loss: 29.9134\n",
      "Processing batch 840/11884 - Loss: 29.2313\n",
      "Processing batch 841/11884 - Loss: 30.2738\n",
      "Processing batch 842/11884 - Loss: 30.0164\n",
      "Processing batch 843/11884 - Loss: 29.6122\n",
      "Processing batch 844/11884 - Loss: 30.9606\n",
      "Processing batch 845/11884 - Loss: 32.0571\n",
      "Processing batch 846/11884 - Loss: 31.7926\n",
      "Processing batch 847/11884 - Loss: 31.0568\n",
      "Processing batch 848/11884 - Loss: 30.1087\n",
      "Processing batch 849/11884 - Loss: 30.0033\n",
      "Processing batch 850/11884 - Loss: 33.0954\n",
      "Processing batch 851/11884 - Loss: 32.5446\n",
      "Processing batch 852/11884 - Loss: 30.6234\n",
      "Processing batch 853/11884 - Loss: 29.5749\n",
      "Processing batch 854/11884 - Loss: 30.7061\n",
      "Processing batch 855/11884 - Loss: 29.6828\n",
      "Processing batch 856/11884 - Loss: 31.9975\n",
      "Processing batch 857/11884 - Loss: 29.9819\n",
      "Processing batch 858/11884 - Loss: 32.3196\n",
      "Processing batch 859/11884 - Loss: 31.2105\n",
      "Processing batch 860/11884 - Loss: 30.8785\n",
      "Processing batch 861/11884 - Loss: 30.1372\n",
      "Processing batch 862/11884 - Loss: 29.5901\n",
      "Processing batch 863/11884 - Loss: 29.6356\n",
      "Processing batch 864/11884 - Loss: 31.3269\n",
      "Processing batch 865/11884 - Loss: 30.3775\n",
      "Processing batch 866/11884 - Loss: 30.3219\n",
      "Processing batch 867/11884 - Loss: 31.2874\n",
      "Processing batch 868/11884 - Loss: 29.4717\n",
      "Processing batch 869/11884 - Loss: 30.2263\n",
      "Processing batch 870/11884 - Loss: 30.1132\n",
      "Processing batch 871/11884 - Loss: 31.6424\n",
      "Processing batch 872/11884 - Loss: 30.6890\n",
      "Processing batch 873/11884 - Loss: 32.3795\n",
      "Processing batch 874/11884 - Loss: 31.2505\n",
      "Processing batch 875/11884 - Loss: 31.2250\n",
      "Processing batch 876/11884 - Loss: 31.2180\n",
      "Processing batch 877/11884 - Loss: 29.6846\n",
      "Processing batch 878/11884 - Loss: 31.3432\n",
      "Processing batch 879/11884 - Loss: 31.0106\n",
      "Processing batch 880/11884 - Loss: 30.0175\n",
      "Processing batch 881/11884 - Loss: 30.8574\n",
      "Processing batch 882/11884 - Loss: 29.1209\n",
      "Processing batch 883/11884 - Loss: 30.4244\n",
      "Processing batch 884/11884 - Loss: 29.9769\n",
      "Processing batch 885/11884 - Loss: 30.9741\n",
      "Processing batch 886/11884 - Loss: 31.1693\n",
      "Processing batch 887/11884 - Loss: 30.7607\n",
      "Processing batch 888/11884 - Loss: 30.6042\n",
      "Processing batch 889/11884 - Loss: 31.6358\n",
      "Processing batch 890/11884 - Loss: 31.0462\n",
      "Processing batch 891/11884 - Loss: 30.0628\n",
      "Processing batch 892/11884 - Loss: 29.5054\n",
      "Processing batch 893/11884 - Loss: 31.0146\n",
      "Processing batch 894/11884 - Loss: 30.5940\n",
      "Processing batch 895/11884 - Loss: 29.1036\n",
      "Processing batch 896/11884 - Loss: 31.3885\n",
      "Processing batch 897/11884 - Loss: 30.8085\n",
      "Processing batch 898/11884 - Loss: 30.3532\n",
      "Processing batch 899/11884 - Loss: 31.7321\n",
      "Processing batch 900/11884 - Loss: 29.9576\n",
      "Processing batch 901/11884 - Loss: 30.6009\n",
      "Processing batch 902/11884 - Loss: 30.6727\n",
      "Processing batch 903/11884 - Loss: 29.8050\n",
      "Processing batch 904/11884 - Loss: 30.9274\n",
      "Processing batch 905/11884 - Loss: 29.9559\n",
      "Processing batch 906/11884 - Loss: 30.8360\n",
      "Processing batch 907/11884 - Loss: 30.9511\n",
      "Processing batch 908/11884 - Loss: 31.5037\n",
      "Processing batch 909/11884 - Loss: 30.8163\n",
      "Processing batch 910/11884 - Loss: 31.6704\n",
      "Processing batch 911/11884 - Loss: 30.2160\n",
      "Processing batch 912/11884 - Loss: 30.3434\n",
      "Processing batch 913/11884 - Loss: 29.7902\n",
      "Processing batch 914/11884 - Loss: 30.1556\n",
      "Processing batch 915/11884 - Loss: 31.7031\n",
      "Processing batch 916/11884 - Loss: 29.9606\n",
      "Processing batch 917/11884 - Loss: 30.1639\n",
      "Processing batch 918/11884 - Loss: 31.3345\n",
      "Processing batch 919/11884 - Loss: 29.3991\n",
      "Processing batch 920/11884 - Loss: 30.9397\n",
      "Processing batch 921/11884 - Loss: 30.4890\n",
      "Processing batch 922/11884 - Loss: 30.3300\n",
      "Processing batch 923/11884 - Loss: 30.4446\n",
      "Processing batch 924/11884 - Loss: 31.7201\n",
      "Processing batch 925/11884 - Loss: 32.0455\n",
      "Processing batch 926/11884 - Loss: 31.1087\n",
      "Processing batch 927/11884 - Loss: 28.9854\n",
      "Processing batch 928/11884 - Loss: 31.8106\n",
      "Processing batch 929/11884 - Loss: 30.7701\n",
      "Processing batch 930/11884 - Loss: 30.6676\n",
      "Processing batch 931/11884 - Loss: 30.8384\n",
      "Processing batch 932/11884 - Loss: 29.7642\n",
      "Processing batch 933/11884 - Loss: 30.6093\n",
      "Processing batch 934/11884 - Loss: 31.4957\n",
      "Processing batch 935/11884 - Loss: 30.1497\n",
      "Processing batch 936/11884 - Loss: 30.9444\n",
      "Processing batch 937/11884 - Loss: 32.6656\n",
      "Processing batch 938/11884 - Loss: 28.9366\n",
      "Processing batch 939/11884 - Loss: 30.3419\n",
      "Processing batch 940/11884 - Loss: 30.3307\n",
      "Processing batch 941/11884 - Loss: 31.4333\n",
      "Processing batch 942/11884 - Loss: 29.3576\n",
      "Processing batch 943/11884 - Loss: 30.8150\n",
      "Processing batch 944/11884 - Loss: 30.4693\n",
      "Processing batch 945/11884 - Loss: 29.7987\n",
      "Processing batch 946/11884 - Loss: 28.7470\n",
      "Processing batch 947/11884 - Loss: 31.4700\n",
      "Processing batch 948/11884 - Loss: 31.1397\n",
      "Processing batch 949/11884 - Loss: 30.9956\n",
      "Processing batch 950/11884 - Loss: 30.2710\n",
      "Processing batch 951/11884 - Loss: 30.4037\n",
      "Processing batch 952/11884 - Loss: 30.4544\n",
      "Processing batch 953/11884 - Loss: 30.3503\n",
      "Processing batch 954/11884 - Loss: 30.2431\n",
      "Processing batch 955/11884 - Loss: 29.8405\n",
      "Processing batch 956/11884 - Loss: 30.7196\n",
      "Processing batch 957/11884 - Loss: 30.6095\n",
      "Processing batch 958/11884 - Loss: 31.4404\n",
      "Processing batch 959/11884 - Loss: 30.0614\n",
      "Processing batch 960/11884 - Loss: 30.4451\n",
      "Processing batch 961/11884 - Loss: 29.7602\n",
      "Processing batch 962/11884 - Loss: 31.0752\n",
      "Processing batch 963/11884 - Loss: 31.2851\n",
      "Processing batch 964/11884 - Loss: 31.5104\n",
      "Processing batch 965/11884 - Loss: 30.7294\n",
      "Processing batch 966/11884 - Loss: 29.7861\n",
      "Processing batch 967/11884 - Loss: 30.0873\n",
      "Processing batch 968/11884 - Loss: 29.8845\n",
      "Processing batch 969/11884 - Loss: 30.9100\n",
      "Processing batch 970/11884 - Loss: 31.3948\n",
      "Processing batch 971/11884 - Loss: 30.0527\n",
      "Processing batch 972/11884 - Loss: 30.6757\n",
      "Processing batch 973/11884 - Loss: 29.5480\n",
      "Processing batch 974/11884 - Loss: 30.8201\n",
      "Processing batch 975/11884 - Loss: 31.2229\n",
      "Processing batch 976/11884 - Loss: 31.1390\n",
      "Processing batch 977/11884 - Loss: 30.9936\n",
      "Processing batch 978/11884 - Loss: 31.0297\n",
      "Processing batch 979/11884 - Loss: 31.0772\n",
      "Processing batch 980/11884 - Loss: 31.6843\n",
      "Processing batch 981/11884 - Loss: 31.6812\n",
      "Processing batch 982/11884 - Loss: 30.4265\n",
      "Processing batch 983/11884 - Loss: 31.4854\n",
      "Processing batch 984/11884 - Loss: 31.1452\n",
      "Processing batch 985/11884 - Loss: 31.0282\n",
      "Processing batch 986/11884 - Loss: 29.5163\n",
      "Processing batch 987/11884 - Loss: 30.7182\n",
      "Processing batch 988/11884 - Loss: 29.9907\n",
      "Processing batch 989/11884 - Loss: 29.7675\n",
      "Processing batch 990/11884 - Loss: 30.8839\n",
      "Processing batch 991/11884 - Loss: 30.3548\n",
      "Processing batch 992/11884 - Loss: 30.2721\n",
      "Processing batch 993/11884 - Loss: 29.9326\n",
      "Processing batch 994/11884 - Loss: 29.6278\n",
      "Processing batch 995/11884 - Loss: 29.9754\n",
      "Processing batch 996/11884 - Loss: 30.5944\n",
      "Processing batch 997/11884 - Loss: 30.2093\n",
      "Processing batch 998/11884 - Loss: 31.2432\n",
      "Processing batch 999/11884 - Loss: 30.1386\n",
      "Processing batch 1000/11884 - Loss: 30.8331\n",
      "Processing batch 1001/11884 - Loss: 29.1948\n",
      "Processing batch 1002/11884 - Loss: 28.8241\n",
      "Processing batch 1003/11884 - Loss: 29.8536\n",
      "Processing batch 1004/11884 - Loss: 30.8807\n",
      "Processing batch 1005/11884 - Loss: 30.0496\n",
      "Processing batch 1006/11884 - Loss: 31.3525\n",
      "Processing batch 1007/11884 - Loss: 29.6088\n",
      "Processing batch 1008/11884 - Loss: 31.2012\n",
      "Processing batch 1009/11884 - Loss: 32.0513\n",
      "Processing batch 1010/11884 - Loss: 30.3589\n",
      "Processing batch 1011/11884 - Loss: 30.8391\n",
      "Processing batch 1012/11884 - Loss: 31.7192\n",
      "Processing batch 1013/11884 - Loss: 31.9092\n",
      "Processing batch 1014/11884 - Loss: 32.1732\n",
      "Processing batch 1015/11884 - Loss: 31.2070\n",
      "Processing batch 1016/11884 - Loss: 28.8787\n",
      "Processing batch 1017/11884 - Loss: 30.8739\n",
      "Processing batch 1018/11884 - Loss: 30.3582\n",
      "Processing batch 1019/11884 - Loss: 30.1563\n",
      "Processing batch 1020/11884 - Loss: 29.7844\n",
      "Processing batch 1021/11884 - Loss: 30.8574\n",
      "Processing batch 1022/11884 - Loss: 30.6937\n",
      "Processing batch 1023/11884 - Loss: 30.0095\n",
      "Processing batch 1024/11884 - Loss: 29.8342\n",
      "Processing batch 1025/11884 - Loss: 30.3110\n",
      "Processing batch 1026/11884 - Loss: 30.3747\n",
      "Processing batch 1027/11884 - Loss: 28.9793\n",
      "Processing batch 1028/11884 - Loss: 31.2821\n",
      "Processing batch 1029/11884 - Loss: 30.6706\n",
      "Processing batch 1030/11884 - Loss: 30.8141\n",
      "Processing batch 1031/11884 - Loss: 30.4830\n",
      "Processing batch 1032/11884 - Loss: 30.2647\n",
      "Processing batch 1033/11884 - Loss: 31.5255\n",
      "Processing batch 1034/11884 - Loss: 29.4829\n",
      "Processing batch 1035/11884 - Loss: 30.2655\n",
      "Processing batch 1036/11884 - Loss: 31.5266\n",
      "Processing batch 1037/11884 - Loss: 30.1078\n",
      "Processing batch 1038/11884 - Loss: 29.7179\n",
      "Processing batch 1039/11884 - Loss: 30.7427\n",
      "Processing batch 1040/11884 - Loss: 30.0586\n",
      "Processing batch 1041/11884 - Loss: 29.1398\n",
      "Processing batch 1042/11884 - Loss: 30.2104\n",
      "Processing batch 1043/11884 - Loss: 29.9626\n",
      "Processing batch 1044/11884 - Loss: 30.6410\n",
      "Processing batch 1045/11884 - Loss: 30.2204\n",
      "Processing batch 1046/11884 - Loss: 31.3462\n",
      "Processing batch 1047/11884 - Loss: 29.8331\n",
      "Processing batch 1048/11884 - Loss: 31.5941\n",
      "Processing batch 1049/11884 - Loss: 30.4751\n",
      "Processing batch 1050/11884 - Loss: 29.9017\n",
      "Processing batch 1051/11884 - Loss: 30.8562\n",
      "Processing batch 1052/11884 - Loss: 30.7634\n",
      "Processing batch 1053/11884 - Loss: 30.8087\n",
      "Processing batch 1054/11884 - Loss: 31.8821\n",
      "Processing batch 1055/11884 - Loss: 28.9311\n",
      "Processing batch 1056/11884 - Loss: 31.1121\n",
      "Processing batch 1057/11884 - Loss: 30.9254\n",
      "Processing batch 1058/11884 - Loss: 29.8786\n",
      "Processing batch 1059/11884 - Loss: 31.9799\n",
      "Processing batch 1060/11884 - Loss: 30.9487\n",
      "Processing batch 1061/11884 - Loss: 32.0558\n",
      "Processing batch 1062/11884 - Loss: 30.8618\n",
      "Processing batch 1063/11884 - Loss: 29.9269\n",
      "Processing batch 1064/11884 - Loss: 31.1957\n",
      "Processing batch 1065/11884 - Loss: 30.9426\n",
      "Processing batch 1066/11884 - Loss: 30.8444\n",
      "Processing batch 1067/11884 - Loss: 30.2853\n",
      "Processing batch 1068/11884 - Loss: 30.3842\n",
      "Processing batch 1069/11884 - Loss: 30.5975\n",
      "Processing batch 1070/11884 - Loss: 30.5486\n",
      "Processing batch 1071/11884 - Loss: 30.0351\n",
      "Processing batch 1072/11884 - Loss: 30.8183\n",
      "Processing batch 1073/11884 - Loss: 31.5757\n",
      "Processing batch 1074/11884 - Loss: 31.5016\n",
      "Processing batch 1075/11884 - Loss: 31.2980\n",
      "Processing batch 1076/11884 - Loss: 31.0837\n",
      "Processing batch 1077/11884 - Loss: 30.5695\n",
      "Processing batch 1078/11884 - Loss: 30.5520\n",
      "Processing batch 1079/11884 - Loss: 30.7543\n",
      "Processing batch 1080/11884 - Loss: 29.9945\n",
      "Processing batch 1081/11884 - Loss: 30.7277\n",
      "Processing batch 1082/11884 - Loss: 29.4510\n",
      "Processing batch 1083/11884 - Loss: 31.8998\n",
      "Processing batch 1084/11884 - Loss: 30.2746\n",
      "Processing batch 1085/11884 - Loss: 30.6902\n",
      "Processing batch 1086/11884 - Loss: 30.4975\n",
      "Processing batch 1087/11884 - Loss: 30.5141\n",
      "Processing batch 1088/11884 - Loss: 31.2432\n",
      "Processing batch 1089/11884 - Loss: 29.4992\n",
      "Processing batch 1090/11884 - Loss: 31.4447\n",
      "Processing batch 1091/11884 - Loss: 31.1063\n",
      "Processing batch 1092/11884 - Loss: 30.3959\n",
      "Processing batch 1093/11884 - Loss: 31.5013\n",
      "Processing batch 1094/11884 - Loss: 33.0053\n",
      "Processing batch 1095/11884 - Loss: 30.5262\n",
      "Processing batch 1096/11884 - Loss: 30.1721\n",
      "Processing batch 1097/11884 - Loss: 30.5614\n",
      "Processing batch 1098/11884 - Loss: 31.4502\n",
      "Processing batch 1099/11884 - Loss: 31.3848\n",
      "Processing batch 1100/11884 - Loss: 31.7622\n",
      "Processing batch 1101/11884 - Loss: 31.3037\n",
      "Processing batch 1102/11884 - Loss: 30.0561\n",
      "Processing batch 1103/11884 - Loss: 29.6967\n",
      "Processing batch 1104/11884 - Loss: 31.1620\n",
      "Processing batch 1105/11884 - Loss: 30.3216\n",
      "Processing batch 1106/11884 - Loss: 30.8738\n",
      "Processing batch 1107/11884 - Loss: 30.7932\n",
      "Processing batch 1108/11884 - Loss: 31.0838\n",
      "Processing batch 1109/11884 - Loss: 30.7934\n",
      "Processing batch 1110/11884 - Loss: 30.9530\n",
      "Processing batch 1111/11884 - Loss: 30.4916\n",
      "Processing batch 1112/11884 - Loss: 29.7017\n",
      "Processing batch 1113/11884 - Loss: 30.5368\n",
      "Processing batch 1114/11884 - Loss: 30.8601\n",
      "Processing batch 1115/11884 - Loss: 32.3772\n",
      "Processing batch 1116/11884 - Loss: 32.0012\n",
      "Processing batch 1117/11884 - Loss: 31.3305\n",
      "Processing batch 1118/11884 - Loss: 30.4579\n",
      "Processing batch 1119/11884 - Loss: 30.9844\n",
      "Processing batch 1120/11884 - Loss: 29.4606\n",
      "Processing batch 1121/11884 - Loss: 30.9731\n",
      "Processing batch 1122/11884 - Loss: 29.4841\n",
      "Processing batch 1123/11884 - Loss: 32.2218\n",
      "Processing batch 1124/11884 - Loss: 30.7344\n",
      "Processing batch 1125/11884 - Loss: 29.8201\n",
      "Processing batch 1126/11884 - Loss: 31.8389\n",
      "Processing batch 1127/11884 - Loss: 31.2332\n",
      "Processing batch 1128/11884 - Loss: 30.9363\n",
      "Processing batch 1129/11884 - Loss: 31.8252\n",
      "Processing batch 1130/11884 - Loss: 31.2301\n",
      "Processing batch 1131/11884 - Loss: 29.8006\n",
      "Processing batch 1132/11884 - Loss: 30.4859\n",
      "Processing batch 1133/11884 - Loss: 31.3647\n",
      "Processing batch 1134/11884 - Loss: 30.8110\n",
      "Processing batch 1135/11884 - Loss: 30.3507\n",
      "Processing batch 1136/11884 - Loss: 30.8081\n",
      "Processing batch 1137/11884 - Loss: 30.4692\n",
      "Processing batch 1138/11884 - Loss: 31.1007\n",
      "Processing batch 1139/11884 - Loss: 31.0182\n",
      "Processing batch 1140/11884 - Loss: 31.7150\n",
      "Processing batch 1141/11884 - Loss: 29.6095\n",
      "Processing batch 1142/11884 - Loss: 30.5560\n",
      "Processing batch 1143/11884 - Loss: 31.6931\n",
      "Processing batch 1144/11884 - Loss: 30.2067\n",
      "Processing batch 1145/11884 - Loss: 30.6145\n",
      "Processing batch 1146/11884 - Loss: 31.6701\n",
      "Processing batch 1147/11884 - Loss: 30.0004\n",
      "Processing batch 1148/11884 - Loss: 30.8217\n",
      "Processing batch 1149/11884 - Loss: 32.6097\n",
      "Processing batch 1150/11884 - Loss: 29.1500\n",
      "Processing batch 1151/11884 - Loss: 30.4315\n",
      "Processing batch 1152/11884 - Loss: 30.6006\n",
      "Processing batch 1153/11884 - Loss: 30.5159\n",
      "Processing batch 1154/11884 - Loss: 30.6637\n",
      "Processing batch 1155/11884 - Loss: 29.7036\n",
      "Processing batch 1156/11884 - Loss: 31.3350\n",
      "Processing batch 1157/11884 - Loss: 31.1754\n",
      "Processing batch 1158/11884 - Loss: 30.8054\n",
      "Processing batch 1159/11884 - Loss: 29.3948\n",
      "Processing batch 1160/11884 - Loss: 31.5045\n",
      "Processing batch 1161/11884 - Loss: 31.0761\n",
      "Processing batch 1162/11884 - Loss: 30.6062\n",
      "Processing batch 1163/11884 - Loss: 30.3432\n",
      "Processing batch 1164/11884 - Loss: 31.1575\n",
      "Processing batch 1165/11884 - Loss: 29.7554\n",
      "Processing batch 1166/11884 - Loss: 29.4216\n",
      "Processing batch 1167/11884 - Loss: 30.6918\n",
      "Processing batch 1168/11884 - Loss: 31.2171\n",
      "Processing batch 1169/11884 - Loss: 29.4323\n",
      "Processing batch 1170/11884 - Loss: 30.5256\n",
      "Processing batch 1171/11884 - Loss: 30.8890\n",
      "Processing batch 1172/11884 - Loss: 30.5412\n",
      "Processing batch 1173/11884 - Loss: 31.1113\n",
      "Processing batch 1174/11884 - Loss: 29.8463\n",
      "Processing batch 1175/11884 - Loss: 32.3464\n",
      "Processing batch 1176/11884 - Loss: 30.4745\n",
      "Processing batch 1177/11884 - Loss: 30.0982\n",
      "Processing batch 1178/11884 - Loss: 30.4511\n",
      "Processing batch 1179/11884 - Loss: 31.0945\n",
      "Processing batch 1180/11884 - Loss: 31.2180\n",
      "Processing batch 1181/11884 - Loss: 29.1814\n",
      "Processing batch 1182/11884 - Loss: 28.6395\n",
      "Processing batch 1183/11884 - Loss: 31.7486\n",
      "Processing batch 1184/11884 - Loss: 30.4619\n",
      "Processing batch 1185/11884 - Loss: 29.9276\n",
      "Processing batch 1186/11884 - Loss: 30.4939\n",
      "Processing batch 1187/11884 - Loss: 31.3150\n",
      "Processing batch 1188/11884 - Loss: 31.0782\n",
      "Processing batch 1189/11884 - Loss: 30.2541\n",
      "Processing batch 1190/11884 - Loss: 29.5816\n",
      "Processing batch 1191/11884 - Loss: 30.8808\n",
      "Processing batch 1192/11884 - Loss: 30.8457\n",
      "Processing batch 1193/11884 - Loss: 31.3625\n",
      "Processing batch 1194/11884 - Loss: 29.0660\n",
      "Processing batch 1195/11884 - Loss: 30.7335\n",
      "Processing batch 1196/11884 - Loss: 30.9605\n",
      "Processing batch 1197/11884 - Loss: 30.4283\n",
      "Processing batch 1198/11884 - Loss: 30.9687\n",
      "Processing batch 1199/11884 - Loss: 30.2444\n",
      "Processing batch 1200/11884 - Loss: 31.8279\n",
      "Processing batch 1201/11884 - Loss: 31.2013\n",
      "Processing batch 1202/11884 - Loss: 30.8949\n",
      "Processing batch 1203/11884 - Loss: 30.3783\n",
      "Processing batch 1204/11884 - Loss: 30.1289\n",
      "Processing batch 1205/11884 - Loss: 30.2892\n",
      "Processing batch 1206/11884 - Loss: 29.8270\n",
      "Processing batch 1207/11884 - Loss: 30.6238\n",
      "Processing batch 1208/11884 - Loss: 29.8349\n",
      "Processing batch 1209/11884 - Loss: 30.2707\n",
      "Processing batch 1210/11884 - Loss: 28.8618\n",
      "Processing batch 1211/11884 - Loss: 31.5765\n",
      "Processing batch 1212/11884 - Loss: 30.5700\n",
      "Processing batch 1213/11884 - Loss: 29.8316\n",
      "Processing batch 1214/11884 - Loss: 29.7830\n",
      "Processing batch 1215/11884 - Loss: 30.1604\n",
      "Processing batch 1216/11884 - Loss: 31.3377\n",
      "Processing batch 1217/11884 - Loss: 30.1625\n",
      "Processing batch 1218/11884 - Loss: 30.2337\n",
      "Processing batch 1219/11884 - Loss: 30.7783\n",
      "Processing batch 1220/11884 - Loss: 30.5837\n",
      "Processing batch 1221/11884 - Loss: 30.0378\n",
      "Processing batch 1222/11884 - Loss: 31.4416\n",
      "Processing batch 1223/11884 - Loss: 31.0715\n",
      "Processing batch 1224/11884 - Loss: 30.3794\n",
      "Processing batch 1225/11884 - Loss: 29.3833\n",
      "Processing batch 1226/11884 - Loss: 31.2625\n",
      "Processing batch 1227/11884 - Loss: 29.9674\n",
      "Processing batch 1228/11884 - Loss: 29.4457\n",
      "Processing batch 1229/11884 - Loss: 31.4662\n",
      "Processing batch 1230/11884 - Loss: 30.9886\n",
      "Processing batch 1231/11884 - Loss: 30.7882\n",
      "Processing batch 1232/11884 - Loss: 31.5237\n",
      "Processing batch 1233/11884 - Loss: 31.1238\n",
      "Processing batch 1234/11884 - Loss: 30.0784\n",
      "Processing batch 1235/11884 - Loss: 30.3364\n",
      "Processing batch 1236/11884 - Loss: 30.6774\n",
      "Processing batch 1237/11884 - Loss: 31.2386\n",
      "Processing batch 1238/11884 - Loss: 29.4721\n",
      "Processing batch 1239/11884 - Loss: 30.5468\n",
      "Processing batch 1240/11884 - Loss: 31.4408\n",
      "Processing batch 1241/11884 - Loss: 30.9166\n",
      "Processing batch 1242/11884 - Loss: 29.1723\n",
      "Processing batch 1243/11884 - Loss: 31.4859\n",
      "Processing batch 1244/11884 - Loss: 30.3586\n",
      "Processing batch 1245/11884 - Loss: 29.9296\n",
      "Processing batch 1246/11884 - Loss: 30.2821\n",
      "Processing batch 1247/11884 - Loss: 30.3756\n",
      "Processing batch 1248/11884 - Loss: 30.8766\n",
      "Processing batch 1249/11884 - Loss: 30.2345\n",
      "Processing batch 1250/11884 - Loss: 32.2133\n",
      "Processing batch 1251/11884 - Loss: 30.6959\n",
      "Processing batch 1252/11884 - Loss: 30.1948\n",
      "Processing batch 1253/11884 - Loss: 29.8789\n",
      "Processing batch 1254/11884 - Loss: 31.5781\n",
      "Processing batch 1255/11884 - Loss: 31.5918\n",
      "Processing batch 1256/11884 - Loss: 30.2000\n",
      "Processing batch 1257/11884 - Loss: 30.1020\n",
      "Processing batch 1258/11884 - Loss: 30.5741\n",
      "Processing batch 1259/11884 - Loss: 30.6566\n",
      "Processing batch 1260/11884 - Loss: 30.4338\n",
      "Processing batch 1261/11884 - Loss: 31.3379\n",
      "Processing batch 1262/11884 - Loss: 30.9288\n",
      "Processing batch 1263/11884 - Loss: 31.1765\n",
      "Processing batch 1264/11884 - Loss: 31.2842\n",
      "Processing batch 1265/11884 - Loss: 31.8501\n",
      "Processing batch 1266/11884 - Loss: 30.5617\n",
      "Processing batch 1267/11884 - Loss: 30.6657\n",
      "Processing batch 1268/11884 - Loss: 31.1496\n",
      "Processing batch 1269/11884 - Loss: 31.2244\n",
      "Processing batch 1270/11884 - Loss: 30.2826\n",
      "Processing batch 1271/11884 - Loss: 32.1873\n",
      "Processing batch 1272/11884 - Loss: 30.8611\n",
      "Processing batch 1273/11884 - Loss: 30.8081\n",
      "Processing batch 1274/11884 - Loss: 30.7419\n",
      "Processing batch 1275/11884 - Loss: 30.8020\n",
      "Processing batch 1276/11884 - Loss: 30.1198\n",
      "Processing batch 1277/11884 - Loss: 30.1976\n",
      "Processing batch 1278/11884 - Loss: 30.3708\n",
      "Processing batch 1279/11884 - Loss: 29.2196\n",
      "Processing batch 1280/11884 - Loss: 30.9493\n",
      "Processing batch 1281/11884 - Loss: 30.0427\n",
      "Processing batch 1282/11884 - Loss: 29.2752\n",
      "Processing batch 1283/11884 - Loss: 30.1322\n",
      "Processing batch 1284/11884 - Loss: 31.2416\n",
      "Processing batch 1285/11884 - Loss: 30.7583\n",
      "Processing batch 1286/11884 - Loss: 29.3254\n",
      "Processing batch 1287/11884 - Loss: 30.6188\n",
      "Processing batch 1288/11884 - Loss: 31.1856\n",
      "Processing batch 1289/11884 - Loss: 29.6060\n",
      "Processing batch 1290/11884 - Loss: 30.8947\n",
      "Processing batch 1291/11884 - Loss: 30.1824\n",
      "Processing batch 1292/11884 - Loss: 28.8345\n",
      "Processing batch 1293/11884 - Loss: 31.1658\n",
      "Processing batch 1294/11884 - Loss: 32.3942\n",
      "Processing batch 1295/11884 - Loss: 29.1714\n",
      "Processing batch 1296/11884 - Loss: 30.3534\n",
      "Processing batch 1297/11884 - Loss: 32.5985\n",
      "Processing batch 1298/11884 - Loss: 30.6633\n",
      "Processing batch 1299/11884 - Loss: 30.1295\n",
      "Processing batch 1300/11884 - Loss: 30.5533\n",
      "Processing batch 1301/11884 - Loss: 29.3336\n",
      "Processing batch 1302/11884 - Loss: 30.9037\n",
      "Processing batch 1303/11884 - Loss: 30.7982\n",
      "Processing batch 1304/11884 - Loss: 30.5686\n",
      "Processing batch 1305/11884 - Loss: 30.6779\n",
      "Processing batch 1306/11884 - Loss: 28.7324\n",
      "Processing batch 1307/11884 - Loss: 30.5065\n",
      "Processing batch 1308/11884 - Loss: 30.9294\n",
      "Processing batch 1309/11884 - Loss: 29.9035\n",
      "Processing batch 1310/11884 - Loss: 31.0429\n",
      "Processing batch 1311/11884 - Loss: 30.2210\n",
      "Processing batch 1312/11884 - Loss: 32.3894\n",
      "Processing batch 1313/11884 - Loss: 30.7369\n",
      "Processing batch 1314/11884 - Loss: 30.1378\n",
      "Processing batch 1315/11884 - Loss: 29.9202\n",
      "Processing batch 1316/11884 - Loss: 31.1430\n",
      "Processing batch 1317/11884 - Loss: 31.9291\n",
      "Processing batch 1318/11884 - Loss: 29.7148\n",
      "Processing batch 1319/11884 - Loss: 30.9197\n",
      "Processing batch 1320/11884 - Loss: 31.8534\n",
      "Processing batch 1321/11884 - Loss: 30.3008\n",
      "Processing batch 1322/11884 - Loss: 29.3803\n",
      "Processing batch 1323/11884 - Loss: 31.7849\n",
      "Processing batch 1324/11884 - Loss: 30.2275\n",
      "Processing batch 1325/11884 - Loss: 30.9346\n",
      "Processing batch 1326/11884 - Loss: 30.3475\n",
      "Processing batch 1327/11884 - Loss: 30.2769\n",
      "Processing batch 1328/11884 - Loss: 30.6748\n",
      "Processing batch 1329/11884 - Loss: 30.3853\n",
      "Processing batch 1330/11884 - Loss: 29.7021\n",
      "Processing batch 1331/11884 - Loss: 29.6350\n",
      "Processing batch 1332/11884 - Loss: 30.6315\n",
      "Processing batch 1333/11884 - Loss: 29.5741\n",
      "Processing batch 1334/11884 - Loss: 30.7258\n",
      "Processing batch 1335/11884 - Loss: 30.2156\n",
      "Processing batch 1336/11884 - Loss: 31.4414\n",
      "Processing batch 1337/11884 - Loss: 29.9564\n",
      "Processing batch 1338/11884 - Loss: 30.4920\n",
      "Processing batch 1339/11884 - Loss: 30.0168\n",
      "Processing batch 1340/11884 - Loss: 29.5548\n",
      "Processing batch 1341/11884 - Loss: 31.1982\n",
      "Processing batch 1342/11884 - Loss: 31.8791\n",
      "Processing batch 1343/11884 - Loss: 29.9269\n",
      "Processing batch 1344/11884 - Loss: 32.4288\n",
      "Processing batch 1345/11884 - Loss: 29.2840\n",
      "Processing batch 1346/11884 - Loss: 31.5350\n",
      "Processing batch 1347/11884 - Loss: 30.1628\n",
      "Processing batch 1348/11884 - Loss: 31.7150\n",
      "Processing batch 1349/11884 - Loss: 28.7824\n",
      "Processing batch 1350/11884 - Loss: 31.4376\n",
      "Processing batch 1351/11884 - Loss: 32.6314\n",
      "Processing batch 1352/11884 - Loss: 31.1644\n",
      "Processing batch 1353/11884 - Loss: 30.3665\n",
      "Processing batch 1354/11884 - Loss: 30.5313\n",
      "Processing batch 1355/11884 - Loss: 29.5274\n",
      "Processing batch 1356/11884 - Loss: 30.5639\n",
      "Processing batch 1357/11884 - Loss: 30.0150\n",
      "Processing batch 1358/11884 - Loss: 31.0158\n",
      "Processing batch 1359/11884 - Loss: 30.2037\n",
      "Processing batch 1360/11884 - Loss: 30.9297\n",
      "Processing batch 1361/11884 - Loss: 30.6244\n",
      "Processing batch 1362/11884 - Loss: 29.5870\n",
      "Processing batch 1363/11884 - Loss: 30.2743\n",
      "Processing batch 1364/11884 - Loss: 30.5163\n",
      "Processing batch 1365/11884 - Loss: 30.6028\n",
      "Processing batch 1366/11884 - Loss: 30.8465\n",
      "Processing batch 1367/11884 - Loss: 30.8271\n",
      "Processing batch 1368/11884 - Loss: 30.0237\n",
      "Processing batch 1369/11884 - Loss: 30.3816\n",
      "Processing batch 1370/11884 - Loss: 31.8978\n",
      "Processing batch 1371/11884 - Loss: 30.7247\n",
      "Processing batch 1372/11884 - Loss: 31.6007\n",
      "Processing batch 1373/11884 - Loss: 31.3988\n",
      "Processing batch 1374/11884 - Loss: 29.3819\n",
      "Processing batch 1375/11884 - Loss: 29.5426\n",
      "Processing batch 1376/11884 - Loss: 30.9021\n",
      "Processing batch 1377/11884 - Loss: 30.6801\n",
      "Processing batch 1378/11884 - Loss: 29.7190\n",
      "Processing batch 1379/11884 - Loss: 29.1858\n",
      "Processing batch 1380/11884 - Loss: 29.7312\n",
      "Processing batch 1381/11884 - Loss: 30.9102\n",
      "Processing batch 1382/11884 - Loss: 31.2792\n",
      "Processing batch 1383/11884 - Loss: 29.6043\n",
      "Processing batch 1384/11884 - Loss: 30.2841\n",
      "Processing batch 1385/11884 - Loss: 30.9550\n",
      "Processing batch 1386/11884 - Loss: 30.2836\n",
      "Processing batch 1387/11884 - Loss: 29.8580\n",
      "Processing batch 1388/11884 - Loss: 30.8672\n",
      "Processing batch 1389/11884 - Loss: 30.4147\n",
      "Processing batch 1390/11884 - Loss: 29.9704\n",
      "Processing batch 1391/11884 - Loss: 30.1232\n",
      "Processing batch 1392/11884 - Loss: 29.0646\n",
      "Processing batch 1393/11884 - Loss: 29.3146\n",
      "Processing batch 1394/11884 - Loss: 32.0062\n",
      "Processing batch 1395/11884 - Loss: 31.4055\n",
      "Processing batch 1396/11884 - Loss: 30.0791\n",
      "Processing batch 1397/11884 - Loss: 29.6159\n",
      "Processing batch 1398/11884 - Loss: 30.7143\n",
      "Processing batch 1399/11884 - Loss: 30.7185\n",
      "Processing batch 1400/11884 - Loss: 30.4516\n",
      "Processing batch 1401/11884 - Loss: 29.1540\n",
      "Processing batch 1402/11884 - Loss: 30.2326\n",
      "Processing batch 1403/11884 - Loss: 29.6692\n",
      "Processing batch 1404/11884 - Loss: 30.1495\n",
      "Processing batch 1405/11884 - Loss: 29.3377\n",
      "Processing batch 1406/11884 - Loss: 30.1396\n",
      "Processing batch 1407/11884 - Loss: 31.0512\n",
      "Processing batch 1408/11884 - Loss: 30.3756\n",
      "Processing batch 1409/11884 - Loss: 30.6508\n",
      "Processing batch 1410/11884 - Loss: 31.0216\n",
      "Processing batch 1411/11884 - Loss: 31.7987\n",
      "Processing batch 1412/11884 - Loss: 30.4113\n",
      "Processing batch 1413/11884 - Loss: 30.3618\n",
      "Processing batch 1414/11884 - Loss: 31.6018\n",
      "Processing batch 1415/11884 - Loss: 30.5043\n",
      "Processing batch 1416/11884 - Loss: 29.3737\n",
      "Processing batch 1417/11884 - Loss: 31.1805\n",
      "Processing batch 1418/11884 - Loss: 29.8756\n",
      "Processing batch 1419/11884 - Loss: 30.5193\n",
      "Processing batch 1420/11884 - Loss: 30.7764\n",
      "Processing batch 1421/11884 - Loss: 30.1811\n",
      "Processing batch 1422/11884 - Loss: 30.8889\n",
      "Processing batch 1423/11884 - Loss: 29.1726\n",
      "Processing batch 1424/11884 - Loss: 31.0597\n",
      "Processing batch 1425/11884 - Loss: 30.0570\n",
      "Processing batch 1426/11884 - Loss: 32.4814\n",
      "Processing batch 1427/11884 - Loss: 29.6543\n",
      "Processing batch 1428/11884 - Loss: 29.6126\n",
      "Processing batch 1429/11884 - Loss: 30.3792\n",
      "Processing batch 1430/11884 - Loss: 31.0080\n",
      "Processing batch 1431/11884 - Loss: 29.6119\n",
      "Processing batch 1432/11884 - Loss: 30.9666\n",
      "Processing batch 1433/11884 - Loss: 30.3693\n",
      "Processing batch 1434/11884 - Loss: 31.8802\n",
      "Processing batch 1435/11884 - Loss: 30.6558\n",
      "Processing batch 1436/11884 - Loss: 32.6503\n",
      "Processing batch 1437/11884 - Loss: 29.5568\n",
      "Processing batch 1438/11884 - Loss: 30.9790\n",
      "Processing batch 1439/11884 - Loss: 30.4596\n",
      "Processing batch 1440/11884 - Loss: 29.9476\n",
      "Processing batch 1441/11884 - Loss: 29.5425\n",
      "Processing batch 1442/11884 - Loss: 31.1747\n",
      "Processing batch 1443/11884 - Loss: 29.7584\n",
      "Processing batch 1444/11884 - Loss: 32.4011\n",
      "Processing batch 1445/11884 - Loss: 30.2998\n",
      "Processing batch 1446/11884 - Loss: 30.9416\n",
      "Processing batch 1447/11884 - Loss: 29.8155\n",
      "Processing batch 1448/11884 - Loss: 29.2494\n",
      "Processing batch 1449/11884 - Loss: 31.0845\n",
      "Processing batch 1450/11884 - Loss: 30.8939\n",
      "Processing batch 1451/11884 - Loss: 31.5206\n",
      "Processing batch 1452/11884 - Loss: 31.8983\n",
      "Processing batch 1453/11884 - Loss: 31.2779\n",
      "Processing batch 1454/11884 - Loss: 32.3280\n",
      "Processing batch 1455/11884 - Loss: 31.3797\n",
      "Processing batch 1456/11884 - Loss: 31.5385\n",
      "Processing batch 1457/11884 - Loss: 31.3460\n",
      "Processing batch 1458/11884 - Loss: 32.2151\n",
      "Processing batch 1459/11884 - Loss: 31.1781\n",
      "Processing batch 1460/11884 - Loss: 29.7285\n",
      "Processing batch 1461/11884 - Loss: 31.4276\n",
      "Processing batch 1462/11884 - Loss: 30.1960\n",
      "Processing batch 1463/11884 - Loss: 30.3962\n",
      "Processing batch 1464/11884 - Loss: 29.9399\n",
      "Processing batch 1465/11884 - Loss: 32.8636\n",
      "Processing batch 1466/11884 - Loss: 29.8128\n",
      "Processing batch 1467/11884 - Loss: 30.3440\n",
      "Processing batch 1468/11884 - Loss: 30.4586\n",
      "Processing batch 1469/11884 - Loss: 30.6817\n",
      "Processing batch 1470/11884 - Loss: 30.9349\n",
      "Processing batch 1471/11884 - Loss: 31.5928\n",
      "Processing batch 1472/11884 - Loss: 29.2074\n",
      "Processing batch 1473/11884 - Loss: 30.9230\n",
      "Processing batch 1474/11884 - Loss: 29.7956\n",
      "Processing batch 1475/11884 - Loss: 31.7134\n",
      "Processing batch 1476/11884 - Loss: 30.8729\n",
      "Processing batch 1477/11884 - Loss: 30.4998\n",
      "Processing batch 1478/11884 - Loss: 28.9200\n",
      "Processing batch 1479/11884 - Loss: 29.5915\n",
      "Processing batch 1480/11884 - Loss: 30.9197\n",
      "Processing batch 1481/11884 - Loss: 31.1155\n",
      "Processing batch 1482/11884 - Loss: 31.1382\n",
      "Processing batch 1483/11884 - Loss: 29.9950\n",
      "Processing batch 1484/11884 - Loss: 30.4717\n",
      "Processing batch 1485/11884 - Loss: 32.0605\n",
      "Processing batch 1486/11884 - Loss: 31.2212\n",
      "Processing batch 1487/11884 - Loss: 30.9275\n",
      "Processing batch 1488/11884 - Loss: 32.2436\n",
      "Processing batch 1489/11884 - Loss: 31.0167\n",
      "Processing batch 1490/11884 - Loss: 30.4667\n",
      "Processing batch 1491/11884 - Loss: 32.4927\n",
      "Processing batch 1492/11884 - Loss: 30.1000\n",
      "Processing batch 1493/11884 - Loss: 31.4609\n",
      "Processing batch 1494/11884 - Loss: 30.9043\n",
      "Processing batch 1495/11884 - Loss: 29.4259\n",
      "Processing batch 1496/11884 - Loss: 30.8978\n",
      "Processing batch 1497/11884 - Loss: 31.2939\n",
      "Processing batch 1498/11884 - Loss: 29.6268\n",
      "Processing batch 1499/11884 - Loss: 30.7835\n",
      "Processing batch 1500/11884 - Loss: 30.5526\n",
      "Processing batch 1501/11884 - Loss: 31.5001\n",
      "Processing batch 1502/11884 - Loss: 30.7093\n",
      "Processing batch 1503/11884 - Loss: 30.7204\n",
      "Processing batch 1504/11884 - Loss: 28.5211\n",
      "Processing batch 1505/11884 - Loss: 31.2464\n",
      "Processing batch 1506/11884 - Loss: 31.3386\n",
      "Processing batch 1507/11884 - Loss: 30.3653\n",
      "Processing batch 1508/11884 - Loss: 31.1559\n",
      "Processing batch 1509/11884 - Loss: 31.3667\n",
      "Processing batch 1510/11884 - Loss: 29.9860\n",
      "Processing batch 1511/11884 - Loss: 31.4330\n",
      "Processing batch 1512/11884 - Loss: 30.4057\n",
      "Processing batch 1513/11884 - Loss: 30.0770\n",
      "Processing batch 1514/11884 - Loss: 29.6077\n",
      "Processing batch 1515/11884 - Loss: 30.2345\n",
      "Processing batch 1516/11884 - Loss: 31.5409\n",
      "Processing batch 1517/11884 - Loss: 30.0600\n",
      "Processing batch 1518/11884 - Loss: 29.3393\n",
      "Processing batch 1519/11884 - Loss: 30.4851\n",
      "Processing batch 1520/11884 - Loss: 31.3136\n",
      "Processing batch 1521/11884 - Loss: 31.3232\n",
      "Processing batch 1522/11884 - Loss: 31.3208\n",
      "Processing batch 1523/11884 - Loss: 31.3732\n",
      "Processing batch 1524/11884 - Loss: 29.9744\n",
      "Processing batch 1525/11884 - Loss: 31.6610\n",
      "Processing batch 1526/11884 - Loss: 30.8375\n",
      "Processing batch 1527/11884 - Loss: 30.4360\n",
      "Processing batch 1528/11884 - Loss: 30.9089\n",
      "Processing batch 1529/11884 - Loss: 30.0375\n",
      "Processing batch 1530/11884 - Loss: 32.2752\n",
      "Processing batch 1531/11884 - Loss: 30.7507\n",
      "Processing batch 1532/11884 - Loss: 29.4441\n",
      "Processing batch 1533/11884 - Loss: 30.5088\n",
      "Processing batch 1534/11884 - Loss: 31.4522\n",
      "Processing batch 1535/11884 - Loss: 31.7109\n",
      "Processing batch 1536/11884 - Loss: 31.3065\n",
      "Processing batch 1537/11884 - Loss: 31.6437\n",
      "Processing batch 1538/11884 - Loss: 31.3301\n",
      "Processing batch 1539/11884 - Loss: 30.0151\n",
      "Processing batch 1540/11884 - Loss: 31.9182\n",
      "Processing batch 1541/11884 - Loss: 31.0884\n",
      "Processing batch 1542/11884 - Loss: 30.3315\n",
      "Processing batch 1543/11884 - Loss: 30.7341\n",
      "Processing batch 1544/11884 - Loss: 30.0404\n",
      "Processing batch 1545/11884 - Loss: 31.1843\n",
      "Processing batch 1546/11884 - Loss: 29.4304\n",
      "Processing batch 1547/11884 - Loss: 30.2748\n",
      "Processing batch 1548/11884 - Loss: 30.7597\n",
      "Processing batch 1549/11884 - Loss: 31.0816\n",
      "Processing batch 1550/11884 - Loss: 31.5073\n",
      "Processing batch 1551/11884 - Loss: 31.5433\n",
      "Processing batch 1552/11884 - Loss: 29.5096\n",
      "Processing batch 1553/11884 - Loss: 29.4525\n",
      "Processing batch 1554/11884 - Loss: 30.1699\n",
      "Processing batch 1555/11884 - Loss: 31.6688\n",
      "Processing batch 1556/11884 - Loss: 31.5143\n",
      "Processing batch 1557/11884 - Loss: 32.2206\n",
      "Processing batch 1558/11884 - Loss: 31.1668\n",
      "Processing batch 1559/11884 - Loss: 30.7254\n",
      "Processing batch 1560/11884 - Loss: 32.0633\n",
      "Processing batch 1561/11884 - Loss: 29.4580\n",
      "Processing batch 1562/11884 - Loss: 30.5124\n",
      "Processing batch 1563/11884 - Loss: 31.0589\n",
      "Processing batch 1564/11884 - Loss: 31.0841\n",
      "Processing batch 1565/11884 - Loss: 30.2093\n",
      "Processing batch 1566/11884 - Loss: 29.8157\n",
      "Processing batch 1567/11884 - Loss: 29.8769\n",
      "Processing batch 1568/11884 - Loss: 30.3805\n",
      "Processing batch 1569/11884 - Loss: 30.2501\n",
      "Processing batch 1570/11884 - Loss: 31.4565\n",
      "Processing batch 1571/11884 - Loss: 29.5447\n",
      "Processing batch 1572/11884 - Loss: 30.2623\n",
      "Processing batch 1573/11884 - Loss: 30.9372\n",
      "Processing batch 1574/11884 - Loss: 30.3956\n",
      "Processing batch 1575/11884 - Loss: 30.6156\n",
      "Processing batch 1576/11884 - Loss: 31.5904\n",
      "Processing batch 1577/11884 - Loss: 29.9430\n",
      "Processing batch 1578/11884 - Loss: 30.4434\n",
      "Processing batch 1579/11884 - Loss: 29.5445\n",
      "Processing batch 1580/11884 - Loss: 30.8246\n",
      "Processing batch 1581/11884 - Loss: 30.9107\n",
      "Processing batch 1582/11884 - Loss: 30.4093\n",
      "Processing batch 1583/11884 - Loss: 31.2908\n",
      "Processing batch 1584/11884 - Loss: 30.5588\n",
      "Processing batch 1585/11884 - Loss: 30.2992\n",
      "Processing batch 1586/11884 - Loss: 30.5887\n",
      "Processing batch 1587/11884 - Loss: 30.2162\n",
      "Processing batch 1588/11884 - Loss: 29.9668\n",
      "Processing batch 1589/11884 - Loss: 30.6882\n",
      "Processing batch 1590/11884 - Loss: 31.9353\n",
      "Processing batch 1591/11884 - Loss: 30.0129\n",
      "Processing batch 1592/11884 - Loss: 29.5018\n",
      "Processing batch 1593/11884 - Loss: 31.0268\n",
      "Processing batch 1594/11884 - Loss: 30.0130\n",
      "Processing batch 1595/11884 - Loss: 31.0044\n",
      "Processing batch 1596/11884 - Loss: 32.0175\n",
      "Processing batch 1597/11884 - Loss: 30.5427\n",
      "Processing batch 1598/11884 - Loss: 30.8037\n",
      "Processing batch 1599/11884 - Loss: 30.5500\n",
      "Processing batch 1600/11884 - Loss: 29.6105\n",
      "Processing batch 1601/11884 - Loss: 30.5973\n",
      "Processing batch 1602/11884 - Loss: 31.7709\n",
      "Processing batch 1603/11884 - Loss: 29.7584\n",
      "Processing batch 1604/11884 - Loss: 30.9705\n",
      "Processing batch 1605/11884 - Loss: 30.3701\n",
      "Processing batch 1606/11884 - Loss: 30.3894\n",
      "Processing batch 1607/11884 - Loss: 29.4562\n",
      "Processing batch 1608/11884 - Loss: 29.9064\n",
      "Processing batch 1609/11884 - Loss: 31.2347\n",
      "Processing batch 1610/11884 - Loss: 30.0653\n",
      "Processing batch 1611/11884 - Loss: 29.9430\n",
      "Processing batch 1612/11884 - Loss: 30.2962\n",
      "Processing batch 1613/11884 - Loss: 32.4412\n",
      "Processing batch 1614/11884 - Loss: 31.8525\n",
      "Processing batch 1615/11884 - Loss: 30.5930\n",
      "Processing batch 1616/11884 - Loss: 31.7524\n",
      "Processing batch 1617/11884 - Loss: 31.3574\n",
      "Processing batch 1618/11884 - Loss: 30.7549\n",
      "Processing batch 1619/11884 - Loss: 30.0895\n",
      "Processing batch 1620/11884 - Loss: 31.2123\n",
      "Processing batch 1621/11884 - Loss: 31.3164\n",
      "Processing batch 1622/11884 - Loss: 30.2876\n",
      "Processing batch 1623/11884 - Loss: 32.3510\n",
      "Processing batch 1624/11884 - Loss: 31.4605\n",
      "Processing batch 1625/11884 - Loss: 31.3357\n",
      "Processing batch 1626/11884 - Loss: 30.7567\n",
      "Processing batch 1627/11884 - Loss: 30.4016\n",
      "Processing batch 1628/11884 - Loss: 31.3049\n",
      "Processing batch 1629/11884 - Loss: 33.0015\n",
      "Processing batch 1630/11884 - Loss: 30.6084\n",
      "Processing batch 1631/11884 - Loss: 31.2742\n",
      "Processing batch 1632/11884 - Loss: 30.8968\n",
      "Processing batch 1633/11884 - Loss: 32.1769\n",
      "Processing batch 1634/11884 - Loss: 30.3707\n",
      "Processing batch 1635/11884 - Loss: 29.5714\n",
      "Processing batch 1636/11884 - Loss: 30.1839\n",
      "Processing batch 1637/11884 - Loss: 31.2282\n",
      "Processing batch 1638/11884 - Loss: 29.6704\n",
      "Processing batch 1639/11884 - Loss: 30.2690\n",
      "Processing batch 1640/11884 - Loss: 32.0619\n",
      "Processing batch 1641/11884 - Loss: 30.9350\n",
      "Processing batch 1642/11884 - Loss: 29.6632\n",
      "Processing batch 1643/11884 - Loss: 31.5818\n",
      "Processing batch 1644/11884 - Loss: 30.5483\n",
      "Processing batch 1645/11884 - Loss: 29.8343\n",
      "Processing batch 1646/11884 - Loss: 30.4044\n",
      "Processing batch 1647/11884 - Loss: 31.6625\n",
      "Processing batch 1648/11884 - Loss: 31.8543\n",
      "Processing batch 1649/11884 - Loss: 29.5968\n",
      "Processing batch 1650/11884 - Loss: 31.1637\n",
      "Processing batch 1651/11884 - Loss: 32.7011\n",
      "Processing batch 1652/11884 - Loss: 29.9123\n",
      "Processing batch 1653/11884 - Loss: 31.0071\n",
      "Processing batch 1654/11884 - Loss: 30.9585\n",
      "Processing batch 1655/11884 - Loss: 31.0493\n",
      "Processing batch 1656/11884 - Loss: 30.1879\n",
      "Processing batch 1657/11884 - Loss: 30.6854\n",
      "Processing batch 1658/11884 - Loss: 29.6944\n",
      "Processing batch 1659/11884 - Loss: 31.5642\n",
      "Processing batch 1660/11884 - Loss: 32.7277\n",
      "Processing batch 1661/11884 - Loss: 30.9266\n",
      "Processing batch 1662/11884 - Loss: 29.7455\n",
      "Processing batch 1663/11884 - Loss: 30.0924\n",
      "Processing batch 1664/11884 - Loss: 29.7524\n",
      "Processing batch 1665/11884 - Loss: 31.0424\n",
      "Processing batch 1666/11884 - Loss: 30.9180\n",
      "Processing batch 1667/11884 - Loss: 30.8540\n",
      "Processing batch 1668/11884 - Loss: 30.5669\n",
      "Processing batch 1669/11884 - Loss: 29.5970\n",
      "Processing batch 1670/11884 - Loss: 30.5016\n",
      "Processing batch 1671/11884 - Loss: 30.6489\n",
      "Processing batch 1672/11884 - Loss: 30.0665\n",
      "Processing batch 1673/11884 - Loss: 29.9930\n",
      "Processing batch 1674/11884 - Loss: 31.4984\n",
      "Processing batch 1675/11884 - Loss: 30.1440\n",
      "Processing batch 1676/11884 - Loss: 30.9036\n",
      "Processing batch 1677/11884 - Loss: 33.1421\n",
      "Processing batch 1678/11884 - Loss: 30.8327\n",
      "Processing batch 1679/11884 - Loss: 29.9030\n",
      "Processing batch 1680/11884 - Loss: 30.9785\n",
      "Processing batch 1681/11884 - Loss: 30.6869\n",
      "Processing batch 1682/11884 - Loss: 30.7306\n",
      "Processing batch 1683/11884 - Loss: 32.0074\n",
      "Processing batch 1684/11884 - Loss: 30.0413\n",
      "Processing batch 1685/11884 - Loss: 30.6376\n",
      "Processing batch 1686/11884 - Loss: 31.9652\n",
      "Processing batch 1687/11884 - Loss: 31.1029\n",
      "Processing batch 1688/11884 - Loss: 28.6515\n",
      "Processing batch 1689/11884 - Loss: 30.9262\n",
      "Processing batch 1690/11884 - Loss: 31.4894\n",
      "Processing batch 1691/11884 - Loss: 31.2209\n",
      "Processing batch 1692/11884 - Loss: 29.2119\n",
      "Processing batch 1693/11884 - Loss: 31.2503\n",
      "Processing batch 1694/11884 - Loss: 32.2627\n",
      "Processing batch 1695/11884 - Loss: 29.4224\n",
      "Processing batch 1696/11884 - Loss: 29.3687\n",
      "Processing batch 1697/11884 - Loss: 29.7266\n",
      "Processing batch 1698/11884 - Loss: 29.8567\n",
      "Processing batch 1699/11884 - Loss: 30.6637\n",
      "Processing batch 1700/11884 - Loss: 30.9025\n",
      "Processing batch 1701/11884 - Loss: 30.1060\n",
      "Processing batch 1702/11884 - Loss: 31.7008\n",
      "Processing batch 1703/11884 - Loss: 29.9909\n",
      "Processing batch 1704/11884 - Loss: 30.7266\n",
      "Processing batch 1705/11884 - Loss: 30.6904\n",
      "Processing batch 1706/11884 - Loss: 30.4254\n",
      "Processing batch 1707/11884 - Loss: 30.8260\n",
      "Processing batch 1708/11884 - Loss: 31.3348\n",
      "Processing batch 1709/11884 - Loss: 30.8437\n",
      "Processing batch 1710/11884 - Loss: 32.4825\n",
      "Processing batch 1711/11884 - Loss: 30.8689\n",
      "Processing batch 1712/11884 - Loss: 30.8408\n",
      "Processing batch 1713/11884 - Loss: 29.3926\n",
      "Processing batch 1714/11884 - Loss: 30.3026\n",
      "Processing batch 1715/11884 - Loss: 31.2758\n",
      "Processing batch 1716/11884 - Loss: 31.2242\n",
      "Processing batch 1717/11884 - Loss: 30.2927\n",
      "Processing batch 1718/11884 - Loss: 30.7119\n",
      "Processing batch 1719/11884 - Loss: 30.1454\n",
      "Processing batch 1720/11884 - Loss: 31.1452\n",
      "Processing batch 1721/11884 - Loss: 29.3341\n",
      "Processing batch 1722/11884 - Loss: 30.5400\n",
      "Processing batch 1723/11884 - Loss: 29.5088\n",
      "Processing batch 1724/11884 - Loss: 30.0888\n",
      "Processing batch 1725/11884 - Loss: 30.9341\n",
      "Processing batch 1726/11884 - Loss: 30.7831\n",
      "Processing batch 1727/11884 - Loss: 31.3050\n",
      "Processing batch 1728/11884 - Loss: 29.5332\n",
      "Processing batch 1729/11884 - Loss: 31.0872\n",
      "Processing batch 1730/11884 - Loss: 31.6905\n",
      "Processing batch 1731/11884 - Loss: 31.8564\n",
      "Processing batch 1732/11884 - Loss: 30.8671\n",
      "Processing batch 1733/11884 - Loss: 31.5987\n",
      "Processing batch 1734/11884 - Loss: 30.9069\n",
      "Processing batch 1735/11884 - Loss: 29.0459\n",
      "Processing batch 1736/11884 - Loss: 30.6070\n",
      "Processing batch 1737/11884 - Loss: 31.5080\n",
      "Processing batch 1738/11884 - Loss: 31.3699\n",
      "Processing batch 1739/11884 - Loss: 30.6884\n",
      "Processing batch 1740/11884 - Loss: 30.6882\n",
      "Processing batch 1741/11884 - Loss: 32.1439\n",
      "Processing batch 1742/11884 - Loss: 32.0198\n",
      "Processing batch 1743/11884 - Loss: 31.1531\n",
      "Processing batch 1744/11884 - Loss: 30.1569\n",
      "Processing batch 1745/11884 - Loss: 29.7576\n",
      "Processing batch 1746/11884 - Loss: 30.5848\n",
      "Processing batch 1747/11884 - Loss: 31.1608\n",
      "Processing batch 1748/11884 - Loss: 30.9681\n",
      "Processing batch 1749/11884 - Loss: 30.2911\n",
      "Processing batch 1750/11884 - Loss: 30.5160\n",
      "Processing batch 1751/11884 - Loss: 30.9076\n",
      "Processing batch 1752/11884 - Loss: 30.5090\n",
      "Processing batch 1753/11884 - Loss: 30.1013\n",
      "Processing batch 1754/11884 - Loss: 29.1109\n",
      "Processing batch 1755/11884 - Loss: 32.5562\n",
      "Processing batch 1756/11884 - Loss: 29.6257\n",
      "Processing batch 1757/11884 - Loss: 30.4739\n",
      "Processing batch 1758/11884 - Loss: 31.1774\n",
      "Processing batch 1759/11884 - Loss: 30.2275\n",
      "Processing batch 1760/11884 - Loss: 29.7533\n",
      "Processing batch 1761/11884 - Loss: 29.8283\n",
      "Processing batch 1762/11884 - Loss: 30.8455\n",
      "Processing batch 1763/11884 - Loss: 30.7728\n",
      "Processing batch 1764/11884 - Loss: 30.9703\n",
      "Processing batch 1765/11884 - Loss: 30.8691\n",
      "Processing batch 1766/11884 - Loss: 31.6411\n",
      "Processing batch 1767/11884 - Loss: 30.5858\n",
      "Processing batch 1768/11884 - Loss: 29.6108\n",
      "Processing batch 1769/11884 - Loss: 30.5832\n",
      "Processing batch 1770/11884 - Loss: 30.0959\n",
      "Processing batch 1771/11884 - Loss: 31.3843\n",
      "Processing batch 1772/11884 - Loss: 31.7617\n",
      "Processing batch 1773/11884 - Loss: 31.6120\n",
      "Processing batch 1774/11884 - Loss: 30.7834\n",
      "Processing batch 1775/11884 - Loss: 29.6412\n",
      "Processing batch 1776/11884 - Loss: 30.4520\n",
      "Processing batch 1777/11884 - Loss: 29.6169\n",
      "Processing batch 1778/11884 - Loss: 30.6405\n",
      "Processing batch 1779/11884 - Loss: 31.1417\n",
      "Processing batch 1780/11884 - Loss: 30.0809\n",
      "Processing batch 1781/11884 - Loss: 29.8533\n",
      "Processing batch 1782/11884 - Loss: 30.6186\n",
      "Processing batch 1783/11884 - Loss: 30.1590\n",
      "Processing batch 1784/11884 - Loss: 30.1083\n",
      "Processing batch 1785/11884 - Loss: 30.7441\n",
      "Processing batch 1786/11884 - Loss: 29.6813\n",
      "Processing batch 1787/11884 - Loss: 31.6909\n",
      "Processing batch 1788/11884 - Loss: 29.7525\n",
      "Processing batch 1789/11884 - Loss: 30.9561\n",
      "Processing batch 1790/11884 - Loss: 30.8490\n",
      "Processing batch 1791/11884 - Loss: 30.2179\n",
      "Processing batch 1792/11884 - Loss: 30.6804\n",
      "Processing batch 1793/11884 - Loss: 31.4193\n",
      "Processing batch 1794/11884 - Loss: 32.0811\n",
      "Processing batch 1795/11884 - Loss: 29.5169\n",
      "Processing batch 1796/11884 - Loss: 29.2195\n",
      "Processing batch 1797/11884 - Loss: 30.1685\n",
      "Processing batch 1798/11884 - Loss: 32.1724\n",
      "Processing batch 1799/11884 - Loss: 30.7226\n",
      "Processing batch 1800/11884 - Loss: 30.8997\n",
      "Processing batch 1801/11884 - Loss: 30.6628\n",
      "Processing batch 1802/11884 - Loss: 30.9416\n",
      "Processing batch 1803/11884 - Loss: 30.4683\n",
      "Processing batch 1804/11884 - Loss: 30.8326\n",
      "Processing batch 1805/11884 - Loss: 30.7467\n",
      "Processing batch 1806/11884 - Loss: 30.3228\n",
      "Processing batch 1807/11884 - Loss: 30.5006\n",
      "Processing batch 1808/11884 - Loss: 31.3964\n",
      "Processing batch 1809/11884 - Loss: 29.9055\n",
      "Processing batch 1810/11884 - Loss: 29.0417\n",
      "Processing batch 1811/11884 - Loss: 31.5143\n",
      "Processing batch 1812/11884 - Loss: 29.5502\n",
      "Processing batch 1813/11884 - Loss: 29.5755\n",
      "Processing batch 1814/11884 - Loss: 31.2897\n",
      "Processing batch 1815/11884 - Loss: 29.9802\n",
      "Processing batch 1816/11884 - Loss: 29.8783\n",
      "Processing batch 1817/11884 - Loss: 30.7487\n",
      "Processing batch 1818/11884 - Loss: 29.1409\n",
      "Processing batch 1819/11884 - Loss: 29.8969\n",
      "Processing batch 1820/11884 - Loss: 30.7669\n",
      "Processing batch 1821/11884 - Loss: 30.4996\n",
      "Processing batch 1822/11884 - Loss: 29.3044\n",
      "Processing batch 1823/11884 - Loss: 29.9889\n",
      "Processing batch 1824/11884 - Loss: 30.4178\n",
      "Processing batch 1825/11884 - Loss: 30.0812\n",
      "Processing batch 1826/11884 - Loss: 30.2602\n",
      "Processing batch 1827/11884 - Loss: 30.5673\n",
      "Processing batch 1828/11884 - Loss: 30.4356\n",
      "Processing batch 1829/11884 - Loss: 30.8637\n",
      "Processing batch 1830/11884 - Loss: 31.1825\n",
      "Processing batch 1831/11884 - Loss: 31.8470\n",
      "Processing batch 1832/11884 - Loss: 30.5243\n",
      "Processing batch 1833/11884 - Loss: 30.8052\n",
      "Processing batch 1834/11884 - Loss: 31.0791\n",
      "Processing batch 1835/11884 - Loss: 29.6274\n",
      "Processing batch 1836/11884 - Loss: 30.1434\n",
      "Processing batch 1837/11884 - Loss: 30.4424\n",
      "Processing batch 1838/11884 - Loss: 31.9466\n",
      "Processing batch 1839/11884 - Loss: 30.6200\n",
      "Processing batch 1840/11884 - Loss: 30.3226\n",
      "Processing batch 1841/11884 - Loss: 30.4490\n",
      "Processing batch 1842/11884 - Loss: 30.1186\n",
      "Processing batch 1843/11884 - Loss: 30.5827\n",
      "Processing batch 1844/11884 - Loss: 30.0145\n",
      "Processing batch 1845/11884 - Loss: 30.7507\n",
      "Processing batch 1846/11884 - Loss: 30.4755\n",
      "Processing batch 1847/11884 - Loss: 31.8505\n",
      "Processing batch 1848/11884 - Loss: 29.6476\n",
      "Processing batch 1849/11884 - Loss: 31.5215\n",
      "Processing batch 1850/11884 - Loss: 28.9089\n",
      "Processing batch 1851/11884 - Loss: 29.8279\n",
      "Processing batch 1852/11884 - Loss: 31.3720\n",
      "Processing batch 1853/11884 - Loss: 28.6829\n",
      "Processing batch 1854/11884 - Loss: 30.6486\n",
      "Processing batch 1855/11884 - Loss: 29.9276\n",
      "Processing batch 1856/11884 - Loss: 31.3895\n",
      "Processing batch 1857/11884 - Loss: 30.4428\n",
      "Processing batch 1858/11884 - Loss: 28.8138\n",
      "Processing batch 1859/11884 - Loss: 31.4228\n",
      "Processing batch 1860/11884 - Loss: 32.1857\n",
      "Processing batch 1861/11884 - Loss: 29.0969\n",
      "Processing batch 1862/11884 - Loss: 29.5443\n",
      "Processing batch 1863/11884 - Loss: 30.8816\n",
      "Processing batch 1864/11884 - Loss: 30.9696\n",
      "Processing batch 1865/11884 - Loss: 32.7079\n",
      "Processing batch 1866/11884 - Loss: 30.1972\n",
      "Processing batch 1867/11884 - Loss: 31.1001\n",
      "Processing batch 1868/11884 - Loss: 30.4290\n",
      "Processing batch 1869/11884 - Loss: 29.5827\n",
      "Processing batch 1870/11884 - Loss: 29.8482\n",
      "Processing batch 1871/11884 - Loss: 30.0374\n",
      "Processing batch 1872/11884 - Loss: 31.3674\n",
      "Processing batch 1873/11884 - Loss: 30.4839\n",
      "Processing batch 1874/11884 - Loss: 31.4694\n",
      "Processing batch 1875/11884 - Loss: 30.8295\n",
      "Processing batch 1876/11884 - Loss: 30.3011\n",
      "Processing batch 1877/11884 - Loss: 31.1936\n",
      "Processing batch 1878/11884 - Loss: 31.8347\n",
      "Processing batch 1879/11884 - Loss: 28.9382\n",
      "Processing batch 1880/11884 - Loss: 30.0922\n",
      "Processing batch 1881/11884 - Loss: 30.5230\n",
      "Processing batch 1882/11884 - Loss: 30.0945\n",
      "Processing batch 1883/11884 - Loss: 28.8926\n",
      "Processing batch 1884/11884 - Loss: 31.3991\n",
      "Processing batch 1885/11884 - Loss: 29.8104\n",
      "Processing batch 1886/11884 - Loss: 29.0688\n",
      "Processing batch 1887/11884 - Loss: 31.3556\n",
      "Processing batch 1888/11884 - Loss: 29.2569\n",
      "Processing batch 1889/11884 - Loss: 30.8288\n",
      "Processing batch 1890/11884 - Loss: 31.5782\n",
      "Processing batch 1891/11884 - Loss: 30.0377\n",
      "Processing batch 1892/11884 - Loss: 31.6632\n",
      "Processing batch 1893/11884 - Loss: 30.4161\n",
      "Processing batch 1894/11884 - Loss: 30.3284\n",
      "Processing batch 1895/11884 - Loss: 29.5628\n",
      "Processing batch 1896/11884 - Loss: 29.7544\n",
      "Processing batch 1897/11884 - Loss: 30.8113\n",
      "Processing batch 1898/11884 - Loss: 30.9885\n",
      "Processing batch 1899/11884 - Loss: 29.8070\n",
      "Processing batch 1900/11884 - Loss: 30.6782\n",
      "Processing batch 1901/11884 - Loss: 30.6391\n",
      "Processing batch 1902/11884 - Loss: 30.2740\n",
      "Processing batch 1903/11884 - Loss: 30.8771\n",
      "Processing batch 1904/11884 - Loss: 30.1865\n",
      "Processing batch 1905/11884 - Loss: 31.6556\n",
      "Processing batch 1906/11884 - Loss: 30.1971\n",
      "Processing batch 1907/11884 - Loss: 30.3736\n",
      "Processing batch 1908/11884 - Loss: 30.6348\n",
      "Processing batch 1909/11884 - Loss: 31.0895\n",
      "Processing batch 1910/11884 - Loss: 30.0966\n",
      "Processing batch 1911/11884 - Loss: 29.9583\n",
      "Processing batch 1912/11884 - Loss: 31.5588\n",
      "Processing batch 1913/11884 - Loss: 31.3297\n",
      "Processing batch 1914/11884 - Loss: 31.4194\n",
      "Processing batch 1915/11884 - Loss: 31.1422\n",
      "Processing batch 1916/11884 - Loss: 31.5629\n",
      "Processing batch 1917/11884 - Loss: 30.3913\n",
      "Processing batch 1918/11884 - Loss: 30.3184\n",
      "Processing batch 1919/11884 - Loss: 29.8485\n",
      "Processing batch 1920/11884 - Loss: 30.0043\n",
      "Processing batch 1921/11884 - Loss: 29.6705\n",
      "Processing batch 1922/11884 - Loss: 30.8284\n",
      "Processing batch 1923/11884 - Loss: 30.2110\n",
      "Processing batch 1924/11884 - Loss: 29.6340\n",
      "Processing batch 1925/11884 - Loss: 29.8408\n",
      "Processing batch 1926/11884 - Loss: 31.3225\n",
      "Processing batch 1927/11884 - Loss: 31.4697\n",
      "Processing batch 1928/11884 - Loss: 29.9313\n",
      "Processing batch 1929/11884 - Loss: 30.9374\n",
      "Processing batch 1930/11884 - Loss: 31.0310\n",
      "Processing batch 1931/11884 - Loss: 30.9111\n",
      "Processing batch 1932/11884 - Loss: 29.3703\n",
      "Processing batch 1933/11884 - Loss: 31.3280\n",
      "Processing batch 1934/11884 - Loss: 29.0416\n",
      "Processing batch 1935/11884 - Loss: 31.9095\n",
      "Processing batch 1936/11884 - Loss: 30.3006\n",
      "Processing batch 1937/11884 - Loss: 30.1371\n",
      "Processing batch 1938/11884 - Loss: 30.4934\n",
      "Processing batch 1939/11884 - Loss: 29.6738\n",
      "Processing batch 1940/11884 - Loss: 31.8464\n",
      "Processing batch 1941/11884 - Loss: 30.6166\n",
      "Processing batch 1942/11884 - Loss: 30.6779\n",
      "Processing batch 1943/11884 - Loss: 31.2285\n",
      "Processing batch 1944/11884 - Loss: 29.9403\n",
      "Processing batch 1945/11884 - Loss: 29.2547\n",
      "Processing batch 1946/11884 - Loss: 30.0808\n",
      "Processing batch 1947/11884 - Loss: 29.8828\n",
      "Processing batch 1948/11884 - Loss: 29.8209\n",
      "Processing batch 1949/11884 - Loss: 30.4037\n",
      "Processing batch 1950/11884 - Loss: 31.9605\n",
      "Processing batch 1951/11884 - Loss: 30.3659\n",
      "Processing batch 1952/11884 - Loss: 29.6795\n",
      "Processing batch 1953/11884 - Loss: 31.4038\n",
      "Processing batch 1954/11884 - Loss: 31.2148\n",
      "Processing batch 1955/11884 - Loss: 31.0335\n",
      "Processing batch 1956/11884 - Loss: 29.8326\n",
      "Processing batch 1957/11884 - Loss: 30.1038\n",
      "Processing batch 1958/11884 - Loss: 30.0962\n",
      "Processing batch 1959/11884 - Loss: 31.7275\n",
      "Processing batch 1960/11884 - Loss: 30.9373\n",
      "Processing batch 1961/11884 - Loss: 30.8142\n",
      "Processing batch 1962/11884 - Loss: 31.1783\n",
      "Processing batch 1963/11884 - Loss: 31.5782\n",
      "Processing batch 1964/11884 - Loss: 29.7821\n",
      "Processing batch 1965/11884 - Loss: 29.8784\n",
      "Processing batch 1966/11884 - Loss: 31.7996\n",
      "Processing batch 1967/11884 - Loss: 31.1217\n",
      "Processing batch 1968/11884 - Loss: 28.7772\n",
      "Processing batch 1969/11884 - Loss: 30.1820\n",
      "Processing batch 1970/11884 - Loss: 32.2210\n",
      "Processing batch 1971/11884 - Loss: 29.1617\n",
      "Processing batch 1972/11884 - Loss: 29.9048\n",
      "Processing batch 1973/11884 - Loss: 30.4609\n",
      "Processing batch 1974/11884 - Loss: 29.5933\n",
      "Processing batch 1975/11884 - Loss: 29.6924\n",
      "Processing batch 1976/11884 - Loss: 30.9689\n",
      "Processing batch 1977/11884 - Loss: 29.9583\n",
      "Processing batch 1978/11884 - Loss: 30.1093\n",
      "Processing batch 1979/11884 - Loss: 30.8538\n",
      "Processing batch 1980/11884 - Loss: 30.6980\n",
      "Processing batch 1981/11884 - Loss: 30.8256\n",
      "Processing batch 1982/11884 - Loss: 30.2167\n",
      "Processing batch 1983/11884 - Loss: 31.4041\n",
      "Processing batch 1984/11884 - Loss: 30.4131\n",
      "Processing batch 1985/11884 - Loss: 29.4296\n",
      "Processing batch 1986/11884 - Loss: 31.8185\n",
      "Processing batch 1987/11884 - Loss: 30.1433\n",
      "Processing batch 1988/11884 - Loss: 31.1702\n",
      "Processing batch 1989/11884 - Loss: 31.7244\n",
      "Processing batch 1990/11884 - Loss: 32.3473\n",
      "Processing batch 1991/11884 - Loss: 30.8370\n",
      "Processing batch 1992/11884 - Loss: 31.2951\n",
      "Processing batch 1993/11884 - Loss: 30.6560\n",
      "Processing batch 1994/11884 - Loss: 29.5644\n",
      "Processing batch 1995/11884 - Loss: 31.3201\n",
      "Processing batch 1996/11884 - Loss: 29.7716\n",
      "Processing batch 1997/11884 - Loss: 31.0998\n",
      "Processing batch 1998/11884 - Loss: 31.0643\n",
      "Processing batch 1999/11884 - Loss: 32.2989\n",
      "Processing batch 2000/11884 - Loss: 29.8551\n",
      "Processing batch 2001/11884 - Loss: 29.5110\n",
      "Processing batch 2002/11884 - Loss: 30.4694\n",
      "Processing batch 2003/11884 - Loss: 31.4509\n",
      "Processing batch 2004/11884 - Loss: 30.7705\n",
      "Processing batch 2005/11884 - Loss: 31.4249\n",
      "Processing batch 2006/11884 - Loss: 31.5819\n",
      "Processing batch 2007/11884 - Loss: 31.0606\n",
      "Processing batch 2008/11884 - Loss: 29.5256\n",
      "Processing batch 2009/11884 - Loss: 29.7311\n",
      "Processing batch 2010/11884 - Loss: 29.6894\n",
      "Processing batch 2011/11884 - Loss: 29.7835\n",
      "Processing batch 2012/11884 - Loss: 31.5172\n",
      "Processing batch 2013/11884 - Loss: 30.1923\n",
      "Processing batch 2014/11884 - Loss: 30.3240\n",
      "Processing batch 2015/11884 - Loss: 30.0568\n",
      "Processing batch 2016/11884 - Loss: 31.0939\n",
      "Processing batch 2017/11884 - Loss: 29.3989\n",
      "Processing batch 2018/11884 - Loss: 30.6418\n",
      "Processing batch 2019/11884 - Loss: 29.4966\n",
      "Processing batch 2020/11884 - Loss: 29.6092\n",
      "Processing batch 2021/11884 - Loss: 30.4480\n",
      "Processing batch 2022/11884 - Loss: 30.3660\n",
      "Processing batch 2023/11884 - Loss: 31.8387\n",
      "Processing batch 2024/11884 - Loss: 30.1907\n",
      "Processing batch 2025/11884 - Loss: 30.7116\n",
      "Processing batch 2026/11884 - Loss: 30.1885\n",
      "Processing batch 2027/11884 - Loss: 31.1242\n",
      "Processing batch 2028/11884 - Loss: 30.7560\n",
      "Processing batch 2029/11884 - Loss: 30.9131\n",
      "Processing batch 2030/11884 - Loss: 31.1041\n",
      "Processing batch 2031/11884 - Loss: 29.8864\n",
      "Processing batch 2032/11884 - Loss: 30.1190\n",
      "Processing batch 2033/11884 - Loss: 29.2353\n",
      "Processing batch 2034/11884 - Loss: 31.1726\n",
      "Processing batch 2035/11884 - Loss: 31.8819\n",
      "Processing batch 2036/11884 - Loss: 28.3833\n",
      "Processing batch 2037/11884 - Loss: 31.0472\n",
      "Processing batch 2038/11884 - Loss: 30.6683\n",
      "Processing batch 2039/11884 - Loss: 31.3527\n",
      "Processing batch 2040/11884 - Loss: 29.7888\n",
      "Processing batch 2041/11884 - Loss: 31.6847\n",
      "Processing batch 2042/11884 - Loss: 30.5908\n",
      "Processing batch 2043/11884 - Loss: 30.6383\n",
      "Processing batch 2044/11884 - Loss: 31.0779\n",
      "Processing batch 2045/11884 - Loss: 30.4859\n",
      "Processing batch 2046/11884 - Loss: 30.0383\n",
      "Processing batch 2047/11884 - Loss: 30.3059\n",
      "Processing batch 2048/11884 - Loss: 30.0564\n",
      "Processing batch 2049/11884 - Loss: 31.2518\n",
      "Processing batch 2050/11884 - Loss: 30.7141\n",
      "Processing batch 2051/11884 - Loss: 30.5526\n",
      "Processing batch 2052/11884 - Loss: 29.4889\n",
      "Processing batch 2053/11884 - Loss: 31.1305\n",
      "Processing batch 2054/11884 - Loss: 29.0165\n",
      "Processing batch 2055/11884 - Loss: 30.2551\n",
      "Processing batch 2056/11884 - Loss: 29.3011\n",
      "Processing batch 2057/11884 - Loss: 31.8958\n",
      "Processing batch 2058/11884 - Loss: 31.0622\n",
      "Processing batch 2059/11884 - Loss: 29.2832\n",
      "Processing batch 2060/11884 - Loss: 31.2568\n",
      "Processing batch 2061/11884 - Loss: 29.8839\n",
      "Processing batch 2062/11884 - Loss: 32.4138\n",
      "Processing batch 2063/11884 - Loss: 30.9232\n",
      "Processing batch 2064/11884 - Loss: 30.7690\n",
      "Processing batch 2065/11884 - Loss: 31.5413\n",
      "Processing batch 2066/11884 - Loss: 30.1187\n",
      "Processing batch 2067/11884 - Loss: 29.3233\n",
      "Processing batch 2068/11884 - Loss: 32.0169\n",
      "Processing batch 2069/11884 - Loss: 30.6779\n",
      "Processing batch 2070/11884 - Loss: 29.5645\n",
      "Processing batch 2071/11884 - Loss: 30.7471\n",
      "Processing batch 2072/11884 - Loss: 30.4255\n",
      "Processing batch 2073/11884 - Loss: 30.4373\n",
      "Processing batch 2074/11884 - Loss: 31.4953\n",
      "Processing batch 2075/11884 - Loss: 30.7339\n",
      "Processing batch 2076/11884 - Loss: 30.8314\n",
      "Processing batch 2077/11884 - Loss: 30.0921\n",
      "Processing batch 2078/11884 - Loss: 29.4566\n",
      "Processing batch 2079/11884 - Loss: 30.4337\n",
      "Processing batch 2080/11884 - Loss: 29.3200\n",
      "Processing batch 2081/11884 - Loss: 30.4294\n",
      "Processing batch 2082/11884 - Loss: 30.7702\n",
      "Processing batch 2083/11884 - Loss: 30.8013\n",
      "Processing batch 2084/11884 - Loss: 30.9244\n",
      "Processing batch 2085/11884 - Loss: 31.7708\n",
      "Processing batch 2086/11884 - Loss: 30.6748\n",
      "Processing batch 2087/11884 - Loss: 31.6136\n",
      "Processing batch 2088/11884 - Loss: 31.6184\n",
      "Processing batch 2089/11884 - Loss: 30.2028\n",
      "Processing batch 2090/11884 - Loss: 30.7534\n",
      "Processing batch 2091/11884 - Loss: 29.6489\n",
      "Processing batch 2092/11884 - Loss: 30.3071\n",
      "Processing batch 2093/11884 - Loss: 31.2316\n",
      "Processing batch 2094/11884 - Loss: 30.2729\n",
      "Processing batch 2095/11884 - Loss: 29.8181\n",
      "Processing batch 2096/11884 - Loss: 31.7513\n",
      "Processing batch 2097/11884 - Loss: 31.7768\n",
      "Processing batch 2098/11884 - Loss: 30.5587\n",
      "Processing batch 2099/11884 - Loss: 29.3107\n",
      "Processing batch 2100/11884 - Loss: 29.4564\n",
      "Processing batch 2101/11884 - Loss: 31.1443\n",
      "Processing batch 2102/11884 - Loss: 30.5626\n",
      "Processing batch 2103/11884 - Loss: 29.0659\n",
      "Processing batch 2104/11884 - Loss: 30.6470\n",
      "Processing batch 2105/11884 - Loss: 31.7550\n",
      "Processing batch 2106/11884 - Loss: 31.8882\n",
      "Processing batch 2107/11884 - Loss: 30.8908\n",
      "Processing batch 2108/11884 - Loss: 30.8650\n",
      "Processing batch 2109/11884 - Loss: 30.3005\n",
      "Processing batch 2110/11884 - Loss: 31.5019\n",
      "Processing batch 2111/11884 - Loss: 29.6359\n",
      "Processing batch 2112/11884 - Loss: 30.9644\n",
      "Processing batch 2113/11884 - Loss: 31.1691\n",
      "Processing batch 2114/11884 - Loss: 30.6573\n",
      "Processing batch 2115/11884 - Loss: 30.5576\n",
      "Processing batch 2116/11884 - Loss: 29.9530\n",
      "Processing batch 2117/11884 - Loss: 30.3980\n",
      "Processing batch 2118/11884 - Loss: 30.1659\n",
      "Processing batch 2119/11884 - Loss: 30.8675\n",
      "Processing batch 2120/11884 - Loss: 30.6056\n",
      "Processing batch 2121/11884 - Loss: 31.4127\n",
      "Processing batch 2122/11884 - Loss: 31.3375\n",
      "Processing batch 2123/11884 - Loss: 31.1698\n",
      "Processing batch 2124/11884 - Loss: 32.1009\n",
      "Processing batch 2125/11884 - Loss: 31.1835\n",
      "Processing batch 2126/11884 - Loss: 31.3597\n",
      "Processing batch 2127/11884 - Loss: 30.2288\n",
      "Processing batch 2128/11884 - Loss: 30.2882\n",
      "Processing batch 2129/11884 - Loss: 30.2373\n",
      "Processing batch 2130/11884 - Loss: 30.8930\n",
      "Processing batch 2131/11884 - Loss: 30.0188\n",
      "Processing batch 2132/11884 - Loss: 31.2681\n",
      "Processing batch 2133/11884 - Loss: 29.8747\n",
      "Processing batch 2134/11884 - Loss: 30.2044\n",
      "Processing batch 2135/11884 - Loss: 31.2493\n",
      "Processing batch 2136/11884 - Loss: 29.4914\n",
      "Processing batch 2137/11884 - Loss: 31.6079\n",
      "Processing batch 2138/11884 - Loss: 31.5735\n",
      "Processing batch 2139/11884 - Loss: 30.9140\n",
      "Processing batch 2140/11884 - Loss: 31.1225\n",
      "Processing batch 2141/11884 - Loss: 30.8502\n",
      "Processing batch 2142/11884 - Loss: 30.9087\n",
      "Processing batch 2143/11884 - Loss: 30.9806\n",
      "Processing batch 2144/11884 - Loss: 29.8497\n",
      "Processing batch 2145/11884 - Loss: 30.7989\n",
      "Processing batch 2146/11884 - Loss: 30.0494\n",
      "Processing batch 2147/11884 - Loss: 31.8351\n",
      "Processing batch 2148/11884 - Loss: 31.3020\n",
      "Processing batch 2149/11884 - Loss: 30.3796\n",
      "Processing batch 2150/11884 - Loss: 30.3117\n",
      "Processing batch 2151/11884 - Loss: 30.7239\n",
      "Processing batch 2152/11884 - Loss: 29.3360\n",
      "Processing batch 2153/11884 - Loss: 29.4135\n",
      "Processing batch 2154/11884 - Loss: 30.4603\n",
      "Processing batch 2155/11884 - Loss: 30.5627\n",
      "Processing batch 2156/11884 - Loss: 30.5118\n",
      "Processing batch 2157/11884 - Loss: 30.4794\n",
      "Processing batch 2158/11884 - Loss: 29.1170\n",
      "Processing batch 2159/11884 - Loss: 30.7923\n",
      "Processing batch 2160/11884 - Loss: 30.5472\n",
      "Processing batch 2161/11884 - Loss: 30.6730\n",
      "Processing batch 2162/11884 - Loss: 30.4476\n",
      "Processing batch 2163/11884 - Loss: 31.8073\n",
      "Processing batch 2164/11884 - Loss: 28.5352\n",
      "Processing batch 2165/11884 - Loss: 29.9042\n",
      "Processing batch 2166/11884 - Loss: 30.4796\n",
      "Processing batch 2167/11884 - Loss: 31.5763\n",
      "Processing batch 2168/11884 - Loss: 30.1675\n",
      "Processing batch 2169/11884 - Loss: 31.0095\n",
      "Processing batch 2170/11884 - Loss: 31.4952\n",
      "Processing batch 2171/11884 - Loss: 31.5112\n",
      "Processing batch 2172/11884 - Loss: 31.0415\n",
      "Processing batch 2173/11884 - Loss: 30.9845\n",
      "Processing batch 2174/11884 - Loss: 30.9277\n",
      "Processing batch 2175/11884 - Loss: 31.2167\n",
      "Processing batch 2176/11884 - Loss: 31.2811\n",
      "Processing batch 2177/11884 - Loss: 31.8122\n",
      "Processing batch 2178/11884 - Loss: 29.6034\n",
      "Processing batch 2179/11884 - Loss: 30.4787\n",
      "Processing batch 2180/11884 - Loss: 29.6174\n",
      "Processing batch 2181/11884 - Loss: 29.2670\n",
      "Processing batch 2182/11884 - Loss: 30.2982\n",
      "Processing batch 2183/11884 - Loss: 30.0007\n",
      "Processing batch 2184/11884 - Loss: 30.1261\n",
      "Processing batch 2185/11884 - Loss: 30.2136\n",
      "Processing batch 2186/11884 - Loss: 29.7612\n",
      "Processing batch 2187/11884 - Loss: 31.0674\n",
      "Processing batch 2188/11884 - Loss: 31.2072\n",
      "Processing batch 2189/11884 - Loss: 31.1733\n",
      "Processing batch 2190/11884 - Loss: 31.3413\n",
      "Processing batch 2191/11884 - Loss: 31.7838\n",
      "Processing batch 2192/11884 - Loss: 31.1158\n",
      "Processing batch 2193/11884 - Loss: 29.8805\n",
      "Processing batch 2194/11884 - Loss: 30.3581\n",
      "Processing batch 2195/11884 - Loss: 31.1799\n",
      "Processing batch 2196/11884 - Loss: 31.5763\n",
      "Processing batch 2197/11884 - Loss: 29.7966\n",
      "Processing batch 2198/11884 - Loss: 30.0314\n",
      "Processing batch 2199/11884 - Loss: 31.5812\n",
      "Processing batch 2200/11884 - Loss: 30.9476\n",
      "Processing batch 2201/11884 - Loss: 29.8419\n",
      "Processing batch 2202/11884 - Loss: 31.3024\n",
      "Processing batch 2203/11884 - Loss: 29.3228\n",
      "Processing batch 2204/11884 - Loss: 31.0709\n",
      "Processing batch 2205/11884 - Loss: 31.3451\n",
      "Processing batch 2206/11884 - Loss: 31.0440\n",
      "Processing batch 2207/11884 - Loss: 32.6137\n",
      "Processing batch 2208/11884 - Loss: 29.3230\n",
      "Processing batch 2209/11884 - Loss: 29.2275\n",
      "Processing batch 2210/11884 - Loss: 31.3324\n",
      "Processing batch 2211/11884 - Loss: 30.5504\n",
      "Processing batch 2212/11884 - Loss: 29.9309\n",
      "Processing batch 2213/11884 - Loss: 30.5716\n",
      "Processing batch 2214/11884 - Loss: 30.9819\n",
      "Processing batch 2215/11884 - Loss: 30.2234\n",
      "Processing batch 2216/11884 - Loss: 28.8962\n",
      "Processing batch 2217/11884 - Loss: 29.8682\n",
      "Processing batch 2218/11884 - Loss: 29.7571\n",
      "Processing batch 2219/11884 - Loss: 29.6300\n",
      "Processing batch 2220/11884 - Loss: 32.2321\n",
      "Processing batch 2221/11884 - Loss: 30.1679\n",
      "Processing batch 2222/11884 - Loss: 29.8099\n",
      "Processing batch 2223/11884 - Loss: 30.6770\n",
      "Processing batch 2224/11884 - Loss: 30.9089\n",
      "Processing batch 2225/11884 - Loss: 30.4520\n",
      "Processing batch 2226/11884 - Loss: 30.0753\n",
      "Processing batch 2227/11884 - Loss: 30.3991\n",
      "Processing batch 2228/11884 - Loss: 30.6415\n",
      "Processing batch 2229/11884 - Loss: 29.5852\n",
      "Processing batch 2230/11884 - Loss: 30.4279\n",
      "Processing batch 2231/11884 - Loss: 32.4521\n",
      "Processing batch 2232/11884 - Loss: 31.2935\n",
      "Processing batch 2233/11884 - Loss: 30.3630\n",
      "Processing batch 2234/11884 - Loss: 31.5740\n",
      "Processing batch 2235/11884 - Loss: 30.1562\n",
      "Processing batch 2236/11884 - Loss: 29.7513\n",
      "Processing batch 2237/11884 - Loss: 30.9571\n",
      "Processing batch 2238/11884 - Loss: 31.5906\n",
      "Processing batch 2239/11884 - Loss: 29.8342\n",
      "Processing batch 2240/11884 - Loss: 29.8053\n",
      "Processing batch 2241/11884 - Loss: 31.9611\n",
      "Processing batch 2242/11884 - Loss: 29.7630\n",
      "Processing batch 2243/11884 - Loss: 31.7205\n",
      "Processing batch 2244/11884 - Loss: 31.2780\n",
      "Processing batch 2245/11884 - Loss: 30.3639\n",
      "Processing batch 2246/11884 - Loss: 31.8805\n",
      "Processing batch 2247/11884 - Loss: 29.9727\n",
      "Processing batch 2248/11884 - Loss: 31.2410\n",
      "Processing batch 2249/11884 - Loss: 31.8223\n",
      "Processing batch 2250/11884 - Loss: 30.6809\n",
      "Processing batch 2251/11884 - Loss: 31.0086\n",
      "Processing batch 2252/11884 - Loss: 31.6692\n",
      "Processing batch 2253/11884 - Loss: 30.7233\n",
      "Processing batch 2254/11884 - Loss: 31.7790\n",
      "Processing batch 2255/11884 - Loss: 30.6456\n",
      "Processing batch 2256/11884 - Loss: 30.0460\n",
      "Processing batch 2257/11884 - Loss: 28.8315\n",
      "Processing batch 2258/11884 - Loss: 30.9678\n",
      "Processing batch 2259/11884 - Loss: 31.0842\n",
      "Processing batch 2260/11884 - Loss: 30.0599\n",
      "Processing batch 2261/11884 - Loss: 30.7179\n",
      "Processing batch 2262/11884 - Loss: 31.2333\n",
      "Processing batch 2263/11884 - Loss: 30.4640\n",
      "Processing batch 2264/11884 - Loss: 31.4078\n",
      "Processing batch 2265/11884 - Loss: 31.3598\n",
      "Processing batch 2266/11884 - Loss: 30.1834\n",
      "Processing batch 2267/11884 - Loss: 31.3974\n",
      "Processing batch 2268/11884 - Loss: 31.0143\n",
      "Processing batch 2269/11884 - Loss: 29.8874\n",
      "Processing batch 2270/11884 - Loss: 31.1714\n",
      "Processing batch 2271/11884 - Loss: 32.0340\n",
      "Processing batch 2272/11884 - Loss: 31.2608\n",
      "Processing batch 2273/11884 - Loss: 31.6673\n",
      "Processing batch 2274/11884 - Loss: 30.8087\n",
      "Processing batch 2275/11884 - Loss: 30.2183\n",
      "Processing batch 2276/11884 - Loss: 31.6445\n",
      "Processing batch 2277/11884 - Loss: 30.4973\n",
      "Processing batch 2278/11884 - Loss: 29.4828\n",
      "Processing batch 2279/11884 - Loss: 30.4277\n",
      "Processing batch 2280/11884 - Loss: 30.5462\n",
      "Processing batch 2281/11884 - Loss: 30.3682\n",
      "Processing batch 2282/11884 - Loss: 31.9399\n",
      "Processing batch 2283/11884 - Loss: 28.6761\n",
      "Processing batch 2284/11884 - Loss: 30.3898\n",
      "Processing batch 2285/11884 - Loss: 30.0674\n",
      "Processing batch 2286/11884 - Loss: 30.5030\n",
      "Processing batch 2287/11884 - Loss: 31.1866\n",
      "Processing batch 2288/11884 - Loss: 31.9278\n",
      "Processing batch 2289/11884 - Loss: 30.0734\n",
      "Processing batch 2290/11884 - Loss: 31.0870\n",
      "Processing batch 2291/11884 - Loss: 29.4622\n",
      "Processing batch 2292/11884 - Loss: 30.8256\n",
      "Processing batch 2293/11884 - Loss: 30.0366\n",
      "Processing batch 2294/11884 - Loss: 29.6630\n",
      "Processing batch 2295/11884 - Loss: 32.0046\n",
      "Processing batch 2296/11884 - Loss: 31.8046\n",
      "Processing batch 2297/11884 - Loss: 31.4703\n",
      "Processing batch 2298/11884 - Loss: 30.8076\n",
      "Processing batch 2299/11884 - Loss: 30.4412\n",
      "Processing batch 2300/11884 - Loss: 31.1952\n",
      "Processing batch 2301/11884 - Loss: 30.7512\n",
      "Processing batch 2302/11884 - Loss: 29.9632\n",
      "Processing batch 2303/11884 - Loss: 31.4990\n",
      "Processing batch 2304/11884 - Loss: 30.4760\n",
      "Processing batch 2305/11884 - Loss: 30.6291\n",
      "Processing batch 2306/11884 - Loss: 29.9966\n",
      "Processing batch 2307/11884 - Loss: 29.4707\n",
      "Processing batch 2308/11884 - Loss: 30.7383\n",
      "Processing batch 2309/11884 - Loss: 31.2350\n",
      "Processing batch 2310/11884 - Loss: 29.4415\n",
      "Processing batch 2311/11884 - Loss: 29.0577\n",
      "Processing batch 2312/11884 - Loss: 28.9373\n",
      "Processing batch 2313/11884 - Loss: 30.9844\n",
      "Processing batch 2314/11884 - Loss: 31.0566\n",
      "Processing batch 2315/11884 - Loss: 29.5048\n",
      "Processing batch 2316/11884 - Loss: 32.0146\n",
      "Processing batch 2317/11884 - Loss: 30.8197\n",
      "Processing batch 2318/11884 - Loss: 31.7473\n",
      "Processing batch 2319/11884 - Loss: 31.0441\n",
      "Processing batch 2320/11884 - Loss: 30.2370\n",
      "Processing batch 2321/11884 - Loss: 29.8095\n",
      "Processing batch 2322/11884 - Loss: 29.7728\n",
      "Processing batch 2323/11884 - Loss: 31.1905\n",
      "Processing batch 2324/11884 - Loss: 30.9241\n",
      "Processing batch 2325/11884 - Loss: 31.8079\n",
      "Processing batch 2326/11884 - Loss: 30.6658\n",
      "Processing batch 2327/11884 - Loss: 30.6911\n",
      "Processing batch 2328/11884 - Loss: 30.7388\n",
      "Processing batch 2329/11884 - Loss: 31.0523\n",
      "Processing batch 2330/11884 - Loss: 30.4503\n",
      "Processing batch 2331/11884 - Loss: 30.7868\n",
      "Processing batch 2332/11884 - Loss: 31.8217\n",
      "Processing batch 2333/11884 - Loss: 30.6333\n",
      "Processing batch 2334/11884 - Loss: 29.5318\n",
      "Processing batch 2335/11884 - Loss: 29.8084\n",
      "Processing batch 2336/11884 - Loss: 30.9171\n",
      "Processing batch 2337/11884 - Loss: 30.7674\n",
      "Processing batch 2338/11884 - Loss: 30.0407\n",
      "Processing batch 2339/11884 - Loss: 30.6780\n",
      "Processing batch 2340/11884 - Loss: 29.3960\n",
      "Processing batch 2341/11884 - Loss: 30.2327\n",
      "Processing batch 2342/11884 - Loss: 30.7531\n",
      "Processing batch 2343/11884 - Loss: 29.4995\n",
      "Processing batch 2344/11884 - Loss: 30.4966\n",
      "Processing batch 2345/11884 - Loss: 30.7236\n",
      "Processing batch 2346/11884 - Loss: 30.4932\n",
      "Processing batch 2347/11884 - Loss: 32.0685\n",
      "Processing batch 2348/11884 - Loss: 30.4918\n",
      "Processing batch 2349/11884 - Loss: 31.9390\n",
      "Processing batch 2350/11884 - Loss: 30.8989\n",
      "Processing batch 2351/11884 - Loss: 30.8662\n",
      "Processing batch 2352/11884 - Loss: 30.8327\n",
      "Processing batch 2353/11884 - Loss: 31.0446\n",
      "Processing batch 2354/11884 - Loss: 31.6781\n",
      "Processing batch 2355/11884 - Loss: 30.1677\n",
      "Processing batch 2356/11884 - Loss: 31.4166\n",
      "Processing batch 2357/11884 - Loss: 32.2436\n",
      "Processing batch 2358/11884 - Loss: 28.9568\n",
      "Processing batch 2359/11884 - Loss: 30.9036\n",
      "Processing batch 2360/11884 - Loss: 31.0413\n",
      "Processing batch 2361/11884 - Loss: 30.2529\n",
      "Processing batch 2362/11884 - Loss: 32.6101\n",
      "Processing batch 2363/11884 - Loss: 30.2593\n",
      "Processing batch 2364/11884 - Loss: 30.1173\n",
      "Processing batch 2365/11884 - Loss: 30.3059\n",
      "Processing batch 2366/11884 - Loss: 31.2284\n",
      "Processing batch 2367/11884 - Loss: 30.4014\n",
      "Processing batch 2368/11884 - Loss: 30.1782\n",
      "Processing batch 2369/11884 - Loss: 30.6255\n",
      "Processing batch 2370/11884 - Loss: 30.5946\n",
      "Processing batch 2371/11884 - Loss: 30.3316\n",
      "Processing batch 2372/11884 - Loss: 29.8606\n",
      "Processing batch 2373/11884 - Loss: 30.8700\n",
      "Processing batch 2374/11884 - Loss: 29.5056\n",
      "Processing batch 2375/11884 - Loss: 29.0782\n",
      "Processing batch 2376/11884 - Loss: 30.5218\n",
      "Processing batch 2377/11884 - Loss: 30.4418\n",
      "Processing batch 2378/11884 - Loss: 31.0793\n",
      "Processing batch 2379/11884 - Loss: 31.3738\n",
      "Processing batch 2380/11884 - Loss: 30.0691\n",
      "Processing batch 2381/11884 - Loss: 31.0339\n",
      "Processing batch 2382/11884 - Loss: 30.5542\n",
      "Processing batch 2383/11884 - Loss: 29.2423\n",
      "Processing batch 2384/11884 - Loss: 29.6926\n",
      "Processing batch 2385/11884 - Loss: 31.6869\n",
      "Processing batch 2386/11884 - Loss: 30.9603\n",
      "Processing batch 2387/11884 - Loss: 29.7477\n",
      "Processing batch 2388/11884 - Loss: 31.2180\n",
      "Processing batch 2389/11884 - Loss: 30.5262\n",
      "Processing batch 2390/11884 - Loss: 30.6923\n",
      "Processing batch 2391/11884 - Loss: 30.5772\n",
      "Processing batch 2392/11884 - Loss: 30.0362\n",
      "Processing batch 2393/11884 - Loss: 31.0876\n",
      "Processing batch 2394/11884 - Loss: 30.9583\n",
      "Processing batch 2395/11884 - Loss: 31.3996\n",
      "Processing batch 2396/11884 - Loss: 30.6635\n",
      "Processing batch 2397/11884 - Loss: 31.6763\n",
      "Processing batch 2398/11884 - Loss: 31.5784\n",
      "Processing batch 2399/11884 - Loss: 29.8831\n",
      "Processing batch 2400/11884 - Loss: 31.4552\n",
      "Processing batch 2401/11884 - Loss: 30.8324\n",
      "Processing batch 2402/11884 - Loss: 30.1275\n",
      "Processing batch 2403/11884 - Loss: 30.9262\n",
      "Processing batch 2404/11884 - Loss: 29.1689\n",
      "Processing batch 2405/11884 - Loss: 30.8703\n",
      "Processing batch 2406/11884 - Loss: 31.4702\n",
      "Processing batch 2407/11884 - Loss: 30.1739\n",
      "Processing batch 2408/11884 - Loss: 29.8604\n",
      "Processing batch 2409/11884 - Loss: 30.2397\n",
      "Processing batch 2410/11884 - Loss: 31.1020\n",
      "Processing batch 2411/11884 - Loss: 31.9029\n",
      "Processing batch 2412/11884 - Loss: 30.7728\n",
      "Processing batch 2413/11884 - Loss: 30.6912\n",
      "Processing batch 2414/11884 - Loss: 30.6938\n",
      "Processing batch 2415/11884 - Loss: 30.2222\n",
      "Processing batch 2416/11884 - Loss: 31.2808\n",
      "Processing batch 2417/11884 - Loss: 31.3903\n",
      "Processing batch 2418/11884 - Loss: 29.8211\n",
      "Processing batch 2419/11884 - Loss: 29.5679\n",
      "Processing batch 2420/11884 - Loss: 31.6396\n",
      "Processing batch 2421/11884 - Loss: 29.4180\n",
      "Processing batch 2422/11884 - Loss: 30.9806\n",
      "Processing batch 2423/11884 - Loss: 30.6118\n",
      "Processing batch 2424/11884 - Loss: 29.4958\n",
      "Processing batch 2425/11884 - Loss: 30.3031\n",
      "Processing batch 2426/11884 - Loss: 31.8304\n",
      "Processing batch 2427/11884 - Loss: 30.8655\n",
      "Processing batch 2428/11884 - Loss: 29.8018\n",
      "Processing batch 2429/11884 - Loss: 30.1719\n",
      "Processing batch 2430/11884 - Loss: 29.2771\n",
      "Processing batch 2431/11884 - Loss: 30.7742\n",
      "Processing batch 2432/11884 - Loss: 30.2764\n",
      "Processing batch 2433/11884 - Loss: 31.3590\n",
      "Processing batch 2434/11884 - Loss: 29.6290\n",
      "Processing batch 2435/11884 - Loss: 30.7836\n",
      "Processing batch 2436/11884 - Loss: 29.5247\n",
      "Processing batch 2437/11884 - Loss: 29.0151\n",
      "Processing batch 2438/11884 - Loss: 29.5802\n",
      "Processing batch 2439/11884 - Loss: 29.9889\n",
      "Processing batch 2440/11884 - Loss: 31.0589\n",
      "Processing batch 2441/11884 - Loss: 29.9412\n",
      "Processing batch 2442/11884 - Loss: 30.7404\n",
      "Processing batch 2443/11884 - Loss: 30.5089\n",
      "Processing batch 2444/11884 - Loss: 30.7336\n",
      "Processing batch 2445/11884 - Loss: 30.2965\n",
      "Processing batch 2446/11884 - Loss: 30.7927\n",
      "Processing batch 2447/11884 - Loss: 30.8101\n",
      "Processing batch 2448/11884 - Loss: 31.3269\n",
      "Processing batch 2449/11884 - Loss: 31.9723\n",
      "Processing batch 2450/11884 - Loss: 29.4087\n",
      "Processing batch 2451/11884 - Loss: 30.3169\n",
      "Processing batch 2452/11884 - Loss: 31.5855\n",
      "Processing batch 2453/11884 - Loss: 29.8338\n",
      "Processing batch 2454/11884 - Loss: 32.1712\n",
      "Processing batch 2455/11884 - Loss: 30.0921\n",
      "Processing batch 2456/11884 - Loss: 30.1063\n",
      "Processing batch 2457/11884 - Loss: 30.1726\n",
      "Processing batch 2458/11884 - Loss: 30.9453\n",
      "Processing batch 2459/11884 - Loss: 30.6400\n",
      "Processing batch 2460/11884 - Loss: 29.6505\n",
      "Processing batch 2461/11884 - Loss: 31.7359\n",
      "Processing batch 2462/11884 - Loss: 29.0192\n",
      "Processing batch 2463/11884 - Loss: 30.1225\n",
      "Processing batch 2464/11884 - Loss: 30.8514\n",
      "Processing batch 2465/11884 - Loss: 30.0765\n",
      "Processing batch 2466/11884 - Loss: 30.6895\n",
      "Processing batch 2467/11884 - Loss: 29.8479\n",
      "Processing batch 2468/11884 - Loss: 30.4953\n",
      "Processing batch 2469/11884 - Loss: 29.1485\n",
      "Processing batch 2470/11884 - Loss: 29.5606\n",
      "Processing batch 2471/11884 - Loss: 31.8615\n",
      "Processing batch 2472/11884 - Loss: 29.6212\n",
      "Processing batch 2473/11884 - Loss: 30.2103\n",
      "Processing batch 2474/11884 - Loss: 30.1677\n",
      "Processing batch 2475/11884 - Loss: 31.1804\n",
      "Processing batch 2476/11884 - Loss: 31.9204\n",
      "Processing batch 2477/11884 - Loss: 29.9095\n",
      "Processing batch 2478/11884 - Loss: 30.1286\n",
      "Processing batch 2479/11884 - Loss: 30.4607\n",
      "Processing batch 2480/11884 - Loss: 31.1054\n",
      "Processing batch 2481/11884 - Loss: 31.1330\n",
      "Processing batch 2482/11884 - Loss: 30.7924\n",
      "Processing batch 2483/11884 - Loss: 31.5273\n",
      "Processing batch 2484/11884 - Loss: 29.4705\n",
      "Processing batch 2485/11884 - Loss: 29.7889\n",
      "Processing batch 2486/11884 - Loss: 31.6028\n",
      "Processing batch 2487/11884 - Loss: 30.8555\n",
      "Processing batch 2488/11884 - Loss: 30.2549\n",
      "Processing batch 2489/11884 - Loss: 29.1748\n",
      "Processing batch 2490/11884 - Loss: 30.8362\n",
      "Processing batch 2491/11884 - Loss: 30.0316\n",
      "Processing batch 2492/11884 - Loss: 30.4344\n",
      "Processing batch 2493/11884 - Loss: 30.6259\n",
      "Processing batch 2494/11884 - Loss: 31.2305\n",
      "Processing batch 2495/11884 - Loss: 29.8444\n",
      "Processing batch 2496/11884 - Loss: 29.9403\n",
      "Processing batch 2497/11884 - Loss: 30.9194\n",
      "Processing batch 2498/11884 - Loss: 30.8638\n",
      "Processing batch 2499/11884 - Loss: 31.0997\n",
      "Processing batch 2500/11884 - Loss: 31.1356\n",
      "Processing batch 2501/11884 - Loss: 31.4872\n",
      "Processing batch 2502/11884 - Loss: 31.6718\n",
      "Processing batch 2503/11884 - Loss: 29.6047\n",
      "Processing batch 2504/11884 - Loss: 30.6928\n",
      "Processing batch 2505/11884 - Loss: 31.6028\n",
      "Processing batch 2506/11884 - Loss: 31.7145\n",
      "Processing batch 2507/11884 - Loss: 30.2498\n",
      "Processing batch 2508/11884 - Loss: 30.0797\n",
      "Processing batch 2509/11884 - Loss: 29.6092\n",
      "Processing batch 2510/11884 - Loss: 30.8681\n",
      "Processing batch 2511/11884 - Loss: 31.4347\n",
      "Processing batch 2512/11884 - Loss: 30.6591\n",
      "Processing batch 2513/11884 - Loss: 30.6789\n",
      "Processing batch 2514/11884 - Loss: 29.9935\n",
      "Processing batch 2515/11884 - Loss: 31.1699\n",
      "Processing batch 2516/11884 - Loss: 29.5815\n",
      "Processing batch 2517/11884 - Loss: 30.6339\n",
      "Processing batch 2518/11884 - Loss: 31.4393\n",
      "Processing batch 2519/11884 - Loss: 30.5890\n",
      "Processing batch 2520/11884 - Loss: 31.4501\n",
      "Processing batch 2521/11884 - Loss: 30.2167\n",
      "Processing batch 2522/11884 - Loss: 30.6896\n",
      "Processing batch 2523/11884 - Loss: 30.4157\n",
      "Processing batch 2524/11884 - Loss: 28.5580\n",
      "Processing batch 2525/11884 - Loss: 30.7183\n",
      "Processing batch 2526/11884 - Loss: 30.6777\n",
      "Processing batch 2527/11884 - Loss: 31.6713\n",
      "Processing batch 2528/11884 - Loss: 29.5391\n",
      "Processing batch 2529/11884 - Loss: 30.8430\n",
      "Processing batch 2530/11884 - Loss: 30.6361\n",
      "Processing batch 2531/11884 - Loss: 31.1620\n",
      "Processing batch 2532/11884 - Loss: 32.1908\n",
      "Processing batch 2533/11884 - Loss: 31.2669\n",
      "Processing batch 2534/11884 - Loss: 31.0372\n",
      "Processing batch 2535/11884 - Loss: 30.7020\n",
      "Processing batch 2536/11884 - Loss: 30.0807\n",
      "Processing batch 2537/11884 - Loss: 31.8496\n",
      "Processing batch 2538/11884 - Loss: 31.2490\n",
      "Processing batch 2539/11884 - Loss: 29.8495\n",
      "Processing batch 2540/11884 - Loss: 30.5683\n",
      "Processing batch 2541/11884 - Loss: 30.3239\n",
      "Processing batch 2542/11884 - Loss: 30.4862\n",
      "Processing batch 2543/11884 - Loss: 31.1530\n",
      "Processing batch 2544/11884 - Loss: 31.4793\n",
      "Processing batch 2545/11884 - Loss: 31.9111\n",
      "Processing batch 2546/11884 - Loss: 31.8589\n",
      "Processing batch 2547/11884 - Loss: 31.0984\n",
      "Processing batch 2548/11884 - Loss: 30.8276\n",
      "Processing batch 2549/11884 - Loss: 29.2334\n",
      "Processing batch 2550/11884 - Loss: 31.2418\n",
      "Processing batch 2551/11884 - Loss: 30.5074\n",
      "Processing batch 2552/11884 - Loss: 30.2020\n",
      "Processing batch 2553/11884 - Loss: 30.4055\n",
      "Processing batch 2554/11884 - Loss: 29.1769\n",
      "Processing batch 2555/11884 - Loss: 31.4601\n",
      "Processing batch 2556/11884 - Loss: 30.1562\n",
      "Processing batch 2557/11884 - Loss: 30.3892\n",
      "Processing batch 2558/11884 - Loss: 30.7405\n",
      "Processing batch 2559/11884 - Loss: 30.4222\n",
      "Processing batch 2560/11884 - Loss: 31.0363\n",
      "Processing batch 2561/11884 - Loss: 30.4293\n",
      "Processing batch 2562/11884 - Loss: 29.6860\n",
      "Processing batch 2563/11884 - Loss: 30.2122\n",
      "Processing batch 2564/11884 - Loss: 31.3405\n",
      "Processing batch 2565/11884 - Loss: 31.0916\n",
      "Processing batch 2566/11884 - Loss: 29.1022\n",
      "Processing batch 2567/11884 - Loss: 30.7913\n",
      "Processing batch 2568/11884 - Loss: 31.0281\n",
      "Processing batch 2569/11884 - Loss: 30.7624\n",
      "Processing batch 2570/11884 - Loss: 29.8366\n",
      "Processing batch 2571/11884 - Loss: 30.4310\n",
      "Processing batch 2572/11884 - Loss: 30.6904\n",
      "Processing batch 2573/11884 - Loss: 30.3445\n",
      "Processing batch 2574/11884 - Loss: 29.6714\n",
      "Processing batch 2575/11884 - Loss: 31.4164\n",
      "Processing batch 2576/11884 - Loss: 31.1566\n",
      "Processing batch 2577/11884 - Loss: 30.6391\n",
      "Processing batch 2578/11884 - Loss: 29.5998\n",
      "Processing batch 2579/11884 - Loss: 30.9564\n",
      "Processing batch 2580/11884 - Loss: 31.1877\n",
      "Processing batch 2581/11884 - Loss: 29.0017\n",
      "Processing batch 2582/11884 - Loss: 32.2008\n",
      "Processing batch 2583/11884 - Loss: 30.8555\n",
      "Processing batch 2584/11884 - Loss: 31.1697\n",
      "Processing batch 2585/11884 - Loss: 29.6296\n",
      "Processing batch 2586/11884 - Loss: 30.4365\n",
      "Processing batch 2587/11884 - Loss: 30.9685\n",
      "Processing batch 2588/11884 - Loss: 31.5811\n",
      "Processing batch 2589/11884 - Loss: 30.3429\n",
      "Processing batch 2590/11884 - Loss: 30.3172\n",
      "Processing batch 2591/11884 - Loss: 29.8879\n",
      "Processing batch 2592/11884 - Loss: 30.7403\n",
      "Processing batch 2593/11884 - Loss: 30.0332\n",
      "Processing batch 2594/11884 - Loss: 29.5472\n",
      "Processing batch 2595/11884 - Loss: 30.5071\n",
      "Processing batch 2596/11884 - Loss: 30.8077\n",
      "Processing batch 2597/11884 - Loss: 30.9209\n",
      "Processing batch 2598/11884 - Loss: 30.2685\n",
      "Processing batch 2599/11884 - Loss: 30.9439\n",
      "Processing batch 2600/11884 - Loss: 28.8646\n",
      "Processing batch 2601/11884 - Loss: 31.9758\n",
      "Processing batch 2602/11884 - Loss: 31.5006\n",
      "Processing batch 2603/11884 - Loss: 31.2074\n",
      "Processing batch 2604/11884 - Loss: 30.1592\n",
      "Processing batch 2605/11884 - Loss: 30.5569\n",
      "Processing batch 2606/11884 - Loss: 30.2953\n",
      "Processing batch 2607/11884 - Loss: 31.6378\n",
      "Processing batch 2608/11884 - Loss: 31.5844\n",
      "Processing batch 2609/11884 - Loss: 29.8981\n",
      "Processing batch 2610/11884 - Loss: 31.3410\n",
      "Processing batch 2611/11884 - Loss: 30.8856\n",
      "Processing batch 2612/11884 - Loss: 30.3581\n",
      "Processing batch 2613/11884 - Loss: 30.4410\n",
      "Processing batch 2614/11884 - Loss: 30.9554\n",
      "Processing batch 2615/11884 - Loss: 31.2305\n",
      "Processing batch 2616/11884 - Loss: 29.3288\n",
      "Processing batch 2617/11884 - Loss: 30.3136\n",
      "Processing batch 2618/11884 - Loss: 30.6354\n",
      "Processing batch 2619/11884 - Loss: 30.6139\n",
      "Processing batch 2620/11884 - Loss: 31.5521\n",
      "Processing batch 2621/11884 - Loss: 30.3607\n",
      "Processing batch 2622/11884 - Loss: 29.6866\n",
      "Processing batch 2623/11884 - Loss: 30.5267\n",
      "Processing batch 2624/11884 - Loss: 30.5573\n",
      "Processing batch 2625/11884 - Loss: 30.1427\n",
      "Processing batch 2626/11884 - Loss: 29.8233\n",
      "Processing batch 2627/11884 - Loss: 30.5734\n",
      "Processing batch 2628/11884 - Loss: 29.8486\n",
      "Processing batch 2629/11884 - Loss: 30.3140\n",
      "Processing batch 2630/11884 - Loss: 29.5353\n",
      "Processing batch 2631/11884 - Loss: 29.8385\n",
      "Processing batch 2632/11884 - Loss: 29.4379\n",
      "Processing batch 2633/11884 - Loss: 30.6109\n",
      "Processing batch 2634/11884 - Loss: 31.4652\n",
      "Processing batch 2635/11884 - Loss: 31.8745\n",
      "Processing batch 2636/11884 - Loss: 31.4995\n",
      "Processing batch 2637/11884 - Loss: 30.4142\n",
      "Processing batch 2638/11884 - Loss: 31.9570\n",
      "Processing batch 2639/11884 - Loss: 30.3642\n",
      "Processing batch 2640/11884 - Loss: 29.9784\n",
      "Processing batch 2641/11884 - Loss: 30.8390\n",
      "Processing batch 2642/11884 - Loss: 30.2600\n",
      "Processing batch 2643/11884 - Loss: 30.0131\n",
      "Processing batch 2644/11884 - Loss: 31.7855\n",
      "Processing batch 2645/11884 - Loss: 30.1204\n",
      "Processing batch 2646/11884 - Loss: 31.3181\n",
      "Processing batch 2647/11884 - Loss: 29.4464\n",
      "Processing batch 2648/11884 - Loss: 30.5409\n",
      "Processing batch 2649/11884 - Loss: 31.5938\n",
      "Processing batch 2650/11884 - Loss: 30.5412\n",
      "Processing batch 2651/11884 - Loss: 29.7280\n",
      "Processing batch 2652/11884 - Loss: 31.1478\n",
      "Processing batch 2653/11884 - Loss: 31.3023\n",
      "Processing batch 2654/11884 - Loss: 30.3195\n",
      "Processing batch 2655/11884 - Loss: 30.5241\n",
      "Processing batch 2656/11884 - Loss: 30.7221\n",
      "Processing batch 2657/11884 - Loss: 30.6309\n",
      "Processing batch 2658/11884 - Loss: 30.9193\n",
      "Processing batch 2659/11884 - Loss: 30.4877\n",
      "Processing batch 2660/11884 - Loss: 31.1953\n",
      "Processing batch 2661/11884 - Loss: 29.9318\n",
      "Processing batch 2662/11884 - Loss: 29.9186\n",
      "Processing batch 2663/11884 - Loss: 30.5113\n",
      "Processing batch 2664/11884 - Loss: 30.2843\n",
      "Processing batch 2665/11884 - Loss: 30.6245\n",
      "Processing batch 2666/11884 - Loss: 31.4073\n",
      "Processing batch 2667/11884 - Loss: 31.4045\n",
      "Processing batch 2668/11884 - Loss: 30.6365\n",
      "Processing batch 2669/11884 - Loss: 30.3079\n",
      "Processing batch 2670/11884 - Loss: 31.6265\n",
      "Processing batch 2671/11884 - Loss: 30.6630\n",
      "Processing batch 2672/11884 - Loss: 28.3915\n",
      "Processing batch 2673/11884 - Loss: 29.5373\n",
      "Processing batch 2674/11884 - Loss: 30.3866\n",
      "Processing batch 2675/11884 - Loss: 31.0128\n",
      "Processing batch 2676/11884 - Loss: 29.9841\n",
      "Processing batch 2677/11884 - Loss: 29.8011\n",
      "Processing batch 2678/11884 - Loss: 30.8851\n",
      "Processing batch 2679/11884 - Loss: 29.9756\n",
      "Processing batch 2680/11884 - Loss: 31.0124\n",
      "Processing batch 2681/11884 - Loss: 30.7578\n",
      "Processing batch 2682/11884 - Loss: 30.1968\n",
      "Processing batch 2683/11884 - Loss: 29.9787\n",
      "Processing batch 2684/11884 - Loss: 29.9303\n",
      "Processing batch 2685/11884 - Loss: 31.8620\n",
      "Processing batch 2686/11884 - Loss: 29.1650\n",
      "Processing batch 2687/11884 - Loss: 30.6667\n",
      "Processing batch 2688/11884 - Loss: 30.1267\n",
      "Processing batch 2689/11884 - Loss: 31.5843\n",
      "Processing batch 2690/11884 - Loss: 30.3792\n",
      "Processing batch 2691/11884 - Loss: 29.6712\n",
      "Processing batch 2692/11884 - Loss: 31.2779\n",
      "Processing batch 2693/11884 - Loss: 31.5728\n",
      "Processing batch 2694/11884 - Loss: 31.2262\n",
      "Processing batch 2695/11884 - Loss: 32.6238\n",
      "Processing batch 2696/11884 - Loss: 29.9941\n",
      "Processing batch 2697/11884 - Loss: 32.4198\n",
      "Processing batch 2698/11884 - Loss: 31.2832\n",
      "Processing batch 2699/11884 - Loss: 30.2407\n",
      "Processing batch 2700/11884 - Loss: 30.9649\n",
      "Processing batch 2701/11884 - Loss: 30.7646\n",
      "Processing batch 2702/11884 - Loss: 28.7560\n",
      "Processing batch 2703/11884 - Loss: 30.0652\n",
      "Processing batch 2704/11884 - Loss: 30.8102\n",
      "Processing batch 2705/11884 - Loss: 30.1478\n",
      "Processing batch 2706/11884 - Loss: 30.3773\n",
      "Processing batch 2707/11884 - Loss: 30.8674\n",
      "Processing batch 2708/11884 - Loss: 30.8843\n",
      "Processing batch 2709/11884 - Loss: 30.8381\n",
      "Processing batch 2710/11884 - Loss: 31.3908\n",
      "Processing batch 2711/11884 - Loss: 31.3069\n",
      "Processing batch 2712/11884 - Loss: 30.6013\n",
      "Processing batch 2713/11884 - Loss: 30.9488\n",
      "Processing batch 2714/11884 - Loss: 28.4283\n",
      "Processing batch 2715/11884 - Loss: 30.5539\n",
      "Processing batch 2716/11884 - Loss: 31.6556\n",
      "Processing batch 2717/11884 - Loss: 30.7006\n",
      "Processing batch 2718/11884 - Loss: 31.3955\n",
      "Processing batch 2719/11884 - Loss: 30.1411\n",
      "Processing batch 2720/11884 - Loss: 30.7013\n",
      "Processing batch 2721/11884 - Loss: 31.3914\n",
      "Processing batch 2722/11884 - Loss: 30.9506\n",
      "Processing batch 2723/11884 - Loss: 30.7129\n",
      "Processing batch 2724/11884 - Loss: 29.5910\n",
      "Processing batch 2725/11884 - Loss: 29.5349\n",
      "Processing batch 2726/11884 - Loss: 30.1423\n",
      "Processing batch 2727/11884 - Loss: 30.5811\n",
      "Processing batch 2728/11884 - Loss: 31.8614\n",
      "Processing batch 2729/11884 - Loss: 29.5637\n",
      "Processing batch 2730/11884 - Loss: 30.2680\n",
      "Processing batch 2731/11884 - Loss: 30.6894\n",
      "Processing batch 2732/11884 - Loss: 30.5079\n",
      "Processing batch 2733/11884 - Loss: 29.6852\n",
      "Processing batch 2734/11884 - Loss: 29.3442\n",
      "Processing batch 2735/11884 - Loss: 31.1478\n",
      "Processing batch 2736/11884 - Loss: 30.8691\n",
      "Processing batch 2737/11884 - Loss: 31.1969\n",
      "Processing batch 2738/11884 - Loss: 31.7794\n",
      "Processing batch 2739/11884 - Loss: 31.7503\n",
      "Processing batch 2740/11884 - Loss: 28.9555\n",
      "Processing batch 2741/11884 - Loss: 29.3939\n",
      "Processing batch 2742/11884 - Loss: 30.0360\n",
      "Processing batch 2743/11884 - Loss: 30.9776\n",
      "Processing batch 2744/11884 - Loss: 30.5370\n",
      "Processing batch 2745/11884 - Loss: 31.5822\n",
      "Processing batch 2746/11884 - Loss: 30.4491\n",
      "Processing batch 2747/11884 - Loss: 28.8729\n",
      "Processing batch 2748/11884 - Loss: 30.7230\n",
      "Processing batch 2749/11884 - Loss: 30.7835\n",
      "Processing batch 2750/11884 - Loss: 31.3858\n",
      "Processing batch 2751/11884 - Loss: 30.0892\n",
      "Processing batch 2752/11884 - Loss: 31.4324\n",
      "Processing batch 2753/11884 - Loss: 31.3602\n",
      "Processing batch 2754/11884 - Loss: 29.9476\n",
      "Processing batch 2755/11884 - Loss: 30.5411\n",
      "Processing batch 2756/11884 - Loss: 30.3924\n",
      "Processing batch 2757/11884 - Loss: 30.8008\n",
      "Processing batch 2758/11884 - Loss: 31.9525\n",
      "Processing batch 2759/11884 - Loss: 30.3471\n",
      "Processing batch 2760/11884 - Loss: 30.6644\n",
      "Processing batch 2761/11884 - Loss: 29.4955\n",
      "Processing batch 2762/11884 - Loss: 30.8113\n",
      "Processing batch 2763/11884 - Loss: 30.9026\n",
      "Processing batch 2764/11884 - Loss: 32.0787\n",
      "Processing batch 2765/11884 - Loss: 30.6973\n",
      "Processing batch 2766/11884 - Loss: 29.4117\n",
      "Processing batch 2767/11884 - Loss: 30.7691\n",
      "Processing batch 2768/11884 - Loss: 30.5501\n",
      "Processing batch 2769/11884 - Loss: 30.9699\n",
      "Processing batch 2770/11884 - Loss: 30.2393\n",
      "Processing batch 2771/11884 - Loss: 30.8004\n",
      "Processing batch 2772/11884 - Loss: 30.4819\n",
      "Processing batch 2773/11884 - Loss: 30.6557\n",
      "Processing batch 2774/11884 - Loss: 29.6275\n",
      "Processing batch 2775/11884 - Loss: 30.3116\n",
      "Processing batch 2776/11884 - Loss: 29.8204\n",
      "Processing batch 2777/11884 - Loss: 30.3607\n",
      "Processing batch 2778/11884 - Loss: 31.3448\n",
      "Processing batch 2779/11884 - Loss: 28.7686\n",
      "Processing batch 2780/11884 - Loss: 30.6321\n",
      "Processing batch 2781/11884 - Loss: 30.6792\n",
      "Processing batch 2782/11884 - Loss: 28.6677\n",
      "Processing batch 2783/11884 - Loss: 30.8605\n",
      "Processing batch 2784/11884 - Loss: 31.0577\n",
      "Processing batch 2785/11884 - Loss: 31.6346\n",
      "Processing batch 2786/11884 - Loss: 31.0116\n",
      "Processing batch 2787/11884 - Loss: 31.4580\n",
      "Processing batch 2788/11884 - Loss: 30.6392\n",
      "Processing batch 2789/11884 - Loss: 30.0353\n",
      "Processing batch 2790/11884 - Loss: 29.6309\n",
      "Processing batch 2791/11884 - Loss: 30.8697\n",
      "Processing batch 2792/11884 - Loss: 31.6048\n",
      "Processing batch 2793/11884 - Loss: 30.5051\n",
      "Processing batch 2794/11884 - Loss: 28.9085\n",
      "Processing batch 2795/11884 - Loss: 29.8962\n",
      "Processing batch 2796/11884 - Loss: 30.5507\n",
      "Processing batch 2797/11884 - Loss: 30.2432\n",
      "Processing batch 2798/11884 - Loss: 30.1087\n",
      "Processing batch 2799/11884 - Loss: 30.0460\n",
      "Processing batch 2800/11884 - Loss: 31.5996\n",
      "Processing batch 2801/11884 - Loss: 31.1036\n",
      "Processing batch 2802/11884 - Loss: 31.4324\n",
      "Processing batch 2803/11884 - Loss: 31.7114\n",
      "Processing batch 2804/11884 - Loss: 30.3000\n",
      "Processing batch 2805/11884 - Loss: 30.5431\n",
      "Processing batch 2806/11884 - Loss: 31.1094\n",
      "Processing batch 2807/11884 - Loss: 28.9906\n",
      "Processing batch 2808/11884 - Loss: 31.7660\n",
      "Processing batch 2809/11884 - Loss: 30.9251\n",
      "Processing batch 2810/11884 - Loss: 29.5451\n",
      "Processing batch 2811/11884 - Loss: 31.5008\n",
      "Processing batch 2812/11884 - Loss: 30.7559\n",
      "Processing batch 2813/11884 - Loss: 31.4124\n",
      "Processing batch 2814/11884 - Loss: 31.1170\n",
      "Processing batch 2815/11884 - Loss: 31.1655\n",
      "Processing batch 2816/11884 - Loss: 29.2548\n",
      "Processing batch 2817/11884 - Loss: 31.1666\n",
      "Processing batch 2818/11884 - Loss: 30.4645\n",
      "Processing batch 2819/11884 - Loss: 30.7547\n",
      "Processing batch 2820/11884 - Loss: 32.7937\n",
      "Processing batch 2821/11884 - Loss: 30.6691\n",
      "Processing batch 2822/11884 - Loss: 30.6360\n",
      "Processing batch 2823/11884 - Loss: 30.1578\n",
      "Processing batch 2824/11884 - Loss: 31.3345\n",
      "Processing batch 2825/11884 - Loss: 31.0411\n",
      "Processing batch 2826/11884 - Loss: 30.7802\n",
      "Processing batch 2827/11884 - Loss: 30.8114\n",
      "Processing batch 2828/11884 - Loss: 30.3591\n",
      "Processing batch 2829/11884 - Loss: 30.3008\n",
      "Processing batch 2830/11884 - Loss: 29.5620\n",
      "Processing batch 2831/11884 - Loss: 30.1944\n",
      "Processing batch 2832/11884 - Loss: 30.7928\n",
      "Processing batch 2833/11884 - Loss: 29.0926\n",
      "Processing batch 2834/11884 - Loss: 30.1216\n",
      "Processing batch 2835/11884 - Loss: 32.2045\n",
      "Processing batch 2836/11884 - Loss: 30.1985\n",
      "Processing batch 2837/11884 - Loss: 29.9108\n",
      "Processing batch 2838/11884 - Loss: 30.2180\n",
      "Processing batch 2839/11884 - Loss: 30.4056\n",
      "Processing batch 2840/11884 - Loss: 29.4817\n",
      "Processing batch 2841/11884 - Loss: 30.5374\n",
      "Processing batch 2842/11884 - Loss: 30.6131\n",
      "Processing batch 2843/11884 - Loss: 30.6993\n",
      "Processing batch 2844/11884 - Loss: 30.6066\n",
      "Processing batch 2845/11884 - Loss: 30.2207\n",
      "Processing batch 2846/11884 - Loss: 29.7991\n",
      "Processing batch 2847/11884 - Loss: 29.8796\n",
      "Processing batch 2848/11884 - Loss: 30.6956\n",
      "Processing batch 2849/11884 - Loss: 31.6404\n",
      "Processing batch 2850/11884 - Loss: 30.5261\n",
      "Processing batch 2851/11884 - Loss: 31.1154\n",
      "Processing batch 2852/11884 - Loss: 29.5381\n",
      "Processing batch 2853/11884 - Loss: 30.5245\n",
      "Processing batch 2854/11884 - Loss: 28.8568\n",
      "Processing batch 2855/11884 - Loss: 31.0206\n",
      "Processing batch 2856/11884 - Loss: 31.0470\n",
      "Processing batch 2857/11884 - Loss: 30.3953\n",
      "Processing batch 2858/11884 - Loss: 30.5067\n",
      "Processing batch 2859/11884 - Loss: 32.1317\n",
      "Processing batch 2860/11884 - Loss: 30.7127\n",
      "Processing batch 2861/11884 - Loss: 31.0393\n",
      "Processing batch 2862/11884 - Loss: 29.6262\n",
      "Processing batch 2863/11884 - Loss: 31.5214\n",
      "Processing batch 2864/11884 - Loss: 31.6007\n",
      "Processing batch 2865/11884 - Loss: 30.1795\n",
      "Processing batch 2866/11884 - Loss: 31.1016\n",
      "Processing batch 2867/11884 - Loss: 31.4876\n",
      "Processing batch 2868/11884 - Loss: 31.3199\n",
      "Processing batch 2869/11884 - Loss: 29.7263\n",
      "Processing batch 2870/11884 - Loss: 31.5177\n",
      "Processing batch 2871/11884 - Loss: 31.4884\n",
      "Processing batch 2872/11884 - Loss: 30.0759\n",
      "Processing batch 2873/11884 - Loss: 30.5062\n",
      "Processing batch 2874/11884 - Loss: 31.1331\n",
      "Processing batch 2875/11884 - Loss: 30.3207\n",
      "Processing batch 2876/11884 - Loss: 31.4225\n",
      "Processing batch 2877/11884 - Loss: 31.2764\n",
      "Processing batch 2878/11884 - Loss: 29.5085\n",
      "Processing batch 2879/11884 - Loss: 30.2306\n",
      "Processing batch 2880/11884 - Loss: 32.4400\n",
      "Processing batch 2881/11884 - Loss: 30.6711\n",
      "Processing batch 2882/11884 - Loss: 30.4656\n",
      "Processing batch 2883/11884 - Loss: 31.9330\n",
      "Processing batch 2884/11884 - Loss: 31.1076\n",
      "Processing batch 2885/11884 - Loss: 30.5755\n",
      "Processing batch 2886/11884 - Loss: 32.1665\n",
      "Processing batch 2887/11884 - Loss: 30.5555\n",
      "Processing batch 2888/11884 - Loss: 31.4289\n",
      "Processing batch 2889/11884 - Loss: 31.2536\n",
      "Processing batch 2890/11884 - Loss: 30.3747\n",
      "Processing batch 2891/11884 - Loss: 30.6546\n",
      "Processing batch 2892/11884 - Loss: 30.7655\n",
      "Processing batch 2893/11884 - Loss: 30.7915\n",
      "Processing batch 2894/11884 - Loss: 30.3974\n",
      "Processing batch 2895/11884 - Loss: 30.3049\n",
      "Processing batch 2896/11884 - Loss: 29.7942\n",
      "Processing batch 2897/11884 - Loss: 31.7702\n",
      "Processing batch 2898/11884 - Loss: 30.1042\n",
      "Processing batch 2899/11884 - Loss: 30.4518\n",
      "Processing batch 2900/11884 - Loss: 30.4305\n",
      "Processing batch 2901/11884 - Loss: 31.4252\n",
      "Processing batch 2902/11884 - Loss: 29.8238\n",
      "Processing batch 2903/11884 - Loss: 29.6994\n",
      "Processing batch 2904/11884 - Loss: 30.6234\n",
      "Processing batch 2905/11884 - Loss: 30.0146\n",
      "Processing batch 2906/11884 - Loss: 30.1853\n",
      "Processing batch 2907/11884 - Loss: 31.1472\n",
      "Processing batch 2908/11884 - Loss: 30.3872\n",
      "Processing batch 2909/11884 - Loss: 30.1793\n",
      "Processing batch 2910/11884 - Loss: 29.1413\n",
      "Processing batch 2911/11884 - Loss: 30.7118\n",
      "Processing batch 2912/11884 - Loss: 30.4530\n",
      "Processing batch 2913/11884 - Loss: 30.4282\n",
      "Processing batch 2914/11884 - Loss: 30.6533\n",
      "Processing batch 2915/11884 - Loss: 31.0934\n",
      "Processing batch 2916/11884 - Loss: 31.5161\n",
      "Processing batch 2917/11884 - Loss: 30.7279\n",
      "Processing batch 2918/11884 - Loss: 30.8765\n",
      "Processing batch 2919/11884 - Loss: 30.7918\n",
      "Processing batch 2920/11884 - Loss: 31.5055\n",
      "Processing batch 2921/11884 - Loss: 30.6173\n",
      "Processing batch 2922/11884 - Loss: 29.9235\n",
      "Processing batch 2923/11884 - Loss: 30.7464\n",
      "Processing batch 2924/11884 - Loss: 31.2710\n",
      "Processing batch 2925/11884 - Loss: 30.3947\n",
      "Processing batch 2926/11884 - Loss: 31.4491\n",
      "Processing batch 2927/11884 - Loss: 29.6973\n",
      "Processing batch 2928/11884 - Loss: 30.5395\n",
      "Processing batch 2929/11884 - Loss: 30.4136\n",
      "Processing batch 2930/11884 - Loss: 31.4062\n",
      "Processing batch 2931/11884 - Loss: 31.1464\n",
      "Processing batch 2932/11884 - Loss: 31.0558\n",
      "Processing batch 2933/11884 - Loss: 30.0935\n",
      "Processing batch 2934/11884 - Loss: 30.6899\n",
      "Processing batch 2935/11884 - Loss: 29.9739\n",
      "Processing batch 2936/11884 - Loss: 31.4699\n",
      "Processing batch 2937/11884 - Loss: 29.3322\n",
      "Processing batch 2938/11884 - Loss: 31.2091\n",
      "Processing batch 2939/11884 - Loss: 30.5140\n",
      "Processing batch 2940/11884 - Loss: 30.5272\n",
      "Processing batch 2941/11884 - Loss: 30.0752\n",
      "Processing batch 2942/11884 - Loss: 31.1097\n",
      "Processing batch 2943/11884 - Loss: 29.8003\n",
      "Processing batch 2944/11884 - Loss: 29.6699\n",
      "Processing batch 2945/11884 - Loss: 30.3916\n",
      "Processing batch 2946/11884 - Loss: 30.4432\n",
      "Processing batch 2947/11884 - Loss: 30.8825\n",
      "Processing batch 2948/11884 - Loss: 29.5613\n",
      "Processing batch 2949/11884 - Loss: 30.4479\n",
      "Processing batch 2950/11884 - Loss: 30.3580\n",
      "Processing batch 2951/11884 - Loss: 30.9551\n",
      "Processing batch 2952/11884 - Loss: 32.1834\n",
      "Processing batch 2953/11884 - Loss: 30.4124\n",
      "Processing batch 2954/11884 - Loss: 30.6144\n",
      "Processing batch 2955/11884 - Loss: 30.1579\n",
      "Processing batch 2956/11884 - Loss: 31.5012\n",
      "Processing batch 2957/11884 - Loss: 31.0709\n",
      "Processing batch 2958/11884 - Loss: 30.2993\n",
      "Processing batch 2959/11884 - Loss: 30.8049\n",
      "Processing batch 2960/11884 - Loss: 32.1149\n",
      "Processing batch 2961/11884 - Loss: 32.0641\n",
      "Processing batch 2962/11884 - Loss: 30.4561\n",
      "Processing batch 2963/11884 - Loss: 30.7568\n",
      "Processing batch 2964/11884 - Loss: 29.6454\n",
      "Processing batch 2965/11884 - Loss: 31.0049\n",
      "Processing batch 2966/11884 - Loss: 29.3788\n",
      "Processing batch 2967/11884 - Loss: 30.3151\n",
      "Processing batch 2968/11884 - Loss: 31.5972\n",
      "Processing batch 2969/11884 - Loss: 30.7804\n",
      "Processing batch 2970/11884 - Loss: 31.2444\n",
      "Processing batch 2971/11884 - Loss: 29.1710\n",
      "Processing batch 2972/11884 - Loss: 31.5071\n",
      "Processing batch 2973/11884 - Loss: 29.6336\n",
      "Processing batch 2974/11884 - Loss: 31.9127\n",
      "Processing batch 2975/11884 - Loss: 31.4783\n",
      "Processing batch 2976/11884 - Loss: 31.5515\n",
      "Processing batch 2977/11884 - Loss: 30.2726\n",
      "Processing batch 2978/11884 - Loss: 31.6568\n",
      "Processing batch 2979/11884 - Loss: 30.9584\n",
      "Processing batch 2980/11884 - Loss: 31.0990\n",
      "Processing batch 2981/11884 - Loss: 30.5373\n",
      "Processing batch 2982/11884 - Loss: 30.0065\n",
      "Processing batch 2983/11884 - Loss: 31.3750\n",
      "Processing batch 2984/11884 - Loss: 31.1102\n",
      "Processing batch 2985/11884 - Loss: 31.7442\n",
      "Processing batch 2986/11884 - Loss: 31.3654\n",
      "Processing batch 2987/11884 - Loss: 32.0221\n",
      "Processing batch 2988/11884 - Loss: 30.2019\n",
      "Processing batch 2989/11884 - Loss: 28.8253\n",
      "Processing batch 2990/11884 - Loss: 31.8588\n",
      "Processing batch 2991/11884 - Loss: 29.4132\n",
      "Processing batch 2992/11884 - Loss: 29.3293\n",
      "Processing batch 2993/11884 - Loss: 31.0988\n",
      "Processing batch 2994/11884 - Loss: 31.1878\n",
      "Processing batch 2995/11884 - Loss: 31.0908\n",
      "Processing batch 2996/11884 - Loss: 29.4083\n",
      "Processing batch 2997/11884 - Loss: 31.1997\n",
      "Processing batch 2998/11884 - Loss: 31.3225\n",
      "Processing batch 2999/11884 - Loss: 31.0950\n",
      "Processing batch 3000/11884 - Loss: 29.5674\n",
      "Processing batch 3001/11884 - Loss: 30.8713\n",
      "Processing batch 3002/11884 - Loss: 30.2963\n",
      "Processing batch 3003/11884 - Loss: 30.4505\n",
      "Processing batch 3004/11884 - Loss: 30.3705\n",
      "Processing batch 3005/11884 - Loss: 30.2328\n",
      "Processing batch 3006/11884 - Loss: 31.2761\n",
      "Processing batch 3007/11884 - Loss: 30.1211\n",
      "Processing batch 3008/11884 - Loss: 29.7942\n",
      "Processing batch 3009/11884 - Loss: 30.2540\n",
      "Processing batch 3010/11884 - Loss: 30.3001\n",
      "Processing batch 3011/11884 - Loss: 30.0122\n",
      "Processing batch 3012/11884 - Loss: 31.4781\n",
      "Processing batch 3013/11884 - Loss: 29.9370\n",
      "Processing batch 3014/11884 - Loss: 30.5755\n",
      "Processing batch 3015/11884 - Loss: 30.8368\n",
      "Processing batch 3016/11884 - Loss: 30.7681\n",
      "Processing batch 3017/11884 - Loss: 30.5890\n",
      "Processing batch 3018/11884 - Loss: 29.9977\n",
      "Processing batch 3019/11884 - Loss: 29.2766\n",
      "Processing batch 3020/11884 - Loss: 28.6774\n",
      "Processing batch 3021/11884 - Loss: 30.0677\n",
      "Processing batch 3022/11884 - Loss: 30.1286\n",
      "Processing batch 3023/11884 - Loss: 30.1928\n",
      "Processing batch 3024/11884 - Loss: 30.5995\n",
      "Processing batch 3025/11884 - Loss: 29.7760\n",
      "Processing batch 3026/11884 - Loss: 29.9526\n",
      "Processing batch 3027/11884 - Loss: 29.1818\n",
      "Processing batch 3028/11884 - Loss: 29.8649\n",
      "Processing batch 3029/11884 - Loss: 31.0397\n",
      "Processing batch 3030/11884 - Loss: 30.7327\n",
      "Processing batch 3031/11884 - Loss: 31.3650\n",
      "Processing batch 3032/11884 - Loss: 29.4443\n",
      "Processing batch 3033/11884 - Loss: 29.4996\n",
      "Processing batch 3034/11884 - Loss: 30.3283\n",
      "Processing batch 3035/11884 - Loss: 31.5806\n",
      "Processing batch 3036/11884 - Loss: 30.9585\n",
      "Processing batch 3037/11884 - Loss: 30.3049\n",
      "Processing batch 3038/11884 - Loss: 30.1942\n",
      "Processing batch 3039/11884 - Loss: 30.5530\n",
      "Processing batch 3040/11884 - Loss: 31.1060\n",
      "Processing batch 3041/11884 - Loss: 29.7913\n",
      "Processing batch 3042/11884 - Loss: 30.9928\n",
      "Processing batch 3043/11884 - Loss: 31.0805\n",
      "Processing batch 3044/11884 - Loss: 31.4272\n",
      "Processing batch 3045/11884 - Loss: 31.7184\n",
      "Processing batch 3046/11884 - Loss: 29.9766\n",
      "Processing batch 3047/11884 - Loss: 31.1151\n",
      "Processing batch 3048/11884 - Loss: 30.8739\n",
      "Processing batch 3049/11884 - Loss: 29.8855\n",
      "Processing batch 3050/11884 - Loss: 29.2777\n",
      "Processing batch 3051/11884 - Loss: 31.7872\n",
      "Processing batch 3052/11884 - Loss: 30.7189\n",
      "Processing batch 3053/11884 - Loss: 30.5900\n",
      "Processing batch 3054/11884 - Loss: 30.5734\n",
      "Processing batch 3055/11884 - Loss: 29.6764\n",
      "Processing batch 3056/11884 - Loss: 30.3085\n",
      "Processing batch 3057/11884 - Loss: 30.5776\n",
      "Processing batch 3058/11884 - Loss: 29.6989\n",
      "Processing batch 3059/11884 - Loss: 30.8443\n",
      "Processing batch 3060/11884 - Loss: 30.4580\n",
      "Processing batch 3061/11884 - Loss: 31.7455\n",
      "Processing batch 3062/11884 - Loss: 31.5121\n",
      "Processing batch 3063/11884 - Loss: 31.6046\n",
      "Processing batch 3064/11884 - Loss: 31.0511\n",
      "Processing batch 3065/11884 - Loss: 29.3351\n",
      "Processing batch 3066/11884 - Loss: 30.3880\n",
      "Processing batch 3067/11884 - Loss: 31.5545\n",
      "Processing batch 3068/11884 - Loss: 30.2945\n",
      "Processing batch 3069/11884 - Loss: 31.8947\n",
      "Processing batch 3070/11884 - Loss: 31.2473\n",
      "Processing batch 3071/11884 - Loss: 31.0666\n",
      "Processing batch 3072/11884 - Loss: 30.6855\n",
      "Processing batch 3073/11884 - Loss: 31.2931\n",
      "Processing batch 3074/11884 - Loss: 32.2198\n",
      "Processing batch 3075/11884 - Loss: 30.3077\n",
      "Processing batch 3076/11884 - Loss: 30.0289\n",
      "Processing batch 3077/11884 - Loss: 30.5522\n",
      "Processing batch 3078/11884 - Loss: 31.3481\n",
      "Processing batch 3079/11884 - Loss: 29.4888\n",
      "Processing batch 3080/11884 - Loss: 29.9398\n",
      "Processing batch 3081/11884 - Loss: 31.1462\n",
      "Processing batch 3082/11884 - Loss: 30.1227\n",
      "Processing batch 3083/11884 - Loss: 30.9927\n",
      "Processing batch 3084/11884 - Loss: 31.0949\n",
      "Processing batch 3085/11884 - Loss: 31.0063\n",
      "Processing batch 3086/11884 - Loss: 29.5699\n",
      "Processing batch 3087/11884 - Loss: 31.0989\n",
      "Processing batch 3088/11884 - Loss: 29.3206\n",
      "Processing batch 3089/11884 - Loss: 30.6641\n",
      "Processing batch 3090/11884 - Loss: 30.9722\n",
      "Processing batch 3091/11884 - Loss: 29.9488\n",
      "Processing batch 3092/11884 - Loss: 30.9980\n",
      "Processing batch 3093/11884 - Loss: 30.4496\n",
      "Processing batch 3094/11884 - Loss: 30.9712\n",
      "Processing batch 3095/11884 - Loss: 31.2411\n",
      "Processing batch 3096/11884 - Loss: 29.9702\n",
      "Processing batch 3097/11884 - Loss: 30.1261\n",
      "Processing batch 3098/11884 - Loss: 31.1654\n",
      "Processing batch 3099/11884 - Loss: 29.7458\n",
      "Processing batch 3100/11884 - Loss: 30.7206\n",
      "Processing batch 3101/11884 - Loss: 31.5819\n",
      "Processing batch 3102/11884 - Loss: 29.6973\n",
      "Processing batch 3103/11884 - Loss: 30.8193\n",
      "Processing batch 3104/11884 - Loss: 31.1653\n",
      "Processing batch 3105/11884 - Loss: 30.7118\n",
      "Processing batch 3106/11884 - Loss: 31.4455\n",
      "Processing batch 3107/11884 - Loss: 31.2290\n",
      "Processing batch 3108/11884 - Loss: 28.9945\n",
      "Processing batch 3109/11884 - Loss: 29.8217\n",
      "Processing batch 3110/11884 - Loss: 29.6776\n",
      "Processing batch 3111/11884 - Loss: 29.6341\n",
      "Processing batch 3112/11884 - Loss: 30.7507\n",
      "Processing batch 3113/11884 - Loss: 31.2135\n",
      "Processing batch 3114/11884 - Loss: 31.5814\n",
      "Processing batch 3115/11884 - Loss: 29.3092\n",
      "Processing batch 3116/11884 - Loss: 30.1711\n",
      "Processing batch 3117/11884 - Loss: 29.4923\n",
      "Processing batch 3118/11884 - Loss: 30.5240\n",
      "Processing batch 3119/11884 - Loss: 29.6732\n",
      "Processing batch 3120/11884 - Loss: 30.9858\n",
      "Processing batch 3121/11884 - Loss: 30.7386\n",
      "Processing batch 3122/11884 - Loss: 29.5379\n",
      "Processing batch 3123/11884 - Loss: 29.0998\n",
      "Processing batch 3124/11884 - Loss: 32.2534\n",
      "Processing batch 3125/11884 - Loss: 30.7862\n",
      "Processing batch 3126/11884 - Loss: 31.5788\n",
      "Processing batch 3127/11884 - Loss: 30.0061\n",
      "Processing batch 3128/11884 - Loss: 31.2282\n",
      "Processing batch 3129/11884 - Loss: 31.2880\n",
      "Processing batch 3130/11884 - Loss: 30.3330\n",
      "Processing batch 3131/11884 - Loss: 30.3280\n",
      "Processing batch 3132/11884 - Loss: 31.3818\n",
      "Processing batch 3133/11884 - Loss: 30.1309\n",
      "Processing batch 3134/11884 - Loss: 29.9510\n",
      "Processing batch 3135/11884 - Loss: 30.4553\n",
      "Processing batch 3136/11884 - Loss: 31.0239\n",
      "Processing batch 3137/11884 - Loss: 30.6385\n",
      "Processing batch 3138/11884 - Loss: 31.6098\n",
      "Processing batch 3139/11884 - Loss: 29.4496\n",
      "Processing batch 3140/11884 - Loss: 31.0130\n",
      "Processing batch 3141/11884 - Loss: 30.7468\n",
      "Processing batch 3142/11884 - Loss: 30.5400\n",
      "Processing batch 3143/11884 - Loss: 31.0907\n",
      "Processing batch 3144/11884 - Loss: 31.6270\n",
      "Processing batch 3145/11884 - Loss: 31.1194\n",
      "Processing batch 3146/11884 - Loss: 28.8862\n",
      "Processing batch 3147/11884 - Loss: 31.1560\n",
      "Processing batch 3148/11884 - Loss: 29.9418\n",
      "Processing batch 3149/11884 - Loss: 31.0232\n",
      "Processing batch 3150/11884 - Loss: 29.1678\n",
      "Processing batch 3151/11884 - Loss: 30.7550\n",
      "Processing batch 3152/11884 - Loss: 30.2636\n",
      "Processing batch 3153/11884 - Loss: 29.0880\n",
      "Processing batch 3154/11884 - Loss: 30.6494\n",
      "Processing batch 3155/11884 - Loss: 29.8555\n",
      "Processing batch 3156/11884 - Loss: 29.7593\n",
      "Processing batch 3157/11884 - Loss: 28.8524\n",
      "Processing batch 3158/11884 - Loss: 30.8770\n",
      "Processing batch 3159/11884 - Loss: 31.2886\n",
      "Processing batch 3160/11884 - Loss: 30.5817\n",
      "Processing batch 3161/11884 - Loss: 30.6742\n",
      "Processing batch 3162/11884 - Loss: 30.2042\n",
      "Processing batch 3163/11884 - Loss: 30.7863\n",
      "Processing batch 3164/11884 - Loss: 31.8235\n",
      "Processing batch 3165/11884 - Loss: 30.2860\n",
      "Processing batch 3166/11884 - Loss: 31.0519\n",
      "Processing batch 3167/11884 - Loss: 30.0359\n",
      "Processing batch 3168/11884 - Loss: 30.7890\n",
      "Processing batch 3169/11884 - Loss: 30.2924\n",
      "Processing batch 3170/11884 - Loss: 30.5474\n",
      "Processing batch 3171/11884 - Loss: 30.2856\n",
      "Processing batch 3172/11884 - Loss: 31.5244\n",
      "Processing batch 3173/11884 - Loss: 30.6631\n",
      "Processing batch 3174/11884 - Loss: 30.0711\n",
      "Processing batch 3175/11884 - Loss: 31.1273\n",
      "Processing batch 3176/11884 - Loss: 29.9132\n",
      "Processing batch 3177/11884 - Loss: 30.2118\n",
      "Processing batch 3178/11884 - Loss: 28.9792\n",
      "Processing batch 3179/11884 - Loss: 31.2157\n",
      "Processing batch 3180/11884 - Loss: 30.4370\n",
      "Processing batch 3181/11884 - Loss: 30.6589\n",
      "Processing batch 3182/11884 - Loss: 30.7003\n",
      "Processing batch 3183/11884 - Loss: 30.7961\n",
      "Processing batch 3184/11884 - Loss: 32.9071\n",
      "Processing batch 3185/11884 - Loss: 31.0622\n",
      "Processing batch 3186/11884 - Loss: 30.2792\n",
      "Processing batch 3187/11884 - Loss: 30.1075\n",
      "Processing batch 3188/11884 - Loss: 31.2165\n",
      "Processing batch 3189/11884 - Loss: 31.8007\n",
      "Processing batch 3190/11884 - Loss: 30.9103\n",
      "Processing batch 3191/11884 - Loss: 29.2971\n",
      "Processing batch 3192/11884 - Loss: 31.5725\n",
      "Processing batch 3193/11884 - Loss: 30.7682\n",
      "Processing batch 3194/11884 - Loss: 31.6523\n",
      "Processing batch 3195/11884 - Loss: 30.3549\n",
      "Processing batch 3196/11884 - Loss: 29.7000\n",
      "Processing batch 3197/11884 - Loss: 30.6194\n",
      "Processing batch 3198/11884 - Loss: 30.9131\n",
      "Processing batch 3199/11884 - Loss: 32.0197\n",
      "Processing batch 3200/11884 - Loss: 29.9585\n",
      "Processing batch 3201/11884 - Loss: 32.5327\n",
      "Processing batch 3202/11884 - Loss: 30.4355\n",
      "Processing batch 3203/11884 - Loss: 30.8425\n",
      "Processing batch 3204/11884 - Loss: 30.0491\n",
      "Processing batch 3205/11884 - Loss: 30.6765\n",
      "Processing batch 3206/11884 - Loss: 29.4726\n",
      "Processing batch 3207/11884 - Loss: 30.6419\n",
      "Processing batch 3208/11884 - Loss: 30.5554\n",
      "Processing batch 3209/11884 - Loss: 30.8964\n",
      "Processing batch 3210/11884 - Loss: 30.6990\n",
      "Processing batch 3211/11884 - Loss: 31.3239\n",
      "Processing batch 3212/11884 - Loss: 30.6646\n",
      "Processing batch 3213/11884 - Loss: 30.7218\n",
      "Processing batch 3214/11884 - Loss: 31.7255\n",
      "Processing batch 3215/11884 - Loss: 30.7888\n",
      "Processing batch 3216/11884 - Loss: 30.4172\n",
      "Processing batch 3217/11884 - Loss: 33.4873\n",
      "Processing batch 3218/11884 - Loss: 29.8953\n",
      "Processing batch 3219/11884 - Loss: 31.5559\n",
      "Processing batch 3220/11884 - Loss: 30.4368\n",
      "Processing batch 3221/11884 - Loss: 30.6172\n",
      "Processing batch 3222/11884 - Loss: 31.6792\n",
      "Processing batch 3223/11884 - Loss: 29.4215\n",
      "Processing batch 3224/11884 - Loss: 30.8759\n",
      "Processing batch 3225/11884 - Loss: 28.9758\n",
      "Processing batch 3226/11884 - Loss: 31.5992\n",
      "Processing batch 3227/11884 - Loss: 30.8155\n",
      "Processing batch 3228/11884 - Loss: 29.3707\n",
      "Processing batch 3229/11884 - Loss: 30.8313\n",
      "Processing batch 3230/11884 - Loss: 30.4121\n",
      "Processing batch 3231/11884 - Loss: 31.5346\n",
      "Processing batch 3232/11884 - Loss: 30.5353\n",
      "Processing batch 3233/11884 - Loss: 30.1133\n",
      "Processing batch 3234/11884 - Loss: 30.3681\n",
      "Processing batch 3235/11884 - Loss: 30.9899\n",
      "Processing batch 3236/11884 - Loss: 29.8087\n",
      "Processing batch 3237/11884 - Loss: 30.4939\n",
      "Processing batch 3238/11884 - Loss: 30.8936\n",
      "Processing batch 3239/11884 - Loss: 30.1244\n",
      "Processing batch 3240/11884 - Loss: 31.5581\n",
      "Processing batch 3241/11884 - Loss: 31.3553\n",
      "Processing batch 3242/11884 - Loss: 30.0520\n",
      "Processing batch 3243/11884 - Loss: 30.9021\n",
      "Processing batch 3244/11884 - Loss: 29.8530\n",
      "Processing batch 3245/11884 - Loss: 30.4442\n",
      "Processing batch 3246/11884 - Loss: 29.2775\n",
      "Processing batch 3247/11884 - Loss: 30.2863\n",
      "Processing batch 3248/11884 - Loss: 30.8633\n",
      "Processing batch 3249/11884 - Loss: 31.2328\n",
      "Processing batch 3250/11884 - Loss: 31.6249\n",
      "Processing batch 3251/11884 - Loss: 29.8281\n",
      "Processing batch 3252/11884 - Loss: 31.5057\n",
      "Processing batch 3253/11884 - Loss: 29.6223\n",
      "Processing batch 3254/11884 - Loss: 28.6267\n",
      "Processing batch 3255/11884 - Loss: 30.6045\n",
      "Processing batch 3256/11884 - Loss: 31.5109\n",
      "Processing batch 3257/11884 - Loss: 30.7793\n",
      "Processing batch 3258/11884 - Loss: 30.7491\n",
      "Processing batch 3259/11884 - Loss: 29.7153\n",
      "Processing batch 3260/11884 - Loss: 30.9217\n",
      "Processing batch 3261/11884 - Loss: 31.5256\n",
      "Processing batch 3262/11884 - Loss: 31.5395\n",
      "Processing batch 3263/11884 - Loss: 30.4071\n",
      "Processing batch 3264/11884 - Loss: 31.7491\n",
      "Processing batch 3265/11884 - Loss: 31.6205\n",
      "Processing batch 3266/11884 - Loss: 29.9415\n",
      "Processing batch 3267/11884 - Loss: 31.3076\n",
      "Processing batch 3268/11884 - Loss: 30.2564\n",
      "Processing batch 3269/11884 - Loss: 30.2082\n",
      "Processing batch 3270/11884 - Loss: 29.8909\n",
      "Processing batch 3271/11884 - Loss: 29.2493\n",
      "Processing batch 3272/11884 - Loss: 28.6681\n",
      "Processing batch 3273/11884 - Loss: 30.3619\n",
      "Processing batch 3274/11884 - Loss: 30.3977\n",
      "Processing batch 3275/11884 - Loss: 30.6950\n",
      "Processing batch 3276/11884 - Loss: 31.0929\n",
      "Processing batch 3277/11884 - Loss: 30.2044\n",
      "Processing batch 3278/11884 - Loss: 30.4491\n",
      "Processing batch 3279/11884 - Loss: 30.4448\n",
      "Processing batch 3280/11884 - Loss: 30.2271\n",
      "Processing batch 3281/11884 - Loss: 29.4478\n",
      "Processing batch 3282/11884 - Loss: 31.1062\n",
      "Processing batch 3283/11884 - Loss: 30.1628\n",
      "Processing batch 3284/11884 - Loss: 31.5179\n",
      "Processing batch 3285/11884 - Loss: 31.0940\n",
      "Processing batch 3286/11884 - Loss: 30.6244\n",
      "Processing batch 3287/11884 - Loss: 28.9005\n",
      "Processing batch 3288/11884 - Loss: 30.1484\n",
      "Processing batch 3289/11884 - Loss: 29.9388\n",
      "Processing batch 3290/11884 - Loss: 30.2786\n",
      "Processing batch 3291/11884 - Loss: 31.0793\n",
      "Processing batch 3292/11884 - Loss: 29.9311\n",
      "Processing batch 3293/11884 - Loss: 30.8747\n",
      "Processing batch 3294/11884 - Loss: 29.8283\n",
      "Processing batch 3295/11884 - Loss: 29.6000\n",
      "Processing batch 3296/11884 - Loss: 30.3941\n",
      "Processing batch 3297/11884 - Loss: 31.6867\n",
      "Processing batch 3298/11884 - Loss: 30.9476\n",
      "Processing batch 3299/11884 - Loss: 31.4209\n",
      "Processing batch 3300/11884 - Loss: 30.3336\n",
      "Processing batch 3301/11884 - Loss: 30.8399\n",
      "Processing batch 3302/11884 - Loss: 28.6814\n",
      "Processing batch 3303/11884 - Loss: 31.6788\n",
      "Processing batch 3304/11884 - Loss: 30.7130\n",
      "Processing batch 3305/11884 - Loss: 30.3627\n",
      "Processing batch 3306/11884 - Loss: 31.2613\n",
      "Processing batch 3307/11884 - Loss: 30.5916\n",
      "Processing batch 3308/11884 - Loss: 30.7838\n",
      "Processing batch 3309/11884 - Loss: 31.0901\n",
      "Processing batch 3310/11884 - Loss: 30.6712\n",
      "Processing batch 3311/11884 - Loss: 30.1978\n",
      "Processing batch 3312/11884 - Loss: 29.9475\n",
      "Processing batch 3313/11884 - Loss: 30.8743\n",
      "Processing batch 3314/11884 - Loss: 30.6958\n",
      "Processing batch 3315/11884 - Loss: 30.1651\n",
      "Processing batch 3316/11884 - Loss: 31.7517\n",
      "Processing batch 3317/11884 - Loss: 31.0357\n",
      "Processing batch 3318/11884 - Loss: 29.3412\n",
      "Processing batch 3319/11884 - Loss: 30.6429\n",
      "Processing batch 3320/11884 - Loss: 29.4920\n",
      "Processing batch 3321/11884 - Loss: 30.7670\n",
      "Processing batch 3322/11884 - Loss: 30.6863\n",
      "Processing batch 3323/11884 - Loss: 29.9715\n",
      "Processing batch 3324/11884 - Loss: 30.7854\n",
      "Processing batch 3325/11884 - Loss: 30.7504\n",
      "Processing batch 3326/11884 - Loss: 30.0117\n",
      "Processing batch 3327/11884 - Loss: 30.4404\n",
      "Processing batch 3328/11884 - Loss: 30.5071\n",
      "Processing batch 3329/11884 - Loss: 30.2148\n",
      "Processing batch 3330/11884 - Loss: 29.7210\n",
      "Processing batch 3331/11884 - Loss: 29.0452\n",
      "Processing batch 3332/11884 - Loss: 32.0858\n",
      "Processing batch 3333/11884 - Loss: 30.5251\n",
      "Processing batch 3334/11884 - Loss: 30.0815\n",
      "Processing batch 3335/11884 - Loss: 31.0306\n",
      "Processing batch 3336/11884 - Loss: 30.8732\n",
      "Processing batch 3337/11884 - Loss: 29.0052\n",
      "Processing batch 3338/11884 - Loss: 29.2664\n",
      "Processing batch 3339/11884 - Loss: 30.3727\n",
      "Processing batch 3340/11884 - Loss: 30.2344\n",
      "Processing batch 3341/11884 - Loss: 30.8899\n",
      "Processing batch 3342/11884 - Loss: 31.3934\n",
      "Processing batch 3343/11884 - Loss: 30.3629\n",
      "Processing batch 3344/11884 - Loss: 30.0769\n",
      "Processing batch 3345/11884 - Loss: 31.3323\n",
      "Processing batch 3346/11884 - Loss: 31.0005\n",
      "Processing batch 3347/11884 - Loss: 31.3414\n",
      "Processing batch 3348/11884 - Loss: 30.5039\n",
      "Processing batch 3349/11884 - Loss: 30.6699\n",
      "Processing batch 3350/11884 - Loss: 30.4534\n",
      "Processing batch 3351/11884 - Loss: 29.6058\n",
      "Processing batch 3352/11884 - Loss: 31.6794\n",
      "Processing batch 3353/11884 - Loss: 31.4642\n",
      "Processing batch 3354/11884 - Loss: 29.9811\n",
      "Processing batch 3355/11884 - Loss: 29.5548\n",
      "Processing batch 3356/11884 - Loss: 30.4701\n",
      "Processing batch 3357/11884 - Loss: 31.7961\n",
      "Processing batch 3358/11884 - Loss: 29.1493\n",
      "Processing batch 3359/11884 - Loss: 30.1790\n",
      "Processing batch 3360/11884 - Loss: 30.2648\n",
      "Processing batch 3361/11884 - Loss: 31.9469\n",
      "Processing batch 3362/11884 - Loss: 31.4908\n",
      "Processing batch 3363/11884 - Loss: 31.1822\n",
      "Processing batch 3364/11884 - Loss: 30.8909\n",
      "Processing batch 3365/11884 - Loss: 31.3437\n",
      "Processing batch 3366/11884 - Loss: 30.8891\n",
      "Processing batch 3367/11884 - Loss: 30.0083\n",
      "Processing batch 3368/11884 - Loss: 31.5838\n",
      "Processing batch 3369/11884 - Loss: 31.5354\n",
      "Processing batch 3370/11884 - Loss: 31.1377\n",
      "Processing batch 3371/11884 - Loss: 31.3571\n",
      "Processing batch 3372/11884 - Loss: 30.8310\n",
      "Processing batch 3373/11884 - Loss: 31.8347\n",
      "Processing batch 3374/11884 - Loss: 31.0300\n",
      "Processing batch 3375/11884 - Loss: 29.9874\n",
      "Processing batch 3376/11884 - Loss: 29.5884\n",
      "Processing batch 3377/11884 - Loss: 30.6316\n",
      "Processing batch 3378/11884 - Loss: 30.0855\n",
      "Processing batch 3379/11884 - Loss: 30.6862\n",
      "Processing batch 3380/11884 - Loss: 30.4022\n",
      "Processing batch 3381/11884 - Loss: 29.8492\n",
      "Processing batch 3382/11884 - Loss: 30.6401\n",
      "Processing batch 3383/11884 - Loss: 30.4624\n",
      "Processing batch 3384/11884 - Loss: 30.8457\n",
      "Processing batch 3385/11884 - Loss: 30.4731\n",
      "Processing batch 3386/11884 - Loss: 31.4000\n",
      "Processing batch 3387/11884 - Loss: 30.6995\n",
      "Processing batch 3388/11884 - Loss: 31.2688\n",
      "Processing batch 3389/11884 - Loss: 29.9340\n",
      "Processing batch 3390/11884 - Loss: 30.6190\n",
      "Processing batch 3391/11884 - Loss: 31.3426\n",
      "Processing batch 3392/11884 - Loss: 30.6979\n",
      "Processing batch 3393/11884 - Loss: 29.2484\n",
      "Processing batch 3394/11884 - Loss: 30.0053\n",
      "Processing batch 3395/11884 - Loss: 29.5013\n",
      "Processing batch 3396/11884 - Loss: 32.3610\n",
      "Processing batch 3397/11884 - Loss: 30.7288\n",
      "Processing batch 3398/11884 - Loss: 29.8326\n",
      "Processing batch 3399/11884 - Loss: 30.7543\n",
      "Processing batch 3400/11884 - Loss: 30.8242\n",
      "Processing batch 3401/11884 - Loss: 31.3004\n",
      "Processing batch 3402/11884 - Loss: 31.5992\n",
      "Processing batch 3403/11884 - Loss: 30.9158\n",
      "Processing batch 3404/11884 - Loss: 31.9365\n",
      "Processing batch 3405/11884 - Loss: 30.1498\n",
      "Processing batch 3406/11884 - Loss: 31.2693\n",
      "Processing batch 3407/11884 - Loss: 29.8880\n",
      "Processing batch 3408/11884 - Loss: 31.9257\n",
      "Processing batch 3409/11884 - Loss: 31.3274\n",
      "Processing batch 3410/11884 - Loss: 31.0162\n",
      "Processing batch 3411/11884 - Loss: 30.7071\n",
      "Processing batch 3412/11884 - Loss: 30.3813\n",
      "Processing batch 3413/11884 - Loss: 30.2715\n",
      "Processing batch 3414/11884 - Loss: 30.5808\n",
      "Processing batch 3415/11884 - Loss: 30.5016\n",
      "Processing batch 3416/11884 - Loss: 30.0435\n",
      "Processing batch 3417/11884 - Loss: 28.8210\n",
      "Processing batch 3418/11884 - Loss: 30.0529\n",
      "Processing batch 3419/11884 - Loss: 30.6973\n",
      "Processing batch 3420/11884 - Loss: 30.2545\n",
      "Processing batch 3421/11884 - Loss: 30.5342\n",
      "Processing batch 3422/11884 - Loss: 31.2051\n",
      "Processing batch 3423/11884 - Loss: 30.8522\n",
      "Processing batch 3424/11884 - Loss: 30.8394\n",
      "Processing batch 3425/11884 - Loss: 31.5956\n",
      "Processing batch 3426/11884 - Loss: 29.5940\n",
      "Processing batch 3427/11884 - Loss: 30.4573\n",
      "Processing batch 3428/11884 - Loss: 32.6161\n",
      "Processing batch 3429/11884 - Loss: 30.0892\n",
      "Processing batch 3430/11884 - Loss: 30.7353\n",
      "Processing batch 3431/11884 - Loss: 30.8194\n",
      "Processing batch 3432/11884 - Loss: 30.7413\n",
      "Processing batch 3433/11884 - Loss: 29.2127\n",
      "Processing batch 3434/11884 - Loss: 28.8719\n",
      "Processing batch 3435/11884 - Loss: 30.4322\n",
      "Processing batch 3436/11884 - Loss: 32.1677\n",
      "Processing batch 3437/11884 - Loss: 31.1482\n",
      "Processing batch 3438/11884 - Loss: 30.5565\n",
      "Processing batch 3439/11884 - Loss: 31.3766\n",
      "Processing batch 3440/11884 - Loss: 30.1639\n",
      "Processing batch 3441/11884 - Loss: 30.4015\n",
      "Processing batch 3442/11884 - Loss: 30.6108\n",
      "Processing batch 3443/11884 - Loss: 31.5066\n",
      "Processing batch 3444/11884 - Loss: 30.4586\n",
      "Processing batch 3445/11884 - Loss: 30.5165\n",
      "Processing batch 3446/11884 - Loss: 29.8184\n",
      "Processing batch 3447/11884 - Loss: 31.5294\n",
      "Processing batch 3448/11884 - Loss: 29.8109\n",
      "Processing batch 3449/11884 - Loss: 31.1022\n",
      "Processing batch 3450/11884 - Loss: 30.2891\n",
      "Processing batch 3451/11884 - Loss: 31.6416\n",
      "Processing batch 3452/11884 - Loss: 31.0860\n",
      "Processing batch 3453/11884 - Loss: 30.6394\n",
      "Processing batch 3454/11884 - Loss: 30.7987\n",
      "Processing batch 3455/11884 - Loss: 31.3894\n",
      "Processing batch 3456/11884 - Loss: 30.7527\n",
      "Processing batch 3457/11884 - Loss: 29.9765\n",
      "Processing batch 3458/11884 - Loss: 31.6136\n",
      "Processing batch 3459/11884 - Loss: 29.8131\n",
      "Processing batch 3460/11884 - Loss: 30.8801\n",
      "Processing batch 3461/11884 - Loss: 30.5941\n",
      "Processing batch 3462/11884 - Loss: 31.6206\n",
      "Processing batch 3463/11884 - Loss: 30.7019\n",
      "Processing batch 3464/11884 - Loss: 31.2349\n",
      "Processing batch 3465/11884 - Loss: 31.0520\n",
      "Processing batch 3466/11884 - Loss: 30.5683\n",
      "Processing batch 3467/11884 - Loss: 29.8728\n",
      "Processing batch 3468/11884 - Loss: 29.9691\n",
      "Processing batch 3469/11884 - Loss: 30.8380\n",
      "Processing batch 3470/11884 - Loss: 30.6747\n",
      "Processing batch 3471/11884 - Loss: 29.7365\n",
      "Processing batch 3472/11884 - Loss: 31.0054\n",
      "Processing batch 3473/11884 - Loss: 31.1240\n",
      "Processing batch 3474/11884 - Loss: 31.2067\n",
      "Processing batch 3475/11884 - Loss: 31.7779\n",
      "Processing batch 3476/11884 - Loss: 29.3265\n",
      "Processing batch 3477/11884 - Loss: 28.9207\n",
      "Processing batch 3478/11884 - Loss: 30.3185\n",
      "Processing batch 3479/11884 - Loss: 31.8425\n",
      "Processing batch 3480/11884 - Loss: 30.8342\n",
      "Processing batch 3481/11884 - Loss: 31.1089\n",
      "Processing batch 3482/11884 - Loss: 32.1228\n",
      "Processing batch 3483/11884 - Loss: 31.6462\n",
      "Processing batch 3484/11884 - Loss: 32.5194\n",
      "Processing batch 3485/11884 - Loss: 31.2726\n",
      "Processing batch 3486/11884 - Loss: 31.0259\n",
      "Processing batch 3487/11884 - Loss: 29.9097\n",
      "Processing batch 3488/11884 - Loss: 30.0686\n",
      "Processing batch 3489/11884 - Loss: 30.5450\n",
      "Processing batch 3490/11884 - Loss: 29.0243\n",
      "Processing batch 3491/11884 - Loss: 32.6001\n",
      "Processing batch 3492/11884 - Loss: 29.8813\n",
      "Processing batch 3493/11884 - Loss: 31.2328\n",
      "Processing batch 3494/11884 - Loss: 30.3252\n",
      "Processing batch 3495/11884 - Loss: 31.4641\n",
      "Processing batch 3496/11884 - Loss: 30.8078\n",
      "Processing batch 3497/11884 - Loss: 29.6429\n",
      "Processing batch 3498/11884 - Loss: 29.7334\n",
      "Processing batch 3499/11884 - Loss: 30.3497\n",
      "Processing batch 3500/11884 - Loss: 30.3997\n",
      "Processing batch 3501/11884 - Loss: 29.4617\n",
      "Processing batch 3502/11884 - Loss: 30.0387\n",
      "Processing batch 3503/11884 - Loss: 31.0411\n",
      "Processing batch 3504/11884 - Loss: 30.6760\n",
      "Processing batch 3505/11884 - Loss: 30.2280\n",
      "Processing batch 3506/11884 - Loss: 30.8544\n",
      "Processing batch 3507/11884 - Loss: 31.3127\n",
      "Processing batch 3508/11884 - Loss: 31.9285\n",
      "Processing batch 3509/11884 - Loss: 29.9726\n",
      "Processing batch 3510/11884 - Loss: 30.8600\n",
      "Processing batch 3511/11884 - Loss: 30.8189\n",
      "Processing batch 3512/11884 - Loss: 30.8739\n",
      "Processing batch 3513/11884 - Loss: 30.0254\n",
      "Processing batch 3514/11884 - Loss: 30.0871\n",
      "Processing batch 3515/11884 - Loss: 29.9871\n",
      "Processing batch 3516/11884 - Loss: 30.7704\n",
      "Processing batch 3517/11884 - Loss: 30.7338\n",
      "Processing batch 3518/11884 - Loss: 29.8551\n",
      "Processing batch 3519/11884 - Loss: 30.9732\n",
      "Processing batch 3520/11884 - Loss: 31.0009\n",
      "Processing batch 3521/11884 - Loss: 30.7685\n",
      "Processing batch 3522/11884 - Loss: 31.3153\n",
      "Processing batch 3523/11884 - Loss: 30.7565\n",
      "Processing batch 3524/11884 - Loss: 31.2251\n",
      "Processing batch 3525/11884 - Loss: 30.4171\n",
      "Processing batch 3526/11884 - Loss: 29.3236\n",
      "Processing batch 3527/11884 - Loss: 29.9001\n",
      "Processing batch 3528/11884 - Loss: 31.0207\n",
      "Processing batch 3529/11884 - Loss: 30.8773\n",
      "Processing batch 3530/11884 - Loss: 30.2699\n",
      "Processing batch 3531/11884 - Loss: 31.0592\n",
      "Processing batch 3532/11884 - Loss: 30.8454\n",
      "Processing batch 3533/11884 - Loss: 31.6194\n",
      "Processing batch 3534/11884 - Loss: 30.3353\n",
      "Processing batch 3535/11884 - Loss: 30.6673\n",
      "Processing batch 3536/11884 - Loss: 30.0853\n",
      "Processing batch 3537/11884 - Loss: 30.1992\n",
      "Processing batch 3538/11884 - Loss: 30.0609\n",
      "Processing batch 3539/11884 - Loss: 30.1044\n",
      "Processing batch 3540/11884 - Loss: 31.4900\n",
      "Processing batch 3541/11884 - Loss: 29.4839\n",
      "Processing batch 3542/11884 - Loss: 30.0916\n",
      "Processing batch 3543/11884 - Loss: 30.5434\n",
      "Processing batch 3544/11884 - Loss: 30.6733\n",
      "Processing batch 3545/11884 - Loss: 30.4552\n",
      "Processing batch 3546/11884 - Loss: 31.0426\n",
      "Processing batch 3547/11884 - Loss: 29.4506\n",
      "Processing batch 3548/11884 - Loss: 31.7909\n",
      "Processing batch 3549/11884 - Loss: 29.6208\n",
      "Processing batch 3550/11884 - Loss: 31.2192\n",
      "Processing batch 3551/11884 - Loss: 28.8836\n",
      "Processing batch 3552/11884 - Loss: 29.5679\n",
      "Processing batch 3553/11884 - Loss: 30.6616\n",
      "Processing batch 3554/11884 - Loss: 29.5266\n",
      "Processing batch 3555/11884 - Loss: 30.2390\n",
      "Processing batch 3556/11884 - Loss: 30.4012\n",
      "Processing batch 3557/11884 - Loss: 29.0988\n",
      "Processing batch 3558/11884 - Loss: 32.0530\n",
      "Processing batch 3559/11884 - Loss: 29.9201\n",
      "Processing batch 3560/11884 - Loss: 31.0301\n",
      "Processing batch 3561/11884 - Loss: 30.0469\n",
      "Processing batch 3562/11884 - Loss: 31.3707\n",
      "Processing batch 3563/11884 - Loss: 31.4300\n",
      "Processing batch 3564/11884 - Loss: 31.1113\n",
      "Processing batch 3565/11884 - Loss: 29.2737\n",
      "Processing batch 3566/11884 - Loss: 29.5619\n",
      "Processing batch 3567/11884 - Loss: 30.9975\n",
      "Processing batch 3568/11884 - Loss: 29.8238\n",
      "Processing batch 3569/11884 - Loss: 29.4636\n",
      "Processing batch 3570/11884 - Loss: 29.0460\n",
      "Processing batch 3571/11884 - Loss: 29.4330\n",
      "Processing batch 3572/11884 - Loss: 29.7043\n",
      "Processing batch 3573/11884 - Loss: 30.0084\n",
      "Processing batch 3574/11884 - Loss: 28.7347\n",
      "Processing batch 3575/11884 - Loss: 30.1947\n",
      "Processing batch 3576/11884 - Loss: 30.9655\n",
      "Processing batch 3577/11884 - Loss: 30.2539\n",
      "Processing batch 3578/11884 - Loss: 30.3548\n",
      "Processing batch 3579/11884 - Loss: 30.2661\n",
      "Processing batch 3580/11884 - Loss: 30.3460\n",
      "Processing batch 3581/11884 - Loss: 28.6888\n",
      "Processing batch 3582/11884 - Loss: 30.5095\n",
      "Processing batch 3583/11884 - Loss: 29.7980\n",
      "Processing batch 3584/11884 - Loss: 30.7072\n",
      "Processing batch 3585/11884 - Loss: 31.5288\n",
      "Processing batch 3586/11884 - Loss: 30.8905\n",
      "Processing batch 3587/11884 - Loss: 29.9745\n",
      "Processing batch 3588/11884 - Loss: 30.5495\n",
      "Processing batch 3589/11884 - Loss: 29.8923\n",
      "Processing batch 3590/11884 - Loss: 30.4406\n",
      "Processing batch 3591/11884 - Loss: 31.3529\n",
      "Processing batch 3592/11884 - Loss: 30.9690\n",
      "Processing batch 3593/11884 - Loss: 29.8868\n",
      "Processing batch 3594/11884 - Loss: 30.4044\n",
      "Processing batch 3595/11884 - Loss: 31.7367\n",
      "Processing batch 3596/11884 - Loss: 30.9126\n",
      "Processing batch 3597/11884 - Loss: 31.7943\n",
      "Processing batch 3598/11884 - Loss: 29.2964\n",
      "Processing batch 3599/11884 - Loss: 30.2823\n",
      "Processing batch 3600/11884 - Loss: 30.4440\n",
      "Processing batch 3601/11884 - Loss: 30.9470\n",
      "Processing batch 3602/11884 - Loss: 30.4729\n",
      "Processing batch 3603/11884 - Loss: 29.7644\n",
      "Processing batch 3604/11884 - Loss: 27.8073\n",
      "Processing batch 3605/11884 - Loss: 30.3460\n",
      "Processing batch 3606/11884 - Loss: 30.1312\n",
      "Processing batch 3607/11884 - Loss: 30.2950\n",
      "Processing batch 3608/11884 - Loss: 30.1770\n",
      "Processing batch 3609/11884 - Loss: 30.8880\n",
      "Processing batch 3610/11884 - Loss: 29.2112\n",
      "Processing batch 3611/11884 - Loss: 29.0491\n",
      "Processing batch 3612/11884 - Loss: 31.3554\n",
      "Processing batch 3613/11884 - Loss: 30.6429\n",
      "Processing batch 3614/11884 - Loss: 32.2200\n",
      "Processing batch 3615/11884 - Loss: 31.2440\n",
      "Processing batch 3616/11884 - Loss: 30.2759\n",
      "Processing batch 3617/11884 - Loss: 30.5902\n",
      "Processing batch 3618/11884 - Loss: 31.0938\n",
      "Processing batch 3619/11884 - Loss: 31.2033\n",
      "Processing batch 3620/11884 - Loss: 29.8219\n",
      "Processing batch 3621/11884 - Loss: 30.4736\n",
      "Processing batch 3622/11884 - Loss: 31.2592\n",
      "Processing batch 3623/11884 - Loss: 30.4336\n",
      "Processing batch 3624/11884 - Loss: 30.5041\n",
      "Processing batch 3625/11884 - Loss: 30.9609\n",
      "Processing batch 3626/11884 - Loss: 31.3145\n",
      "Processing batch 3627/11884 - Loss: 30.5802\n",
      "Processing batch 3628/11884 - Loss: 30.0108\n",
      "Processing batch 3629/11884 - Loss: 29.9422\n",
      "Processing batch 3630/11884 - Loss: 30.7566\n",
      "Processing batch 3631/11884 - Loss: 31.4386\n",
      "Processing batch 3632/11884 - Loss: 30.2880\n",
      "Processing batch 3633/11884 - Loss: 31.2282\n",
      "Processing batch 3634/11884 - Loss: 30.0344\n",
      "Processing batch 3635/11884 - Loss: 30.0285\n",
      "Processing batch 3636/11884 - Loss: 30.2653\n",
      "Processing batch 3637/11884 - Loss: 29.7916\n",
      "Processing batch 3638/11884 - Loss: 29.8765\n",
      "Processing batch 3639/11884 - Loss: 30.7835\n",
      "Processing batch 3640/11884 - Loss: 30.8308\n",
      "Processing batch 3641/11884 - Loss: 31.4807\n",
      "Processing batch 3642/11884 - Loss: 31.8606\n",
      "Processing batch 3643/11884 - Loss: 31.2994\n",
      "Processing batch 3644/11884 - Loss: 30.4695\n",
      "Processing batch 3645/11884 - Loss: 30.5165\n",
      "Processing batch 3646/11884 - Loss: 31.2789\n",
      "Processing batch 3647/11884 - Loss: 29.8135\n",
      "Processing batch 3648/11884 - Loss: 31.2718\n",
      "Processing batch 3649/11884 - Loss: 31.1149\n",
      "Processing batch 3650/11884 - Loss: 29.6275\n",
      "Processing batch 3651/11884 - Loss: 30.3874\n",
      "Processing batch 3652/11884 - Loss: 30.8190\n",
      "Processing batch 3653/11884 - Loss: 29.9478\n",
      "Processing batch 3654/11884 - Loss: 30.7903\n",
      "Processing batch 3655/11884 - Loss: 31.0899\n",
      "Processing batch 3656/11884 - Loss: 30.7474\n",
      "Processing batch 3657/11884 - Loss: 30.5923\n",
      "Processing batch 3658/11884 - Loss: 30.8802\n",
      "Processing batch 3659/11884 - Loss: 32.2984\n",
      "Processing batch 3660/11884 - Loss: 30.4018\n",
      "Processing batch 3661/11884 - Loss: 30.4655\n",
      "Processing batch 3662/11884 - Loss: 31.5220\n",
      "Processing batch 3663/11884 - Loss: 31.8553\n",
      "Processing batch 3664/11884 - Loss: 31.2573\n",
      "Processing batch 3665/11884 - Loss: 30.2766\n",
      "Processing batch 3666/11884 - Loss: 30.8963\n",
      "Processing batch 3667/11884 - Loss: 30.1116\n",
      "Processing batch 3668/11884 - Loss: 30.7502\n",
      "Processing batch 3669/11884 - Loss: 31.4387\n",
      "Processing batch 3670/11884 - Loss: 31.0905\n",
      "Processing batch 3671/11884 - Loss: 29.9068\n",
      "Processing batch 3672/11884 - Loss: 30.9094\n",
      "Processing batch 3673/11884 - Loss: 30.3602\n",
      "Processing batch 3674/11884 - Loss: 29.4156\n",
      "Processing batch 3675/11884 - Loss: 30.9044\n",
      "Processing batch 3676/11884 - Loss: 31.0228\n",
      "Processing batch 3677/11884 - Loss: 29.3494\n",
      "Processing batch 3678/11884 - Loss: 31.0260\n",
      "Processing batch 3679/11884 - Loss: 30.0892\n",
      "Processing batch 3680/11884 - Loss: 31.1624\n",
      "Processing batch 3681/11884 - Loss: 30.2782\n",
      "Processing batch 3682/11884 - Loss: 31.8153\n",
      "Processing batch 3683/11884 - Loss: 30.5199\n",
      "Processing batch 3684/11884 - Loss: 32.7423\n",
      "Processing batch 3685/11884 - Loss: 30.3257\n",
      "Processing batch 3686/11884 - Loss: 30.4625\n",
      "Processing batch 3687/11884 - Loss: 30.5571\n",
      "Processing batch 3688/11884 - Loss: 30.2527\n",
      "Processing batch 3689/11884 - Loss: 30.3194\n",
      "Processing batch 3690/11884 - Loss: 30.0677\n",
      "Processing batch 3691/11884 - Loss: 30.2210\n",
      "Processing batch 3692/11884 - Loss: 30.0331\n",
      "Processing batch 3693/11884 - Loss: 30.3358\n",
      "Processing batch 3694/11884 - Loss: 30.0308\n",
      "Processing batch 3695/11884 - Loss: 31.7943\n",
      "Processing batch 3696/11884 - Loss: 31.5739\n",
      "Processing batch 3697/11884 - Loss: 30.8835\n",
      "Processing batch 3698/11884 - Loss: 30.6360\n",
      "Processing batch 3699/11884 - Loss: 30.6166\n",
      "Processing batch 3700/11884 - Loss: 30.1691\n",
      "Processing batch 3701/11884 - Loss: 29.7080\n",
      "Processing batch 3702/11884 - Loss: 31.1923\n",
      "Processing batch 3703/11884 - Loss: 30.4840\n",
      "Processing batch 3704/11884 - Loss: 31.5174\n",
      "Processing batch 3705/11884 - Loss: 30.8220\n",
      "Processing batch 3706/11884 - Loss: 29.7143\n",
      "Processing batch 3707/11884 - Loss: 29.3490\n",
      "Processing batch 3708/11884 - Loss: 30.7888\n",
      "Processing batch 3709/11884 - Loss: 30.5370\n",
      "Processing batch 3710/11884 - Loss: 29.1216\n",
      "Processing batch 3711/11884 - Loss: 31.1593\n",
      "Processing batch 3712/11884 - Loss: 31.7492\n",
      "Processing batch 3713/11884 - Loss: 30.3541\n",
      "Processing batch 3714/11884 - Loss: 30.6000\n",
      "Processing batch 3715/11884 - Loss: 31.2680\n",
      "Processing batch 3716/11884 - Loss: 30.6153\n",
      "Processing batch 3717/11884 - Loss: 30.5953\n",
      "Processing batch 3718/11884 - Loss: 31.2505\n",
      "Processing batch 3719/11884 - Loss: 31.2110\n",
      "Processing batch 3720/11884 - Loss: 31.6099\n",
      "Processing batch 3721/11884 - Loss: 30.9338\n",
      "Processing batch 3722/11884 - Loss: 32.1491\n",
      "Processing batch 3723/11884 - Loss: 30.4287\n",
      "Processing batch 3724/11884 - Loss: 30.7586\n",
      "Processing batch 3725/11884 - Loss: 31.9906\n",
      "Processing batch 3726/11884 - Loss: 30.1336\n",
      "Processing batch 3727/11884 - Loss: 29.5601\n",
      "Processing batch 3728/11884 - Loss: 28.7611\n",
      "Processing batch 3729/11884 - Loss: 30.6485\n",
      "Processing batch 3730/11884 - Loss: 29.5461\n",
      "Processing batch 3731/11884 - Loss: 29.8470\n",
      "Processing batch 3732/11884 - Loss: 30.4019\n",
      "Processing batch 3733/11884 - Loss: 29.8698\n",
      "Processing batch 3734/11884 - Loss: 30.0553\n",
      "Processing batch 3735/11884 - Loss: 31.1093\n",
      "Processing batch 3736/11884 - Loss: 30.6821\n",
      "Processing batch 3737/11884 - Loss: 30.7428\n",
      "Processing batch 3738/11884 - Loss: 31.3915\n",
      "Processing batch 3739/11884 - Loss: 30.8477\n",
      "Processing batch 3740/11884 - Loss: 30.2601\n",
      "Processing batch 3741/11884 - Loss: 30.8954\n",
      "Processing batch 3742/11884 - Loss: 30.0827\n",
      "Processing batch 3743/11884 - Loss: 30.2941\n",
      "Processing batch 3744/11884 - Loss: 29.8914\n",
      "Processing batch 3745/11884 - Loss: 30.6618\n",
      "Processing batch 3746/11884 - Loss: 31.3599\n",
      "Processing batch 3747/11884 - Loss: 30.3752\n",
      "Processing batch 3748/11884 - Loss: 30.8653\n",
      "Processing batch 3749/11884 - Loss: 30.4727\n",
      "Processing batch 3750/11884 - Loss: 30.1045\n",
      "Processing batch 3751/11884 - Loss: 31.7231\n",
      "Processing batch 3752/11884 - Loss: 30.5716\n",
      "Processing batch 3753/11884 - Loss: 29.5052\n",
      "Processing batch 3754/11884 - Loss: 29.7404\n",
      "Processing batch 3755/11884 - Loss: 29.0222\n",
      "Processing batch 3756/11884 - Loss: 31.5209\n",
      "Processing batch 3757/11884 - Loss: 29.8518\n",
      "Processing batch 3758/11884 - Loss: 31.4014\n",
      "Processing batch 3759/11884 - Loss: 29.1239\n",
      "Processing batch 3760/11884 - Loss: 30.1196\n",
      "Processing batch 3761/11884 - Loss: 32.2676\n",
      "Processing batch 3762/11884 - Loss: 30.5409\n",
      "Processing batch 3763/11884 - Loss: 31.5805\n",
      "Processing batch 3764/11884 - Loss: 30.8191\n",
      "Processing batch 3765/11884 - Loss: 31.0254\n",
      "Processing batch 3766/11884 - Loss: 31.8944\n",
      "Processing batch 3767/11884 - Loss: 30.0999\n",
      "Processing batch 3768/11884 - Loss: 30.6529\n",
      "Processing batch 3769/11884 - Loss: 31.5372\n",
      "Processing batch 3770/11884 - Loss: 30.1692\n",
      "Processing batch 3771/11884 - Loss: 30.0692\n",
      "Processing batch 3772/11884 - Loss: 29.8480\n",
      "Processing batch 3773/11884 - Loss: 31.0543\n",
      "Processing batch 3774/11884 - Loss: 31.0015\n",
      "Processing batch 3775/11884 - Loss: 30.1080\n",
      "Processing batch 3776/11884 - Loss: 30.0142\n",
      "Processing batch 3777/11884 - Loss: 30.8174\n",
      "Processing batch 3778/11884 - Loss: 31.3857\n",
      "Processing batch 3779/11884 - Loss: 30.5903\n",
      "Processing batch 3780/11884 - Loss: 30.8669\n",
      "Processing batch 3781/11884 - Loss: 29.7819\n",
      "Processing batch 3782/11884 - Loss: 30.6415\n",
      "Processing batch 3783/11884 - Loss: 30.7760\n",
      "Processing batch 3784/11884 - Loss: 31.0255\n",
      "Processing batch 3785/11884 - Loss: 29.4842\n",
      "Processing batch 3786/11884 - Loss: 30.9283\n",
      "Processing batch 3787/11884 - Loss: 29.9663\n",
      "Processing batch 3788/11884 - Loss: 29.8244\n",
      "Processing batch 3789/11884 - Loss: 30.8120\n",
      "Processing batch 3790/11884 - Loss: 29.6592\n",
      "Processing batch 3791/11884 - Loss: 31.0022\n",
      "Processing batch 3792/11884 - Loss: 32.2823\n",
      "Processing batch 3793/11884 - Loss: 31.5934\n",
      "Processing batch 3794/11884 - Loss: 31.3983\n",
      "Processing batch 3795/11884 - Loss: 31.0127\n",
      "Processing batch 3796/11884 - Loss: 29.2578\n",
      "Processing batch 3797/11884 - Loss: 31.0356\n",
      "Processing batch 3798/11884 - Loss: 30.4370\n",
      "Processing batch 3799/11884 - Loss: 30.6704\n",
      "Processing batch 3800/11884 - Loss: 31.6533\n",
      "Processing batch 3801/11884 - Loss: 31.4763\n",
      "Processing batch 3802/11884 - Loss: 29.7239\n",
      "Processing batch 3803/11884 - Loss: 31.7933\n",
      "Processing batch 3804/11884 - Loss: 31.5871\n",
      "Processing batch 3805/11884 - Loss: 30.9457\n",
      "Processing batch 3806/11884 - Loss: 29.1915\n",
      "Processing batch 3807/11884 - Loss: 31.1064\n",
      "Processing batch 3808/11884 - Loss: 30.0284\n",
      "Processing batch 3809/11884 - Loss: 31.2794\n",
      "Processing batch 3810/11884 - Loss: 31.4952\n",
      "Processing batch 3811/11884 - Loss: 30.2864\n",
      "Processing batch 3812/11884 - Loss: 29.3068\n",
      "Processing batch 3813/11884 - Loss: 29.3819\n",
      "Processing batch 3814/11884 - Loss: 30.8834\n",
      "Processing batch 3815/11884 - Loss: 29.2926\n",
      "Processing batch 3816/11884 - Loss: 28.5234\n",
      "Processing batch 3817/11884 - Loss: 31.4281\n",
      "Processing batch 3818/11884 - Loss: 30.3489\n",
      "Processing batch 3819/11884 - Loss: 30.4591\n",
      "Processing batch 3820/11884 - Loss: 31.7857\n",
      "Processing batch 3821/11884 - Loss: 29.8052\n",
      "Processing batch 3822/11884 - Loss: 30.0646\n",
      "Processing batch 3823/11884 - Loss: 30.7487\n",
      "Processing batch 3824/11884 - Loss: 30.6433\n",
      "Processing batch 3825/11884 - Loss: 29.4245\n",
      "Processing batch 3826/11884 - Loss: 30.8830\n",
      "Processing batch 3827/11884 - Loss: 31.6167\n",
      "Processing batch 3828/11884 - Loss: 31.7701\n",
      "Processing batch 3829/11884 - Loss: 30.3056\n",
      "Processing batch 3830/11884 - Loss: 30.6011\n",
      "Processing batch 3831/11884 - Loss: 30.8686\n",
      "Processing batch 3832/11884 - Loss: 31.1561\n",
      "Processing batch 3833/11884 - Loss: 30.5761\n",
      "Processing batch 3834/11884 - Loss: 31.2405\n",
      "Processing batch 3835/11884 - Loss: 30.5168\n",
      "Processing batch 3836/11884 - Loss: 31.0824\n",
      "Processing batch 3837/11884 - Loss: 31.6106\n",
      "Processing batch 3838/11884 - Loss: 30.1065\n",
      "Processing batch 3839/11884 - Loss: 31.5331\n",
      "Processing batch 3840/11884 - Loss: 29.1541\n",
      "Processing batch 3841/11884 - Loss: 30.8964\n",
      "Processing batch 3842/11884 - Loss: 28.2980\n",
      "Processing batch 3843/11884 - Loss: 30.2672\n",
      "Processing batch 3844/11884 - Loss: 31.5198\n",
      "Processing batch 3845/11884 - Loss: 30.8673\n",
      "Processing batch 3846/11884 - Loss: 32.2333\n",
      "Processing batch 3847/11884 - Loss: 31.0735\n",
      "Processing batch 3848/11884 - Loss: 29.9908\n",
      "Processing batch 3849/11884 - Loss: 30.7422\n",
      "Processing batch 3850/11884 - Loss: 30.7821\n",
      "Processing batch 3851/11884 - Loss: 30.9988\n",
      "Processing batch 3852/11884 - Loss: 31.3374\n",
      "Processing batch 3853/11884 - Loss: 30.6200\n",
      "Processing batch 3854/11884 - Loss: 30.7713\n",
      "Processing batch 3855/11884 - Loss: 31.6841\n",
      "Processing batch 3856/11884 - Loss: 31.0276\n",
      "Processing batch 3857/11884 - Loss: 30.9480\n",
      "Processing batch 3858/11884 - Loss: 29.4628\n",
      "Processing batch 3859/11884 - Loss: 29.5884\n",
      "Processing batch 3860/11884 - Loss: 29.7180\n",
      "Processing batch 3861/11884 - Loss: 29.7031\n",
      "Processing batch 3862/11884 - Loss: 30.3655\n",
      "Processing batch 3863/11884 - Loss: 30.7465\n",
      "Processing batch 3864/11884 - Loss: 30.4234\n",
      "Processing batch 3865/11884 - Loss: 32.1609\n",
      "Processing batch 3866/11884 - Loss: 28.8684\n",
      "Processing batch 3867/11884 - Loss: 30.3641\n",
      "Processing batch 3868/11884 - Loss: 30.7167\n",
      "Processing batch 3869/11884 - Loss: 31.4018\n",
      "Processing batch 3870/11884 - Loss: 30.0879\n",
      "Processing batch 3871/11884 - Loss: 31.3367\n",
      "Processing batch 3872/11884 - Loss: 30.2188\n",
      "Processing batch 3873/11884 - Loss: 30.4661\n",
      "Processing batch 3874/11884 - Loss: 30.0271\n",
      "Processing batch 3875/11884 - Loss: 30.2300\n",
      "Processing batch 3876/11884 - Loss: 30.4824\n",
      "Processing batch 3877/11884 - Loss: 31.5338\n",
      "Processing batch 3878/11884 - Loss: 30.0953\n",
      "Processing batch 3879/11884 - Loss: 30.6270\n",
      "Processing batch 3880/11884 - Loss: 30.7793\n",
      "Processing batch 3881/11884 - Loss: 29.6635\n",
      "Processing batch 3882/11884 - Loss: 31.4527\n",
      "Processing batch 3883/11884 - Loss: 30.3734\n",
      "Processing batch 3884/11884 - Loss: 31.1836\n",
      "Processing batch 3885/11884 - Loss: 30.1296\n",
      "Processing batch 3886/11884 - Loss: 29.9667\n",
      "Processing batch 3887/11884 - Loss: 30.6543\n",
      "Processing batch 3888/11884 - Loss: 29.4891\n",
      "Processing batch 3889/11884 - Loss: 31.8476\n",
      "Processing batch 3890/11884 - Loss: 30.2808\n",
      "Processing batch 3891/11884 - Loss: 31.0090\n",
      "Processing batch 3892/11884 - Loss: 30.8205\n",
      "Processing batch 3893/11884 - Loss: 31.8734\n",
      "Processing batch 3894/11884 - Loss: 30.6910\n",
      "Processing batch 3895/11884 - Loss: 31.0363\n",
      "Processing batch 3896/11884 - Loss: 30.5808\n",
      "Processing batch 3897/11884 - Loss: 31.0545\n",
      "Processing batch 3898/11884 - Loss: 31.1727\n",
      "Processing batch 3899/11884 - Loss: 30.8954\n",
      "Processing batch 3900/11884 - Loss: 30.1881\n",
      "Processing batch 3901/11884 - Loss: 29.9943\n",
      "Processing batch 3902/11884 - Loss: 29.8223\n",
      "Processing batch 3903/11884 - Loss: 29.9152\n",
      "Processing batch 3904/11884 - Loss: 31.5261\n",
      "Processing batch 3905/11884 - Loss: 30.2782\n",
      "Processing batch 3906/11884 - Loss: 30.0826\n",
      "Processing batch 3907/11884 - Loss: 30.3982\n",
      "Processing batch 3908/11884 - Loss: 30.5338\n",
      "Processing batch 3909/11884 - Loss: 30.0761\n",
      "Processing batch 3910/11884 - Loss: 31.5429\n",
      "Processing batch 3911/11884 - Loss: 30.2398\n",
      "Processing batch 3912/11884 - Loss: 30.0469\n",
      "Processing batch 3913/11884 - Loss: 30.1608\n",
      "Processing batch 3914/11884 - Loss: 30.3572\n",
      "Processing batch 3915/11884 - Loss: 30.7514\n",
      "Processing batch 3916/11884 - Loss: 31.2736\n",
      "Processing batch 3917/11884 - Loss: 31.2241\n",
      "Processing batch 3918/11884 - Loss: 30.0934\n",
      "Processing batch 3919/11884 - Loss: 29.5784\n",
      "Processing batch 3920/11884 - Loss: 30.5229\n",
      "Processing batch 3921/11884 - Loss: 30.2421\n",
      "Processing batch 3922/11884 - Loss: 29.8591\n",
      "Processing batch 3923/11884 - Loss: 31.5107\n",
      "Processing batch 3924/11884 - Loss: 31.9771\n",
      "Processing batch 3925/11884 - Loss: 30.3610\n",
      "Processing batch 3926/11884 - Loss: 31.2456\n",
      "Processing batch 3927/11884 - Loss: 30.9983\n",
      "Processing batch 3928/11884 - Loss: 30.4832\n",
      "Processing batch 3929/11884 - Loss: 30.0071\n",
      "Processing batch 3930/11884 - Loss: 31.8698\n",
      "Processing batch 3931/11884 - Loss: 29.8419\n",
      "Processing batch 3932/11884 - Loss: 29.4633\n",
      "Processing batch 3933/11884 - Loss: 30.4158\n",
      "Processing batch 3934/11884 - Loss: 30.2936\n",
      "Processing batch 3935/11884 - Loss: 29.3489\n",
      "Processing batch 3936/11884 - Loss: 30.3211\n",
      "Processing batch 3937/11884 - Loss: 30.9100\n",
      "Processing batch 3938/11884 - Loss: 31.1676\n",
      "Processing batch 3939/11884 - Loss: 29.4580\n",
      "Processing batch 3940/11884 - Loss: 29.6767\n",
      "Processing batch 3941/11884 - Loss: 29.6518\n",
      "Processing batch 3942/11884 - Loss: 30.4410\n",
      "Processing batch 3943/11884 - Loss: 29.6599\n",
      "Processing batch 3944/11884 - Loss: 29.7663\n",
      "Processing batch 3945/11884 - Loss: 30.6177\n",
      "Processing batch 3946/11884 - Loss: 30.0582\n",
      "Processing batch 3947/11884 - Loss: 31.8222\n",
      "Processing batch 3948/11884 - Loss: 30.6978\n",
      "Processing batch 3949/11884 - Loss: 29.9744\n",
      "Processing batch 3950/11884 - Loss: 31.0845\n",
      "Processing batch 3951/11884 - Loss: 31.2078\n",
      "Processing batch 3952/11884 - Loss: 29.6683\n",
      "Processing batch 3953/11884 - Loss: 30.9550\n",
      "Processing batch 3954/11884 - Loss: 31.8481\n",
      "Processing batch 3955/11884 - Loss: 30.9302\n",
      "Processing batch 3956/11884 - Loss: 29.9986\n",
      "Processing batch 3957/11884 - Loss: 30.0750\n",
      "Processing batch 3958/11884 - Loss: 31.0763\n",
      "Processing batch 3959/11884 - Loss: 29.1646\n",
      "Processing batch 3960/11884 - Loss: 30.3038\n",
      "Processing batch 3961/11884 - Loss: 31.3620\n",
      "Processing batch 3962/11884 - Loss: 30.6331\n",
      "Processing batch 3963/11884 - Loss: 30.7396\n",
      "Processing batch 3964/11884 - Loss: 30.4389\n",
      "Processing batch 3965/11884 - Loss: 30.9262\n",
      "Processing batch 3966/11884 - Loss: 29.8129\n",
      "Processing batch 3967/11884 - Loss: 31.8490\n",
      "Processing batch 3968/11884 - Loss: 30.8279\n",
      "Processing batch 3969/11884 - Loss: 32.1335\n",
      "Processing batch 3970/11884 - Loss: 30.9463\n",
      "Processing batch 3971/11884 - Loss: 28.8342\n",
      "Processing batch 3972/11884 - Loss: 29.9250\n",
      "Processing batch 3973/11884 - Loss: 31.2887\n",
      "Processing batch 3974/11884 - Loss: 29.7102\n",
      "Processing batch 3975/11884 - Loss: 29.8831\n",
      "Processing batch 3976/11884 - Loss: 31.3933\n",
      "Processing batch 3977/11884 - Loss: 29.5839\n",
      "Processing batch 3978/11884 - Loss: 31.4292\n",
      "Processing batch 3979/11884 - Loss: 30.6251\n",
      "Processing batch 3980/11884 - Loss: 29.9736\n",
      "Processing batch 3981/11884 - Loss: 30.3368\n",
      "Processing batch 3982/11884 - Loss: 30.7194\n",
      "Processing batch 3983/11884 - Loss: 31.1781\n",
      "Processing batch 3984/11884 - Loss: 29.4160\n",
      "Processing batch 3985/11884 - Loss: 30.6867\n",
      "Processing batch 3986/11884 - Loss: 29.7699\n",
      "Processing batch 3987/11884 - Loss: 31.1412\n",
      "Processing batch 3988/11884 - Loss: 32.2346\n",
      "Processing batch 3989/11884 - Loss: 31.3218\n",
      "Processing batch 3990/11884 - Loss: 31.2425\n",
      "Processing batch 3991/11884 - Loss: 30.2545\n",
      "Processing batch 3992/11884 - Loss: 30.3579\n",
      "Processing batch 3993/11884 - Loss: 31.1197\n",
      "Processing batch 3994/11884 - Loss: 30.9347\n",
      "Processing batch 3995/11884 - Loss: 32.3231\n",
      "Processing batch 3996/11884 - Loss: 30.2968\n",
      "Processing batch 3997/11884 - Loss: 30.4051\n",
      "Processing batch 3998/11884 - Loss: 31.4853\n",
      "Processing batch 3999/11884 - Loss: 31.4016\n",
      "Processing batch 4000/11884 - Loss: 29.8490\n",
      "Processing batch 4001/11884 - Loss: 31.2494\n",
      "Processing batch 4002/11884 - Loss: 29.8120\n",
      "Processing batch 4003/11884 - Loss: 29.3695\n",
      "Processing batch 4004/11884 - Loss: 31.1970\n",
      "Processing batch 4005/11884 - Loss: 30.8019\n",
      "Processing batch 4006/11884 - Loss: 30.2931\n",
      "Processing batch 4007/11884 - Loss: 30.4033\n",
      "Processing batch 4008/11884 - Loss: 30.6180\n",
      "Processing batch 4009/11884 - Loss: 30.1199\n",
      "Processing batch 4010/11884 - Loss: 30.1897\n",
      "Processing batch 4011/11884 - Loss: 30.1065\n",
      "Processing batch 4012/11884 - Loss: 30.6416\n",
      "Processing batch 4013/11884 - Loss: 31.3741\n",
      "Processing batch 4014/11884 - Loss: 29.6596\n",
      "Processing batch 4015/11884 - Loss: 30.9543\n",
      "Processing batch 4016/11884 - Loss: 29.3389\n",
      "Processing batch 4017/11884 - Loss: 29.2763\n",
      "Processing batch 4018/11884 - Loss: 30.7821\n",
      "Processing batch 4019/11884 - Loss: 30.4283\n",
      "Processing batch 4020/11884 - Loss: 30.3716\n",
      "Processing batch 4021/11884 - Loss: 29.8623\n",
      "Processing batch 4022/11884 - Loss: 29.8475\n",
      "Processing batch 4023/11884 - Loss: 29.3916\n",
      "Processing batch 4024/11884 - Loss: 29.6304\n",
      "Processing batch 4025/11884 - Loss: 28.9948\n",
      "Processing batch 4026/11884 - Loss: 30.4996\n",
      "Processing batch 4027/11884 - Loss: 30.8910\n",
      "Processing batch 4028/11884 - Loss: 30.1590\n",
      "Processing batch 4029/11884 - Loss: 30.8272\n",
      "Processing batch 4030/11884 - Loss: 29.2087\n",
      "Processing batch 4031/11884 - Loss: 30.2444\n",
      "Processing batch 4032/11884 - Loss: 30.3807\n",
      "Processing batch 4033/11884 - Loss: 29.8455\n",
      "Processing batch 4034/11884 - Loss: 31.0117\n",
      "Processing batch 4035/11884 - Loss: 29.6200\n",
      "Processing batch 4036/11884 - Loss: 30.2462\n",
      "Processing batch 4037/11884 - Loss: 31.2636\n",
      "Processing batch 4038/11884 - Loss: 29.6234\n",
      "Processing batch 4039/11884 - Loss: 29.2001\n",
      "Processing batch 4040/11884 - Loss: 29.1821\n",
      "Processing batch 4041/11884 - Loss: 30.8637\n",
      "Processing batch 4042/11884 - Loss: 30.5928\n",
      "Processing batch 4043/11884 - Loss: 30.2085\n",
      "Processing batch 4044/11884 - Loss: 30.5236\n",
      "Processing batch 4045/11884 - Loss: 30.9675\n",
      "Processing batch 4046/11884 - Loss: 31.4701\n",
      "Processing batch 4047/11884 - Loss: 31.6313\n",
      "Processing batch 4048/11884 - Loss: 30.2048\n",
      "Processing batch 4049/11884 - Loss: 28.3180\n",
      "Processing batch 4050/11884 - Loss: 30.2604\n",
      "Processing batch 4051/11884 - Loss: 30.8049\n",
      "Processing batch 4052/11884 - Loss: 33.1630\n",
      "Processing batch 4053/11884 - Loss: 31.3834\n",
      "Processing batch 4054/11884 - Loss: 30.5254\n",
      "Processing batch 4055/11884 - Loss: 29.0676\n",
      "Processing batch 4056/11884 - Loss: 31.0913\n",
      "Processing batch 4057/11884 - Loss: 31.1900\n",
      "Processing batch 4058/11884 - Loss: 29.9536\n",
      "Processing batch 4059/11884 - Loss: 30.7618\n",
      "Processing batch 4060/11884 - Loss: 31.3251\n",
      "Processing batch 4061/11884 - Loss: 30.1071\n",
      "Processing batch 4062/11884 - Loss: 30.7273\n",
      "Processing batch 4063/11884 - Loss: 30.7540\n",
      "Processing batch 4064/11884 - Loss: 29.5329\n",
      "Processing batch 4065/11884 - Loss: 31.4194\n",
      "Processing batch 4066/11884 - Loss: 30.2689\n",
      "Processing batch 4067/11884 - Loss: 29.6129\n",
      "Processing batch 4068/11884 - Loss: 30.9041\n",
      "Processing batch 4069/11884 - Loss: 30.5514\n",
      "Processing batch 4070/11884 - Loss: 29.6565\n",
      "Processing batch 4071/11884 - Loss: 31.2147\n",
      "Processing batch 4072/11884 - Loss: 31.8421\n",
      "Processing batch 4073/11884 - Loss: 30.9106\n",
      "Processing batch 4074/11884 - Loss: 30.1858\n",
      "Processing batch 4075/11884 - Loss: 31.5198\n",
      "Processing batch 4076/11884 - Loss: 29.0471\n",
      "Processing batch 4077/11884 - Loss: 31.6638\n",
      "Processing batch 4078/11884 - Loss: 31.0662\n",
      "Processing batch 4079/11884 - Loss: 29.4610\n",
      "Processing batch 4080/11884 - Loss: 30.9077\n",
      "Processing batch 4081/11884 - Loss: 30.7422\n",
      "Processing batch 4082/11884 - Loss: 30.8605\n",
      "Processing batch 4083/11884 - Loss: 29.8413\n",
      "Processing batch 4084/11884 - Loss: 30.8042\n",
      "Processing batch 4085/11884 - Loss: 30.8375\n",
      "Processing batch 4086/11884 - Loss: 32.3568\n",
      "Processing batch 4087/11884 - Loss: 29.9529\n",
      "Processing batch 4088/11884 - Loss: 30.3550\n",
      "Processing batch 4089/11884 - Loss: 30.5697\n",
      "Processing batch 4090/11884 - Loss: 30.9243\n",
      "Processing batch 4091/11884 - Loss: 29.9093\n",
      "Processing batch 4092/11884 - Loss: 31.1424\n",
      "Processing batch 4093/11884 - Loss: 30.2006\n",
      "Processing batch 4094/11884 - Loss: 31.7373\n",
      "Processing batch 4095/11884 - Loss: 30.7197\n",
      "Processing batch 4096/11884 - Loss: 28.7575\n",
      "Processing batch 4097/11884 - Loss: 30.4056\n",
      "Processing batch 4098/11884 - Loss: 30.8225\n",
      "Processing batch 4099/11884 - Loss: 30.7896\n",
      "Processing batch 4100/11884 - Loss: 31.5211\n",
      "Processing batch 4101/11884 - Loss: 31.1201\n",
      "Processing batch 4102/11884 - Loss: 30.1062\n",
      "Processing batch 4103/11884 - Loss: 31.4163\n",
      "Processing batch 4104/11884 - Loss: 30.9761\n",
      "Processing batch 4105/11884 - Loss: 30.9840\n",
      "Processing batch 4106/11884 - Loss: 30.2054\n",
      "Processing batch 4107/11884 - Loss: 29.5156\n",
      "Processing batch 4108/11884 - Loss: 30.8192\n",
      "Processing batch 4109/11884 - Loss: 30.6551\n",
      "Processing batch 4110/11884 - Loss: 30.3380\n",
      "Processing batch 4111/11884 - Loss: 31.1070\n",
      "Processing batch 4112/11884 - Loss: 31.1902\n",
      "Processing batch 4113/11884 - Loss: 29.0640\n",
      "Processing batch 4114/11884 - Loss: 29.8688\n",
      "Processing batch 4115/11884 - Loss: 30.6866\n",
      "Processing batch 4116/11884 - Loss: 29.7514\n",
      "Processing batch 4117/11884 - Loss: 31.4006\n",
      "Processing batch 4118/11884 - Loss: 33.0728\n",
      "Processing batch 4119/11884 - Loss: 31.2045\n",
      "Processing batch 4120/11884 - Loss: 29.6964\n",
      "Processing batch 4121/11884 - Loss: 31.2220\n",
      "Processing batch 4122/11884 - Loss: 31.2882\n",
      "Processing batch 4123/11884 - Loss: 30.3985\n",
      "Processing batch 4124/11884 - Loss: 30.3371\n",
      "Processing batch 4125/11884 - Loss: 30.8148\n",
      "Processing batch 4126/11884 - Loss: 29.5459\n",
      "Processing batch 4127/11884 - Loss: 30.1808\n",
      "Processing batch 4128/11884 - Loss: 31.5242\n",
      "Processing batch 4129/11884 - Loss: 30.5655\n",
      "Processing batch 4130/11884 - Loss: 32.2084\n",
      "Processing batch 4131/11884 - Loss: 29.9199\n",
      "Processing batch 4132/11884 - Loss: 30.4495\n",
      "Processing batch 4133/11884 - Loss: 29.3588\n",
      "Processing batch 4134/11884 - Loss: 30.6417\n",
      "Processing batch 4135/11884 - Loss: 30.1037\n",
      "Processing batch 4136/11884 - Loss: 30.3179\n",
      "Processing batch 4137/11884 - Loss: 29.1855\n",
      "Processing batch 4138/11884 - Loss: 31.5544\n",
      "Processing batch 4139/11884 - Loss: 31.4805\n",
      "Processing batch 4140/11884 - Loss: 29.5925\n",
      "Processing batch 4141/11884 - Loss: 29.2486\n",
      "Processing batch 4142/11884 - Loss: 30.6949\n",
      "Processing batch 4143/11884 - Loss: 30.4515\n",
      "Processing batch 4144/11884 - Loss: 29.4578\n",
      "Processing batch 4145/11884 - Loss: 30.9225\n",
      "Processing batch 4146/11884 - Loss: 29.9535\n",
      "Processing batch 4147/11884 - Loss: 29.8271\n",
      "Processing batch 4148/11884 - Loss: 29.7117\n",
      "Processing batch 4149/11884 - Loss: 31.1388\n",
      "Processing batch 4150/11884 - Loss: 30.2632\n",
      "Processing batch 4151/11884 - Loss: 30.3785\n",
      "Processing batch 4152/11884 - Loss: 32.0724\n",
      "Processing batch 4153/11884 - Loss: 30.7914\n",
      "Processing batch 4154/11884 - Loss: 30.3093\n",
      "Processing batch 4155/11884 - Loss: 30.0703\n",
      "Processing batch 4156/11884 - Loss: 31.1285\n",
      "Processing batch 4157/11884 - Loss: 30.7546\n",
      "Processing batch 4158/11884 - Loss: 31.9518\n",
      "Processing batch 4159/11884 - Loss: 31.3375\n",
      "Processing batch 4160/11884 - Loss: 29.8452\n",
      "Processing batch 4161/11884 - Loss: 30.7920\n",
      "Processing batch 4162/11884 - Loss: 31.2503\n",
      "Processing batch 4163/11884 - Loss: 29.1378\n",
      "Processing batch 4164/11884 - Loss: 30.3458\n",
      "Processing batch 4165/11884 - Loss: 30.2641\n",
      "Processing batch 4166/11884 - Loss: 30.3593\n",
      "Processing batch 4167/11884 - Loss: 30.4120\n",
      "Processing batch 4168/11884 - Loss: 30.5217\n",
      "Processing batch 4169/11884 - Loss: 28.7569\n",
      "Processing batch 4170/11884 - Loss: 31.4656\n",
      "Processing batch 4171/11884 - Loss: 29.9777\n",
      "Processing batch 4172/11884 - Loss: 31.6363\n",
      "Processing batch 4173/11884 - Loss: 30.2250\n",
      "Processing batch 4174/11884 - Loss: 30.7434\n",
      "Processing batch 4175/11884 - Loss: 31.2943\n",
      "Processing batch 4176/11884 - Loss: 31.0581\n",
      "Processing batch 4177/11884 - Loss: 30.8174\n",
      "Processing batch 4178/11884 - Loss: 30.6162\n",
      "Processing batch 4179/11884 - Loss: 31.0702\n",
      "Processing batch 4180/11884 - Loss: 31.0071\n",
      "Processing batch 4181/11884 - Loss: 30.7754\n",
      "Processing batch 4182/11884 - Loss: 31.0699\n",
      "Processing batch 4183/11884 - Loss: 30.7124\n",
      "Processing batch 4184/11884 - Loss: 29.4124\n",
      "Processing batch 4185/11884 - Loss: 29.4327\n",
      "Processing batch 4186/11884 - Loss: 29.9324\n",
      "Processing batch 4187/11884 - Loss: 31.2303\n",
      "Processing batch 4188/11884 - Loss: 31.5658\n",
      "Processing batch 4189/11884 - Loss: 30.0820\n",
      "Processing batch 4190/11884 - Loss: 31.0478\n",
      "Processing batch 4191/11884 - Loss: 30.7747\n",
      "Processing batch 4192/11884 - Loss: 30.9619\n",
      "Processing batch 4193/11884 - Loss: 30.6341\n",
      "Processing batch 4194/11884 - Loss: 30.3297\n",
      "Processing batch 4195/11884 - Loss: 28.8934\n",
      "Processing batch 4196/11884 - Loss: 29.2153\n",
      "Processing batch 4197/11884 - Loss: 30.8285\n",
      "Processing batch 4198/11884 - Loss: 32.8564\n",
      "Processing batch 4199/11884 - Loss: 29.6665\n",
      "Processing batch 4200/11884 - Loss: 31.0701\n",
      "Processing batch 4201/11884 - Loss: 30.1842\n",
      "Processing batch 4202/11884 - Loss: 29.2140\n",
      "Processing batch 4203/11884 - Loss: 29.7080\n",
      "Processing batch 4204/11884 - Loss: 29.7628\n",
      "Processing batch 4205/11884 - Loss: 30.7179\n",
      "Processing batch 4206/11884 - Loss: 32.3298\n",
      "Processing batch 4207/11884 - Loss: 30.2789\n",
      "Processing batch 4208/11884 - Loss: 30.0084\n",
      "Processing batch 4209/11884 - Loss: 30.6699\n",
      "Processing batch 4210/11884 - Loss: 30.1066\n",
      "Processing batch 4211/11884 - Loss: 29.7073\n",
      "Processing batch 4212/11884 - Loss: 30.6254\n",
      "Processing batch 4213/11884 - Loss: 29.4393\n",
      "Processing batch 4214/11884 - Loss: 29.7351\n",
      "Processing batch 4215/11884 - Loss: 30.5345\n",
      "Processing batch 4216/11884 - Loss: 29.9878\n",
      "Processing batch 4217/11884 - Loss: 32.1835\n",
      "Processing batch 4218/11884 - Loss: 30.7435\n",
      "Processing batch 4219/11884 - Loss: 31.5721\n",
      "Processing batch 4220/11884 - Loss: 30.8014\n",
      "Processing batch 4221/11884 - Loss: 31.7884\n",
      "Processing batch 4222/11884 - Loss: 29.9785\n",
      "Processing batch 4223/11884 - Loss: 29.7138\n",
      "Processing batch 4224/11884 - Loss: 32.0049\n",
      "Processing batch 4225/11884 - Loss: 29.9562\n",
      "Processing batch 4226/11884 - Loss: 30.5893\n",
      "Processing batch 4227/11884 - Loss: 31.0354\n",
      "Processing batch 4228/11884 - Loss: 30.3937\n",
      "Processing batch 4229/11884 - Loss: 31.9923\n",
      "Processing batch 4230/11884 - Loss: 30.3440\n",
      "Processing batch 4231/11884 - Loss: 31.7816\n",
      "Processing batch 4232/11884 - Loss: 30.9878\n",
      "Processing batch 4233/11884 - Loss: 30.3312\n",
      "Processing batch 4234/11884 - Loss: 31.2429\n",
      "Processing batch 4235/11884 - Loss: 31.8603\n",
      "Processing batch 4236/11884 - Loss: 29.0510\n",
      "Processing batch 4237/11884 - Loss: 30.7064\n",
      "Processing batch 4238/11884 - Loss: 31.4394\n",
      "Processing batch 4239/11884 - Loss: 29.5503\n",
      "Processing batch 4240/11884 - Loss: 30.4603\n",
      "Processing batch 4241/11884 - Loss: 31.2857\n",
      "Processing batch 4242/11884 - Loss: 31.3955\n",
      "Processing batch 4243/11884 - Loss: 29.5452\n",
      "Processing batch 4244/11884 - Loss: 31.1930\n",
      "Processing batch 4245/11884 - Loss: 29.5801\n",
      "Processing batch 4246/11884 - Loss: 31.1466\n",
      "Processing batch 4247/11884 - Loss: 30.7492\n",
      "Processing batch 4248/11884 - Loss: 31.4687\n",
      "Processing batch 4249/11884 - Loss: 29.7146\n",
      "Processing batch 4250/11884 - Loss: 31.1147\n",
      "Processing batch 4251/11884 - Loss: 30.4414\n",
      "Processing batch 4252/11884 - Loss: 30.2748\n",
      "Processing batch 4253/11884 - Loss: 30.6693\n",
      "Processing batch 4254/11884 - Loss: 29.9458\n",
      "Processing batch 4255/11884 - Loss: 31.0140\n",
      "Processing batch 4256/11884 - Loss: 31.1807\n",
      "Processing batch 4257/11884 - Loss: 30.1005\n",
      "Processing batch 4258/11884 - Loss: 29.7810\n",
      "Processing batch 4259/11884 - Loss: 30.4865\n",
      "Processing batch 4260/11884 - Loss: 30.8112\n",
      "Processing batch 4261/11884 - Loss: 31.6778\n",
      "Processing batch 4262/11884 - Loss: 29.8823\n",
      "Processing batch 4263/11884 - Loss: 29.0970\n",
      "Processing batch 4264/11884 - Loss: 29.9429\n",
      "Processing batch 4265/11884 - Loss: 29.7284\n",
      "Processing batch 4266/11884 - Loss: 30.1281\n",
      "Processing batch 4267/11884 - Loss: 30.7827\n",
      "Processing batch 4268/11884 - Loss: 30.4154\n",
      "Processing batch 4269/11884 - Loss: 29.3108\n",
      "Processing batch 4270/11884 - Loss: 31.2224\n",
      "Processing batch 4271/11884 - Loss: 30.7373\n",
      "Processing batch 4272/11884 - Loss: 30.7695\n",
      "Processing batch 4273/11884 - Loss: 30.8576\n",
      "Processing batch 4274/11884 - Loss: 29.7362\n",
      "Processing batch 4275/11884 - Loss: 30.9591\n",
      "Processing batch 4276/11884 - Loss: 31.3360\n",
      "Processing batch 4277/11884 - Loss: 30.2029\n",
      "Processing batch 4278/11884 - Loss: 31.1779\n",
      "Processing batch 4279/11884 - Loss: 31.1735\n",
      "Processing batch 4280/11884 - Loss: 29.4397\n",
      "Processing batch 4281/11884 - Loss: 29.5349\n",
      "Processing batch 4282/11884 - Loss: 30.6310\n",
      "Processing batch 4283/11884 - Loss: 31.6332\n",
      "Processing batch 4284/11884 - Loss: 29.6919\n",
      "Processing batch 4285/11884 - Loss: 31.5858\n",
      "Processing batch 4286/11884 - Loss: 30.3118\n",
      "Processing batch 4287/11884 - Loss: 30.3879\n",
      "Processing batch 4288/11884 - Loss: 30.1141\n",
      "Processing batch 4289/11884 - Loss: 30.6707\n",
      "Processing batch 4290/11884 - Loss: 30.6014\n",
      "Processing batch 4291/11884 - Loss: 31.0461\n",
      "Processing batch 4292/11884 - Loss: 29.7706\n",
      "Processing batch 4293/11884 - Loss: 32.9858\n",
      "Processing batch 4294/11884 - Loss: 31.2788\n",
      "Processing batch 4295/11884 - Loss: 30.3995\n",
      "Processing batch 4296/11884 - Loss: 30.0289\n",
      "Processing batch 4297/11884 - Loss: 30.6039\n",
      "Processing batch 4298/11884 - Loss: 30.6608\n",
      "Processing batch 4299/11884 - Loss: 31.0562\n",
      "Processing batch 4300/11884 - Loss: 30.2738\n",
      "Processing batch 4301/11884 - Loss: 32.0796\n",
      "Processing batch 4302/11884 - Loss: 31.6843\n",
      "Processing batch 4303/11884 - Loss: 31.2107\n",
      "Processing batch 4304/11884 - Loss: 30.8680\n",
      "Processing batch 4305/11884 - Loss: 31.2132\n",
      "Processing batch 4306/11884 - Loss: 30.8527\n",
      "Processing batch 4307/11884 - Loss: 29.2125\n",
      "Processing batch 4308/11884 - Loss: 29.2646\n",
      "Processing batch 4309/11884 - Loss: 31.5950\n",
      "Processing batch 4310/11884 - Loss: 29.7852\n",
      "Processing batch 4311/11884 - Loss: 29.0019\n",
      "Processing batch 4312/11884 - Loss: 30.5617\n",
      "Processing batch 4313/11884 - Loss: 30.7267\n",
      "Processing batch 4314/11884 - Loss: 30.4797\n",
      "Processing batch 4315/11884 - Loss: 30.4131\n",
      "Processing batch 4316/11884 - Loss: 31.2402\n",
      "Processing batch 4317/11884 - Loss: 30.8606\n",
      "Processing batch 4318/11884 - Loss: 31.0090\n",
      "Processing batch 4319/11884 - Loss: 30.3928\n",
      "Processing batch 4320/11884 - Loss: 30.8923\n",
      "Processing batch 4321/11884 - Loss: 30.0440\n",
      "Processing batch 4322/11884 - Loss: 31.2293\n",
      "Processing batch 4323/11884 - Loss: 30.7837\n",
      "Processing batch 4324/11884 - Loss: 28.9023\n",
      "Processing batch 4325/11884 - Loss: 30.6665\n",
      "Processing batch 4326/11884 - Loss: 30.4524\n",
      "Processing batch 4327/11884 - Loss: 31.2009\n",
      "Processing batch 4328/11884 - Loss: 31.3555\n",
      "Processing batch 4329/11884 - Loss: 31.0739\n",
      "Processing batch 4330/11884 - Loss: 30.4504\n",
      "Processing batch 4331/11884 - Loss: 29.5887\n",
      "Processing batch 4332/11884 - Loss: 31.6525\n",
      "Processing batch 4333/11884 - Loss: 30.4136\n",
      "Processing batch 4334/11884 - Loss: 31.0156\n",
      "Processing batch 4335/11884 - Loss: 30.4281\n",
      "Processing batch 4336/11884 - Loss: 30.8585\n",
      "Processing batch 4337/11884 - Loss: 31.3303\n",
      "Processing batch 4338/11884 - Loss: 28.8419\n",
      "Processing batch 4339/11884 - Loss: 30.6765\n",
      "Processing batch 4340/11884 - Loss: 32.4346\n",
      "Processing batch 4341/11884 - Loss: 31.9779\n",
      "Processing batch 4342/11884 - Loss: 30.1365\n",
      "Processing batch 4343/11884 - Loss: 30.3822\n",
      "Processing batch 4344/11884 - Loss: 29.9511\n",
      "Processing batch 4345/11884 - Loss: 30.6104\n",
      "Processing batch 4346/11884 - Loss: 30.8952\n",
      "Processing batch 4347/11884 - Loss: 30.8779\n",
      "Processing batch 4348/11884 - Loss: 30.4305\n",
      "Processing batch 4349/11884 - Loss: 30.5353\n",
      "Processing batch 4350/11884 - Loss: 29.8546\n",
      "Processing batch 4351/11884 - Loss: 30.3277\n",
      "Processing batch 4352/11884 - Loss: 29.6370\n",
      "Processing batch 4353/11884 - Loss: 31.3318\n",
      "Processing batch 4354/11884 - Loss: 30.4932\n",
      "Processing batch 4355/11884 - Loss: 31.9246\n",
      "Processing batch 4356/11884 - Loss: 30.2849\n",
      "Processing batch 4357/11884 - Loss: 30.0119\n",
      "Processing batch 4358/11884 - Loss: 31.0669\n",
      "Processing batch 4359/11884 - Loss: 30.2943\n",
      "Processing batch 4360/11884 - Loss: 30.3721\n",
      "Processing batch 4361/11884 - Loss: 30.1604\n",
      "Processing batch 4362/11884 - Loss: 30.0384\n",
      "Processing batch 4363/11884 - Loss: 31.8116\n",
      "Processing batch 4364/11884 - Loss: 30.2277\n",
      "Processing batch 4365/11884 - Loss: 31.4111\n",
      "Processing batch 4366/11884 - Loss: 30.9651\n",
      "Processing batch 4367/11884 - Loss: 32.0809\n",
      "Processing batch 4368/11884 - Loss: 30.2351\n",
      "Processing batch 4369/11884 - Loss: 31.0361\n",
      "Processing batch 4370/11884 - Loss: 30.7340\n",
      "Processing batch 4371/11884 - Loss: 29.6503\n",
      "Processing batch 4372/11884 - Loss: 30.7391\n",
      "Processing batch 4373/11884 - Loss: 31.2516\n",
      "Processing batch 4374/11884 - Loss: 31.0135\n",
      "Processing batch 4375/11884 - Loss: 31.7286\n",
      "Processing batch 4376/11884 - Loss: 30.2561\n",
      "Processing batch 4377/11884 - Loss: 30.9659\n",
      "Processing batch 4378/11884 - Loss: 30.0629\n",
      "Processing batch 4379/11884 - Loss: 30.5361\n",
      "Processing batch 4380/11884 - Loss: 30.4864\n",
      "Processing batch 4381/11884 - Loss: 30.3438\n",
      "Processing batch 4382/11884 - Loss: 29.9641\n",
      "Processing batch 4383/11884 - Loss: 31.3562\n",
      "Processing batch 4384/11884 - Loss: 30.9430\n",
      "Processing batch 4385/11884 - Loss: 31.0325\n",
      "Processing batch 4386/11884 - Loss: 29.1895\n",
      "Processing batch 4387/11884 - Loss: 30.0500\n",
      "Processing batch 4388/11884 - Loss: 31.7121\n",
      "Processing batch 4389/11884 - Loss: 30.5423\n",
      "Processing batch 4390/11884 - Loss: 30.1617\n",
      "Processing batch 4391/11884 - Loss: 30.1616\n",
      "Processing batch 4392/11884 - Loss: 30.2137\n",
      "Processing batch 4393/11884 - Loss: 31.4272\n",
      "Processing batch 4394/11884 - Loss: 30.9566\n",
      "Processing batch 4395/11884 - Loss: 30.3822\n",
      "Processing batch 4396/11884 - Loss: 29.2397\n",
      "Processing batch 4397/11884 - Loss: 30.2523\n",
      "Processing batch 4398/11884 - Loss: 31.4421\n",
      "Processing batch 4399/11884 - Loss: 31.4025\n",
      "Processing batch 4400/11884 - Loss: 30.9125\n",
      "Processing batch 4401/11884 - Loss: 30.0310\n",
      "Processing batch 4402/11884 - Loss: 30.4037\n",
      "Processing batch 4403/11884 - Loss: 29.9962\n",
      "Processing batch 4404/11884 - Loss: 31.1725\n",
      "Processing batch 4405/11884 - Loss: 30.9298\n",
      "Processing batch 4406/11884 - Loss: 30.8581\n",
      "Processing batch 4407/11884 - Loss: 29.7638\n",
      "Processing batch 4408/11884 - Loss: 30.2272\n",
      "Processing batch 4409/11884 - Loss: 32.9597\n",
      "Processing batch 4410/11884 - Loss: 29.9418\n",
      "Processing batch 4411/11884 - Loss: 29.7783\n",
      "Processing batch 4412/11884 - Loss: 30.6422\n",
      "Processing batch 4413/11884 - Loss: 29.9589\n",
      "Processing batch 4414/11884 - Loss: 30.6603\n",
      "Processing batch 4415/11884 - Loss: 30.5576\n",
      "Processing batch 4416/11884 - Loss: 30.5095\n",
      "Processing batch 4417/11884 - Loss: 31.7010\n",
      "Processing batch 4418/11884 - Loss: 31.0485\n",
      "Processing batch 4419/11884 - Loss: 31.0655\n",
      "Processing batch 4420/11884 - Loss: 31.0476\n",
      "Processing batch 4421/11884 - Loss: 29.8038\n",
      "Processing batch 4422/11884 - Loss: 32.4062\n",
      "Processing batch 4423/11884 - Loss: 30.8449\n",
      "Processing batch 4424/11884 - Loss: 30.1306\n",
      "Processing batch 4425/11884 - Loss: 31.0580\n",
      "Processing batch 4426/11884 - Loss: 31.3226\n",
      "Processing batch 4427/11884 - Loss: 31.7567\n",
      "Processing batch 4428/11884 - Loss: 29.8016\n",
      "Processing batch 4429/11884 - Loss: 30.9424\n",
      "Processing batch 4430/11884 - Loss: 29.2457\n",
      "Processing batch 4431/11884 - Loss: 30.7135\n",
      "Processing batch 4432/11884 - Loss: 30.1609\n",
      "Processing batch 4433/11884 - Loss: 30.4931\n",
      "Processing batch 4434/11884 - Loss: 30.0680\n",
      "Processing batch 4435/11884 - Loss: 31.3311\n",
      "Processing batch 4436/11884 - Loss: 30.1679\n",
      "Processing batch 4437/11884 - Loss: 30.8682\n",
      "Processing batch 4438/11884 - Loss: 30.7759\n",
      "Processing batch 4439/11884 - Loss: 31.2467\n",
      "Processing batch 4440/11884 - Loss: 29.6547\n",
      "Processing batch 4441/11884 - Loss: 30.0332\n",
      "Processing batch 4442/11884 - Loss: 30.3818\n",
      "Processing batch 4443/11884 - Loss: 30.2931\n",
      "Processing batch 4444/11884 - Loss: 29.3523\n",
      "Processing batch 4445/11884 - Loss: 30.3889\n",
      "Processing batch 4446/11884 - Loss: 30.2260\n",
      "Processing batch 4447/11884 - Loss: 30.9082\n",
      "Processing batch 4448/11884 - Loss: 31.7228\n",
      "Processing batch 4449/11884 - Loss: 30.7240\n",
      "Processing batch 4450/11884 - Loss: 31.5210\n",
      "Processing batch 4451/11884 - Loss: 31.4212\n",
      "Processing batch 4452/11884 - Loss: 30.5925\n",
      "Processing batch 4453/11884 - Loss: 30.6125\n",
      "Processing batch 4454/11884 - Loss: 30.3330\n",
      "Processing batch 4455/11884 - Loss: 31.1073\n",
      "Processing batch 4456/11884 - Loss: 30.8758\n",
      "Processing batch 4457/11884 - Loss: 29.8800\n",
      "Processing batch 4458/11884 - Loss: 29.5537\n",
      "Processing batch 4459/11884 - Loss: 30.1200\n",
      "Processing batch 4460/11884 - Loss: 31.1594\n",
      "Processing batch 4461/11884 - Loss: 31.9170\n",
      "Processing batch 4462/11884 - Loss: 30.1230\n",
      "Processing batch 4463/11884 - Loss: 29.9493\n",
      "Processing batch 4464/11884 - Loss: 30.4115\n",
      "Processing batch 4465/11884 - Loss: 30.3306\n",
      "Processing batch 4466/11884 - Loss: 31.6155\n",
      "Processing batch 4467/11884 - Loss: 31.8578\n",
      "Processing batch 4468/11884 - Loss: 30.4886\n",
      "Processing batch 4469/11884 - Loss: 30.2532\n",
      "Processing batch 4470/11884 - Loss: 30.4894\n",
      "Processing batch 4471/11884 - Loss: 28.2103\n",
      "Processing batch 4472/11884 - Loss: 31.3198\n",
      "Processing batch 4473/11884 - Loss: 29.7511\n",
      "Processing batch 4474/11884 - Loss: 31.4598\n",
      "Processing batch 4475/11884 - Loss: 31.3452\n",
      "Processing batch 4476/11884 - Loss: 30.5348\n",
      "Processing batch 4477/11884 - Loss: 31.8092\n",
      "Processing batch 4478/11884 - Loss: 31.1723\n",
      "Processing batch 4479/11884 - Loss: 30.2265\n",
      "Processing batch 4480/11884 - Loss: 31.6482\n",
      "Processing batch 4481/11884 - Loss: 31.3174\n",
      "Processing batch 4482/11884 - Loss: 29.7187\n",
      "Processing batch 4483/11884 - Loss: 32.0115\n",
      "Processing batch 4484/11884 - Loss: 31.2740\n",
      "Processing batch 4485/11884 - Loss: 30.2414\n",
      "Processing batch 4486/11884 - Loss: 32.4725\n",
      "Processing batch 4487/11884 - Loss: 30.7063\n",
      "Processing batch 4488/11884 - Loss: 31.0265\n",
      "Processing batch 4489/11884 - Loss: 31.7561\n",
      "Processing batch 4490/11884 - Loss: 29.8260\n",
      "Processing batch 4491/11884 - Loss: 30.5634\n",
      "Processing batch 4492/11884 - Loss: 30.7706\n",
      "Processing batch 4493/11884 - Loss: 31.0908\n",
      "Processing batch 4494/11884 - Loss: 29.9513\n",
      "Processing batch 4495/11884 - Loss: 30.0365\n",
      "Processing batch 4496/11884 - Loss: 30.0074\n",
      "Processing batch 4497/11884 - Loss: 30.6015\n",
      "Processing batch 4498/11884 - Loss: 28.6763\n",
      "Processing batch 4499/11884 - Loss: 31.1221\n",
      "Processing batch 4500/11884 - Loss: 29.0077\n",
      "Processing batch 4501/11884 - Loss: 30.5841\n",
      "Processing batch 4502/11884 - Loss: 30.6427\n",
      "Processing batch 4503/11884 - Loss: 31.5331\n",
      "Processing batch 4504/11884 - Loss: 30.8139\n",
      "Processing batch 4505/11884 - Loss: 30.1751\n",
      "Processing batch 4506/11884 - Loss: 31.3755\n",
      "Processing batch 4507/11884 - Loss: 30.8762\n",
      "Processing batch 4508/11884 - Loss: 29.7275\n",
      "Processing batch 4509/11884 - Loss: 30.0516\n",
      "Processing batch 4510/11884 - Loss: 29.9565\n",
      "Processing batch 4511/11884 - Loss: 30.1480\n",
      "Processing batch 4512/11884 - Loss: 31.6217\n",
      "Processing batch 4513/11884 - Loss: 31.8786\n",
      "Processing batch 4514/11884 - Loss: 29.8545\n",
      "Processing batch 4515/11884 - Loss: 30.5911\n",
      "Processing batch 4516/11884 - Loss: 32.6958\n",
      "Processing batch 4517/11884 - Loss: 30.0435\n",
      "Processing batch 4518/11884 - Loss: 30.5345\n",
      "Processing batch 4519/11884 - Loss: 30.9392\n",
      "Processing batch 4520/11884 - Loss: 31.2651\n",
      "Processing batch 4521/11884 - Loss: 31.6868\n",
      "Processing batch 4522/11884 - Loss: 30.9314\n",
      "Processing batch 4523/11884 - Loss: 31.1703\n",
      "Processing batch 4524/11884 - Loss: 30.2997\n",
      "Processing batch 4525/11884 - Loss: 30.1130\n",
      "Processing batch 4526/11884 - Loss: 30.0112\n",
      "Processing batch 4527/11884 - Loss: 30.1320\n",
      "Processing batch 4528/11884 - Loss: 30.9207\n",
      "Processing batch 4529/11884 - Loss: 30.0042\n",
      "Processing batch 4530/11884 - Loss: 29.3629\n",
      "Processing batch 4531/11884 - Loss: 30.3152\n",
      "Processing batch 4532/11884 - Loss: 31.2203\n",
      "Processing batch 4533/11884 - Loss: 29.2508\n",
      "Processing batch 4534/11884 - Loss: 31.2185\n",
      "Processing batch 4535/11884 - Loss: 30.8820\n",
      "Processing batch 4536/11884 - Loss: 29.3658\n",
      "Processing batch 4537/11884 - Loss: 29.8360\n",
      "Processing batch 4538/11884 - Loss: 30.2855\n",
      "Processing batch 4539/11884 - Loss: 30.5204\n",
      "Processing batch 4540/11884 - Loss: 31.3265\n",
      "Processing batch 4541/11884 - Loss: 29.5747\n",
      "Processing batch 4542/11884 - Loss: 30.7537\n",
      "Processing batch 4543/11884 - Loss: 33.0517\n",
      "Processing batch 4544/11884 - Loss: 30.4278\n",
      "Processing batch 4545/11884 - Loss: 31.2041\n",
      "Processing batch 4546/11884 - Loss: 30.5753\n",
      "Processing batch 4547/11884 - Loss: 31.1173\n",
      "Processing batch 4548/11884 - Loss: 31.4845\n",
      "Processing batch 4549/11884 - Loss: 30.3382\n",
      "Processing batch 4550/11884 - Loss: 32.0255\n",
      "Processing batch 4551/11884 - Loss: 30.8013\n",
      "Processing batch 4552/11884 - Loss: 30.1016\n",
      "Processing batch 4553/11884 - Loss: 31.1795\n",
      "Processing batch 4554/11884 - Loss: 29.9616\n",
      "Processing batch 4555/11884 - Loss: 31.4510\n",
      "Processing batch 4556/11884 - Loss: 30.6253\n",
      "Processing batch 4557/11884 - Loss: 29.9250\n",
      "Processing batch 4558/11884 - Loss: 30.2582\n",
      "Processing batch 4559/11884 - Loss: 30.4671\n",
      "Processing batch 4560/11884 - Loss: 31.6549\n",
      "Processing batch 4561/11884 - Loss: 30.6368\n",
      "Processing batch 4562/11884 - Loss: 30.1852\n",
      "Processing batch 4563/11884 - Loss: 30.7477\n",
      "Processing batch 4564/11884 - Loss: 31.0758\n",
      "Processing batch 4565/11884 - Loss: 31.6815\n",
      "Processing batch 4566/11884 - Loss: 30.7244\n",
      "Processing batch 4567/11884 - Loss: 30.9500\n",
      "Processing batch 4568/11884 - Loss: 28.8964\n",
      "Processing batch 4569/11884 - Loss: 30.1430\n",
      "Processing batch 4570/11884 - Loss: 29.9498\n",
      "Processing batch 4571/11884 - Loss: 29.8800\n",
      "Processing batch 4572/11884 - Loss: 31.2771\n",
      "Processing batch 4573/11884 - Loss: 30.6015\n",
      "Processing batch 4574/11884 - Loss: 30.6719\n",
      "Processing batch 4575/11884 - Loss: 29.7731\n",
      "Processing batch 4576/11884 - Loss: 30.2983\n",
      "Processing batch 4577/11884 - Loss: 30.7205\n",
      "Processing batch 4578/11884 - Loss: 30.9381\n",
      "Processing batch 4579/11884 - Loss: 30.9454\n",
      "Processing batch 4580/11884 - Loss: 30.1414\n",
      "Processing batch 4581/11884 - Loss: 29.7077\n",
      "Processing batch 4582/11884 - Loss: 31.1023\n",
      "Processing batch 4583/11884 - Loss: 29.0256\n",
      "Processing batch 4584/11884 - Loss: 31.4832\n",
      "Processing batch 4585/11884 - Loss: 31.5987\n",
      "Processing batch 4586/11884 - Loss: 30.4637\n",
      "Processing batch 4587/11884 - Loss: 29.8521\n",
      "Processing batch 4588/11884 - Loss: 31.0746\n",
      "Processing batch 4589/11884 - Loss: 30.7525\n",
      "Processing batch 4590/11884 - Loss: 30.8043\n",
      "Processing batch 4591/11884 - Loss: 29.6675\n",
      "Processing batch 4592/11884 - Loss: 31.0311\n",
      "Processing batch 4593/11884 - Loss: 31.0766\n",
      "Processing batch 4594/11884 - Loss: 30.6071\n",
      "Processing batch 4595/11884 - Loss: 29.8664\n",
      "Processing batch 4596/11884 - Loss: 31.6419\n",
      "Processing batch 4597/11884 - Loss: 30.1651\n",
      "Processing batch 4598/11884 - Loss: 29.5518\n",
      "Processing batch 4599/11884 - Loss: 31.7465\n",
      "Processing batch 4600/11884 - Loss: 30.0327\n",
      "Processing batch 4601/11884 - Loss: 30.7973\n",
      "Processing batch 4602/11884 - Loss: 30.9235\n",
      "Processing batch 4603/11884 - Loss: 31.7834\n",
      "Processing batch 4604/11884 - Loss: 29.7974\n",
      "Processing batch 4605/11884 - Loss: 31.7731\n",
      "Processing batch 4606/11884 - Loss: 29.3918\n",
      "Processing batch 4607/11884 - Loss: 31.5097\n",
      "Processing batch 4608/11884 - Loss: 28.9185\n",
      "Processing batch 4609/11884 - Loss: 30.2402\n",
      "Processing batch 4610/11884 - Loss: 30.3163\n",
      "Processing batch 4611/11884 - Loss: 30.4968\n",
      "Processing batch 4612/11884 - Loss: 31.7250\n",
      "Processing batch 4613/11884 - Loss: 30.5063\n",
      "Processing batch 4614/11884 - Loss: 29.8873\n",
      "Processing batch 4615/11884 - Loss: 30.8251\n",
      "Processing batch 4616/11884 - Loss: 30.5153\n",
      "Processing batch 4617/11884 - Loss: 30.4637\n",
      "Processing batch 4618/11884 - Loss: 30.7417\n",
      "Processing batch 4619/11884 - Loss: 29.4157\n",
      "Processing batch 4620/11884 - Loss: 31.0968\n",
      "Processing batch 4621/11884 - Loss: 31.1076\n",
      "Processing batch 4622/11884 - Loss: 30.8029\n",
      "Processing batch 4623/11884 - Loss: 30.0254\n",
      "Processing batch 4624/11884 - Loss: 31.1731\n",
      "Processing batch 4625/11884 - Loss: 29.8864\n",
      "Processing batch 4626/11884 - Loss: 30.3385\n",
      "Processing batch 4627/11884 - Loss: 30.6542\n",
      "Processing batch 4628/11884 - Loss: 30.6858\n",
      "Processing batch 4629/11884 - Loss: 30.1006\n",
      "Processing batch 4630/11884 - Loss: 30.1387\n",
      "Processing batch 4631/11884 - Loss: 30.8058\n",
      "Processing batch 4632/11884 - Loss: 29.4445\n",
      "Processing batch 4633/11884 - Loss: 30.7401\n",
      "Processing batch 4634/11884 - Loss: 30.5601\n",
      "Processing batch 4635/11884 - Loss: 31.2180\n",
      "Processing batch 4636/11884 - Loss: 31.2128\n",
      "Processing batch 4637/11884 - Loss: 29.8587\n",
      "Processing batch 4638/11884 - Loss: 29.7893\n",
      "Processing batch 4639/11884 - Loss: 32.1928\n",
      "Processing batch 4640/11884 - Loss: 30.3214\n",
      "Processing batch 4641/11884 - Loss: 30.8274\n",
      "Processing batch 4642/11884 - Loss: 30.1587\n",
      "Processing batch 4643/11884 - Loss: 29.6060\n",
      "Processing batch 4644/11884 - Loss: 29.9792\n",
      "Processing batch 4645/11884 - Loss: 30.3605\n",
      "Processing batch 4646/11884 - Loss: 30.1069\n",
      "Processing batch 4647/11884 - Loss: 29.8846\n",
      "Processing batch 4648/11884 - Loss: 30.8741\n",
      "Processing batch 4649/11884 - Loss: 30.7878\n",
      "Processing batch 4650/11884 - Loss: 29.9595\n",
      "Processing batch 4651/11884 - Loss: 30.6382\n",
      "Processing batch 4652/11884 - Loss: 30.4336\n",
      "Processing batch 4653/11884 - Loss: 31.2683\n",
      "Processing batch 4654/11884 - Loss: 31.1830\n",
      "Processing batch 4655/11884 - Loss: 31.9142\n",
      "Processing batch 4656/11884 - Loss: 30.6125\n",
      "Processing batch 4657/11884 - Loss: 30.2761\n",
      "Processing batch 4658/11884 - Loss: 29.4710\n",
      "Processing batch 4659/11884 - Loss: 30.5443\n",
      "Processing batch 4660/11884 - Loss: 29.9366\n",
      "Processing batch 4661/11884 - Loss: 31.5549\n",
      "Processing batch 4662/11884 - Loss: 31.3086\n",
      "Processing batch 4663/11884 - Loss: 30.2015\n",
      "Processing batch 4664/11884 - Loss: 31.1844\n",
      "Processing batch 4665/11884 - Loss: 30.9739\n",
      "Processing batch 4666/11884 - Loss: 32.2411\n",
      "Processing batch 4667/11884 - Loss: 30.1993\n",
      "Processing batch 4668/11884 - Loss: 31.9698\n",
      "Processing batch 4669/11884 - Loss: 30.2899\n",
      "Processing batch 4670/11884 - Loss: 31.2415\n",
      "Processing batch 4671/11884 - Loss: 31.1254\n",
      "Processing batch 4672/11884 - Loss: 31.1201\n",
      "Processing batch 4673/11884 - Loss: 30.0312\n",
      "Processing batch 4674/11884 - Loss: 31.6207\n",
      "Processing batch 4675/11884 - Loss: 30.1959\n",
      "Processing batch 4676/11884 - Loss: 31.3186\n",
      "Processing batch 4677/11884 - Loss: 31.2990\n",
      "Processing batch 4678/11884 - Loss: 28.9044\n",
      "Processing batch 4679/11884 - Loss: 29.8941\n",
      "Processing batch 4680/11884 - Loss: 30.7397\n",
      "Processing batch 4681/11884 - Loss: 31.2414\n",
      "Processing batch 4682/11884 - Loss: 31.8578\n",
      "Processing batch 4683/11884 - Loss: 29.5070\n",
      "Processing batch 4684/11884 - Loss: 30.2403\n",
      "Processing batch 4685/11884 - Loss: 30.3797\n",
      "Processing batch 4686/11884 - Loss: 30.3549\n",
      "Processing batch 4687/11884 - Loss: 29.8948\n",
      "Processing batch 4688/11884 - Loss: 30.4531\n",
      "Processing batch 4689/11884 - Loss: 31.0743\n",
      "Processing batch 4690/11884 - Loss: 30.3577\n",
      "Processing batch 4691/11884 - Loss: 28.7330\n",
      "Processing batch 4692/11884 - Loss: 30.8414\n",
      "Processing batch 4693/11884 - Loss: 30.3055\n",
      "Processing batch 4694/11884 - Loss: 31.2914\n",
      "Processing batch 4695/11884 - Loss: 30.8471\n",
      "Processing batch 4696/11884 - Loss: 30.0614\n",
      "Processing batch 4697/11884 - Loss: 30.2686\n",
      "Processing batch 4698/11884 - Loss: 29.8959\n",
      "Processing batch 4699/11884 - Loss: 31.6926\n",
      "Processing batch 4700/11884 - Loss: 30.7781\n",
      "Processing batch 4701/11884 - Loss: 31.1399\n",
      "Processing batch 4702/11884 - Loss: 29.9944\n",
      "Processing batch 4703/11884 - Loss: 31.1572\n",
      "Processing batch 4704/11884 - Loss: 31.5819\n",
      "Processing batch 4705/11884 - Loss: 30.1824\n",
      "Processing batch 4706/11884 - Loss: 30.7116\n",
      "Processing batch 4707/11884 - Loss: 30.8379\n",
      "Processing batch 4708/11884 - Loss: 32.3416\n",
      "Processing batch 4709/11884 - Loss: 30.2758\n",
      "Processing batch 4710/11884 - Loss: 30.1246\n",
      "Processing batch 4711/11884 - Loss: 31.1799\n",
      "Processing batch 4712/11884 - Loss: 30.9576\n",
      "Processing batch 4713/11884 - Loss: 31.0593\n",
      "Processing batch 4714/11884 - Loss: 30.7386\n",
      "Processing batch 4715/11884 - Loss: 30.5553\n",
      "Processing batch 4716/11884 - Loss: 31.4098\n",
      "Processing batch 4717/11884 - Loss: 30.6267\n",
      "Processing batch 4718/11884 - Loss: 30.4535\n",
      "Processing batch 4719/11884 - Loss: 30.3627\n",
      "Processing batch 4720/11884 - Loss: 30.6463\n",
      "Processing batch 4721/11884 - Loss: 30.0552\n",
      "Processing batch 4722/11884 - Loss: 30.4854\n",
      "Processing batch 4723/11884 - Loss: 30.3633\n",
      "Processing batch 4724/11884 - Loss: 30.3514\n",
      "Processing batch 4725/11884 - Loss: 30.7168\n",
      "Processing batch 4726/11884 - Loss: 31.6735\n",
      "Processing batch 4727/11884 - Loss: 30.9811\n",
      "Processing batch 4728/11884 - Loss: 29.6712\n",
      "Processing batch 4729/11884 - Loss: 30.9085\n",
      "Processing batch 4730/11884 - Loss: 30.6279\n",
      "Processing batch 4731/11884 - Loss: 30.3828\n",
      "Processing batch 4732/11884 - Loss: 31.3386\n",
      "Processing batch 4733/11884 - Loss: 31.0840\n",
      "Processing batch 4734/11884 - Loss: 31.3630\n",
      "Processing batch 4735/11884 - Loss: 31.4326\n",
      "Processing batch 4736/11884 - Loss: 30.6392\n",
      "Processing batch 4737/11884 - Loss: 30.4737\n",
      "Processing batch 4738/11884 - Loss: 31.4825\n",
      "Processing batch 4739/11884 - Loss: 29.7861\n",
      "Processing batch 4740/11884 - Loss: 30.4705\n",
      "Processing batch 4741/11884 - Loss: 30.1761\n",
      "Processing batch 4742/11884 - Loss: 30.5423\n",
      "Processing batch 4743/11884 - Loss: 31.2710\n",
      "Processing batch 4744/11884 - Loss: 29.9366\n",
      "Processing batch 4745/11884 - Loss: 30.7741\n",
      "Processing batch 4746/11884 - Loss: 31.4389\n",
      "Processing batch 4747/11884 - Loss: 30.7753\n",
      "Processing batch 4748/11884 - Loss: 30.9600\n",
      "Processing batch 4749/11884 - Loss: 30.8748\n",
      "Processing batch 4750/11884 - Loss: 30.7378\n",
      "Processing batch 4751/11884 - Loss: 30.0454\n",
      "Processing batch 4752/11884 - Loss: 29.0081\n",
      "Processing batch 4753/11884 - Loss: 31.1122\n",
      "Processing batch 4754/11884 - Loss: 30.7085\n",
      "Processing batch 4755/11884 - Loss: 30.5735\n",
      "Processing batch 4756/11884 - Loss: 29.0766\n",
      "Processing batch 4757/11884 - Loss: 31.2974\n",
      "Processing batch 4758/11884 - Loss: 31.0382\n",
      "Processing batch 4759/11884 - Loss: 30.0729\n",
      "Processing batch 4760/11884 - Loss: 30.5581\n",
      "Processing batch 4761/11884 - Loss: 31.3871\n",
      "Processing batch 4762/11884 - Loss: 31.6347\n",
      "Processing batch 4763/11884 - Loss: 30.8438\n",
      "Processing batch 4764/11884 - Loss: 31.2138\n",
      "Processing batch 4765/11884 - Loss: 31.4435\n",
      "Processing batch 4766/11884 - Loss: 32.4202\n",
      "Processing batch 4767/11884 - Loss: 30.7526\n",
      "Processing batch 4768/11884 - Loss: 31.0582\n",
      "Processing batch 4769/11884 - Loss: 28.8276\n",
      "Processing batch 4770/11884 - Loss: 28.5350\n",
      "Processing batch 4771/11884 - Loss: 29.8425\n",
      "Processing batch 4772/11884 - Loss: 31.0073\n",
      "Processing batch 4773/11884 - Loss: 29.9130\n",
      "Processing batch 4774/11884 - Loss: 30.5809\n",
      "Processing batch 4775/11884 - Loss: 30.6926\n",
      "Processing batch 4776/11884 - Loss: 29.6148\n",
      "Processing batch 4777/11884 - Loss: 29.7251\n",
      "Processing batch 4778/11884 - Loss: 28.9029\n",
      "Processing batch 4779/11884 - Loss: 30.4769\n",
      "Processing batch 4780/11884 - Loss: 30.2587\n",
      "Processing batch 4781/11884 - Loss: 30.9646\n",
      "Processing batch 4782/11884 - Loss: 31.0252\n",
      "Processing batch 4783/11884 - Loss: 30.6358\n",
      "Processing batch 4784/11884 - Loss: 30.8784\n",
      "Processing batch 4785/11884 - Loss: 30.0172\n",
      "Processing batch 4786/11884 - Loss: 30.6953\n",
      "Processing batch 4787/11884 - Loss: 29.9681\n",
      "Processing batch 4788/11884 - Loss: 30.7213\n",
      "Processing batch 4789/11884 - Loss: 31.4809\n",
      "Processing batch 4790/11884 - Loss: 31.0581\n",
      "Processing batch 4791/11884 - Loss: 30.4959\n",
      "Processing batch 4792/11884 - Loss: 28.8801\n",
      "Processing batch 4793/11884 - Loss: 30.4235\n",
      "Processing batch 4794/11884 - Loss: 28.5761\n",
      "Processing batch 4795/11884 - Loss: 30.5961\n",
      "Processing batch 4796/11884 - Loss: 30.0419\n",
      "Processing batch 4797/11884 - Loss: 30.3230\n",
      "Processing batch 4798/11884 - Loss: 31.7704\n",
      "Processing batch 4799/11884 - Loss: 30.2833\n",
      "Processing batch 4800/11884 - Loss: 30.8712\n",
      "Processing batch 4801/11884 - Loss: 31.4053\n",
      "Processing batch 4802/11884 - Loss: 31.0034\n",
      "Processing batch 4803/11884 - Loss: 29.7549\n",
      "Processing batch 4804/11884 - Loss: 31.0948\n",
      "Processing batch 4805/11884 - Loss: 30.4506\n",
      "Processing batch 4806/11884 - Loss: 31.5343\n",
      "Processing batch 4807/11884 - Loss: 30.0376\n",
      "Processing batch 4808/11884 - Loss: 30.7361\n",
      "Processing batch 4809/11884 - Loss: 30.6119\n",
      "Processing batch 4810/11884 - Loss: 31.3276\n",
      "Processing batch 4811/11884 - Loss: 29.8629\n",
      "Processing batch 4812/11884 - Loss: 31.8980\n",
      "Processing batch 4813/11884 - Loss: 31.0725\n",
      "Processing batch 4814/11884 - Loss: 30.8400\n",
      "Processing batch 4815/11884 - Loss: 31.1116\n",
      "Processing batch 4816/11884 - Loss: 29.1713\n",
      "Processing batch 4817/11884 - Loss: 30.0227\n",
      "Processing batch 4818/11884 - Loss: 30.1215\n",
      "Processing batch 4819/11884 - Loss: 31.2020\n",
      "Processing batch 4820/11884 - Loss: 28.9502\n",
      "Processing batch 4821/11884 - Loss: 31.0326\n",
      "Processing batch 4822/11884 - Loss: 30.1027\n",
      "Processing batch 4823/11884 - Loss: 32.8381\n",
      "Processing batch 4824/11884 - Loss: 30.4498\n",
      "Processing batch 4825/11884 - Loss: 28.9853\n",
      "Processing batch 4826/11884 - Loss: 31.3980\n",
      "Processing batch 4827/11884 - Loss: 31.1555\n",
      "Processing batch 4828/11884 - Loss: 30.7037\n",
      "Processing batch 4829/11884 - Loss: 29.9438\n",
      "Processing batch 4830/11884 - Loss: 30.4861\n",
      "Processing batch 4831/11884 - Loss: 29.1913\n",
      "Processing batch 4832/11884 - Loss: 29.9360\n",
      "Processing batch 4833/11884 - Loss: 30.6966\n",
      "Processing batch 4834/11884 - Loss: 30.7415\n",
      "Processing batch 4835/11884 - Loss: 30.8468\n",
      "Processing batch 4836/11884 - Loss: 31.4081\n",
      "Processing batch 4837/11884 - Loss: 29.6894\n",
      "Processing batch 4838/11884 - Loss: 31.6176\n",
      "Processing batch 4839/11884 - Loss: 29.8542\n",
      "Processing batch 4840/11884 - Loss: 32.8846\n",
      "Processing batch 4841/11884 - Loss: 30.9742\n",
      "Processing batch 4842/11884 - Loss: 29.9329\n",
      "Processing batch 4843/11884 - Loss: 30.6430\n",
      "Processing batch 4844/11884 - Loss: 30.5317\n",
      "Processing batch 4845/11884 - Loss: 29.4230\n",
      "Processing batch 4846/11884 - Loss: 29.8159\n",
      "Processing batch 4847/11884 - Loss: 31.6936\n",
      "Processing batch 4848/11884 - Loss: 30.5163\n",
      "Processing batch 4849/11884 - Loss: 30.4232\n",
      "Processing batch 4850/11884 - Loss: 30.7234\n",
      "Processing batch 4851/11884 - Loss: 31.6458\n",
      "Processing batch 4852/11884 - Loss: 30.3402\n",
      "Processing batch 4853/11884 - Loss: 30.9864\n",
      "Processing batch 4854/11884 - Loss: 30.0032\n",
      "Processing batch 4855/11884 - Loss: 32.8433\n",
      "Processing batch 4856/11884 - Loss: 30.1629\n",
      "Processing batch 4857/11884 - Loss: 29.7998\n",
      "Processing batch 4858/11884 - Loss: 30.3938\n",
      "Processing batch 4859/11884 - Loss: 30.7394\n",
      "Processing batch 4860/11884 - Loss: 30.2845\n",
      "Processing batch 4861/11884 - Loss: 30.4030\n",
      "Processing batch 4862/11884 - Loss: 30.7050\n",
      "Processing batch 4863/11884 - Loss: 30.5444\n",
      "Processing batch 4864/11884 - Loss: 30.2319\n",
      "Processing batch 4865/11884 - Loss: 29.6269\n",
      "Processing batch 4866/11884 - Loss: 30.3782\n",
      "Processing batch 4867/11884 - Loss: 30.5608\n",
      "Processing batch 4868/11884 - Loss: 31.4834\n",
      "Processing batch 4869/11884 - Loss: 29.4203\n",
      "Processing batch 4870/11884 - Loss: 30.6218\n",
      "Processing batch 4871/11884 - Loss: 30.6982\n",
      "Processing batch 4872/11884 - Loss: 29.8646\n",
      "Processing batch 4873/11884 - Loss: 31.1931\n",
      "Processing batch 4874/11884 - Loss: 30.2819\n",
      "Processing batch 4875/11884 - Loss: 30.7889\n",
      "Processing batch 4876/11884 - Loss: 30.1711\n",
      "Processing batch 4877/11884 - Loss: 30.7590\n",
      "Processing batch 4878/11884 - Loss: 31.4610\n",
      "Processing batch 4879/11884 - Loss: 30.5318\n",
      "Processing batch 4880/11884 - Loss: 30.0193\n",
      "Processing batch 4881/11884 - Loss: 32.0345\n",
      "Processing batch 4882/11884 - Loss: 29.7938\n",
      "Processing batch 4883/11884 - Loss: 31.6009\n",
      "Processing batch 4884/11884 - Loss: 29.8127\n",
      "Processing batch 4885/11884 - Loss: 28.5647\n",
      "Processing batch 4886/11884 - Loss: 29.0017\n",
      "Processing batch 4887/11884 - Loss: 30.3524\n",
      "Processing batch 4888/11884 - Loss: 30.7870\n",
      "Processing batch 4889/11884 - Loss: 31.4210\n",
      "Processing batch 4890/11884 - Loss: 30.2725\n",
      "Processing batch 4891/11884 - Loss: 30.1219\n",
      "Processing batch 4892/11884 - Loss: 30.4979\n",
      "Processing batch 4893/11884 - Loss: 29.5676\n",
      "Processing batch 4894/11884 - Loss: 29.7224\n",
      "Processing batch 4895/11884 - Loss: 30.1931\n",
      "Processing batch 4896/11884 - Loss: 32.1514\n",
      "Processing batch 4897/11884 - Loss: 30.4187\n",
      "Processing batch 4898/11884 - Loss: 31.8096\n",
      "Processing batch 4899/11884 - Loss: 31.8027\n",
      "Processing batch 4900/11884 - Loss: 30.7031\n",
      "Processing batch 4901/11884 - Loss: 29.1854\n",
      "Processing batch 4902/11884 - Loss: 29.6321\n",
      "Processing batch 4903/11884 - Loss: 31.1295\n",
      "Processing batch 4904/11884 - Loss: 30.4021\n",
      "Processing batch 4905/11884 - Loss: 30.2952\n",
      "Processing batch 4906/11884 - Loss: 32.1729\n",
      "Processing batch 4907/11884 - Loss: 31.8554\n",
      "Processing batch 4908/11884 - Loss: 31.0631\n",
      "Processing batch 4909/11884 - Loss: 31.1978\n",
      "Processing batch 4910/11884 - Loss: 29.7490\n",
      "Processing batch 4911/11884 - Loss: 30.3471\n",
      "Processing batch 4912/11884 - Loss: 29.8700\n",
      "Processing batch 4913/11884 - Loss: 30.7653\n",
      "Processing batch 4914/11884 - Loss: 31.4490\n",
      "Processing batch 4915/11884 - Loss: 30.5795\n",
      "Processing batch 4916/11884 - Loss: 30.0918\n",
      "Processing batch 4917/11884 - Loss: 30.8798\n",
      "Processing batch 4918/11884 - Loss: 30.2414\n",
      "Processing batch 4919/11884 - Loss: 30.3830\n",
      "Processing batch 4920/11884 - Loss: 31.2835\n",
      "Processing batch 4921/11884 - Loss: 29.8484\n",
      "Processing batch 4922/11884 - Loss: 29.9253\n",
      "Processing batch 4923/11884 - Loss: 31.3867\n",
      "Processing batch 4924/11884 - Loss: 29.2006\n",
      "Processing batch 4925/11884 - Loss: 30.9144\n",
      "Processing batch 4926/11884 - Loss: 31.2647\n",
      "Processing batch 4927/11884 - Loss: 29.2354\n",
      "Processing batch 4928/11884 - Loss: 29.9459\n",
      "Processing batch 4929/11884 - Loss: 30.6184\n",
      "Processing batch 4930/11884 - Loss: 30.0055\n",
      "Processing batch 4931/11884 - Loss: 30.7341\n",
      "Processing batch 4932/11884 - Loss: 30.3381\n",
      "Processing batch 4933/11884 - Loss: 32.0719\n",
      "Processing batch 4934/11884 - Loss: 30.2797\n",
      "Processing batch 4935/11884 - Loss: 29.9210\n",
      "Processing batch 4936/11884 - Loss: 30.8196\n",
      "Processing batch 4937/11884 - Loss: 30.8789\n",
      "Processing batch 4938/11884 - Loss: 29.3968\n",
      "Processing batch 4939/11884 - Loss: 29.9838\n",
      "Processing batch 4940/11884 - Loss: 30.8486\n",
      "Processing batch 4941/11884 - Loss: 31.4486\n",
      "Processing batch 4942/11884 - Loss: 31.9856\n",
      "Processing batch 4943/11884 - Loss: 30.0184\n",
      "Processing batch 4944/11884 - Loss: 31.7379\n",
      "Processing batch 4945/11884 - Loss: 31.0013\n",
      "Processing batch 4946/11884 - Loss: 30.2107\n",
      "Processing batch 4947/11884 - Loss: 30.2439\n",
      "Processing batch 4948/11884 - Loss: 29.6866\n",
      "Processing batch 4949/11884 - Loss: 30.7000\n",
      "Processing batch 4950/11884 - Loss: 29.9640\n",
      "Processing batch 4951/11884 - Loss: 30.6648\n",
      "Processing batch 4952/11884 - Loss: 30.0924\n",
      "Processing batch 4953/11884 - Loss: 29.7720\n",
      "Processing batch 4954/11884 - Loss: 29.5541\n",
      "Processing batch 4955/11884 - Loss: 29.7781\n",
      "Processing batch 4956/11884 - Loss: 29.7364\n",
      "Processing batch 4957/11884 - Loss: 30.8695\n",
      "Processing batch 4958/11884 - Loss: 30.7463\n",
      "Processing batch 4959/11884 - Loss: 30.3532\n",
      "Processing batch 4960/11884 - Loss: 29.2934\n",
      "Processing batch 4961/11884 - Loss: 31.9851\n",
      "Processing batch 4962/11884 - Loss: 29.6471\n",
      "Processing batch 4963/11884 - Loss: 31.1383\n",
      "Processing batch 4964/11884 - Loss: 31.0230\n",
      "Processing batch 4965/11884 - Loss: 31.0369\n",
      "Processing batch 4966/11884 - Loss: 31.2888\n",
      "Processing batch 4967/11884 - Loss: 28.3611\n",
      "Processing batch 4968/11884 - Loss: 29.4854\n",
      "Processing batch 4969/11884 - Loss: 30.3853\n",
      "Processing batch 4970/11884 - Loss: 30.3771\n",
      "Processing batch 4971/11884 - Loss: 29.5947\n",
      "Processing batch 4972/11884 - Loss: 30.5773\n",
      "Processing batch 4973/11884 - Loss: 29.6744\n",
      "Processing batch 4974/11884 - Loss: 31.2217\n",
      "Processing batch 4975/11884 - Loss: 29.9284\n",
      "Processing batch 4976/11884 - Loss: 30.1038\n",
      "Processing batch 4977/11884 - Loss: 30.5669\n",
      "Processing batch 4978/11884 - Loss: 30.4443\n",
      "Processing batch 4979/11884 - Loss: 30.2341\n",
      "Processing batch 4980/11884 - Loss: 31.2819\n",
      "Processing batch 4981/11884 - Loss: 29.3541\n",
      "Processing batch 4982/11884 - Loss: 29.8464\n",
      "Processing batch 4983/11884 - Loss: 31.5266\n",
      "Processing batch 4984/11884 - Loss: 30.5357\n",
      "Processing batch 4985/11884 - Loss: 30.7771\n",
      "Processing batch 4986/11884 - Loss: 30.7176\n",
      "Processing batch 4987/11884 - Loss: 31.8871\n",
      "Processing batch 4988/11884 - Loss: 30.9782\n",
      "Processing batch 4989/11884 - Loss: 29.5043\n",
      "Processing batch 4990/11884 - Loss: 32.0009\n",
      "Processing batch 4991/11884 - Loss: 29.7880\n",
      "Processing batch 4992/11884 - Loss: 30.9295\n",
      "Processing batch 4993/11884 - Loss: 30.1167\n",
      "Processing batch 4994/11884 - Loss: 31.2901\n",
      "Processing batch 4995/11884 - Loss: 30.1980\n",
      "Processing batch 4996/11884 - Loss: 30.7563\n",
      "Processing batch 4997/11884 - Loss: 32.4724\n",
      "Processing batch 4998/11884 - Loss: 29.9364\n",
      "Processing batch 4999/11884 - Loss: 30.4814\n",
      "Processing batch 5000/11884 - Loss: 29.1770\n",
      "Processing batch 5001/11884 - Loss: 31.8371\n",
      "Processing batch 5002/11884 - Loss: 30.4376\n",
      "Processing batch 5003/11884 - Loss: 29.5769\n",
      "Processing batch 5004/11884 - Loss: 30.7473\n",
      "Processing batch 5005/11884 - Loss: 31.0505\n",
      "Processing batch 5006/11884 - Loss: 30.5968\n",
      "Processing batch 5007/11884 - Loss: 29.4860\n",
      "Processing batch 5008/11884 - Loss: 30.8329\n",
      "Processing batch 5009/11884 - Loss: 30.0427\n",
      "Processing batch 5010/11884 - Loss: 29.9523\n",
      "Processing batch 5011/11884 - Loss: 31.3938\n",
      "Processing batch 5012/11884 - Loss: 29.9954\n",
      "Processing batch 5013/11884 - Loss: 30.4382\n",
      "Processing batch 5014/11884 - Loss: 30.9116\n",
      "Processing batch 5015/11884 - Loss: 30.2576\n",
      "Processing batch 5016/11884 - Loss: 31.1843\n",
      "Processing batch 5017/11884 - Loss: 30.8857\n",
      "Processing batch 5018/11884 - Loss: 31.2103\n",
      "Processing batch 5019/11884 - Loss: 29.8560\n",
      "Processing batch 5020/11884 - Loss: 29.1096\n",
      "Processing batch 5021/11884 - Loss: 28.8401\n",
      "Processing batch 5022/11884 - Loss: 30.2140\n",
      "Processing batch 5023/11884 - Loss: 31.8870\n",
      "Processing batch 5024/11884 - Loss: 30.0423\n",
      "Processing batch 5025/11884 - Loss: 29.9381\n",
      "Processing batch 5026/11884 - Loss: 30.1985\n",
      "Processing batch 5027/11884 - Loss: 30.4636\n",
      "Processing batch 5028/11884 - Loss: 30.2527\n",
      "Processing batch 5029/11884 - Loss: 31.3349\n",
      "Processing batch 5030/11884 - Loss: 31.6309\n",
      "Processing batch 5031/11884 - Loss: 31.1797\n",
      "Processing batch 5032/11884 - Loss: 30.1499\n",
      "Processing batch 5033/11884 - Loss: 31.7608\n",
      "Processing batch 5034/11884 - Loss: 30.8082\n",
      "Processing batch 5035/11884 - Loss: 31.7748\n",
      "Processing batch 5036/11884 - Loss: 30.5645\n",
      "Processing batch 5037/11884 - Loss: 29.2558\n",
      "Processing batch 5038/11884 - Loss: 30.2990\n",
      "Processing batch 5039/11884 - Loss: 31.2868\n",
      "Processing batch 5040/11884 - Loss: 30.5153\n",
      "Processing batch 5041/11884 - Loss: 30.1917\n",
      "Processing batch 5042/11884 - Loss: 31.7245\n",
      "Processing batch 5043/11884 - Loss: 32.2864\n",
      "Processing batch 5044/11884 - Loss: 30.8399\n",
      "Processing batch 5045/11884 - Loss: 29.7662\n",
      "Processing batch 5046/11884 - Loss: 31.4197\n",
      "Processing batch 5047/11884 - Loss: 29.2820\n",
      "Processing batch 5048/11884 - Loss: 30.5606\n",
      "Processing batch 5049/11884 - Loss: 30.9019\n",
      "Processing batch 5050/11884 - Loss: 30.1069\n",
      "Processing batch 5051/11884 - Loss: 30.3924\n",
      "Processing batch 5052/11884 - Loss: 30.5196\n",
      "Processing batch 5053/11884 - Loss: 29.9834\n",
      "Processing batch 5054/11884 - Loss: 28.5310\n",
      "Processing batch 5055/11884 - Loss: 31.2602\n",
      "Processing batch 5056/11884 - Loss: 30.1917\n",
      "Processing batch 5057/11884 - Loss: 31.0604\n",
      "Processing batch 5058/11884 - Loss: 32.4602\n",
      "Processing batch 5059/11884 - Loss: 32.1985\n",
      "Processing batch 5060/11884 - Loss: 30.4184\n",
      "Processing batch 5061/11884 - Loss: 29.3772\n",
      "Processing batch 5062/11884 - Loss: 30.9610\n",
      "Processing batch 5063/11884 - Loss: 30.7112\n",
      "Processing batch 5064/11884 - Loss: 31.6361\n",
      "Processing batch 5065/11884 - Loss: 30.5682\n",
      "Processing batch 5066/11884 - Loss: 31.0814\n",
      "Processing batch 5067/11884 - Loss: 30.7602\n",
      "Processing batch 5068/11884 - Loss: 29.7343\n",
      "Processing batch 5069/11884 - Loss: 31.0270\n",
      "Processing batch 5070/11884 - Loss: 32.2541\n",
      "Processing batch 5071/11884 - Loss: 30.9398\n",
      "Processing batch 5072/11884 - Loss: 30.7599\n",
      "Processing batch 5073/11884 - Loss: 30.8817\n",
      "Processing batch 5074/11884 - Loss: 30.1525\n",
      "Processing batch 5075/11884 - Loss: 31.7367\n",
      "Processing batch 5076/11884 - Loss: 30.7070\n",
      "Processing batch 5077/11884 - Loss: 29.5479\n",
      "Processing batch 5078/11884 - Loss: 30.5137\n",
      "Processing batch 5079/11884 - Loss: 29.2549\n",
      "Processing batch 5080/11884 - Loss: 29.8405\n",
      "Processing batch 5081/11884 - Loss: 30.5346\n",
      "Processing batch 5082/11884 - Loss: 30.4450\n",
      "Processing batch 5083/11884 - Loss: 30.3035\n",
      "Processing batch 5084/11884 - Loss: 30.2016\n",
      "Processing batch 5085/11884 - Loss: 29.2430\n",
      "Processing batch 5086/11884 - Loss: 31.9114\n",
      "Processing batch 5087/11884 - Loss: 29.8031\n",
      "Processing batch 5088/11884 - Loss: 31.3596\n",
      "Processing batch 5089/11884 - Loss: 29.9800\n",
      "Processing batch 5090/11884 - Loss: 31.7264\n",
      "Processing batch 5091/11884 - Loss: 30.5197\n",
      "Processing batch 5092/11884 - Loss: 30.1613\n",
      "Processing batch 5093/11884 - Loss: 29.9231\n",
      "Processing batch 5094/11884 - Loss: 31.8326\n",
      "Processing batch 5095/11884 - Loss: 30.3337\n",
      "Processing batch 5096/11884 - Loss: 31.3496\n",
      "Processing batch 5097/11884 - Loss: 30.7664\n",
      "Processing batch 5098/11884 - Loss: 31.3008\n",
      "Processing batch 5099/11884 - Loss: 31.0875\n",
      "Processing batch 5100/11884 - Loss: 30.0536\n",
      "Processing batch 5101/11884 - Loss: 31.2919\n",
      "Processing batch 5102/11884 - Loss: 31.5200\n",
      "Processing batch 5103/11884 - Loss: 30.9197\n",
      "Processing batch 5104/11884 - Loss: 28.7851\n",
      "Processing batch 5105/11884 - Loss: 30.2625\n",
      "Processing batch 5106/11884 - Loss: 29.7403\n",
      "Processing batch 5107/11884 - Loss: 31.9303\n",
      "Processing batch 5108/11884 - Loss: 29.9683\n",
      "Processing batch 5109/11884 - Loss: 30.8061\n",
      "Processing batch 5110/11884 - Loss: 32.2279\n",
      "Processing batch 5111/11884 - Loss: 29.0287\n",
      "Processing batch 5112/11884 - Loss: 30.5101\n",
      "Processing batch 5113/11884 - Loss: 29.8836\n",
      "Processing batch 5114/11884 - Loss: 30.9458\n",
      "Processing batch 5115/11884 - Loss: 29.1754\n",
      "Processing batch 5116/11884 - Loss: 30.8555\n",
      "Processing batch 5117/11884 - Loss: 30.6674\n",
      "Processing batch 5118/11884 - Loss: 30.9993\n",
      "Processing batch 5119/11884 - Loss: 31.0243\n",
      "Processing batch 5120/11884 - Loss: 31.3299\n",
      "Processing batch 5121/11884 - Loss: 30.6296\n",
      "Processing batch 5122/11884 - Loss: 30.8391\n",
      "Processing batch 5123/11884 - Loss: 30.8322\n",
      "Processing batch 5124/11884 - Loss: 30.4246\n",
      "Processing batch 5125/11884 - Loss: 30.6468\n",
      "Processing batch 5126/11884 - Loss: 30.5670\n",
      "Processing batch 5127/11884 - Loss: 30.9843\n",
      "Processing batch 5128/11884 - Loss: 31.2857\n",
      "Processing batch 5129/11884 - Loss: 29.1636\n",
      "Processing batch 5130/11884 - Loss: 29.8339\n",
      "Processing batch 5131/11884 - Loss: 31.0445\n",
      "Processing batch 5132/11884 - Loss: 30.3676\n",
      "Processing batch 5133/11884 - Loss: 29.9140\n",
      "Processing batch 5134/11884 - Loss: 30.5336\n",
      "Processing batch 5135/11884 - Loss: 30.3728\n",
      "Processing batch 5136/11884 - Loss: 30.6986\n",
      "Processing batch 5137/11884 - Loss: 31.0573\n",
      "Processing batch 5138/11884 - Loss: 30.8160\n",
      "Processing batch 5139/11884 - Loss: 30.6436\n",
      "Processing batch 5140/11884 - Loss: 32.0399\n",
      "Processing batch 5141/11884 - Loss: 29.3594\n",
      "Processing batch 5142/11884 - Loss: 29.7508\n",
      "Processing batch 5143/11884 - Loss: 31.1621\n",
      "Processing batch 5144/11884 - Loss: 30.4929\n",
      "Processing batch 5145/11884 - Loss: 31.2377\n",
      "Processing batch 5146/11884 - Loss: 29.5620\n",
      "Processing batch 5147/11884 - Loss: 30.0375\n",
      "Processing batch 5148/11884 - Loss: 31.3564\n",
      "Processing batch 5149/11884 - Loss: 30.1992\n",
      "Processing batch 5150/11884 - Loss: 30.7616\n",
      "Processing batch 5151/11884 - Loss: 30.7243\n",
      "Processing batch 5152/11884 - Loss: 30.3587\n",
      "Processing batch 5153/11884 - Loss: 31.1174\n",
      "Processing batch 5154/11884 - Loss: 30.2442\n",
      "Processing batch 5155/11884 - Loss: 29.1438\n",
      "Processing batch 5156/11884 - Loss: 30.7273\n",
      "Processing batch 5157/11884 - Loss: 31.7582\n",
      "Processing batch 5158/11884 - Loss: 30.3248\n",
      "Processing batch 5159/11884 - Loss: 30.4448\n",
      "Processing batch 5160/11884 - Loss: 30.5973\n",
      "Processing batch 5161/11884 - Loss: 29.3749\n",
      "Processing batch 5162/11884 - Loss: 28.1254\n",
      "Processing batch 5163/11884 - Loss: 30.4846\n",
      "Processing batch 5164/11884 - Loss: 30.0720\n",
      "Processing batch 5165/11884 - Loss: 29.6576\n",
      "Processing batch 5166/11884 - Loss: 31.0028\n",
      "Processing batch 5167/11884 - Loss: 30.3525\n",
      "Processing batch 5168/11884 - Loss: 29.9999\n",
      "Processing batch 5169/11884 - Loss: 30.9306\n",
      "Processing batch 5170/11884 - Loss: 31.6969\n",
      "Processing batch 5171/11884 - Loss: 30.4307\n",
      "Processing batch 5172/11884 - Loss: 28.6867\n",
      "Processing batch 5173/11884 - Loss: 31.0746\n",
      "Processing batch 5174/11884 - Loss: 30.7687\n",
      "Processing batch 5175/11884 - Loss: 30.6839\n",
      "Processing batch 5176/11884 - Loss: 31.3426\n",
      "Processing batch 5177/11884 - Loss: 31.5171\n",
      "Processing batch 5178/11884 - Loss: 31.2813\n",
      "Processing batch 5179/11884 - Loss: 29.8459\n",
      "Processing batch 5180/11884 - Loss: 30.1276\n",
      "Processing batch 5181/11884 - Loss: 30.5186\n",
      "Processing batch 5182/11884 - Loss: 30.6036\n",
      "Processing batch 5183/11884 - Loss: 30.5292\n",
      "Processing batch 5184/11884 - Loss: 30.1821\n",
      "Processing batch 5185/11884 - Loss: 31.8762\n",
      "Processing batch 5186/11884 - Loss: 30.1603\n",
      "Processing batch 5187/11884 - Loss: 31.3024\n",
      "Processing batch 5188/11884 - Loss: 31.1888\n",
      "Processing batch 5189/11884 - Loss: 30.3924\n",
      "Processing batch 5190/11884 - Loss: 28.6246\n",
      "Processing batch 5191/11884 - Loss: 30.3370\n",
      "Processing batch 5192/11884 - Loss: 29.5379\n",
      "Processing batch 5193/11884 - Loss: 30.4331\n",
      "Processing batch 5194/11884 - Loss: 30.7898\n",
      "Processing batch 5195/11884 - Loss: 31.1940\n",
      "Processing batch 5196/11884 - Loss: 32.0150\n",
      "Processing batch 5197/11884 - Loss: 31.5531\n",
      "Processing batch 5198/11884 - Loss: 30.7122\n",
      "Processing batch 5199/11884 - Loss: 30.4420\n",
      "Processing batch 5200/11884 - Loss: 30.0211\n",
      "Processing batch 5201/11884 - Loss: 30.2337\n",
      "Processing batch 5202/11884 - Loss: 31.0123\n",
      "Processing batch 5203/11884 - Loss: 30.3225\n",
      "Processing batch 5204/11884 - Loss: 30.1282\n",
      "Processing batch 5205/11884 - Loss: 30.4273\n",
      "Processing batch 5206/11884 - Loss: 31.0865\n",
      "Processing batch 5207/11884 - Loss: 29.7941\n",
      "Processing batch 5208/11884 - Loss: 31.0889\n",
      "Processing batch 5209/11884 - Loss: 31.9598\n",
      "Processing batch 5210/11884 - Loss: 30.2313\n",
      "Processing batch 5211/11884 - Loss: 31.6436\n",
      "Processing batch 5212/11884 - Loss: 31.7060\n",
      "Processing batch 5213/11884 - Loss: 30.0440\n",
      "Processing batch 5214/11884 - Loss: 31.6096\n",
      "Processing batch 5215/11884 - Loss: 31.5050\n",
      "Processing batch 5216/11884 - Loss: 32.3532\n",
      "Processing batch 5217/11884 - Loss: 31.9486\n",
      "Processing batch 5218/11884 - Loss: 30.7025\n",
      "Processing batch 5219/11884 - Loss: 30.9650\n",
      "Processing batch 5220/11884 - Loss: 29.4293\n",
      "Processing batch 5221/11884 - Loss: 30.4366\n",
      "Processing batch 5222/11884 - Loss: 30.6325\n",
      "Processing batch 5223/11884 - Loss: 31.5157\n",
      "Processing batch 5224/11884 - Loss: 30.2098\n",
      "Processing batch 5225/11884 - Loss: 30.0268\n",
      "Processing batch 5226/11884 - Loss: 30.6914\n",
      "Processing batch 5227/11884 - Loss: 30.7303\n",
      "Processing batch 5228/11884 - Loss: 30.3436\n",
      "Processing batch 5229/11884 - Loss: 29.6927\n",
      "Processing batch 5230/11884 - Loss: 30.5566\n",
      "Processing batch 5231/11884 - Loss: 30.6177\n",
      "Processing batch 5232/11884 - Loss: 30.5329\n",
      "Processing batch 5233/11884 - Loss: 31.5363\n",
      "Processing batch 5234/11884 - Loss: 29.8452\n",
      "Processing batch 5235/11884 - Loss: 29.3408\n",
      "Processing batch 5236/11884 - Loss: 29.9285\n",
      "Processing batch 5237/11884 - Loss: 29.8688\n",
      "Processing batch 5238/11884 - Loss: 32.1681\n",
      "Processing batch 5239/11884 - Loss: 30.4975\n",
      "Processing batch 5240/11884 - Loss: 30.4484\n",
      "Processing batch 5241/11884 - Loss: 32.0717\n",
      "Processing batch 5242/11884 - Loss: 30.4745\n",
      "Processing batch 5243/11884 - Loss: 31.2297\n",
      "Processing batch 5244/11884 - Loss: 30.7444\n",
      "Processing batch 5245/11884 - Loss: 30.6832\n",
      "Processing batch 5246/11884 - Loss: 32.3326\n",
      "Processing batch 5247/11884 - Loss: 31.8194\n",
      "Processing batch 5248/11884 - Loss: 30.5043\n",
      "Processing batch 5249/11884 - Loss: 30.4057\n",
      "Processing batch 5250/11884 - Loss: 30.3892\n",
      "Processing batch 5251/11884 - Loss: 31.1223\n",
      "Processing batch 5252/11884 - Loss: 30.4044\n",
      "Processing batch 5253/11884 - Loss: 29.9085\n",
      "Processing batch 5254/11884 - Loss: 30.2769\n",
      "Processing batch 5255/11884 - Loss: 30.4618\n",
      "Processing batch 5256/11884 - Loss: 30.6693\n",
      "Processing batch 5257/11884 - Loss: 30.3215\n",
      "Processing batch 5258/11884 - Loss: 30.4389\n",
      "Processing batch 5259/11884 - Loss: 30.0422\n",
      "Processing batch 5260/11884 - Loss: 32.1650\n",
      "Processing batch 5261/11884 - Loss: 30.6141\n",
      "Processing batch 5262/11884 - Loss: 29.8637\n",
      "Processing batch 5263/11884 - Loss: 30.7891\n",
      "Processing batch 5264/11884 - Loss: 30.3480\n",
      "Processing batch 5265/11884 - Loss: 31.4146\n",
      "Processing batch 5266/11884 - Loss: 30.8399\n",
      "Processing batch 5267/11884 - Loss: 31.0461\n",
      "Processing batch 5268/11884 - Loss: 30.4740\n",
      "Processing batch 5269/11884 - Loss: 31.3051\n",
      "Processing batch 5270/11884 - Loss: 30.6216\n",
      "Processing batch 5271/11884 - Loss: 30.1303\n",
      "Processing batch 5272/11884 - Loss: 30.3429\n",
      "Processing batch 5273/11884 - Loss: 30.5602\n",
      "Processing batch 5274/11884 - Loss: 30.8868\n",
      "Processing batch 5275/11884 - Loss: 30.4476\n",
      "Processing batch 5276/11884 - Loss: 31.0876\n",
      "Processing batch 5277/11884 - Loss: 30.4612\n",
      "Processing batch 5278/11884 - Loss: 30.1723\n",
      "Processing batch 5279/11884 - Loss: 30.4839\n",
      "Processing batch 5280/11884 - Loss: 29.9462\n",
      "Processing batch 5281/11884 - Loss: 31.1234\n",
      "Processing batch 5282/11884 - Loss: 30.2262\n",
      "Processing batch 5283/11884 - Loss: 30.3079\n",
      "Processing batch 5284/11884 - Loss: 31.3263\n",
      "Processing batch 5285/11884 - Loss: 31.7776\n",
      "Processing batch 5286/11884 - Loss: 30.9408\n",
      "Processing batch 5287/11884 - Loss: 31.5755\n",
      "Processing batch 5288/11884 - Loss: 29.7038\n",
      "Processing batch 5289/11884 - Loss: 30.8789\n",
      "Processing batch 5290/11884 - Loss: 31.1935\n",
      "Processing batch 5291/11884 - Loss: 31.6744\n",
      "Processing batch 5292/11884 - Loss: 30.4434\n",
      "Processing batch 5293/11884 - Loss: 31.8502\n",
      "Processing batch 5294/11884 - Loss: 29.6846\n",
      "Processing batch 5295/11884 - Loss: 30.8432\n",
      "Processing batch 5296/11884 - Loss: 30.0731\n",
      "Processing batch 5297/11884 - Loss: 31.8569\n",
      "Processing batch 5298/11884 - Loss: 31.6777\n",
      "Processing batch 5299/11884 - Loss: 31.4783\n",
      "Processing batch 5300/11884 - Loss: 29.5016\n",
      "Processing batch 5301/11884 - Loss: 29.9460\n",
      "Processing batch 5302/11884 - Loss: 30.9136\n",
      "Processing batch 5303/11884 - Loss: 30.7614\n",
      "Processing batch 5304/11884 - Loss: 30.1054\n",
      "Processing batch 5305/11884 - Loss: 31.0896\n",
      "Processing batch 5306/11884 - Loss: 31.3356\n",
      "Processing batch 5307/11884 - Loss: 31.3643\n",
      "Processing batch 5308/11884 - Loss: 30.6973\n",
      "Processing batch 5309/11884 - Loss: 31.3123\n",
      "Processing batch 5310/11884 - Loss: 29.8597\n",
      "Processing batch 5311/11884 - Loss: 30.8387\n",
      "Processing batch 5312/11884 - Loss: 30.9155\n",
      "Processing batch 5313/11884 - Loss: 29.3355\n",
      "Processing batch 5314/11884 - Loss: 30.5850\n",
      "Processing batch 5315/11884 - Loss: 30.2213\n",
      "Processing batch 5316/11884 - Loss: 32.0070\n",
      "Processing batch 5317/11884 - Loss: 30.0249\n",
      "Processing batch 5318/11884 - Loss: 32.3601\n",
      "Processing batch 5319/11884 - Loss: 30.7228\n",
      "Processing batch 5320/11884 - Loss: 30.9238\n",
      "Processing batch 5321/11884 - Loss: 30.9233\n",
      "Processing batch 5322/11884 - Loss: 29.7538\n",
      "Processing batch 5323/11884 - Loss: 31.4961\n",
      "Processing batch 5324/11884 - Loss: 31.4576\n",
      "Processing batch 5325/11884 - Loss: 29.7486\n",
      "Processing batch 5326/11884 - Loss: 29.7986\n",
      "Processing batch 5327/11884 - Loss: 29.6393\n",
      "Processing batch 5328/11884 - Loss: 30.0371\n",
      "Processing batch 5329/11884 - Loss: 30.6748\n",
      "Processing batch 5330/11884 - Loss: 30.6545\n",
      "Processing batch 5331/11884 - Loss: 30.0242\n",
      "Processing batch 5332/11884 - Loss: 30.3731\n",
      "Processing batch 5333/11884 - Loss: 29.1534\n",
      "Processing batch 5334/11884 - Loss: 29.7795\n",
      "Processing batch 5335/11884 - Loss: 29.5156\n",
      "Processing batch 5336/11884 - Loss: 30.9276\n",
      "Processing batch 5337/11884 - Loss: 30.7109\n",
      "Processing batch 5338/11884 - Loss: 30.2892\n",
      "Processing batch 5339/11884 - Loss: 30.4938\n",
      "Processing batch 5340/11884 - Loss: 30.6471\n",
      "Processing batch 5341/11884 - Loss: 30.6994\n",
      "Processing batch 5342/11884 - Loss: 30.7650\n",
      "Processing batch 5343/11884 - Loss: 30.2739\n",
      "Processing batch 5344/11884 - Loss: 28.8697\n",
      "Processing batch 5345/11884 - Loss: 31.7996\n",
      "Processing batch 5346/11884 - Loss: 30.9458\n",
      "Processing batch 5347/11884 - Loss: 31.9439\n",
      "Processing batch 5348/11884 - Loss: 29.8620\n",
      "Processing batch 5349/11884 - Loss: 30.9680\n",
      "Processing batch 5350/11884 - Loss: 30.4749\n",
      "Processing batch 5351/11884 - Loss: 31.3188\n",
      "Processing batch 5352/11884 - Loss: 29.9597\n",
      "Processing batch 5353/11884 - Loss: 29.5745\n",
      "Processing batch 5354/11884 - Loss: 30.3701\n",
      "Processing batch 5355/11884 - Loss: 30.8836\n",
      "Processing batch 5356/11884 - Loss: 31.4065\n",
      "Processing batch 5357/11884 - Loss: 29.3057\n",
      "Processing batch 5358/11884 - Loss: 30.7273\n",
      "Processing batch 5359/11884 - Loss: 31.3511\n",
      "Processing batch 5360/11884 - Loss: 29.8051\n",
      "Processing batch 5361/11884 - Loss: 30.6121\n",
      "Processing batch 5362/11884 - Loss: 31.6496\n",
      "Processing batch 5363/11884 - Loss: 30.3730\n",
      "Processing batch 5364/11884 - Loss: 30.6496\n",
      "Processing batch 5365/11884 - Loss: 29.6088\n",
      "Processing batch 5366/11884 - Loss: 31.0646\n",
      "Processing batch 5367/11884 - Loss: 30.5672\n",
      "Processing batch 5368/11884 - Loss: 30.6988\n",
      "Processing batch 5369/11884 - Loss: 31.2327\n",
      "Processing batch 5370/11884 - Loss: 31.3038\n",
      "Processing batch 5371/11884 - Loss: 30.3657\n",
      "Processing batch 5372/11884 - Loss: 31.3731\n",
      "Processing batch 5373/11884 - Loss: 31.9859\n",
      "Processing batch 5374/11884 - Loss: 29.8293\n",
      "Processing batch 5375/11884 - Loss: 30.1157\n",
      "Processing batch 5376/11884 - Loss: 30.0516\n",
      "Processing batch 5377/11884 - Loss: 30.4616\n",
      "Processing batch 5378/11884 - Loss: 30.9860\n",
      "Processing batch 5379/11884 - Loss: 30.3106\n",
      "Processing batch 5380/11884 - Loss: 30.3369\n",
      "Processing batch 5381/11884 - Loss: 30.1826\n",
      "Processing batch 5382/11884 - Loss: 30.3765\n",
      "Processing batch 5383/11884 - Loss: 30.6678\n",
      "Processing batch 5384/11884 - Loss: 29.4138\n",
      "Processing batch 5385/11884 - Loss: 30.4382\n",
      "Processing batch 5386/11884 - Loss: 31.1495\n",
      "Processing batch 5387/11884 - Loss: 29.7478\n",
      "Processing batch 5388/11884 - Loss: 31.9201\n",
      "Processing batch 5389/11884 - Loss: 32.3826\n",
      "Processing batch 5390/11884 - Loss: 30.1950\n",
      "Processing batch 5391/11884 - Loss: 30.7359\n",
      "Processing batch 5392/11884 - Loss: 31.3889\n",
      "Processing batch 5393/11884 - Loss: 29.8671\n",
      "Processing batch 5394/11884 - Loss: 31.2265\n",
      "Processing batch 5395/11884 - Loss: 31.4934\n",
      "Processing batch 5396/11884 - Loss: 29.9773\n",
      "Processing batch 5397/11884 - Loss: 31.9302\n",
      "Processing batch 5398/11884 - Loss: 30.9244\n",
      "Processing batch 5399/11884 - Loss: 30.2399\n",
      "Processing batch 5400/11884 - Loss: 30.0258\n",
      "Processing batch 5401/11884 - Loss: 30.2481\n",
      "Processing batch 5402/11884 - Loss: 30.3162\n",
      "Processing batch 5403/11884 - Loss: 29.5364\n",
      "Processing batch 5404/11884 - Loss: 31.0216\n",
      "Processing batch 5405/11884 - Loss: 30.1356\n",
      "Processing batch 5406/11884 - Loss: 30.1573\n",
      "Processing batch 5407/11884 - Loss: 30.9239\n",
      "Processing batch 5408/11884 - Loss: 30.7847\n",
      "Processing batch 5409/11884 - Loss: 30.7645\n",
      "Processing batch 5410/11884 - Loss: 31.6791\n",
      "Processing batch 5411/11884 - Loss: 29.9182\n",
      "Processing batch 5412/11884 - Loss: 30.3470\n",
      "Processing batch 5413/11884 - Loss: 32.2012\n",
      "Processing batch 5414/11884 - Loss: 29.3814\n",
      "Processing batch 5415/11884 - Loss: 29.3208\n",
      "Processing batch 5416/11884 - Loss: 31.2008\n",
      "Processing batch 5417/11884 - Loss: 29.9498\n",
      "Processing batch 5418/11884 - Loss: 30.1858\n",
      "Processing batch 5419/11884 - Loss: 30.5250\n",
      "Processing batch 5420/11884 - Loss: 29.8297\n",
      "Processing batch 5421/11884 - Loss: 29.7339\n",
      "Processing batch 5422/11884 - Loss: 30.1030\n",
      "Processing batch 5423/11884 - Loss: 30.0702\n",
      "Processing batch 5424/11884 - Loss: 30.0918\n",
      "Processing batch 5425/11884 - Loss: 31.1001\n",
      "Processing batch 5426/11884 - Loss: 30.7486\n",
      "Processing batch 5427/11884 - Loss: 30.1849\n",
      "Processing batch 5428/11884 - Loss: 30.8114\n",
      "Processing batch 5429/11884 - Loss: 30.6734\n",
      "Processing batch 5430/11884 - Loss: 32.7096\n",
      "Processing batch 5431/11884 - Loss: 29.5768\n",
      "Processing batch 5432/11884 - Loss: 32.0984\n",
      "Processing batch 5433/11884 - Loss: 29.7227\n",
      "Processing batch 5434/11884 - Loss: 31.2324\n",
      "Processing batch 5435/11884 - Loss: 31.2768\n",
      "Processing batch 5436/11884 - Loss: 30.3277\n",
      "Processing batch 5437/11884 - Loss: 31.0752\n",
      "Processing batch 5438/11884 - Loss: 30.4678\n",
      "Processing batch 5439/11884 - Loss: 29.8889\n",
      "Processing batch 5440/11884 - Loss: 29.6560\n",
      "Processing batch 5441/11884 - Loss: 30.5485\n",
      "Processing batch 5442/11884 - Loss: 31.5034\n",
      "Processing batch 5443/11884 - Loss: 30.3005\n",
      "Processing batch 5444/11884 - Loss: 30.8052\n",
      "Processing batch 5445/11884 - Loss: 32.0861\n",
      "Processing batch 5446/11884 - Loss: 29.4962\n",
      "Processing batch 5447/11884 - Loss: 30.9334\n",
      "Processing batch 5448/11884 - Loss: 31.1396\n",
      "Processing batch 5449/11884 - Loss: 29.9327\n",
      "Processing batch 5450/11884 - Loss: 30.6737\n",
      "Processing batch 5451/11884 - Loss: 29.3697\n",
      "Processing batch 5452/11884 - Loss: 31.6266\n",
      "Processing batch 5453/11884 - Loss: 32.0382\n",
      "Processing batch 5454/11884 - Loss: 30.0936\n",
      "Processing batch 5455/11884 - Loss: 31.3574\n",
      "Processing batch 5456/11884 - Loss: 30.6920\n",
      "Processing batch 5457/11884 - Loss: 30.2090\n",
      "Processing batch 5458/11884 - Loss: 30.8935\n",
      "Processing batch 5459/11884 - Loss: 30.6545\n",
      "Processing batch 5460/11884 - Loss: 30.8561\n",
      "Processing batch 5461/11884 - Loss: 29.4441\n",
      "Processing batch 5462/11884 - Loss: 31.4564\n",
      "Processing batch 5463/11884 - Loss: 29.8377\n",
      "Processing batch 5464/11884 - Loss: 30.6016\n",
      "Processing batch 5465/11884 - Loss: 31.2729\n",
      "Processing batch 5466/11884 - Loss: 29.8043\n",
      "Processing batch 5467/11884 - Loss: 30.5458\n",
      "Processing batch 5468/11884 - Loss: 29.8711\n",
      "Processing batch 5469/11884 - Loss: 30.7281\n",
      "Processing batch 5470/11884 - Loss: 29.4510\n",
      "Processing batch 5471/11884 - Loss: 30.5325\n",
      "Processing batch 5472/11884 - Loss: 30.1954\n",
      "Processing batch 5473/11884 - Loss: 30.5219\n",
      "Processing batch 5474/11884 - Loss: 30.4442\n",
      "Processing batch 5475/11884 - Loss: 29.3201\n",
      "Processing batch 5476/11884 - Loss: 30.7875\n",
      "Processing batch 5477/11884 - Loss: 30.0284\n",
      "Processing batch 5478/11884 - Loss: 30.7242\n",
      "Processing batch 5479/11884 - Loss: 29.9443\n",
      "Processing batch 5480/11884 - Loss: 30.7704\n",
      "Processing batch 5481/11884 - Loss: 29.7191\n",
      "Processing batch 5482/11884 - Loss: 30.5959\n",
      "Processing batch 5483/11884 - Loss: 30.3365\n",
      "Processing batch 5484/11884 - Loss: 31.6206\n",
      "Processing batch 5485/11884 - Loss: 32.2296\n",
      "Processing batch 5486/11884 - Loss: 29.5676\n",
      "Processing batch 5487/11884 - Loss: 30.4773\n",
      "Processing batch 5488/11884 - Loss: 30.1598\n",
      "Processing batch 5489/11884 - Loss: 30.2184\n",
      "Processing batch 5490/11884 - Loss: 31.4114\n",
      "Processing batch 5491/11884 - Loss: 31.2654\n",
      "Processing batch 5492/11884 - Loss: 30.7555\n",
      "Processing batch 5493/11884 - Loss: 31.0325\n",
      "Processing batch 5494/11884 - Loss: 29.6937\n",
      "Processing batch 5495/11884 - Loss: 29.9623\n",
      "Processing batch 5496/11884 - Loss: 30.9699\n",
      "Processing batch 5497/11884 - Loss: 30.0038\n",
      "Processing batch 5498/11884 - Loss: 30.7226\n",
      "Processing batch 5499/11884 - Loss: 30.4834\n",
      "Processing batch 5500/11884 - Loss: 30.6706\n",
      "Processing batch 5501/11884 - Loss: 30.7004\n",
      "Processing batch 5502/11884 - Loss: 30.2617\n",
      "Processing batch 5503/11884 - Loss: 30.8545\n",
      "Processing batch 5504/11884 - Loss: 30.5839\n",
      "Processing batch 5505/11884 - Loss: 30.0187\n",
      "Processing batch 5506/11884 - Loss: 29.1057\n",
      "Processing batch 5507/11884 - Loss: 31.4434\n",
      "Processing batch 5508/11884 - Loss: 30.3230\n",
      "Processing batch 5509/11884 - Loss: 30.3850\n",
      "Processing batch 5510/11884 - Loss: 31.5603\n",
      "Processing batch 5511/11884 - Loss: 30.7982\n",
      "Processing batch 5512/11884 - Loss: 31.3883\n",
      "Processing batch 5513/11884 - Loss: 31.9517\n",
      "Processing batch 5514/11884 - Loss: 31.5770\n",
      "Processing batch 5515/11884 - Loss: 30.3017\n",
      "Processing batch 5516/11884 - Loss: 29.8339\n",
      "Processing batch 5517/11884 - Loss: 31.1190\n",
      "Processing batch 5518/11884 - Loss: 30.3140\n",
      "Processing batch 5519/11884 - Loss: 30.1232\n",
      "Processing batch 5520/11884 - Loss: 30.6615\n",
      "Processing batch 5521/11884 - Loss: 30.0659\n",
      "Processing batch 5522/11884 - Loss: 31.7774\n",
      "Processing batch 5523/11884 - Loss: 29.9613\n",
      "Processing batch 5524/11884 - Loss: 28.8723\n",
      "Processing batch 5525/11884 - Loss: 30.2815\n",
      "Processing batch 5526/11884 - Loss: 30.3128\n",
      "Processing batch 5527/11884 - Loss: 30.9575\n",
      "Processing batch 5528/11884 - Loss: 30.0649\n",
      "Processing batch 5529/11884 - Loss: 31.3236\n",
      "Processing batch 5530/11884 - Loss: 32.5365\n",
      "Processing batch 5531/11884 - Loss: 30.9789\n",
      "Processing batch 5532/11884 - Loss: 30.4170\n",
      "Processing batch 5533/11884 - Loss: 30.3268\n",
      "Processing batch 5534/11884 - Loss: 30.2736\n",
      "Processing batch 5535/11884 - Loss: 31.2830\n",
      "Processing batch 5536/11884 - Loss: 31.4904\n",
      "Processing batch 5537/11884 - Loss: 30.6580\n",
      "Processing batch 5538/11884 - Loss: 31.3824\n",
      "Processing batch 5539/11884 - Loss: 30.9433\n",
      "Processing batch 5540/11884 - Loss: 30.1034\n",
      "Processing batch 5541/11884 - Loss: 30.8217\n",
      "Processing batch 5542/11884 - Loss: 30.8690\n",
      "Processing batch 5543/11884 - Loss: 30.6741\n",
      "Processing batch 5544/11884 - Loss: 30.8537\n",
      "Processing batch 5545/11884 - Loss: 29.8470\n",
      "Processing batch 5546/11884 - Loss: 30.3755\n",
      "Processing batch 5547/11884 - Loss: 30.2055\n",
      "Processing batch 5548/11884 - Loss: 29.7537\n",
      "Processing batch 5549/11884 - Loss: 30.6685\n",
      "Processing batch 5550/11884 - Loss: 30.4376\n",
      "Processing batch 5551/11884 - Loss: 30.5290\n",
      "Processing batch 5552/11884 - Loss: 30.6007\n",
      "Processing batch 5553/11884 - Loss: 30.6477\n",
      "Processing batch 5554/11884 - Loss: 29.7582\n",
      "Processing batch 5555/11884 - Loss: 28.8506\n",
      "Processing batch 5556/11884 - Loss: 30.1184\n",
      "Processing batch 5557/11884 - Loss: 30.3555\n",
      "Processing batch 5558/11884 - Loss: 31.3968\n",
      "Processing batch 5559/11884 - Loss: 29.6758\n",
      "Processing batch 5560/11884 - Loss: 29.3764\n",
      "Processing batch 5561/11884 - Loss: 29.8825\n",
      "Processing batch 5562/11884 - Loss: 29.0828\n",
      "Processing batch 5563/11884 - Loss: 30.2498\n",
      "Processing batch 5564/11884 - Loss: 29.8744\n",
      "Processing batch 5565/11884 - Loss: 30.7019\n",
      "Processing batch 5566/11884 - Loss: 30.9855\n",
      "Processing batch 5567/11884 - Loss: 29.5781\n",
      "Processing batch 5568/11884 - Loss: 30.0541\n",
      "Processing batch 5569/11884 - Loss: 29.3770\n",
      "Processing batch 5570/11884 - Loss: 30.2768\n",
      "Processing batch 5571/11884 - Loss: 30.4345\n",
      "Processing batch 5572/11884 - Loss: 31.0834\n",
      "Processing batch 5573/11884 - Loss: 31.5816\n",
      "Processing batch 5574/11884 - Loss: 29.6816\n",
      "Processing batch 5575/11884 - Loss: 30.1832\n",
      "Processing batch 5576/11884 - Loss: 29.6062\n",
      "Processing batch 5577/11884 - Loss: 29.8751\n",
      "Processing batch 5578/11884 - Loss: 30.1810\n",
      "Processing batch 5579/11884 - Loss: 30.9016\n",
      "Processing batch 5580/11884 - Loss: 31.2647\n",
      "Processing batch 5581/11884 - Loss: 29.5533\n",
      "Processing batch 5582/11884 - Loss: 30.9826\n",
      "Processing batch 5583/11884 - Loss: 29.7151\n",
      "Processing batch 5584/11884 - Loss: 31.2802\n",
      "Processing batch 5585/11884 - Loss: 29.6215\n",
      "Processing batch 5586/11884 - Loss: 30.6111\n",
      "Processing batch 5587/11884 - Loss: 28.6976\n",
      "Processing batch 5588/11884 - Loss: 29.9250\n",
      "Processing batch 5589/11884 - Loss: 29.6436\n",
      "Processing batch 5590/11884 - Loss: 30.9349\n",
      "Processing batch 5591/11884 - Loss: 32.6277\n",
      "Processing batch 5592/11884 - Loss: 29.4071\n",
      "Processing batch 5593/11884 - Loss: 29.9914\n",
      "Processing batch 5594/11884 - Loss: 30.6400\n",
      "Processing batch 5595/11884 - Loss: 31.0829\n",
      "Processing batch 5596/11884 - Loss: 28.9633\n",
      "Processing batch 5597/11884 - Loss: 31.6677\n",
      "Processing batch 5598/11884 - Loss: 30.1454\n",
      "Processing batch 5599/11884 - Loss: 30.2975\n",
      "Processing batch 5600/11884 - Loss: 29.9291\n",
      "Processing batch 5601/11884 - Loss: 31.5145\n",
      "Processing batch 5602/11884 - Loss: 31.4800\n",
      "Processing batch 5603/11884 - Loss: 30.3162\n",
      "Processing batch 5604/11884 - Loss: 30.4821\n",
      "Processing batch 5605/11884 - Loss: 31.9098\n",
      "Processing batch 5606/11884 - Loss: 29.6023\n",
      "Processing batch 5607/11884 - Loss: 30.5729\n",
      "Processing batch 5608/11884 - Loss: 29.9508\n",
      "Processing batch 5609/11884 - Loss: 31.1805\n",
      "Processing batch 5610/11884 - Loss: 29.5237\n",
      "Processing batch 5611/11884 - Loss: 30.6228\n",
      "Processing batch 5612/11884 - Loss: 30.6845\n",
      "Processing batch 5613/11884 - Loss: 31.4085\n",
      "Processing batch 5614/11884 - Loss: 30.0831\n",
      "Processing batch 5615/11884 - Loss: 30.9050\n",
      "Processing batch 5616/11884 - Loss: 30.1006\n",
      "Processing batch 5617/11884 - Loss: 31.0280\n",
      "Processing batch 5618/11884 - Loss: 30.2307\n",
      "Processing batch 5619/11884 - Loss: 31.1642\n",
      "Processing batch 5620/11884 - Loss: 31.4207\n",
      "Processing batch 5621/11884 - Loss: 31.0084\n",
      "Processing batch 5622/11884 - Loss: 28.8355\n",
      "Processing batch 5623/11884 - Loss: 32.3391\n",
      "Processing batch 5624/11884 - Loss: 30.1439\n",
      "Processing batch 5625/11884 - Loss: 30.1893\n",
      "Processing batch 5626/11884 - Loss: 30.3326\n",
      "Processing batch 5627/11884 - Loss: 29.9452\n",
      "Processing batch 5628/11884 - Loss: 31.1366\n",
      "Processing batch 5629/11884 - Loss: 30.1345\n",
      "Processing batch 5630/11884 - Loss: 29.8005\n",
      "Processing batch 5631/11884 - Loss: 31.4526\n",
      "Processing batch 5632/11884 - Loss: 30.8079\n",
      "Processing batch 5633/11884 - Loss: 30.2279\n",
      "Processing batch 5634/11884 - Loss: 29.2545\n",
      "Processing batch 5635/11884 - Loss: 30.1494\n",
      "Processing batch 5636/11884 - Loss: 30.9886\n",
      "Processing batch 5637/11884 - Loss: 28.8436\n",
      "Processing batch 5638/11884 - Loss: 30.1778\n",
      "Processing batch 5639/11884 - Loss: 30.1005\n",
      "Processing batch 5640/11884 - Loss: 30.7022\n",
      "Processing batch 5641/11884 - Loss: 30.3323\n",
      "Processing batch 5642/11884 - Loss: 30.5261\n",
      "Processing batch 5643/11884 - Loss: 29.8740\n",
      "Processing batch 5644/11884 - Loss: 30.2192\n",
      "Processing batch 5645/11884 - Loss: 31.5204\n",
      "Processing batch 5646/11884 - Loss: 29.4695\n",
      "Processing batch 5647/11884 - Loss: 30.7661\n",
      "Processing batch 5648/11884 - Loss: 31.1939\n",
      "Processing batch 5649/11884 - Loss: 30.6187\n",
      "Processing batch 5650/11884 - Loss: 30.7590\n",
      "Processing batch 5651/11884 - Loss: 30.5409\n",
      "Processing batch 5652/11884 - Loss: 30.2253\n",
      "Processing batch 5653/11884 - Loss: 30.4185\n",
      "Processing batch 5654/11884 - Loss: 31.3634\n",
      "Processing batch 5655/11884 - Loss: 30.8947\n",
      "Processing batch 5656/11884 - Loss: 30.1744\n",
      "Processing batch 5657/11884 - Loss: 27.8512\n",
      "Processing batch 5658/11884 - Loss: 30.8384\n",
      "Processing batch 5659/11884 - Loss: 31.7750\n",
      "Processing batch 5660/11884 - Loss: 30.9612\n",
      "Processing batch 5661/11884 - Loss: 30.1974\n",
      "Processing batch 5662/11884 - Loss: 29.5998\n",
      "Processing batch 5663/11884 - Loss: 31.3768\n",
      "Processing batch 5664/11884 - Loss: 31.5584\n",
      "Processing batch 5665/11884 - Loss: 30.4752\n",
      "Processing batch 5666/11884 - Loss: 30.7443\n",
      "Processing batch 5667/11884 - Loss: 31.0350\n",
      "Processing batch 5668/11884 - Loss: 29.7293\n",
      "Processing batch 5669/11884 - Loss: 31.4489\n",
      "Processing batch 5670/11884 - Loss: 30.6188\n",
      "Processing batch 5671/11884 - Loss: 31.8154\n",
      "Processing batch 5672/11884 - Loss: 29.7870\n",
      "Processing batch 5673/11884 - Loss: 30.3524\n",
      "Processing batch 5674/11884 - Loss: 30.1943\n",
      "Processing batch 5675/11884 - Loss: 30.0267\n",
      "Processing batch 5676/11884 - Loss: 30.9770\n",
      "Processing batch 5677/11884 - Loss: 31.1091\n",
      "Processing batch 5678/11884 - Loss: 29.5823\n",
      "Processing batch 5679/11884 - Loss: 30.0493\n",
      "Processing batch 5680/11884 - Loss: 30.5273\n",
      "Processing batch 5681/11884 - Loss: 31.8087\n",
      "Processing batch 5682/11884 - Loss: 29.9683\n",
      "Processing batch 5683/11884 - Loss: 30.7249\n",
      "Processing batch 5684/11884 - Loss: 29.4297\n",
      "Processing batch 5685/11884 - Loss: 30.9301\n",
      "Processing batch 5686/11884 - Loss: 31.0349\n",
      "Processing batch 5687/11884 - Loss: 31.3544\n",
      "Processing batch 5688/11884 - Loss: 28.9824\n",
      "Processing batch 5689/11884 - Loss: 30.5958\n",
      "Processing batch 5690/11884 - Loss: 30.0754\n",
      "Processing batch 5691/11884 - Loss: 31.2380\n",
      "Processing batch 5692/11884 - Loss: 30.8807\n",
      "Processing batch 5693/11884 - Loss: 30.0263\n",
      "Processing batch 5694/11884 - Loss: 31.6594\n",
      "Processing batch 5695/11884 - Loss: 30.9961\n",
      "Processing batch 5696/11884 - Loss: 30.8633\n",
      "Processing batch 5697/11884 - Loss: 30.5636\n",
      "Processing batch 5698/11884 - Loss: 30.0153\n",
      "Processing batch 5699/11884 - Loss: 31.6899\n",
      "Processing batch 5700/11884 - Loss: 30.0586\n",
      "Processing batch 5701/11884 - Loss: 31.3817\n",
      "Processing batch 5702/11884 - Loss: 31.7080\n",
      "Processing batch 5703/11884 - Loss: 31.3957\n",
      "Processing batch 5704/11884 - Loss: 29.5900\n",
      "Processing batch 5705/11884 - Loss: 29.5934\n",
      "Processing batch 5706/11884 - Loss: 30.0725\n",
      "Processing batch 5707/11884 - Loss: 30.1454\n",
      "Processing batch 5708/11884 - Loss: 30.3101\n",
      "Processing batch 5709/11884 - Loss: 31.1872\n",
      "Processing batch 5710/11884 - Loss: 31.3131\n",
      "Processing batch 5711/11884 - Loss: 30.9069\n",
      "Processing batch 5712/11884 - Loss: 29.7084\n",
      "Processing batch 5713/11884 - Loss: 30.6597\n",
      "Processing batch 5714/11884 - Loss: 31.5804\n",
      "Processing batch 5715/11884 - Loss: 31.1018\n",
      "Processing batch 5716/11884 - Loss: 31.2448\n",
      "Processing batch 5717/11884 - Loss: 31.1504\n",
      "Processing batch 5718/11884 - Loss: 30.6369\n",
      "Processing batch 5719/11884 - Loss: 31.1842\n",
      "Processing batch 5720/11884 - Loss: 28.7578\n",
      "Processing batch 5721/11884 - Loss: 29.3957\n",
      "Processing batch 5722/11884 - Loss: 30.4900\n",
      "Processing batch 5723/11884 - Loss: 32.2498\n",
      "Processing batch 5724/11884 - Loss: 30.7905\n",
      "Processing batch 5725/11884 - Loss: 29.6767\n",
      "Processing batch 5726/11884 - Loss: 29.9095\n",
      "Processing batch 5727/11884 - Loss: 30.3856\n",
      "Processing batch 5728/11884 - Loss: 32.3785\n",
      "Processing batch 5729/11884 - Loss: 30.5192\n",
      "Processing batch 5730/11884 - Loss: 30.9083\n",
      "Processing batch 5731/11884 - Loss: 30.5549\n",
      "Processing batch 5732/11884 - Loss: 31.9662\n",
      "Processing batch 5733/11884 - Loss: 30.1602\n",
      "Processing batch 5734/11884 - Loss: 30.7129\n",
      "Processing batch 5735/11884 - Loss: 30.8152\n",
      "Processing batch 5736/11884 - Loss: 30.0989\n",
      "Processing batch 5737/11884 - Loss: 29.8315\n",
      "Processing batch 5738/11884 - Loss: 29.4241\n",
      "Processing batch 5739/11884 - Loss: 31.1423\n",
      "Processing batch 5740/11884 - Loss: 29.2841\n",
      "Processing batch 5741/11884 - Loss: 29.7916\n",
      "Processing batch 5742/11884 - Loss: 30.1601\n",
      "Processing batch 5743/11884 - Loss: 30.8165\n",
      "Processing batch 5744/11884 - Loss: 29.4390\n",
      "Processing batch 5745/11884 - Loss: 29.8664\n",
      "Processing batch 5746/11884 - Loss: 29.0540\n",
      "Processing batch 5747/11884 - Loss: 32.7762\n",
      "Processing batch 5748/11884 - Loss: 31.5546\n",
      "Processing batch 5749/11884 - Loss: 29.4254\n",
      "Processing batch 5750/11884 - Loss: 30.5016\n",
      "Processing batch 5751/11884 - Loss: 31.6499\n",
      "Processing batch 5752/11884 - Loss: 31.2128\n",
      "Processing batch 5753/11884 - Loss: 29.1918\n",
      "Processing batch 5754/11884 - Loss: 29.8858\n",
      "Processing batch 5755/11884 - Loss: 31.0870\n",
      "Processing batch 5756/11884 - Loss: 31.1567\n",
      "Processing batch 5757/11884 - Loss: 30.9532\n",
      "Processing batch 5758/11884 - Loss: 29.8347\n",
      "Processing batch 5759/11884 - Loss: 30.0692\n",
      "Processing batch 5760/11884 - Loss: 31.0167\n",
      "Processing batch 5761/11884 - Loss: 31.1307\n",
      "Processing batch 5762/11884 - Loss: 30.4734\n",
      "Processing batch 5763/11884 - Loss: 31.1869\n",
      "Processing batch 5764/11884 - Loss: 30.6164\n",
      "Processing batch 5765/11884 - Loss: 31.7653\n",
      "Processing batch 5766/11884 - Loss: 30.7311\n",
      "Processing batch 5767/11884 - Loss: 31.5212\n",
      "Processing batch 5768/11884 - Loss: 30.6591\n",
      "Processing batch 5769/11884 - Loss: 31.1765\n",
      "Processing batch 5770/11884 - Loss: 30.8701\n",
      "Processing batch 5771/11884 - Loss: 31.5209\n",
      "Processing batch 5772/11884 - Loss: 30.7320\n",
      "Processing batch 5773/11884 - Loss: 29.6546\n",
      "Processing batch 5774/11884 - Loss: 31.6933\n",
      "Processing batch 5775/11884 - Loss: 30.4412\n",
      "Processing batch 5776/11884 - Loss: 31.0770\n",
      "Processing batch 5777/11884 - Loss: 28.2099\n",
      "Processing batch 5778/11884 - Loss: 30.4511\n",
      "Processing batch 5779/11884 - Loss: 30.8594\n",
      "Processing batch 5780/11884 - Loss: 30.7067\n",
      "Processing batch 5781/11884 - Loss: 30.8691\n",
      "Processing batch 5782/11884 - Loss: 30.6243\n",
      "Processing batch 5783/11884 - Loss: 30.5253\n",
      "Processing batch 5784/11884 - Loss: 31.2521\n",
      "Processing batch 5785/11884 - Loss: 29.2504\n",
      "Processing batch 5786/11884 - Loss: 30.3840\n",
      "Processing batch 5787/11884 - Loss: 29.7953\n",
      "Processing batch 5788/11884 - Loss: 31.7823\n",
      "Processing batch 5789/11884 - Loss: 30.3992\n",
      "Processing batch 5790/11884 - Loss: 30.2752\n",
      "Processing batch 5791/11884 - Loss: 30.8255\n",
      "Processing batch 5792/11884 - Loss: 30.3255\n",
      "Processing batch 5793/11884 - Loss: 29.0977\n",
      "Processing batch 5794/11884 - Loss: 29.3098\n",
      "Processing batch 5795/11884 - Loss: 30.6918\n",
      "Processing batch 5796/11884 - Loss: 30.4287\n",
      "Processing batch 5797/11884 - Loss: 30.4493\n",
      "Processing batch 5798/11884 - Loss: 30.0712\n",
      "Processing batch 5799/11884 - Loss: 30.2324\n",
      "Processing batch 5800/11884 - Loss: 29.3780\n",
      "Processing batch 5801/11884 - Loss: 30.5727\n",
      "Processing batch 5802/11884 - Loss: 31.1643\n",
      "Processing batch 5803/11884 - Loss: 30.1303\n",
      "Processing batch 5804/11884 - Loss: 30.0747\n",
      "Processing batch 5805/11884 - Loss: 31.5514\n",
      "Processing batch 5806/11884 - Loss: 30.6619\n",
      "Processing batch 5807/11884 - Loss: 30.2836\n",
      "Processing batch 5808/11884 - Loss: 29.6708\n",
      "Processing batch 5809/11884 - Loss: 30.8337\n",
      "Processing batch 5810/11884 - Loss: 30.1531\n",
      "Processing batch 5811/11884 - Loss: 31.2281\n",
      "Processing batch 5812/11884 - Loss: 30.0479\n",
      "Processing batch 5813/11884 - Loss: 30.3461\n",
      "Processing batch 5814/11884 - Loss: 29.7274\n",
      "Processing batch 5815/11884 - Loss: 30.6003\n",
      "Processing batch 5816/11884 - Loss: 30.4153\n",
      "Processing batch 5817/11884 - Loss: 29.6826\n",
      "Processing batch 5818/11884 - Loss: 31.5001\n",
      "Processing batch 5819/11884 - Loss: 30.1862\n",
      "Processing batch 5820/11884 - Loss: 33.0455\n",
      "Processing batch 5821/11884 - Loss: 29.7815\n",
      "Processing batch 5822/11884 - Loss: 30.4256\n",
      "Processing batch 5823/11884 - Loss: 29.6635\n",
      "Processing batch 5824/11884 - Loss: 30.6045\n",
      "Processing batch 5825/11884 - Loss: 30.6894\n",
      "Processing batch 5826/11884 - Loss: 31.6451\n",
      "Processing batch 5827/11884 - Loss: 31.1150\n",
      "Processing batch 5828/11884 - Loss: 31.2807\n",
      "Processing batch 5829/11884 - Loss: 30.7102\n",
      "Processing batch 5830/11884 - Loss: 31.1116\n",
      "Processing batch 5831/11884 - Loss: 31.5142\n",
      "Processing batch 5832/11884 - Loss: 30.8351\n",
      "Processing batch 5833/11884 - Loss: 30.5220\n",
      "Processing batch 5834/11884 - Loss: 32.1357\n",
      "Processing batch 5835/11884 - Loss: 32.3238\n",
      "Processing batch 5836/11884 - Loss: 30.3067\n",
      "Processing batch 5837/11884 - Loss: 31.1556\n",
      "Processing batch 5838/11884 - Loss: 31.5285\n",
      "Processing batch 5839/11884 - Loss: 32.0413\n",
      "Processing batch 5840/11884 - Loss: 28.7939\n",
      "Processing batch 5841/11884 - Loss: 29.8656\n",
      "Processing batch 5842/11884 - Loss: 29.6379\n",
      "Processing batch 5843/11884 - Loss: 30.6753\n",
      "Processing batch 5844/11884 - Loss: 31.7700\n",
      "Processing batch 5845/11884 - Loss: 31.5295\n",
      "Processing batch 5846/11884 - Loss: 30.9493\n",
      "Processing batch 5847/11884 - Loss: 30.0429\n",
      "Processing batch 5848/11884 - Loss: 30.7432\n",
      "Processing batch 5849/11884 - Loss: 31.0355\n",
      "Processing batch 5850/11884 - Loss: 30.5771\n",
      "Processing batch 5851/11884 - Loss: 31.0417\n",
      "Processing batch 5852/11884 - Loss: 31.0293\n",
      "Processing batch 5853/11884 - Loss: 31.1273\n",
      "Processing batch 5854/11884 - Loss: 28.9968\n",
      "Processing batch 5855/11884 - Loss: 30.4610\n",
      "Processing batch 5856/11884 - Loss: 30.9307\n",
      "Processing batch 5857/11884 - Loss: 29.8285\n",
      "Processing batch 5858/11884 - Loss: 32.6290\n",
      "Processing batch 5859/11884 - Loss: 31.3106\n",
      "Processing batch 5860/11884 - Loss: 29.7736\n",
      "Processing batch 5861/11884 - Loss: 30.8110\n",
      "Processing batch 5862/11884 - Loss: 29.9910\n",
      "Processing batch 5863/11884 - Loss: 30.8047\n",
      "Processing batch 5864/11884 - Loss: 28.8461\n",
      "Processing batch 5865/11884 - Loss: 30.1070\n",
      "Processing batch 5866/11884 - Loss: 31.5019\n",
      "Processing batch 5867/11884 - Loss: 31.6145\n",
      "Processing batch 5868/11884 - Loss: 29.6375\n",
      "Processing batch 5869/11884 - Loss: 29.4908\n",
      "Processing batch 5870/11884 - Loss: 29.5929\n",
      "Processing batch 5871/11884 - Loss: 30.4277\n",
      "Processing batch 5872/11884 - Loss: 31.6102\n",
      "Processing batch 5873/11884 - Loss: 30.3159\n",
      "Processing batch 5874/11884 - Loss: 31.3332\n",
      "Processing batch 5875/11884 - Loss: 30.2574\n",
      "Processing batch 5876/11884 - Loss: 30.4800\n",
      "Processing batch 5877/11884 - Loss: 29.6260\n",
      "Processing batch 5878/11884 - Loss: 29.6822\n",
      "Processing batch 5879/11884 - Loss: 29.5964\n",
      "Processing batch 5880/11884 - Loss: 29.8615\n",
      "Processing batch 5881/11884 - Loss: 30.6043\n",
      "Processing batch 5882/11884 - Loss: 31.3092\n",
      "Processing batch 5883/11884 - Loss: 31.5526\n",
      "Processing batch 5884/11884 - Loss: 31.1889\n",
      "Processing batch 5885/11884 - Loss: 33.0160\n",
      "Processing batch 5886/11884 - Loss: 29.9898\n",
      "Processing batch 5887/11884 - Loss: 30.7698\n",
      "Processing batch 5888/11884 - Loss: 29.2234\n",
      "Processing batch 5889/11884 - Loss: 31.1984\n",
      "Processing batch 5890/11884 - Loss: 30.1983\n",
      "Processing batch 5891/11884 - Loss: 30.5654\n",
      "Processing batch 5892/11884 - Loss: 31.0396\n",
      "Processing batch 5893/11884 - Loss: 29.3583\n",
      "Processing batch 5894/11884 - Loss: 30.5900\n",
      "Processing batch 5895/11884 - Loss: 30.8373\n",
      "Processing batch 5896/11884 - Loss: 29.5438\n",
      "Processing batch 5897/11884 - Loss: 30.2920\n",
      "Processing batch 5898/11884 - Loss: 30.0313\n",
      "Processing batch 5899/11884 - Loss: 29.3951\n",
      "Processing batch 5900/11884 - Loss: 29.5467\n",
      "Processing batch 5901/11884 - Loss: 31.4726\n",
      "Processing batch 5902/11884 - Loss: 30.7819\n",
      "Processing batch 5903/11884 - Loss: 31.1145\n",
      "Processing batch 5904/11884 - Loss: 30.1321\n",
      "Processing batch 5905/11884 - Loss: 31.2339\n",
      "Processing batch 5906/11884 - Loss: 30.6796\n",
      "Processing batch 5907/11884 - Loss: 29.9940\n",
      "Processing batch 5908/11884 - Loss: 31.2235\n",
      "Processing batch 5909/11884 - Loss: 31.5316\n",
      "Processing batch 5910/11884 - Loss: 30.1460\n",
      "Processing batch 5911/11884 - Loss: 29.9463\n",
      "Processing batch 5912/11884 - Loss: 29.2589\n",
      "Processing batch 5913/11884 - Loss: 30.7617\n",
      "Processing batch 5914/11884 - Loss: 30.0699\n",
      "Processing batch 5915/11884 - Loss: 30.1096\n",
      "Processing batch 5916/11884 - Loss: 29.7683\n",
      "Processing batch 5917/11884 - Loss: 30.4023\n",
      "Processing batch 5918/11884 - Loss: 30.7194\n",
      "Processing batch 5919/11884 - Loss: 30.5683\n",
      "Processing batch 5920/11884 - Loss: 30.5814\n",
      "Processing batch 5921/11884 - Loss: 30.4702\n",
      "Processing batch 5922/11884 - Loss: 29.2600\n",
      "Processing batch 5923/11884 - Loss: 30.1972\n",
      "Processing batch 5924/11884 - Loss: 28.7052\n",
      "Processing batch 5925/11884 - Loss: 31.2159\n",
      "Processing batch 5926/11884 - Loss: 30.9445\n",
      "Processing batch 5927/11884 - Loss: 30.3411\n",
      "Processing batch 5928/11884 - Loss: 30.7061\n",
      "Processing batch 5929/11884 - Loss: 30.6980\n",
      "Processing batch 5930/11884 - Loss: 31.5616\n",
      "Processing batch 5931/11884 - Loss: 31.0170\n",
      "Processing batch 5932/11884 - Loss: 29.6637\n",
      "Processing batch 5933/11884 - Loss: 31.3073\n",
      "Processing batch 5934/11884 - Loss: 32.0328\n",
      "Processing batch 5935/11884 - Loss: 30.4783\n",
      "Processing batch 5936/11884 - Loss: 29.7895\n",
      "Processing batch 5937/11884 - Loss: 30.4278\n",
      "Processing batch 5938/11884 - Loss: 30.6799\n",
      "Processing batch 5939/11884 - Loss: 31.6481\n",
      "Processing batch 5940/11884 - Loss: 30.6529\n",
      "Processing batch 5941/11884 - Loss: 30.5281\n",
      "Processing batch 5942/11884 - Loss: 31.3412\n",
      "Processing batch 5943/11884 - Loss: 30.8469\n",
      "Processing batch 5944/11884 - Loss: 31.1985\n",
      "Processing batch 5945/11884 - Loss: 30.4995\n",
      "Processing batch 5946/11884 - Loss: 30.1780\n",
      "Processing batch 5947/11884 - Loss: 31.4546\n",
      "Processing batch 5948/11884 - Loss: 30.8807\n",
      "Processing batch 5949/11884 - Loss: 29.7671\n",
      "Processing batch 5950/11884 - Loss: 31.0105\n",
      "Processing batch 5951/11884 - Loss: 30.5160\n",
      "Processing batch 5952/11884 - Loss: 31.3993\n",
      "Processing batch 5953/11884 - Loss: 31.8623\n",
      "Processing batch 5954/11884 - Loss: 30.6157\n",
      "Processing batch 5955/11884 - Loss: 31.7794\n",
      "Processing batch 5956/11884 - Loss: 30.2160\n",
      "Processing batch 5957/11884 - Loss: 29.9694\n",
      "Processing batch 5958/11884 - Loss: 30.4584\n",
      "Processing batch 5959/11884 - Loss: 29.6618\n",
      "Processing batch 5960/11884 - Loss: 29.8979\n",
      "Processing batch 5961/11884 - Loss: 30.8864\n",
      "Processing batch 5962/11884 - Loss: 31.1124\n",
      "Processing batch 5963/11884 - Loss: 30.2466\n",
      "Processing batch 5964/11884 - Loss: 30.1667\n",
      "Processing batch 5965/11884 - Loss: 31.6407\n",
      "Processing batch 5966/11884 - Loss: 29.7526\n",
      "Processing batch 5967/11884 - Loss: 29.4118\n",
      "Processing batch 5968/11884 - Loss: 30.0706\n",
      "Processing batch 5969/11884 - Loss: 31.2694\n",
      "Processing batch 5970/11884 - Loss: 30.3849\n",
      "Processing batch 5971/11884 - Loss: 32.0553\n",
      "Processing batch 5972/11884 - Loss: 30.8868\n",
      "Processing batch 5973/11884 - Loss: 30.1036\n",
      "Processing batch 5974/11884 - Loss: 29.4778\n",
      "Processing batch 5975/11884 - Loss: 29.6610\n",
      "Processing batch 5976/11884 - Loss: 30.6118\n",
      "Processing batch 5977/11884 - Loss: 30.0108\n",
      "Processing batch 5978/11884 - Loss: 30.6301\n",
      "Processing batch 5979/11884 - Loss: 29.9047\n",
      "Processing batch 5980/11884 - Loss: 29.8827\n",
      "Processing batch 5981/11884 - Loss: 30.9371\n",
      "Processing batch 5982/11884 - Loss: 29.8933\n",
      "Processing batch 5983/11884 - Loss: 30.5595\n",
      "Processing batch 5984/11884 - Loss: 31.7394\n",
      "Processing batch 5985/11884 - Loss: 30.0222\n",
      "Processing batch 5986/11884 - Loss: 30.2836\n",
      "Processing batch 5987/11884 - Loss: 31.7877\n",
      "Processing batch 5988/11884 - Loss: 29.0055\n",
      "Processing batch 5989/11884 - Loss: 30.7355\n",
      "Processing batch 5990/11884 - Loss: 30.6118\n",
      "Processing batch 5991/11884 - Loss: 31.5187\n",
      "Processing batch 5992/11884 - Loss: 30.6345\n",
      "Processing batch 5993/11884 - Loss: 29.2902\n",
      "Processing batch 5994/11884 - Loss: 30.5455\n",
      "Processing batch 5995/11884 - Loss: 30.1574\n",
      "Processing batch 5996/11884 - Loss: 30.1806\n",
      "Processing batch 5997/11884 - Loss: 31.4407\n",
      "Processing batch 5998/11884 - Loss: 29.3687\n",
      "Processing batch 5999/11884 - Loss: 30.5764\n",
      "Processing batch 6000/11884 - Loss: 30.9519\n",
      "Processing batch 6001/11884 - Loss: 30.2194\n",
      "Processing batch 6002/11884 - Loss: 30.3134\n",
      "Processing batch 6003/11884 - Loss: 31.1381\n",
      "Processing batch 6004/11884 - Loss: 29.8049\n",
      "Processing batch 6005/11884 - Loss: 29.9991\n",
      "Processing batch 6006/11884 - Loss: 29.9914\n",
      "Processing batch 6007/11884 - Loss: 30.2174\n",
      "Processing batch 6008/11884 - Loss: 30.1510\n",
      "Processing batch 6009/11884 - Loss: 30.6568\n",
      "Processing batch 6010/11884 - Loss: 30.5160\n",
      "Processing batch 6011/11884 - Loss: 30.2442\n",
      "Processing batch 6012/11884 - Loss: 29.7967\n",
      "Processing batch 6013/11884 - Loss: 30.4773\n",
      "Processing batch 6014/11884 - Loss: 29.4388\n",
      "Processing batch 6015/11884 - Loss: 30.8357\n",
      "Processing batch 6016/11884 - Loss: 29.3960\n",
      "Processing batch 6017/11884 - Loss: 31.7124\n",
      "Processing batch 6018/11884 - Loss: 30.3478\n",
      "Processing batch 6019/11884 - Loss: 31.1778\n",
      "Processing batch 6020/11884 - Loss: 30.7686\n",
      "Processing batch 6021/11884 - Loss: 29.7854\n",
      "Processing batch 6022/11884 - Loss: 30.3259\n",
      "Processing batch 6023/11884 - Loss: 31.0140\n",
      "Processing batch 6024/11884 - Loss: 30.4672\n",
      "Processing batch 6025/11884 - Loss: 30.6699\n",
      "Processing batch 6026/11884 - Loss: 31.0435\n",
      "Processing batch 6027/11884 - Loss: 29.8009\n",
      "Processing batch 6028/11884 - Loss: 29.4667\n",
      "Processing batch 6029/11884 - Loss: 29.7216\n",
      "Processing batch 6030/11884 - Loss: 31.4956\n",
      "Processing batch 6031/11884 - Loss: 31.0565\n",
      "Processing batch 6032/11884 - Loss: 30.0698\n",
      "Processing batch 6033/11884 - Loss: 31.7924\n",
      "Processing batch 6034/11884 - Loss: 30.0441\n",
      "Processing batch 6035/11884 - Loss: 30.5179\n",
      "Processing batch 6036/11884 - Loss: 29.8790\n",
      "Processing batch 6037/11884 - Loss: 29.7224\n",
      "Processing batch 6038/11884 - Loss: 30.2559\n",
      "Processing batch 6039/11884 - Loss: 29.9197\n",
      "Processing batch 6040/11884 - Loss: 31.4586\n",
      "Processing batch 6041/11884 - Loss: 29.0789\n",
      "Processing batch 6042/11884 - Loss: 30.0126\n",
      "Processing batch 6043/11884 - Loss: 30.6566\n",
      "Processing batch 6044/11884 - Loss: 29.9293\n",
      "Processing batch 6045/11884 - Loss: 30.3794\n",
      "Processing batch 6046/11884 - Loss: 31.9244\n",
      "Processing batch 6047/11884 - Loss: 29.3806\n",
      "Processing batch 6048/11884 - Loss: 31.2512\n",
      "Processing batch 6049/11884 - Loss: 29.6735\n",
      "Processing batch 6050/11884 - Loss: 29.5701\n",
      "Processing batch 6051/11884 - Loss: 30.4866\n",
      "Processing batch 6052/11884 - Loss: 30.9904\n",
      "Processing batch 6053/11884 - Loss: 31.0552\n",
      "Processing batch 6054/11884 - Loss: 29.9070\n",
      "Processing batch 6055/11884 - Loss: 31.5105\n",
      "Processing batch 6056/11884 - Loss: 29.6630\n",
      "Processing batch 6057/11884 - Loss: 31.9799\n",
      "Processing batch 6058/11884 - Loss: 31.0330\n",
      "Processing batch 6059/11884 - Loss: 32.0003\n",
      "Processing batch 6060/11884 - Loss: 29.9575\n",
      "Processing batch 6061/11884 - Loss: 31.5928\n",
      "Processing batch 6062/11884 - Loss: 30.9536\n",
      "Processing batch 6063/11884 - Loss: 30.2572\n",
      "Processing batch 6064/11884 - Loss: 31.6836\n",
      "Processing batch 6065/11884 - Loss: 31.3054\n",
      "Processing batch 6066/11884 - Loss: 30.1640\n",
      "Processing batch 6067/11884 - Loss: 31.0240\n",
      "Processing batch 6068/11884 - Loss: 30.2358\n",
      "Processing batch 6069/11884 - Loss: 32.0204\n",
      "Processing batch 6070/11884 - Loss: 30.2615\n",
      "Processing batch 6071/11884 - Loss: 30.4685\n",
      "Processing batch 6072/11884 - Loss: 31.1743\n",
      "Processing batch 6073/11884 - Loss: 31.4541\n",
      "Processing batch 6074/11884 - Loss: 31.1612\n",
      "Processing batch 6075/11884 - Loss: 30.7346\n",
      "Processing batch 6076/11884 - Loss: 29.3266\n",
      "Processing batch 6077/11884 - Loss: 30.6823\n",
      "Processing batch 6078/11884 - Loss: 30.3523\n",
      "Processing batch 6079/11884 - Loss: 28.2139\n",
      "Processing batch 6080/11884 - Loss: 31.7670\n",
      "Processing batch 6081/11884 - Loss: 32.3339\n",
      "Processing batch 6082/11884 - Loss: 30.3123\n",
      "Processing batch 6083/11884 - Loss: 29.7804\n",
      "Processing batch 6084/11884 - Loss: 31.6367\n",
      "Processing batch 6085/11884 - Loss: 29.3573\n",
      "Processing batch 6086/11884 - Loss: 28.9728\n",
      "Processing batch 6087/11884 - Loss: 29.5687\n",
      "Processing batch 6088/11884 - Loss: 30.7189\n",
      "Processing batch 6089/11884 - Loss: 30.0581\n",
      "Processing batch 6090/11884 - Loss: 30.5706\n",
      "Processing batch 6091/11884 - Loss: 31.0333\n",
      "Processing batch 6092/11884 - Loss: 29.5271\n",
      "Processing batch 6093/11884 - Loss: 29.1080\n",
      "Processing batch 6094/11884 - Loss: 29.3869\n",
      "Processing batch 6095/11884 - Loss: 30.3909\n",
      "Processing batch 6096/11884 - Loss: 31.8921\n",
      "Processing batch 6097/11884 - Loss: 31.1924\n",
      "Processing batch 6098/11884 - Loss: 29.6689\n",
      "Processing batch 6099/11884 - Loss: 30.5184\n",
      "Processing batch 6100/11884 - Loss: 29.7607\n",
      "Processing batch 6101/11884 - Loss: 30.7101\n",
      "Processing batch 6102/11884 - Loss: 30.7122\n",
      "Processing batch 6103/11884 - Loss: 31.5171\n",
      "Processing batch 6104/11884 - Loss: 31.4214\n",
      "Processing batch 6105/11884 - Loss: 30.1048\n",
      "Processing batch 6106/11884 - Loss: 30.8667\n",
      "Processing batch 6107/11884 - Loss: 30.2600\n",
      "Processing batch 6108/11884 - Loss: 30.3682\n",
      "Processing batch 6109/11884 - Loss: 30.5930\n",
      "Processing batch 6110/11884 - Loss: 29.8590\n",
      "Processing batch 6111/11884 - Loss: 30.0066\n",
      "Processing batch 6112/11884 - Loss: 31.4070\n",
      "Processing batch 6113/11884 - Loss: 30.2166\n",
      "Processing batch 6114/11884 - Loss: 29.9513\n",
      "Processing batch 6115/11884 - Loss: 29.9797\n",
      "Processing batch 6116/11884 - Loss: 30.3156\n",
      "Processing batch 6117/11884 - Loss: 31.2108\n",
      "Processing batch 6118/11884 - Loss: 31.3445\n",
      "Processing batch 6119/11884 - Loss: 32.0726\n",
      "Processing batch 6120/11884 - Loss: 30.3043\n",
      "Processing batch 6121/11884 - Loss: 29.4980\n",
      "Processing batch 6122/11884 - Loss: 28.7532\n",
      "Processing batch 6123/11884 - Loss: 30.7883\n",
      "Processing batch 6124/11884 - Loss: 30.1056\n",
      "Processing batch 6125/11884 - Loss: 29.4284\n",
      "Processing batch 6126/11884 - Loss: 30.7519\n",
      "Processing batch 6127/11884 - Loss: 30.5835\n",
      "Processing batch 6128/11884 - Loss: 31.0915\n",
      "Processing batch 6129/11884 - Loss: 30.8153\n",
      "Processing batch 6130/11884 - Loss: 29.9267\n",
      "Processing batch 6131/11884 - Loss: 31.8115\n",
      "Processing batch 6132/11884 - Loss: 31.2181\n",
      "Processing batch 6133/11884 - Loss: 29.5946\n",
      "Processing batch 6134/11884 - Loss: 30.5612\n",
      "Processing batch 6135/11884 - Loss: 32.6023\n",
      "Processing batch 6136/11884 - Loss: 31.0614\n",
      "Processing batch 6137/11884 - Loss: 29.3526\n",
      "Processing batch 6138/11884 - Loss: 29.9706\n",
      "Processing batch 6139/11884 - Loss: 30.0870\n",
      "Processing batch 6140/11884 - Loss: 31.3448\n",
      "Processing batch 6141/11884 - Loss: 30.7166\n",
      "Processing batch 6142/11884 - Loss: 30.0849\n",
      "Processing batch 6143/11884 - Loss: 30.4386\n",
      "Processing batch 6144/11884 - Loss: 30.0789\n",
      "Processing batch 6145/11884 - Loss: 30.9069\n",
      "Processing batch 6146/11884 - Loss: 30.9830\n",
      "Processing batch 6147/11884 - Loss: 30.5740\n",
      "Processing batch 6148/11884 - Loss: 31.3699\n",
      "Processing batch 6149/11884 - Loss: 30.6249\n",
      "Processing batch 6150/11884 - Loss: 30.1273\n",
      "Processing batch 6151/11884 - Loss: 30.2068\n",
      "Processing batch 6152/11884 - Loss: 29.5784\n",
      "Processing batch 6153/11884 - Loss: 30.6711\n",
      "Processing batch 6154/11884 - Loss: 31.6916\n",
      "Processing batch 6155/11884 - Loss: 29.3069\n",
      "Processing batch 6156/11884 - Loss: 30.9344\n",
      "Processing batch 6157/11884 - Loss: 31.1151\n",
      "Processing batch 6158/11884 - Loss: 29.8681\n",
      "Processing batch 6159/11884 - Loss: 31.2266\n",
      "Processing batch 6160/11884 - Loss: 31.2771\n",
      "Processing batch 6161/11884 - Loss: 30.9178\n",
      "Processing batch 6162/11884 - Loss: 30.5264\n",
      "Processing batch 6163/11884 - Loss: 29.2939\n",
      "Processing batch 6164/11884 - Loss: 31.3594\n",
      "Processing batch 6165/11884 - Loss: 30.2583\n",
      "Processing batch 6166/11884 - Loss: 30.5613\n",
      "Processing batch 6167/11884 - Loss: 27.6877\n",
      "Processing batch 6168/11884 - Loss: 30.5500\n",
      "Processing batch 6169/11884 - Loss: 29.9862\n",
      "Processing batch 6170/11884 - Loss: 30.5678\n",
      "Processing batch 6171/11884 - Loss: 30.4035\n",
      "Processing batch 6172/11884 - Loss: 29.3351\n",
      "Processing batch 6173/11884 - Loss: 29.6739\n",
      "Processing batch 6174/11884 - Loss: 30.0901\n",
      "Processing batch 6175/11884 - Loss: 30.9205\n",
      "Processing batch 6176/11884 - Loss: 32.0916\n",
      "Processing batch 6177/11884 - Loss: 31.1158\n",
      "Processing batch 6178/11884 - Loss: 32.0154\n",
      "Processing batch 6179/11884 - Loss: 32.1013\n",
      "Processing batch 6180/11884 - Loss: 30.3179\n",
      "Processing batch 6181/11884 - Loss: 29.2182\n",
      "Processing batch 6182/11884 - Loss: 30.5253\n",
      "Processing batch 6183/11884 - Loss: 29.4885\n",
      "Processing batch 6184/11884 - Loss: 30.6990\n",
      "Processing batch 6185/11884 - Loss: 30.6990\n",
      "Processing batch 6186/11884 - Loss: 29.2197\n",
      "Processing batch 6187/11884 - Loss: 29.3251\n",
      "Processing batch 6188/11884 - Loss: 30.8136\n",
      "Processing batch 6189/11884 - Loss: 32.9895\n",
      "Processing batch 6190/11884 - Loss: 30.6582\n",
      "Processing batch 6191/11884 - Loss: 30.4699\n",
      "Processing batch 6192/11884 - Loss: 31.1608\n",
      "Processing batch 6193/11884 - Loss: 30.7617\n",
      "Processing batch 6194/11884 - Loss: 30.6937\n",
      "Processing batch 6195/11884 - Loss: 30.6364\n",
      "Processing batch 6196/11884 - Loss: 31.2523\n",
      "Processing batch 6197/11884 - Loss: 30.3963\n",
      "Processing batch 6198/11884 - Loss: 30.0525\n",
      "Processing batch 6199/11884 - Loss: 30.0181\n",
      "Processing batch 6200/11884 - Loss: 30.5607\n",
      "Processing batch 6201/11884 - Loss: 30.1948\n",
      "Processing batch 6202/11884 - Loss: 30.9347\n",
      "Processing batch 6203/11884 - Loss: 32.0787\n",
      "Processing batch 6204/11884 - Loss: 29.9511\n",
      "Processing batch 6205/11884 - Loss: 29.8284\n",
      "Processing batch 6206/11884 - Loss: 31.1963\n",
      "Processing batch 6207/11884 - Loss: 30.6424\n",
      "Processing batch 6208/11884 - Loss: 31.0048\n",
      "Processing batch 6209/11884 - Loss: 30.5751\n",
      "Processing batch 6210/11884 - Loss: 30.2962\n",
      "Processing batch 6211/11884 - Loss: 31.2050\n",
      "Processing batch 6212/11884 - Loss: 30.8344\n",
      "Processing batch 6213/11884 - Loss: 29.1544\n",
      "Processing batch 6214/11884 - Loss: 30.5446\n",
      "Processing batch 6215/11884 - Loss: 30.2666\n",
      "Processing batch 6216/11884 - Loss: 30.3761\n",
      "Processing batch 6217/11884 - Loss: 31.5989\n",
      "Processing batch 6218/11884 - Loss: 31.8876\n",
      "Processing batch 6219/11884 - Loss: 30.7713\n",
      "Processing batch 6220/11884 - Loss: 30.6929\n",
      "Processing batch 6221/11884 - Loss: 29.3065\n",
      "Processing batch 6222/11884 - Loss: 30.6851\n",
      "Processing batch 6223/11884 - Loss: 31.4242\n",
      "Processing batch 6224/11884 - Loss: 31.1051\n",
      "Processing batch 6225/11884 - Loss: 30.7551\n",
      "Processing batch 6226/11884 - Loss: 29.7500\n",
      "Processing batch 6227/11884 - Loss: 30.9551\n",
      "Processing batch 6228/11884 - Loss: 29.7894\n",
      "Processing batch 6229/11884 - Loss: 29.7067\n",
      "Processing batch 6230/11884 - Loss: 31.2263\n",
      "Processing batch 6231/11884 - Loss: 28.5040\n",
      "Processing batch 6232/11884 - Loss: 30.4742\n",
      "Processing batch 6233/11884 - Loss: 30.0109\n",
      "Processing batch 6234/11884 - Loss: 30.9358\n",
      "Processing batch 6235/11884 - Loss: 31.5180\n",
      "Processing batch 6236/11884 - Loss: 31.2005\n",
      "Processing batch 6237/11884 - Loss: 30.7496\n",
      "Processing batch 6238/11884 - Loss: 30.0985\n",
      "Processing batch 6239/11884 - Loss: 30.4094\n",
      "Processing batch 6240/11884 - Loss: 29.4906\n",
      "Processing batch 6241/11884 - Loss: 31.9400\n",
      "Processing batch 6242/11884 - Loss: 30.3897\n",
      "Processing batch 6243/11884 - Loss: 31.6511\n",
      "Processing batch 6244/11884 - Loss: 31.2001\n",
      "Processing batch 6245/11884 - Loss: 30.6446\n",
      "Processing batch 6246/11884 - Loss: 32.2748\n",
      "Processing batch 6247/11884 - Loss: 30.6532\n",
      "Processing batch 6248/11884 - Loss: 30.1851\n",
      "Processing batch 6249/11884 - Loss: 30.8638\n",
      "Processing batch 6250/11884 - Loss: 30.3689\n",
      "Processing batch 6251/11884 - Loss: 31.3113\n",
      "Processing batch 6252/11884 - Loss: 30.2681\n",
      "Processing batch 6253/11884 - Loss: 30.5800\n",
      "Processing batch 6254/11884 - Loss: 31.1154\n",
      "Processing batch 6255/11884 - Loss: 28.8155\n",
      "Processing batch 6256/11884 - Loss: 30.8631\n",
      "Processing batch 6257/11884 - Loss: 29.0170\n",
      "Processing batch 6258/11884 - Loss: 29.4970\n",
      "Processing batch 6259/11884 - Loss: 29.7541\n",
      "Processing batch 6260/11884 - Loss: 28.8111\n",
      "Processing batch 6261/11884 - Loss: 29.5907\n",
      "Processing batch 6262/11884 - Loss: 31.1208\n",
      "Processing batch 6263/11884 - Loss: 28.8128\n",
      "Processing batch 6264/11884 - Loss: 30.6851\n",
      "Processing batch 6265/11884 - Loss: 30.0972\n",
      "Processing batch 6266/11884 - Loss: 30.6628\n",
      "Processing batch 6267/11884 - Loss: 30.7726\n",
      "Processing batch 6268/11884 - Loss: 30.4225\n",
      "Processing batch 6269/11884 - Loss: 31.4249\n",
      "Processing batch 6270/11884 - Loss: 31.2670\n",
      "Processing batch 6271/11884 - Loss: 31.8289\n",
      "Processing batch 6272/11884 - Loss: 30.6422\n",
      "Processing batch 6273/11884 - Loss: 31.6220\n",
      "Processing batch 6274/11884 - Loss: 29.9877\n",
      "Processing batch 6275/11884 - Loss: 29.7508\n",
      "Processing batch 6276/11884 - Loss: 30.1079\n",
      "Processing batch 6277/11884 - Loss: 28.8886\n",
      "Processing batch 6278/11884 - Loss: 31.3426\n",
      "Processing batch 6279/11884 - Loss: 30.9135\n",
      "Processing batch 6280/11884 - Loss: 29.8802\n",
      "Processing batch 6281/11884 - Loss: 30.7258\n",
      "Processing batch 6282/11884 - Loss: 28.5888\n",
      "Processing batch 6283/11884 - Loss: 30.2894\n",
      "Processing batch 6284/11884 - Loss: 30.3576\n",
      "Processing batch 6285/11884 - Loss: 30.5696\n",
      "Processing batch 6286/11884 - Loss: 29.7816\n",
      "Processing batch 6287/11884 - Loss: 31.0673\n",
      "Processing batch 6288/11884 - Loss: 30.0287\n",
      "Processing batch 6289/11884 - Loss: 28.3472\n",
      "Processing batch 6290/11884 - Loss: 31.6932\n",
      "Processing batch 6291/11884 - Loss: 28.7108\n",
      "Processing batch 6292/11884 - Loss: 29.9333\n",
      "Processing batch 6293/11884 - Loss: 30.4413\n",
      "Processing batch 6294/11884 - Loss: 30.3925\n",
      "Processing batch 6295/11884 - Loss: 30.9662\n",
      "Processing batch 6296/11884 - Loss: 30.3709\n",
      "Processing batch 6297/11884 - Loss: 32.2393\n",
      "Processing batch 6298/11884 - Loss: 29.7717\n",
      "Processing batch 6299/11884 - Loss: 30.9558\n",
      "Processing batch 6300/11884 - Loss: 31.1801\n",
      "Processing batch 6301/11884 - Loss: 31.2795\n",
      "Processing batch 6302/11884 - Loss: 29.3444\n",
      "Processing batch 6303/11884 - Loss: 30.4896\n",
      "Processing batch 6304/11884 - Loss: 29.9106\n",
      "Processing batch 6305/11884 - Loss: 30.8364\n",
      "Processing batch 6306/11884 - Loss: 31.0925\n",
      "Processing batch 6307/11884 - Loss: 30.4065\n",
      "Processing batch 6308/11884 - Loss: 31.1135\n",
      "Processing batch 6309/11884 - Loss: 31.1498\n",
      "Processing batch 6310/11884 - Loss: 31.6832\n",
      "Processing batch 6311/11884 - Loss: 30.0985\n",
      "Processing batch 6312/11884 - Loss: 30.8763\n",
      "Processing batch 6313/11884 - Loss: 31.2856\n",
      "Processing batch 6314/11884 - Loss: 31.0877\n",
      "Processing batch 6315/11884 - Loss: 29.9358\n",
      "Processing batch 6316/11884 - Loss: 31.1346\n",
      "Processing batch 6317/11884 - Loss: 29.6746\n",
      "Processing batch 6318/11884 - Loss: 30.9471\n",
      "Processing batch 6319/11884 - Loss: 30.6999\n",
      "Processing batch 6320/11884 - Loss: 30.5271\n",
      "Processing batch 6321/11884 - Loss: 30.6530\n",
      "Processing batch 6322/11884 - Loss: 31.2756\n",
      "Processing batch 6323/11884 - Loss: 30.4166\n",
      "Processing batch 6324/11884 - Loss: 30.5655\n",
      "Processing batch 6325/11884 - Loss: 29.9695\n",
      "Processing batch 6326/11884 - Loss: 30.4853\n",
      "Processing batch 6327/11884 - Loss: 31.1294\n",
      "Processing batch 6328/11884 - Loss: 29.4678\n",
      "Processing batch 6329/11884 - Loss: 29.8675\n",
      "Processing batch 6330/11884 - Loss: 30.3939\n",
      "Processing batch 6331/11884 - Loss: 30.8146\n",
      "Processing batch 6332/11884 - Loss: 30.7688\n",
      "Processing batch 6333/11884 - Loss: 29.3617\n",
      "Processing batch 6334/11884 - Loss: 31.2543\n",
      "Processing batch 6335/11884 - Loss: 29.6312\n",
      "Processing batch 6336/11884 - Loss: 30.6904\n",
      "Processing batch 6337/11884 - Loss: 29.8678\n",
      "Processing batch 6338/11884 - Loss: 32.5050\n",
      "Processing batch 6339/11884 - Loss: 30.9632\n",
      "Processing batch 6340/11884 - Loss: 30.5293\n",
      "Processing batch 6341/11884 - Loss: 29.9165\n",
      "Processing batch 6342/11884 - Loss: 31.1992\n",
      "Processing batch 6343/11884 - Loss: 32.1075\n",
      "Processing batch 6344/11884 - Loss: 29.2845\n",
      "Processing batch 6345/11884 - Loss: 31.0287\n",
      "Processing batch 6346/11884 - Loss: 30.1953\n",
      "Processing batch 6347/11884 - Loss: 28.4353\n",
      "Processing batch 6348/11884 - Loss: 30.5641\n",
      "Processing batch 6349/11884 - Loss: 32.5002\n",
      "Processing batch 6350/11884 - Loss: 30.7303\n",
      "Processing batch 6351/11884 - Loss: 30.4933\n",
      "Processing batch 6352/11884 - Loss: 30.6497\n",
      "Processing batch 6353/11884 - Loss: 30.4154\n",
      "Processing batch 6354/11884 - Loss: 30.4634\n",
      "Processing batch 6355/11884 - Loss: 31.0646\n",
      "Processing batch 6356/11884 - Loss: 31.6791\n",
      "Processing batch 6357/11884 - Loss: 30.3695\n",
      "Processing batch 6358/11884 - Loss: 31.1325\n",
      "Processing batch 6359/11884 - Loss: 31.4273\n",
      "Processing batch 6360/11884 - Loss: 30.8438\n",
      "Processing batch 6361/11884 - Loss: 30.4234\n",
      "Processing batch 6362/11884 - Loss: 30.9110\n",
      "Processing batch 6363/11884 - Loss: 28.7360\n",
      "Processing batch 6364/11884 - Loss: 31.4801\n",
      "Processing batch 6365/11884 - Loss: 28.6774\n",
      "Processing batch 6366/11884 - Loss: 31.4472\n",
      "Processing batch 6367/11884 - Loss: 29.6335\n",
      "Processing batch 6368/11884 - Loss: 29.7488\n",
      "Processing batch 6369/11884 - Loss: 31.3404\n",
      "Processing batch 6370/11884 - Loss: 28.9786\n",
      "Processing batch 6371/11884 - Loss: 30.9605\n",
      "Processing batch 6372/11884 - Loss: 29.8581\n",
      "Processing batch 6373/11884 - Loss: 30.8052\n",
      "Processing batch 6374/11884 - Loss: 31.1783\n",
      "Processing batch 6375/11884 - Loss: 30.5701\n",
      "Processing batch 6376/11884 - Loss: 30.6247\n",
      "Processing batch 6377/11884 - Loss: 30.5952\n",
      "Processing batch 6378/11884 - Loss: 31.2522\n",
      "Processing batch 6379/11884 - Loss: 29.5106\n",
      "Processing batch 6380/11884 - Loss: 30.3826\n",
      "Processing batch 6381/11884 - Loss: 30.2797\n",
      "Processing batch 6382/11884 - Loss: 29.7589\n",
      "Processing batch 6383/11884 - Loss: 30.8831\n",
      "Processing batch 6384/11884 - Loss: 30.2184\n",
      "Processing batch 6385/11884 - Loss: 30.0825\n",
      "Processing batch 6386/11884 - Loss: 31.5313\n",
      "Processing batch 6387/11884 - Loss: 31.8632\n",
      "Processing batch 6388/11884 - Loss: 31.7119\n",
      "Processing batch 6389/11884 - Loss: 30.0670\n",
      "Processing batch 6390/11884 - Loss: 30.6179\n",
      "Processing batch 6391/11884 - Loss: 29.4313\n",
      "Processing batch 6392/11884 - Loss: 30.7971\n",
      "Processing batch 6393/11884 - Loss: 30.4148\n",
      "Processing batch 6394/11884 - Loss: 29.8097\n",
      "Processing batch 6395/11884 - Loss: 29.0749\n",
      "Processing batch 6396/11884 - Loss: 31.9023\n",
      "Processing batch 6397/11884 - Loss: 30.7197\n",
      "Processing batch 6398/11884 - Loss: 31.0690\n",
      "Processing batch 6399/11884 - Loss: 29.7299\n",
      "Processing batch 6400/11884 - Loss: 29.6453\n",
      "Processing batch 6401/11884 - Loss: 29.5589\n",
      "Processing batch 6402/11884 - Loss: 29.7966\n",
      "Processing batch 6403/11884 - Loss: 31.0108\n",
      "Processing batch 6404/11884 - Loss: 30.3557\n",
      "Processing batch 6405/11884 - Loss: 30.0090\n",
      "Processing batch 6406/11884 - Loss: 29.8780\n",
      "Processing batch 6407/11884 - Loss: 31.3749\n",
      "Processing batch 6408/11884 - Loss: 31.5948\n",
      "Processing batch 6409/11884 - Loss: 30.6507\n",
      "Processing batch 6410/11884 - Loss: 31.2117\n",
      "Processing batch 6411/11884 - Loss: 30.7230\n",
      "Processing batch 6412/11884 - Loss: 31.7118\n",
      "Processing batch 6413/11884 - Loss: 30.0733\n",
      "Processing batch 6414/11884 - Loss: 30.4433\n",
      "Processing batch 6415/11884 - Loss: 30.6697\n",
      "Processing batch 6416/11884 - Loss: 29.9790\n",
      "Processing batch 6417/11884 - Loss: 29.7042\n",
      "Processing batch 6418/11884 - Loss: 29.9111\n",
      "Processing batch 6419/11884 - Loss: 30.8643\n",
      "Processing batch 6420/11884 - Loss: 29.3887\n",
      "Processing batch 6421/11884 - Loss: 31.4564\n",
      "Processing batch 6422/11884 - Loss: 29.3268\n",
      "Processing batch 6423/11884 - Loss: 29.9725\n",
      "Processing batch 6424/11884 - Loss: 30.8638\n",
      "Processing batch 6425/11884 - Loss: 31.4839\n",
      "Processing batch 6426/11884 - Loss: 30.0931\n",
      "Processing batch 6427/11884 - Loss: 30.4294\n",
      "Processing batch 6428/11884 - Loss: 31.0167\n",
      "Processing batch 6429/11884 - Loss: 30.0986\n",
      "Processing batch 6430/11884 - Loss: 32.1127\n",
      "Processing batch 6431/11884 - Loss: 31.8736\n",
      "Processing batch 6432/11884 - Loss: 30.0642\n",
      "Processing batch 6433/11884 - Loss: 29.2835\n",
      "Processing batch 6434/11884 - Loss: 30.6095\n",
      "Processing batch 6435/11884 - Loss: 31.3864\n",
      "Processing batch 6436/11884 - Loss: 29.9846\n",
      "Processing batch 6437/11884 - Loss: 30.7521\n",
      "Processing batch 6438/11884 - Loss: 30.6128\n",
      "Processing batch 6439/11884 - Loss: 30.8882\n",
      "Processing batch 6440/11884 - Loss: 29.1505\n",
      "Processing batch 6441/11884 - Loss: 29.4173\n",
      "Processing batch 6442/11884 - Loss: 31.1030\n",
      "Processing batch 6443/11884 - Loss: 30.8609\n",
      "Processing batch 6444/11884 - Loss: 30.1907\n",
      "Processing batch 6445/11884 - Loss: 29.7148\n",
      "Processing batch 6446/11884 - Loss: 29.0728\n",
      "Processing batch 6447/11884 - Loss: 29.4968\n",
      "Processing batch 6448/11884 - Loss: 31.6483\n",
      "Processing batch 6449/11884 - Loss: 30.1505\n",
      "Processing batch 6450/11884 - Loss: 31.3151\n",
      "Processing batch 6451/11884 - Loss: 29.8387\n",
      "Processing batch 6452/11884 - Loss: 28.9332\n",
      "Processing batch 6453/11884 - Loss: 29.7261\n",
      "Processing batch 6454/11884 - Loss: 30.5683\n",
      "Processing batch 6455/11884 - Loss: 30.8326\n",
      "Processing batch 6456/11884 - Loss: 30.5551\n",
      "Processing batch 6457/11884 - Loss: 31.1998\n",
      "Processing batch 6458/11884 - Loss: 31.7223\n",
      "Processing batch 6459/11884 - Loss: 32.7127\n",
      "Processing batch 6460/11884 - Loss: 30.0755\n",
      "Processing batch 6461/11884 - Loss: 30.8450\n",
      "Processing batch 6462/11884 - Loss: 30.9613\n",
      "Processing batch 6463/11884 - Loss: 31.0091\n",
      "Processing batch 6464/11884 - Loss: 30.8895\n",
      "Processing batch 6465/11884 - Loss: 32.4612\n",
      "Processing batch 6466/11884 - Loss: 30.2698\n",
      "Processing batch 6467/11884 - Loss: 30.3139\n",
      "Processing batch 6468/11884 - Loss: 29.4418\n",
      "Processing batch 6469/11884 - Loss: 31.1371\n",
      "Processing batch 6470/11884 - Loss: 30.8060\n",
      "Processing batch 6471/11884 - Loss: 30.0837\n",
      "Processing batch 6472/11884 - Loss: 30.8281\n",
      "Processing batch 6473/11884 - Loss: 30.7650\n",
      "Processing batch 6474/11884 - Loss: 31.2310\n",
      "Processing batch 6475/11884 - Loss: 31.1370\n",
      "Processing batch 6476/11884 - Loss: 30.2446\n",
      "Processing batch 6477/11884 - Loss: 29.8947\n",
      "Processing batch 6478/11884 - Loss: 30.6299\n",
      "Processing batch 6479/11884 - Loss: 29.6749\n",
      "Processing batch 6480/11884 - Loss: 31.1963\n",
      "Processing batch 6481/11884 - Loss: 31.1974\n",
      "Processing batch 6482/11884 - Loss: 30.5860\n",
      "Processing batch 6483/11884 - Loss: 30.4088\n",
      "Processing batch 6484/11884 - Loss: 30.9401\n",
      "Processing batch 6485/11884 - Loss: 29.3942\n",
      "Processing batch 6486/11884 - Loss: 30.2888\n",
      "Processing batch 6487/11884 - Loss: 29.6439\n",
      "Processing batch 6488/11884 - Loss: 30.6232\n",
      "Processing batch 6489/11884 - Loss: 29.9970\n",
      "Processing batch 6490/11884 - Loss: 31.6348\n",
      "Processing batch 6491/11884 - Loss: 30.4918\n",
      "Processing batch 6492/11884 - Loss: 29.1526\n",
      "Processing batch 6493/11884 - Loss: 31.0039\n",
      "Processing batch 6494/11884 - Loss: 31.7312\n",
      "Processing batch 6495/11884 - Loss: 29.5217\n",
      "Processing batch 6496/11884 - Loss: 31.4960\n",
      "Processing batch 6497/11884 - Loss: 30.9776\n",
      "Processing batch 6498/11884 - Loss: 29.7819\n",
      "Processing batch 6499/11884 - Loss: 30.9971\n",
      "Processing batch 6500/11884 - Loss: 29.8624\n",
      "Processing batch 6501/11884 - Loss: 31.5345\n",
      "Processing batch 6502/11884 - Loss: 30.1861\n",
      "Processing batch 6503/11884 - Loss: 30.9202\n",
      "Processing batch 6504/11884 - Loss: 31.1148\n",
      "Processing batch 6505/11884 - Loss: 30.3559\n",
      "Processing batch 6506/11884 - Loss: 30.8951\n",
      "Processing batch 6507/11884 - Loss: 30.7182\n",
      "Processing batch 6508/11884 - Loss: 29.9996\n",
      "Processing batch 6509/11884 - Loss: 30.7305\n",
      "Processing batch 6510/11884 - Loss: 30.6313\n",
      "Processing batch 6511/11884 - Loss: 29.3659\n",
      "Processing batch 6512/11884 - Loss: 32.0407\n",
      "Processing batch 6513/11884 - Loss: 29.9402\n",
      "Processing batch 6514/11884 - Loss: 29.4234\n",
      "Processing batch 6515/11884 - Loss: 31.7647\n",
      "Processing batch 6516/11884 - Loss: 29.8794\n",
      "Processing batch 6517/11884 - Loss: 31.1212\n",
      "Processing batch 6518/11884 - Loss: 29.5945\n",
      "Processing batch 6519/11884 - Loss: 30.8471\n",
      "Processing batch 6520/11884 - Loss: 30.9748\n",
      "Processing batch 6521/11884 - Loss: 30.3708\n",
      "Processing batch 6522/11884 - Loss: 32.0959\n",
      "Processing batch 6523/11884 - Loss: 30.3268\n",
      "Processing batch 6524/11884 - Loss: 30.3821\n",
      "Processing batch 6525/11884 - Loss: 29.5601\n",
      "Processing batch 6526/11884 - Loss: 30.7198\n",
      "Processing batch 6527/11884 - Loss: 30.3364\n",
      "Processing batch 6528/11884 - Loss: 30.7537\n",
      "Processing batch 6529/11884 - Loss: 30.5002\n",
      "Processing batch 6530/11884 - Loss: 30.1591\n",
      "Processing batch 6531/11884 - Loss: 29.4157\n",
      "Processing batch 6532/11884 - Loss: 30.5724\n",
      "Processing batch 6533/11884 - Loss: 30.6800\n",
      "Processing batch 6534/11884 - Loss: 30.5379\n",
      "Processing batch 6535/11884 - Loss: 29.5212\n",
      "Processing batch 6536/11884 - Loss: 29.7494\n",
      "Processing batch 6537/11884 - Loss: 30.3507\n",
      "Processing batch 6538/11884 - Loss: 30.9149\n",
      "Processing batch 6539/11884 - Loss: 29.1182\n",
      "Processing batch 6540/11884 - Loss: 28.4011\n",
      "Processing batch 6541/11884 - Loss: 31.7389\n",
      "Processing batch 6542/11884 - Loss: 29.2466\n",
      "Processing batch 6543/11884 - Loss: 31.1950\n",
      "Processing batch 6544/11884 - Loss: 29.5957\n",
      "Processing batch 6545/11884 - Loss: 31.8793\n",
      "Processing batch 6546/11884 - Loss: 31.3780\n",
      "Processing batch 6547/11884 - Loss: 30.2645\n",
      "Processing batch 6548/11884 - Loss: 30.2334\n",
      "Processing batch 6549/11884 - Loss: 29.9023\n",
      "Processing batch 6550/11884 - Loss: 30.5564\n",
      "Processing batch 6551/11884 - Loss: 29.4847\n",
      "Processing batch 6552/11884 - Loss: 30.8692\n",
      "Processing batch 6553/11884 - Loss: 31.4018\n",
      "Processing batch 6554/11884 - Loss: 30.9212\n",
      "Processing batch 6555/11884 - Loss: 30.1098\n",
      "Processing batch 6556/11884 - Loss: 30.8293\n",
      "Processing batch 6557/11884 - Loss: 31.9899\n",
      "Processing batch 6558/11884 - Loss: 30.7484\n",
      "Processing batch 6559/11884 - Loss: 31.4512\n",
      "Processing batch 6560/11884 - Loss: 30.4434\n",
      "Processing batch 6561/11884 - Loss: 29.6636\n",
      "Processing batch 6562/11884 - Loss: 29.7159\n",
      "Processing batch 6563/11884 - Loss: 31.1033\n",
      "Processing batch 6564/11884 - Loss: 30.7225\n",
      "Processing batch 6565/11884 - Loss: 29.8395\n",
      "Processing batch 6566/11884 - Loss: 30.2909\n",
      "Processing batch 6567/11884 - Loss: 30.9161\n",
      "Processing batch 6568/11884 - Loss: 29.7204\n",
      "Processing batch 6569/11884 - Loss: 30.2828\n",
      "Processing batch 6570/11884 - Loss: 31.0565\n",
      "Processing batch 6571/11884 - Loss: 29.8059\n",
      "Processing batch 6572/11884 - Loss: 30.4408\n",
      "Processing batch 6573/11884 - Loss: 31.3406\n",
      "Processing batch 6574/11884 - Loss: 29.1400\n",
      "Processing batch 6575/11884 - Loss: 31.6430\n",
      "Processing batch 6576/11884 - Loss: 30.0293\n",
      "Processing batch 6577/11884 - Loss: 31.0842\n",
      "Processing batch 6578/11884 - Loss: 30.9628\n",
      "Processing batch 6579/11884 - Loss: 30.7369\n",
      "Processing batch 6580/11884 - Loss: 30.7368\n",
      "Processing batch 6581/11884 - Loss: 29.6134\n",
      "Processing batch 6582/11884 - Loss: 31.2330\n",
      "Processing batch 6583/11884 - Loss: 28.9510\n",
      "Processing batch 6584/11884 - Loss: 31.9685\n",
      "Processing batch 6585/11884 - Loss: 29.5160\n",
      "Processing batch 6586/11884 - Loss: 28.9098\n",
      "Processing batch 6587/11884 - Loss: 31.5669\n",
      "Processing batch 6588/11884 - Loss: 30.3720\n",
      "Processing batch 6589/11884 - Loss: 30.9289\n",
      "Processing batch 6590/11884 - Loss: 29.2417\n",
      "Processing batch 6591/11884 - Loss: 31.1492\n",
      "Processing batch 6592/11884 - Loss: 29.6259\n",
      "Processing batch 6593/11884 - Loss: 30.2706\n",
      "Processing batch 6594/11884 - Loss: 31.4322\n",
      "Processing batch 6595/11884 - Loss: 30.0783\n",
      "Processing batch 6596/11884 - Loss: 30.9057\n",
      "Processing batch 6597/11884 - Loss: 29.5854\n",
      "Processing batch 6598/11884 - Loss: 31.3336\n",
      "Processing batch 6599/11884 - Loss: 29.3205\n",
      "Processing batch 6600/11884 - Loss: 29.4036\n",
      "Processing batch 6601/11884 - Loss: 30.6509\n",
      "Processing batch 6602/11884 - Loss: 31.1846\n",
      "Processing batch 6603/11884 - Loss: 30.9673\n",
      "Processing batch 6604/11884 - Loss: 30.1726\n",
      "Processing batch 6605/11884 - Loss: 28.9812\n",
      "Processing batch 6606/11884 - Loss: 31.5261\n",
      "Processing batch 6607/11884 - Loss: 30.4381\n",
      "Processing batch 6608/11884 - Loss: 31.9939\n",
      "Processing batch 6609/11884 - Loss: 32.3667\n",
      "Processing batch 6610/11884 - Loss: 30.2910\n",
      "Processing batch 6611/11884 - Loss: 30.2198\n",
      "Processing batch 6612/11884 - Loss: 30.5573\n",
      "Processing batch 6613/11884 - Loss: 30.4854\n",
      "Processing batch 6614/11884 - Loss: 30.8862\n",
      "Processing batch 6615/11884 - Loss: 30.2555\n",
      "Processing batch 6616/11884 - Loss: 28.8427\n",
      "Processing batch 6617/11884 - Loss: 30.2761\n",
      "Processing batch 6618/11884 - Loss: 30.6173\n",
      "Processing batch 6619/11884 - Loss: 30.1362\n",
      "Processing batch 6620/11884 - Loss: 30.5714\n",
      "Processing batch 6621/11884 - Loss: 29.6947\n",
      "Processing batch 6622/11884 - Loss: 30.8451\n",
      "Processing batch 6623/11884 - Loss: 30.1734\n",
      "Processing batch 6624/11884 - Loss: 29.9433\n",
      "Processing batch 6625/11884 - Loss: 30.8439\n",
      "Processing batch 6626/11884 - Loss: 29.5995\n",
      "Processing batch 6627/11884 - Loss: 31.0495\n",
      "Processing batch 6628/11884 - Loss: 30.1913\n",
      "Processing batch 6629/11884 - Loss: 30.7941\n",
      "Processing batch 6630/11884 - Loss: 31.8005\n",
      "Processing batch 6631/11884 - Loss: 29.8867\n",
      "Processing batch 6632/11884 - Loss: 29.2415\n",
      "Processing batch 6633/11884 - Loss: 30.5175\n",
      "Processing batch 6634/11884 - Loss: 31.3294\n",
      "Processing batch 6635/11884 - Loss: 30.3034\n",
      "Processing batch 6636/11884 - Loss: 31.3358\n",
      "Processing batch 6637/11884 - Loss: 32.1364\n",
      "Processing batch 6638/11884 - Loss: 30.2532\n",
      "Processing batch 6639/11884 - Loss: 31.9823\n",
      "Processing batch 6640/11884 - Loss: 29.7075\n",
      "Processing batch 6641/11884 - Loss: 30.8547\n",
      "Processing batch 6642/11884 - Loss: 29.8779\n",
      "Processing batch 6643/11884 - Loss: 31.7826\n",
      "Processing batch 6644/11884 - Loss: 31.0205\n",
      "Processing batch 6645/11884 - Loss: 30.8669\n",
      "Processing batch 6646/11884 - Loss: 32.2771\n",
      "Processing batch 6647/11884 - Loss: 30.1503\n",
      "Processing batch 6648/11884 - Loss: 29.6384\n",
      "Processing batch 6649/11884 - Loss: 30.6538\n",
      "Processing batch 6650/11884 - Loss: 30.2668\n",
      "Processing batch 6651/11884 - Loss: 29.7288\n",
      "Processing batch 6652/11884 - Loss: 29.5966\n",
      "Processing batch 6653/11884 - Loss: 30.1537\n",
      "Processing batch 6654/11884 - Loss: 29.9603\n",
      "Processing batch 6655/11884 - Loss: 30.9761\n",
      "Processing batch 6656/11884 - Loss: 31.0163\n",
      "Processing batch 6657/11884 - Loss: 30.9286\n",
      "Processing batch 6658/11884 - Loss: 31.9661\n",
      "Processing batch 6659/11884 - Loss: 30.8903\n",
      "Processing batch 6660/11884 - Loss: 31.3292\n",
      "Processing batch 6661/11884 - Loss: 29.6722\n",
      "Processing batch 6662/11884 - Loss: 31.1504\n",
      "Processing batch 6663/11884 - Loss: 30.8789\n",
      "Processing batch 6664/11884 - Loss: 29.7586\n",
      "Processing batch 6665/11884 - Loss: 31.6421\n",
      "Processing batch 6666/11884 - Loss: 29.5559\n",
      "Processing batch 6667/11884 - Loss: 31.0123\n",
      "Processing batch 6668/11884 - Loss: 30.1946\n",
      "Processing batch 6669/11884 - Loss: 31.1731\n",
      "Processing batch 6670/11884 - Loss: 31.1502\n",
      "Processing batch 6671/11884 - Loss: 30.3265\n",
      "Processing batch 6672/11884 - Loss: 31.0390\n",
      "Processing batch 6673/11884 - Loss: 30.5117\n",
      "Processing batch 6674/11884 - Loss: 31.5540\n",
      "Processing batch 6675/11884 - Loss: 29.9168\n",
      "Processing batch 6676/11884 - Loss: 30.5514\n",
      "Processing batch 6677/11884 - Loss: 30.7141\n",
      "Processing batch 6678/11884 - Loss: 31.0279\n",
      "Processing batch 6679/11884 - Loss: 31.2351\n",
      "Processing batch 6680/11884 - Loss: 30.2444\n",
      "Processing batch 6681/11884 - Loss: 30.5807\n",
      "Processing batch 6682/11884 - Loss: 30.3583\n",
      "Processing batch 6683/11884 - Loss: 30.9118\n",
      "Processing batch 6684/11884 - Loss: 30.4098\n",
      "Processing batch 6685/11884 - Loss: 30.2093\n",
      "Processing batch 6686/11884 - Loss: 30.5175\n",
      "Processing batch 6687/11884 - Loss: 29.8828\n",
      "Processing batch 6688/11884 - Loss: 31.1895\n",
      "Processing batch 6689/11884 - Loss: 30.7906\n",
      "Processing batch 6690/11884 - Loss: 31.1389\n",
      "Processing batch 6691/11884 - Loss: 31.1140\n",
      "Processing batch 6692/11884 - Loss: 30.9253\n",
      "Processing batch 6693/11884 - Loss: 30.3551\n",
      "Processing batch 6694/11884 - Loss: 29.9091\n",
      "Processing batch 6695/11884 - Loss: 30.4915\n",
      "Processing batch 6696/11884 - Loss: 30.1119\n",
      "Processing batch 6697/11884 - Loss: 29.5499\n",
      "Processing batch 6698/11884 - Loss: 30.7297\n",
      "Processing batch 6699/11884 - Loss: 30.6638\n",
      "Processing batch 6700/11884 - Loss: 31.1456\n",
      "Processing batch 6701/11884 - Loss: 30.3744\n",
      "Processing batch 6702/11884 - Loss: 31.7175\n",
      "Processing batch 6703/11884 - Loss: 30.5072\n",
      "Processing batch 6704/11884 - Loss: 29.5021\n",
      "Processing batch 6705/11884 - Loss: 31.2545\n",
      "Processing batch 6706/11884 - Loss: 30.8338\n",
      "Processing batch 6707/11884 - Loss: 30.7256\n",
      "Processing batch 6708/11884 - Loss: 29.4041\n",
      "Processing batch 6709/11884 - Loss: 31.5467\n",
      "Processing batch 6710/11884 - Loss: 30.6180\n",
      "Processing batch 6711/11884 - Loss: 31.7704\n",
      "Processing batch 6712/11884 - Loss: 29.6390\n",
      "Processing batch 6713/11884 - Loss: 30.0330\n",
      "Processing batch 6714/11884 - Loss: 30.5847\n",
      "Processing batch 6715/11884 - Loss: 31.0600\n",
      "Processing batch 6716/11884 - Loss: 31.2357\n",
      "Processing batch 6717/11884 - Loss: 30.1445\n",
      "Processing batch 6718/11884 - Loss: 29.7820\n",
      "Processing batch 6719/11884 - Loss: 30.4639\n",
      "Processing batch 6720/11884 - Loss: 31.6338\n",
      "Processing batch 6721/11884 - Loss: 30.8941\n",
      "Processing batch 6722/11884 - Loss: 31.5993\n",
      "Processing batch 6723/11884 - Loss: 30.0734\n",
      "Processing batch 6724/11884 - Loss: 30.1546\n",
      "Processing batch 6725/11884 - Loss: 31.1134\n",
      "Processing batch 6726/11884 - Loss: 29.8691\n",
      "Processing batch 6727/11884 - Loss: 30.1545\n",
      "Processing batch 6728/11884 - Loss: 31.7927\n",
      "Processing batch 6729/11884 - Loss: 30.2931\n",
      "Processing batch 6730/11884 - Loss: 31.3410\n",
      "Processing batch 6731/11884 - Loss: 28.7598\n",
      "Processing batch 6732/11884 - Loss: 30.6342\n",
      "Processing batch 6733/11884 - Loss: 28.9914\n",
      "Processing batch 6734/11884 - Loss: 32.5667\n",
      "Processing batch 6735/11884 - Loss: 31.6212\n",
      "Processing batch 6736/11884 - Loss: 30.0964\n",
      "Processing batch 6737/11884 - Loss: 31.2361\n",
      "Processing batch 6738/11884 - Loss: 30.5346\n",
      "Processing batch 6739/11884 - Loss: 30.3216\n",
      "Processing batch 6740/11884 - Loss: 31.7202\n",
      "Processing batch 6741/11884 - Loss: 29.7518\n",
      "Processing batch 6742/11884 - Loss: 29.8329\n",
      "Processing batch 6743/11884 - Loss: 29.8994\n",
      "Processing batch 6744/11884 - Loss: 31.0893\n",
      "Processing batch 6745/11884 - Loss: 30.2998\n",
      "Processing batch 6746/11884 - Loss: 30.7256\n",
      "Processing batch 6747/11884 - Loss: 31.2884\n",
      "Processing batch 6748/11884 - Loss: 29.8187\n",
      "Processing batch 6749/11884 - Loss: 28.8963\n",
      "Processing batch 6750/11884 - Loss: 30.0626\n",
      "Processing batch 6751/11884 - Loss: 30.6025\n",
      "Processing batch 6752/11884 - Loss: 28.9771\n",
      "Processing batch 6753/11884 - Loss: 30.2150\n",
      "Processing batch 6754/11884 - Loss: 30.8958\n",
      "Processing batch 6755/11884 - Loss: 29.6959\n",
      "Processing batch 6756/11884 - Loss: 31.1161\n",
      "Processing batch 6757/11884 - Loss: 30.4905\n",
      "Processing batch 6758/11884 - Loss: 30.4688\n",
      "Processing batch 6759/11884 - Loss: 31.0575\n",
      "Processing batch 6760/11884 - Loss: 30.5242\n",
      "Processing batch 6761/11884 - Loss: 29.5833\n",
      "Processing batch 6762/11884 - Loss: 31.6165\n",
      "Processing batch 6763/11884 - Loss: 29.6314\n",
      "Processing batch 6764/11884 - Loss: 32.1590\n",
      "Processing batch 6765/11884 - Loss: 29.8754\n",
      "Processing batch 6766/11884 - Loss: 30.8475\n",
      "Processing batch 6767/11884 - Loss: 31.3562\n",
      "Processing batch 6768/11884 - Loss: 29.3900\n",
      "Processing batch 6769/11884 - Loss: 30.3717\n",
      "Processing batch 6770/11884 - Loss: 30.1746\n",
      "Processing batch 6771/11884 - Loss: 29.1508\n",
      "Processing batch 6772/11884 - Loss: 29.6995\n",
      "Processing batch 6773/11884 - Loss: 31.0111\n",
      "Processing batch 6774/11884 - Loss: 30.5965\n",
      "Processing batch 6775/11884 - Loss: 30.6918\n",
      "Processing batch 6776/11884 - Loss: 30.4855\n",
      "Processing batch 6777/11884 - Loss: 29.9029\n",
      "Processing batch 6778/11884 - Loss: 30.5747\n",
      "Processing batch 6779/11884 - Loss: 31.6029\n",
      "Processing batch 6780/11884 - Loss: 30.1755\n",
      "Processing batch 6781/11884 - Loss: 30.5178\n",
      "Processing batch 6782/11884 - Loss: 30.7896\n",
      "Processing batch 6783/11884 - Loss: 29.6442\n",
      "Processing batch 6784/11884 - Loss: 28.6958\n",
      "Processing batch 6785/11884 - Loss: 31.0032\n",
      "Processing batch 6786/11884 - Loss: 29.6760\n",
      "Processing batch 6787/11884 - Loss: 30.4464\n",
      "Processing batch 6788/11884 - Loss: 31.5612\n",
      "Processing batch 6789/11884 - Loss: 31.0769\n",
      "Processing batch 6790/11884 - Loss: 30.6832\n",
      "Processing batch 6791/11884 - Loss: 31.7719\n",
      "Processing batch 6792/11884 - Loss: 29.5182\n",
      "Processing batch 6793/11884 - Loss: 29.9790\n",
      "Processing batch 6794/11884 - Loss: 31.6732\n",
      "Processing batch 6795/11884 - Loss: 29.4616\n",
      "Processing batch 6796/11884 - Loss: 30.1160\n",
      "Processing batch 6797/11884 - Loss: 29.7235\n",
      "Processing batch 6798/11884 - Loss: 30.0702\n",
      "Processing batch 6799/11884 - Loss: 29.7712\n",
      "Processing batch 6800/11884 - Loss: 30.8340\n",
      "Processing batch 6801/11884 - Loss: 29.2117\n",
      "Processing batch 6802/11884 - Loss: 30.3311\n",
      "Processing batch 6803/11884 - Loss: 29.1516\n",
      "Processing batch 6804/11884 - Loss: 31.7770\n",
      "Processing batch 6805/11884 - Loss: 30.0814\n",
      "Processing batch 6806/11884 - Loss: 30.7857\n",
      "Processing batch 6807/11884 - Loss: 28.6511\n",
      "Processing batch 6808/11884 - Loss: 30.4579\n",
      "Processing batch 6809/11884 - Loss: 30.4582\n",
      "Processing batch 6810/11884 - Loss: 30.9384\n",
      "Processing batch 6811/11884 - Loss: 30.2477\n",
      "Processing batch 6812/11884 - Loss: 31.1099\n",
      "Processing batch 6813/11884 - Loss: 31.1156\n",
      "Processing batch 6814/11884 - Loss: 30.6571\n",
      "Processing batch 6815/11884 - Loss: 29.3081\n",
      "Processing batch 6816/11884 - Loss: 29.8482\n",
      "Processing batch 6817/11884 - Loss: 30.8878\n",
      "Processing batch 6818/11884 - Loss: 29.6396\n",
      "Processing batch 6819/11884 - Loss: 29.6615\n",
      "Processing batch 6820/11884 - Loss: 29.5487\n",
      "Processing batch 6821/11884 - Loss: 30.3335\n",
      "Processing batch 6822/11884 - Loss: 30.7257\n",
      "Processing batch 6823/11884 - Loss: 30.7607\n",
      "Processing batch 6824/11884 - Loss: 31.2306\n",
      "Processing batch 6825/11884 - Loss: 30.5746\n",
      "Processing batch 6826/11884 - Loss: 29.6400\n",
      "Processing batch 6827/11884 - Loss: 30.2851\n",
      "Processing batch 6828/11884 - Loss: 30.9010\n",
      "Processing batch 6829/11884 - Loss: 30.5795\n",
      "Processing batch 6830/11884 - Loss: 30.5081\n",
      "Processing batch 6831/11884 - Loss: 31.0919\n",
      "Processing batch 6832/11884 - Loss: 30.7659\n",
      "Processing batch 6833/11884 - Loss: 30.7986\n",
      "Processing batch 6834/11884 - Loss: 30.2964\n",
      "Processing batch 6835/11884 - Loss: 30.9193\n",
      "Processing batch 6836/11884 - Loss: 30.4489\n",
      "Processing batch 6837/11884 - Loss: 30.7786\n",
      "Processing batch 6838/11884 - Loss: 29.7546\n",
      "Processing batch 6839/11884 - Loss: 30.7215\n",
      "Processing batch 6840/11884 - Loss: 30.2824\n",
      "Processing batch 6841/11884 - Loss: 30.1727\n",
      "Processing batch 6842/11884 - Loss: 30.2643\n",
      "Processing batch 6843/11884 - Loss: 32.0621\n",
      "Processing batch 6844/11884 - Loss: 29.9002\n",
      "Processing batch 6845/11884 - Loss: 29.8849\n",
      "Processing batch 6846/11884 - Loss: 29.4737\n",
      "Processing batch 6847/11884 - Loss: 30.4222\n",
      "Processing batch 6848/11884 - Loss: 29.4801\n",
      "Processing batch 6849/11884 - Loss: 31.1121\n",
      "Processing batch 6850/11884 - Loss: 28.7705\n",
      "Processing batch 6851/11884 - Loss: 30.4785\n",
      "Processing batch 6852/11884 - Loss: 30.8290\n",
      "Processing batch 6853/11884 - Loss: 30.5410\n",
      "Processing batch 6854/11884 - Loss: 28.5929\n",
      "Processing batch 6855/11884 - Loss: 30.4632\n",
      "Processing batch 6856/11884 - Loss: 31.5627\n",
      "Processing batch 6857/11884 - Loss: 31.5401\n",
      "Processing batch 6858/11884 - Loss: 30.9506\n",
      "Processing batch 6859/11884 - Loss: 30.8162\n",
      "Processing batch 6860/11884 - Loss: 29.0273\n",
      "Processing batch 6861/11884 - Loss: 30.8484\n",
      "Processing batch 6862/11884 - Loss: 30.2406\n",
      "Processing batch 6863/11884 - Loss: 31.8598\n",
      "Processing batch 6864/11884 - Loss: 31.2797\n",
      "Processing batch 6865/11884 - Loss: 30.9222\n",
      "Processing batch 6866/11884 - Loss: 30.8045\n",
      "Processing batch 6867/11884 - Loss: 28.4190\n",
      "Processing batch 6868/11884 - Loss: 29.7098\n",
      "Processing batch 6869/11884 - Loss: 30.6517\n",
      "Processing batch 6870/11884 - Loss: 30.6585\n",
      "Processing batch 6871/11884 - Loss: 30.9940\n",
      "Processing batch 6872/11884 - Loss: 30.1707\n",
      "Processing batch 6873/11884 - Loss: 30.3761\n",
      "Processing batch 6874/11884 - Loss: 31.3091\n",
      "Processing batch 6875/11884 - Loss: 31.1880\n",
      "Processing batch 6876/11884 - Loss: 30.2870\n",
      "Processing batch 6877/11884 - Loss: 30.8932\n",
      "Processing batch 6878/11884 - Loss: 30.3979\n",
      "Processing batch 6879/11884 - Loss: 30.1381\n",
      "Processing batch 6880/11884 - Loss: 30.0648\n",
      "Processing batch 6881/11884 - Loss: 30.8432\n",
      "Processing batch 6882/11884 - Loss: 31.0691\n",
      "Processing batch 6883/11884 - Loss: 31.9121\n",
      "Processing batch 6884/11884 - Loss: 29.1363\n",
      "Processing batch 6885/11884 - Loss: 31.5143\n",
      "Processing batch 6886/11884 - Loss: 29.9485\n",
      "Processing batch 6887/11884 - Loss: 31.2975\n",
      "Processing batch 6888/11884 - Loss: 29.2048\n",
      "Processing batch 6889/11884 - Loss: 31.2368\n",
      "Processing batch 6890/11884 - Loss: 30.8431\n",
      "Processing batch 6891/11884 - Loss: 30.1753\n",
      "Processing batch 6892/11884 - Loss: 30.1367\n",
      "Processing batch 6893/11884 - Loss: 30.7465\n",
      "Processing batch 6894/11884 - Loss: 30.8674\n",
      "Processing batch 6895/11884 - Loss: 29.5401\n",
      "Processing batch 6896/11884 - Loss: 29.3562\n",
      "Processing batch 6897/11884 - Loss: 29.9087\n",
      "Processing batch 6898/11884 - Loss: 29.8112\n",
      "Processing batch 6899/11884 - Loss: 30.8511\n",
      "Processing batch 6900/11884 - Loss: 32.0614\n",
      "Processing batch 6901/11884 - Loss: 31.2522\n",
      "Processing batch 6902/11884 - Loss: 29.5703\n",
      "Processing batch 6903/11884 - Loss: 29.9998\n",
      "Processing batch 6904/11884 - Loss: 30.9021\n",
      "Processing batch 6905/11884 - Loss: 31.3914\n",
      "Processing batch 6906/11884 - Loss: 30.3518\n",
      "Processing batch 6907/11884 - Loss: 31.0194\n",
      "Processing batch 6908/11884 - Loss: 30.4509\n",
      "Processing batch 6909/11884 - Loss: 30.7623\n",
      "Processing batch 6910/11884 - Loss: 29.6898\n",
      "Processing batch 6911/11884 - Loss: 30.9692\n",
      "Processing batch 6912/11884 - Loss: 30.0752\n",
      "Processing batch 6913/11884 - Loss: 29.4141\n",
      "Processing batch 6914/11884 - Loss: 28.9803\n",
      "Processing batch 6915/11884 - Loss: 30.4559\n",
      "Processing batch 6916/11884 - Loss: 30.1192\n",
      "Processing batch 6917/11884 - Loss: 30.3151\n",
      "Processing batch 6918/11884 - Loss: 30.5673\n",
      "Processing batch 6919/11884 - Loss: 30.3453\n",
      "Processing batch 6920/11884 - Loss: 29.5421\n",
      "Processing batch 6921/11884 - Loss: 29.5541\n",
      "Processing batch 6922/11884 - Loss: 29.4609\n",
      "Processing batch 6923/11884 - Loss: 31.6374\n",
      "Processing batch 6924/11884 - Loss: 31.2915\n",
      "Processing batch 6925/11884 - Loss: 28.7794\n",
      "Processing batch 6926/11884 - Loss: 31.1003\n",
      "Processing batch 6927/11884 - Loss: 30.0090\n",
      "Processing batch 6928/11884 - Loss: 30.7757\n",
      "Processing batch 6929/11884 - Loss: 31.0156\n",
      "Processing batch 6930/11884 - Loss: 30.6892\n",
      "Processing batch 6931/11884 - Loss: 31.0400\n",
      "Processing batch 6932/11884 - Loss: 29.4629\n",
      "Processing batch 6933/11884 - Loss: 29.7576\n",
      "Processing batch 6934/11884 - Loss: 30.7351\n",
      "Processing batch 6935/11884 - Loss: 30.3632\n",
      "Processing batch 6936/11884 - Loss: 30.3106\n",
      "Processing batch 6937/11884 - Loss: 29.6101\n",
      "Processing batch 6938/11884 - Loss: 30.6346\n",
      "Processing batch 6939/11884 - Loss: 31.2470\n",
      "Processing batch 6940/11884 - Loss: 29.7484\n",
      "Processing batch 6941/11884 - Loss: 30.9588\n",
      "Processing batch 6942/11884 - Loss: 29.7716\n",
      "Processing batch 6943/11884 - Loss: 29.9182\n",
      "Processing batch 6944/11884 - Loss: 30.8670\n",
      "Processing batch 6945/11884 - Loss: 30.2122\n",
      "Processing batch 6946/11884 - Loss: 30.5062\n",
      "Processing batch 6947/11884 - Loss: 30.5111\n",
      "Processing batch 6948/11884 - Loss: 31.1727\n",
      "Processing batch 6949/11884 - Loss: 30.8267\n",
      "Processing batch 6950/11884 - Loss: 29.5476\n",
      "Processing batch 6951/11884 - Loss: 31.0309\n",
      "Processing batch 6952/11884 - Loss: 30.6421\n",
      "Processing batch 6953/11884 - Loss: 30.4562\n",
      "Processing batch 6954/11884 - Loss: 29.9024\n",
      "Processing batch 6955/11884 - Loss: 29.8009\n",
      "Processing batch 6956/11884 - Loss: 31.5447\n",
      "Processing batch 6957/11884 - Loss: 30.5572\n",
      "Processing batch 6958/11884 - Loss: 29.6075\n",
      "Processing batch 6959/11884 - Loss: 30.8593\n",
      "Processing batch 6960/11884 - Loss: 29.9586\n",
      "Processing batch 6961/11884 - Loss: 29.2122\n",
      "Processing batch 6962/11884 - Loss: 30.1204\n",
      "Processing batch 6963/11884 - Loss: 30.6876\n",
      "Processing batch 6964/11884 - Loss: 32.0695\n",
      "Processing batch 6965/11884 - Loss: 30.7628\n",
      "Processing batch 6966/11884 - Loss: 30.9541\n",
      "Processing batch 6967/11884 - Loss: 30.4033\n",
      "Processing batch 6968/11884 - Loss: 30.8609\n",
      "Processing batch 6969/11884 - Loss: 30.7914\n",
      "Processing batch 6970/11884 - Loss: 30.5728\n",
      "Processing batch 6971/11884 - Loss: 30.5521\n",
      "Processing batch 6972/11884 - Loss: 31.1375\n",
      "Processing batch 6973/11884 - Loss: 30.4769\n",
      "Processing batch 6974/11884 - Loss: 31.3783\n",
      "Processing batch 6975/11884 - Loss: 29.9370\n",
      "Processing batch 6976/11884 - Loss: 30.5166\n",
      "Processing batch 6977/11884 - Loss: 29.5550\n",
      "Processing batch 6978/11884 - Loss: 30.5140\n",
      "Processing batch 6979/11884 - Loss: 30.6560\n",
      "Processing batch 6980/11884 - Loss: 31.0612\n",
      "Processing batch 6981/11884 - Loss: 32.4266\n",
      "Processing batch 6982/11884 - Loss: 31.0645\n",
      "Processing batch 6983/11884 - Loss: 29.3326\n",
      "Processing batch 6984/11884 - Loss: 31.4275\n",
      "Processing batch 6985/11884 - Loss: 30.5864\n",
      "Processing batch 6986/11884 - Loss: 30.2295\n",
      "Processing batch 6987/11884 - Loss: 30.3174\n",
      "Processing batch 6988/11884 - Loss: 31.7631\n",
      "Processing batch 6989/11884 - Loss: 30.2664\n",
      "Processing batch 6990/11884 - Loss: 30.7767\n",
      "Processing batch 6991/11884 - Loss: 30.7307\n",
      "Processing batch 6992/11884 - Loss: 30.5535\n",
      "Processing batch 6993/11884 - Loss: 29.3015\n",
      "Processing batch 6994/11884 - Loss: 31.1931\n",
      "Processing batch 6995/11884 - Loss: 32.1035\n",
      "Processing batch 6996/11884 - Loss: 29.5743\n",
      "Processing batch 6997/11884 - Loss: 31.0573\n",
      "Processing batch 6998/11884 - Loss: 30.5942\n",
      "Processing batch 6999/11884 - Loss: 28.8760\n",
      "Processing batch 7000/11884 - Loss: 31.3222\n",
      "Processing batch 7001/11884 - Loss: 30.2100\n",
      "Processing batch 7002/11884 - Loss: 31.1275\n",
      "Processing batch 7003/11884 - Loss: 31.2438\n",
      "Processing batch 7004/11884 - Loss: 31.1192\n",
      "Processing batch 7005/11884 - Loss: 31.0832\n",
      "Processing batch 7006/11884 - Loss: 30.6914\n",
      "Processing batch 7007/11884 - Loss: 30.2103\n",
      "Processing batch 7008/11884 - Loss: 31.1397\n",
      "Processing batch 7009/11884 - Loss: 30.7614\n",
      "Processing batch 7010/11884 - Loss: 29.7728\n",
      "Processing batch 7011/11884 - Loss: 30.4479\n",
      "Processing batch 7012/11884 - Loss: 30.8695\n",
      "Processing batch 7013/11884 - Loss: 30.7781\n",
      "Processing batch 7014/11884 - Loss: 30.5921\n",
      "Processing batch 7015/11884 - Loss: 29.5366\n",
      "Processing batch 7016/11884 - Loss: 29.7412\n",
      "Processing batch 7017/11884 - Loss: 30.3539\n",
      "Processing batch 7018/11884 - Loss: 31.1255\n",
      "Processing batch 7019/11884 - Loss: 30.3672\n",
      "Processing batch 7020/11884 - Loss: 31.8426\n",
      "Processing batch 7021/11884 - Loss: 30.5230\n",
      "Processing batch 7022/11884 - Loss: 30.8639\n",
      "Processing batch 7023/11884 - Loss: 30.7077\n",
      "Processing batch 7024/11884 - Loss: 32.2074\n",
      "Processing batch 7025/11884 - Loss: 30.4096\n",
      "Processing batch 7026/11884 - Loss: 30.7972\n",
      "Processing batch 7027/11884 - Loss: 31.8613\n",
      "Processing batch 7028/11884 - Loss: 29.3559\n",
      "Processing batch 7029/11884 - Loss: 29.3059\n",
      "Processing batch 7030/11884 - Loss: 30.5473\n",
      "Processing batch 7031/11884 - Loss: 29.9620\n",
      "Processing batch 7032/11884 - Loss: 30.5372\n",
      "Processing batch 7033/11884 - Loss: 29.8251\n",
      "Processing batch 7034/11884 - Loss: 31.0627\n",
      "Processing batch 7035/11884 - Loss: 30.7875\n",
      "Processing batch 7036/11884 - Loss: 29.9975\n",
      "Processing batch 7037/11884 - Loss: 30.9744\n",
      "Processing batch 7038/11884 - Loss: 29.0917\n",
      "Processing batch 7039/11884 - Loss: 30.8044\n",
      "Processing batch 7040/11884 - Loss: 30.1663\n",
      "Processing batch 7041/11884 - Loss: 30.2716\n",
      "Processing batch 7042/11884 - Loss: 30.9068\n",
      "Processing batch 7043/11884 - Loss: 31.7107\n",
      "Processing batch 7044/11884 - Loss: 30.5337\n",
      "Processing batch 7045/11884 - Loss: 30.4588\n",
      "Processing batch 7046/11884 - Loss: 29.9050\n",
      "Processing batch 7047/11884 - Loss: 28.8072\n",
      "Processing batch 7048/11884 - Loss: 30.6108\n",
      "Processing batch 7049/11884 - Loss: 31.2251\n",
      "Processing batch 7050/11884 - Loss: 31.0968\n",
      "Processing batch 7051/11884 - Loss: 29.0114\n",
      "Processing batch 7052/11884 - Loss: 29.7407\n",
      "Processing batch 7053/11884 - Loss: 28.9542\n",
      "Processing batch 7054/11884 - Loss: 30.4372\n",
      "Processing batch 7055/11884 - Loss: 30.5831\n",
      "Processing batch 7056/11884 - Loss: 29.7164\n",
      "Processing batch 7057/11884 - Loss: 31.2563\n",
      "Processing batch 7058/11884 - Loss: 30.9997\n",
      "Processing batch 7059/11884 - Loss: 29.2368\n",
      "Processing batch 7060/11884 - Loss: 30.5254\n",
      "Processing batch 7061/11884 - Loss: 30.3622\n",
      "Processing batch 7062/11884 - Loss: 29.4471\n",
      "Processing batch 7063/11884 - Loss: 30.1184\n",
      "Processing batch 7064/11884 - Loss: 29.7696\n",
      "Processing batch 7065/11884 - Loss: 30.1489\n",
      "Processing batch 7066/11884 - Loss: 29.1755\n",
      "Processing batch 7067/11884 - Loss: 30.6939\n",
      "Processing batch 7068/11884 - Loss: 31.2633\n",
      "Processing batch 7069/11884 - Loss: 30.2809\n",
      "Processing batch 7070/11884 - Loss: 30.3755\n",
      "Processing batch 7071/11884 - Loss: 29.1455\n",
      "Processing batch 7072/11884 - Loss: 29.9904\n",
      "Processing batch 7073/11884 - Loss: 30.0625\n",
      "Processing batch 7074/11884 - Loss: 31.9425\n",
      "Processing batch 7075/11884 - Loss: 29.9705\n",
      "Processing batch 7076/11884 - Loss: 30.8846\n",
      "Processing batch 7077/11884 - Loss: 30.1682\n",
      "Processing batch 7078/11884 - Loss: 30.9139\n",
      "Processing batch 7079/11884 - Loss: 29.8506\n",
      "Processing batch 7080/11884 - Loss: 30.5753\n",
      "Processing batch 7081/11884 - Loss: 30.4242\n",
      "Processing batch 7082/11884 - Loss: 30.1342\n",
      "Processing batch 7083/11884 - Loss: 32.2407\n",
      "Processing batch 7084/11884 - Loss: 29.8682\n",
      "Processing batch 7085/11884 - Loss: 30.6498\n",
      "Processing batch 7086/11884 - Loss: 30.2621\n",
      "Processing batch 7087/11884 - Loss: 31.2121\n",
      "Processing batch 7088/11884 - Loss: 31.6918\n",
      "Processing batch 7089/11884 - Loss: 31.0719\n",
      "Processing batch 7090/11884 - Loss: 30.6598\n",
      "Processing batch 7091/11884 - Loss: 29.7035\n",
      "Processing batch 7092/11884 - Loss: 29.6811\n",
      "Processing batch 7093/11884 - Loss: 29.7084\n",
      "Processing batch 7094/11884 - Loss: 31.7009\n",
      "Processing batch 7095/11884 - Loss: 29.9919\n",
      "Processing batch 7096/11884 - Loss: 30.1891\n",
      "Processing batch 7097/11884 - Loss: 30.4956\n",
      "Processing batch 7098/11884 - Loss: 30.1555\n",
      "Processing batch 7099/11884 - Loss: 29.6746\n",
      "Processing batch 7100/11884 - Loss: 29.8550\n",
      "Processing batch 7101/11884 - Loss: 30.7753\n",
      "Processing batch 7102/11884 - Loss: 30.6742\n",
      "Processing batch 7103/11884 - Loss: 29.7763\n",
      "Processing batch 7104/11884 - Loss: 32.0400\n",
      "Processing batch 7105/11884 - Loss: 29.7068\n",
      "Processing batch 7106/11884 - Loss: 28.4994\n",
      "Processing batch 7107/11884 - Loss: 29.9332\n",
      "Processing batch 7108/11884 - Loss: 29.1054\n",
      "Processing batch 7109/11884 - Loss: 30.2794\n",
      "Processing batch 7110/11884 - Loss: 29.2397\n",
      "Processing batch 7111/11884 - Loss: 30.8059\n",
      "Processing batch 7112/11884 - Loss: 29.7918\n",
      "Processing batch 7113/11884 - Loss: 29.5680\n",
      "Processing batch 7114/11884 - Loss: 29.3527\n",
      "Processing batch 7115/11884 - Loss: 30.3125\n",
      "Processing batch 7116/11884 - Loss: 31.2044\n",
      "Processing batch 7117/11884 - Loss: 30.4618\n",
      "Processing batch 7118/11884 - Loss: 30.4043\n",
      "Processing batch 7119/11884 - Loss: 29.9893\n",
      "Processing batch 7120/11884 - Loss: 31.0063\n",
      "Processing batch 7121/11884 - Loss: 30.5291\n",
      "Processing batch 7122/11884 - Loss: 29.4394\n",
      "Processing batch 7123/11884 - Loss: 30.3351\n",
      "Processing batch 7124/11884 - Loss: 29.9971\n",
      "Processing batch 7125/11884 - Loss: 31.5597\n",
      "Processing batch 7126/11884 - Loss: 30.6486\n",
      "Processing batch 7127/11884 - Loss: 30.6089\n",
      "Processing batch 7128/11884 - Loss: 30.3288\n",
      "Processing batch 7129/11884 - Loss: 30.1935\n",
      "Processing batch 7130/11884 - Loss: 30.2752\n",
      "Processing batch 7131/11884 - Loss: 30.0807\n",
      "Processing batch 7132/11884 - Loss: 29.7977\n",
      "Processing batch 7133/11884 - Loss: 29.2118\n",
      "Processing batch 7134/11884 - Loss: 31.3999\n",
      "Processing batch 7135/11884 - Loss: 29.9287\n",
      "Processing batch 7136/11884 - Loss: 30.2117\n",
      "Processing batch 7137/11884 - Loss: 29.8478\n",
      "Processing batch 7138/11884 - Loss: 29.7646\n",
      "Processing batch 7139/11884 - Loss: 30.7667\n",
      "Processing batch 7140/11884 - Loss: 31.2869\n",
      "Processing batch 7141/11884 - Loss: 30.6947\n",
      "Processing batch 7142/11884 - Loss: 29.4481\n",
      "Processing batch 7143/11884 - Loss: 31.1570\n",
      "Processing batch 7144/11884 - Loss: 30.8026\n",
      "Processing batch 7145/11884 - Loss: 29.2090\n",
      "Processing batch 7146/11884 - Loss: 31.3331\n",
      "Processing batch 7147/11884 - Loss: 31.5423\n",
      "Processing batch 7148/11884 - Loss: 30.7989\n",
      "Processing batch 7149/11884 - Loss: 31.2608\n",
      "Processing batch 7150/11884 - Loss: 31.3275\n",
      "Processing batch 7151/11884 - Loss: 30.7015\n",
      "Processing batch 7152/11884 - Loss: 29.9219\n",
      "Processing batch 7153/11884 - Loss: 31.6468\n",
      "Processing batch 7154/11884 - Loss: 31.7726\n",
      "Processing batch 7155/11884 - Loss: 31.3253\n",
      "Processing batch 7156/11884 - Loss: 31.6950\n",
      "Processing batch 7157/11884 - Loss: 31.1414\n",
      "Processing batch 7158/11884 - Loss: 31.9335\n",
      "Processing batch 7159/11884 - Loss: 29.8466\n",
      "Processing batch 7160/11884 - Loss: 30.7911\n",
      "Processing batch 7161/11884 - Loss: 30.3198\n",
      "Processing batch 7162/11884 - Loss: 31.5456\n",
      "Processing batch 7163/11884 - Loss: 30.1454\n",
      "Processing batch 7164/11884 - Loss: 30.3887\n",
      "Processing batch 7165/11884 - Loss: 32.0373\n",
      "Processing batch 7166/11884 - Loss: 30.9570\n",
      "Processing batch 7167/11884 - Loss: 29.0015\n",
      "Processing batch 7168/11884 - Loss: 30.6838\n",
      "Processing batch 7169/11884 - Loss: 29.4439\n",
      "Processing batch 7170/11884 - Loss: 30.5854\n",
      "Processing batch 7171/11884 - Loss: 31.2956\n",
      "Processing batch 7172/11884 - Loss: 30.4064\n",
      "Processing batch 7173/11884 - Loss: 30.2834\n",
      "Processing batch 7174/11884 - Loss: 30.7850\n",
      "Processing batch 7175/11884 - Loss: 29.9945\n",
      "Processing batch 7176/11884 - Loss: 30.8305\n",
      "Processing batch 7177/11884 - Loss: 30.7292\n",
      "Processing batch 7178/11884 - Loss: 31.9578\n",
      "Processing batch 7179/11884 - Loss: 30.6195\n",
      "Processing batch 7180/11884 - Loss: 30.4013\n",
      "Processing batch 7181/11884 - Loss: 31.8629\n",
      "Processing batch 7182/11884 - Loss: 29.7733\n",
      "Processing batch 7183/11884 - Loss: 30.4654\n",
      "Processing batch 7184/11884 - Loss: 30.8314\n",
      "Processing batch 7185/11884 - Loss: 29.9615\n",
      "Processing batch 7186/11884 - Loss: 30.4296\n",
      "Processing batch 7187/11884 - Loss: 29.8084\n",
      "Processing batch 7188/11884 - Loss: 31.0217\n",
      "Processing batch 7189/11884 - Loss: 31.4353\n",
      "Processing batch 7190/11884 - Loss: 30.4490\n",
      "Processing batch 7191/11884 - Loss: 30.3453\n",
      "Processing batch 7192/11884 - Loss: 30.4947\n",
      "Processing batch 7193/11884 - Loss: 32.6209\n",
      "Processing batch 7194/11884 - Loss: 29.0491\n",
      "Processing batch 7195/11884 - Loss: 30.8604\n",
      "Processing batch 7196/11884 - Loss: 32.2575\n",
      "Processing batch 7197/11884 - Loss: 29.9773\n",
      "Processing batch 7198/11884 - Loss: 30.9022\n",
      "Processing batch 7199/11884 - Loss: 29.0937\n",
      "Processing batch 7200/11884 - Loss: 29.5816\n",
      "Processing batch 7201/11884 - Loss: 30.5007\n",
      "Processing batch 7202/11884 - Loss: 29.6295\n",
      "Processing batch 7203/11884 - Loss: 30.1081\n",
      "Processing batch 7204/11884 - Loss: 31.0265\n",
      "Processing batch 7205/11884 - Loss: 30.9785\n",
      "Processing batch 7206/11884 - Loss: 30.0562\n",
      "Processing batch 7207/11884 - Loss: 30.2317\n",
      "Processing batch 7208/11884 - Loss: 31.7353\n",
      "Processing batch 7209/11884 - Loss: 31.3413\n",
      "Processing batch 7210/11884 - Loss: 31.9473\n",
      "Processing batch 7211/11884 - Loss: 30.2593\n",
      "Processing batch 7212/11884 - Loss: 31.3433\n",
      "Processing batch 7213/11884 - Loss: 29.3867\n",
      "Processing batch 7214/11884 - Loss: 30.1574\n",
      "Processing batch 7215/11884 - Loss: 32.1757\n",
      "Processing batch 7216/11884 - Loss: 30.4565\n",
      "Processing batch 7217/11884 - Loss: 30.6675\n",
      "Processing batch 7218/11884 - Loss: 29.9249\n",
      "Processing batch 7219/11884 - Loss: 31.3980\n",
      "Processing batch 7220/11884 - Loss: 30.3434\n",
      "Processing batch 7221/11884 - Loss: 30.1770\n",
      "Processing batch 7222/11884 - Loss: 30.0490\n",
      "Processing batch 7223/11884 - Loss: 29.8444\n",
      "Processing batch 7224/11884 - Loss: 30.2884\n",
      "Processing batch 7225/11884 - Loss: 31.2256\n",
      "Processing batch 7226/11884 - Loss: 30.8972\n",
      "Processing batch 7227/11884 - Loss: 29.3773\n",
      "Processing batch 7228/11884 - Loss: 30.3533\n",
      "Processing batch 7229/11884 - Loss: 30.1497\n",
      "Processing batch 7230/11884 - Loss: 30.7477\n",
      "Processing batch 7231/11884 - Loss: 30.0252\n",
      "Processing batch 7232/11884 - Loss: 30.2994\n",
      "Processing batch 7233/11884 - Loss: 30.4356\n",
      "Processing batch 7234/11884 - Loss: 29.5503\n",
      "Processing batch 7235/11884 - Loss: 29.2643\n",
      "Processing batch 7236/11884 - Loss: 30.8205\n",
      "Processing batch 7237/11884 - Loss: 31.1967\n",
      "Processing batch 7238/11884 - Loss: 30.2476\n",
      "Processing batch 7239/11884 - Loss: 31.1143\n",
      "Processing batch 7240/11884 - Loss: 30.6490\n",
      "Processing batch 7241/11884 - Loss: 30.7066\n",
      "Processing batch 7242/11884 - Loss: 29.1547\n",
      "Processing batch 7243/11884 - Loss: 30.0054\n",
      "Processing batch 7244/11884 - Loss: 30.7846\n",
      "Processing batch 7245/11884 - Loss: 31.3712\n",
      "Processing batch 7246/11884 - Loss: 30.3452\n",
      "Processing batch 7247/11884 - Loss: 29.0746\n",
      "Processing batch 7248/11884 - Loss: 31.3699\n",
      "Processing batch 7249/11884 - Loss: 31.6848\n",
      "Processing batch 7250/11884 - Loss: 30.6411\n",
      "Processing batch 7251/11884 - Loss: 30.6800\n",
      "Processing batch 7252/11884 - Loss: 30.4793\n",
      "Processing batch 7253/11884 - Loss: 31.4565\n",
      "Processing batch 7254/11884 - Loss: 31.9433\n",
      "Processing batch 7255/11884 - Loss: 30.6210\n",
      "Processing batch 7256/11884 - Loss: 30.4583\n",
      "Processing batch 7257/11884 - Loss: 31.3414\n",
      "Processing batch 7258/11884 - Loss: 29.5112\n",
      "Processing batch 7259/11884 - Loss: 29.9882\n",
      "Processing batch 7260/11884 - Loss: 29.7218\n",
      "Processing batch 7261/11884 - Loss: 31.3060\n",
      "Processing batch 7262/11884 - Loss: 30.9517\n",
      "Processing batch 7263/11884 - Loss: 30.0842\n",
      "Processing batch 7264/11884 - Loss: 30.5961\n",
      "Processing batch 7265/11884 - Loss: 31.0754\n",
      "Processing batch 7266/11884 - Loss: 31.2208\n",
      "Processing batch 7267/11884 - Loss: 30.7564\n",
      "Processing batch 7268/11884 - Loss: 30.0296\n",
      "Processing batch 7269/11884 - Loss: 29.5640\n",
      "Processing batch 7270/11884 - Loss: 29.7280\n",
      "Processing batch 7271/11884 - Loss: 29.7278\n",
      "Processing batch 7272/11884 - Loss: 29.6979\n",
      "Processing batch 7273/11884 - Loss: 30.1390\n",
      "Processing batch 7274/11884 - Loss: 30.9365\n",
      "Processing batch 7275/11884 - Loss: 29.9743\n",
      "Processing batch 7276/11884 - Loss: 30.4605\n",
      "Processing batch 7277/11884 - Loss: 30.0526\n",
      "Processing batch 7278/11884 - Loss: 29.9738\n",
      "Processing batch 7279/11884 - Loss: 30.9900\n",
      "Processing batch 7280/11884 - Loss: 29.8424\n",
      "Processing batch 7281/11884 - Loss: 30.6319\n",
      "Processing batch 7282/11884 - Loss: 29.7151\n",
      "Processing batch 7283/11884 - Loss: 30.5362\n",
      "Processing batch 7284/11884 - Loss: 29.7711\n",
      "Processing batch 7285/11884 - Loss: 30.0480\n",
      "Processing batch 7286/11884 - Loss: 29.8934\n",
      "Processing batch 7287/11884 - Loss: 31.8306\n",
      "Processing batch 7288/11884 - Loss: 30.3848\n",
      "Processing batch 7289/11884 - Loss: 30.0145\n",
      "Processing batch 7290/11884 - Loss: 30.1996\n",
      "Processing batch 7291/11884 - Loss: 30.0071\n",
      "Processing batch 7292/11884 - Loss: 29.9011\n",
      "Processing batch 7293/11884 - Loss: 30.3095\n",
      "Processing batch 7294/11884 - Loss: 30.8203\n",
      "Processing batch 7295/11884 - Loss: 30.6072\n",
      "Processing batch 7296/11884 - Loss: 31.4196\n",
      "Processing batch 7297/11884 - Loss: 29.8326\n",
      "Processing batch 7298/11884 - Loss: 30.6624\n",
      "Processing batch 7299/11884 - Loss: 30.7895\n",
      "Processing batch 7300/11884 - Loss: 31.4371\n",
      "Processing batch 7301/11884 - Loss: 31.5155\n",
      "Processing batch 7302/11884 - Loss: 31.8427\n",
      "Processing batch 7303/11884 - Loss: 31.5087\n",
      "Processing batch 7304/11884 - Loss: 29.4159\n",
      "Processing batch 7305/11884 - Loss: 29.2953\n",
      "Processing batch 7306/11884 - Loss: 29.7417\n",
      "Processing batch 7307/11884 - Loss: 30.2730\n",
      "Processing batch 7308/11884 - Loss: 31.4309\n",
      "Processing batch 7309/11884 - Loss: 31.0392\n",
      "Processing batch 7310/11884 - Loss: 30.7792\n",
      "Processing batch 7311/11884 - Loss: 29.9091\n",
      "Processing batch 7312/11884 - Loss: 29.3750\n",
      "Processing batch 7313/11884 - Loss: 29.3649\n",
      "Processing batch 7314/11884 - Loss: 29.1120\n",
      "Processing batch 7315/11884 - Loss: 30.7064\n",
      "Processing batch 7316/11884 - Loss: 30.7835\n",
      "Processing batch 7317/11884 - Loss: 30.4553\n",
      "Processing batch 7318/11884 - Loss: 29.6199\n",
      "Processing batch 7319/11884 - Loss: 30.9085\n",
      "Processing batch 7320/11884 - Loss: 29.8732\n",
      "Processing batch 7321/11884 - Loss: 30.5896\n",
      "Processing batch 7322/11884 - Loss: 30.2225\n",
      "Processing batch 7323/11884 - Loss: 30.0761\n",
      "Processing batch 7324/11884 - Loss: 30.4646\n",
      "Processing batch 7325/11884 - Loss: 31.6138\n",
      "Processing batch 7326/11884 - Loss: 31.7031\n",
      "Processing batch 7327/11884 - Loss: 30.3586\n",
      "Processing batch 7328/11884 - Loss: 29.6958\n",
      "Processing batch 7329/11884 - Loss: 31.2904\n",
      "Processing batch 7330/11884 - Loss: 30.8625\n",
      "Processing batch 7331/11884 - Loss: 28.9914\n",
      "Processing batch 7332/11884 - Loss: 30.0008\n",
      "Processing batch 7333/11884 - Loss: 28.9385\n",
      "Processing batch 7334/11884 - Loss: 28.8465\n",
      "Processing batch 7335/11884 - Loss: 30.4702\n",
      "Processing batch 7336/11884 - Loss: 31.6639\n",
      "Processing batch 7337/11884 - Loss: 30.7769\n",
      "Processing batch 7338/11884 - Loss: 30.8195\n",
      "Processing batch 7339/11884 - Loss: 31.7525\n",
      "Processing batch 7340/11884 - Loss: 29.8156\n",
      "Processing batch 7341/11884 - Loss: 29.5238\n",
      "Processing batch 7342/11884 - Loss: 30.2706\n",
      "Processing batch 7343/11884 - Loss: 31.0515\n",
      "Processing batch 7344/11884 - Loss: 31.0323\n",
      "Processing batch 7345/11884 - Loss: 30.0925\n",
      "Processing batch 7346/11884 - Loss: 29.4702\n",
      "Processing batch 7347/11884 - Loss: 29.7610\n",
      "Processing batch 7348/11884 - Loss: 31.3303\n",
      "Processing batch 7349/11884 - Loss: 31.2288\n",
      "Processing batch 7350/11884 - Loss: 30.5525\n",
      "Processing batch 7351/11884 - Loss: 29.7673\n",
      "Processing batch 7352/11884 - Loss: 30.2939\n",
      "Processing batch 7353/11884 - Loss: 30.4208\n",
      "Processing batch 7354/11884 - Loss: 31.0339\n",
      "Processing batch 7355/11884 - Loss: 30.1243\n",
      "Processing batch 7356/11884 - Loss: 31.1062\n",
      "Processing batch 7357/11884 - Loss: 31.4575\n",
      "Processing batch 7358/11884 - Loss: 28.7242\n",
      "Processing batch 7359/11884 - Loss: 30.7366\n",
      "Processing batch 7360/11884 - Loss: 30.1454\n",
      "Processing batch 7361/11884 - Loss: 30.3367\n",
      "Processing batch 7362/11884 - Loss: 30.1825\n",
      "Processing batch 7363/11884 - Loss: 29.2019\n",
      "Processing batch 7364/11884 - Loss: 30.4356\n",
      "Processing batch 7365/11884 - Loss: 30.1344\n",
      "Processing batch 7366/11884 - Loss: 30.8528\n",
      "Processing batch 7367/11884 - Loss: 30.5683\n",
      "Processing batch 7368/11884 - Loss: 31.8386\n",
      "Processing batch 7369/11884 - Loss: 29.9278\n",
      "Processing batch 7370/11884 - Loss: 29.8022\n",
      "Processing batch 7371/11884 - Loss: 30.5949\n",
      "Processing batch 7372/11884 - Loss: 29.4501\n",
      "Processing batch 7373/11884 - Loss: 30.2882\n",
      "Processing batch 7374/11884 - Loss: 30.9559\n",
      "Processing batch 7375/11884 - Loss: 29.9656\n",
      "Processing batch 7376/11884 - Loss: 30.1262\n",
      "Processing batch 7377/11884 - Loss: 31.1333\n",
      "Processing batch 7378/11884 - Loss: 29.2136\n",
      "Processing batch 7379/11884 - Loss: 29.8070\n",
      "Processing batch 7380/11884 - Loss: 30.2655\n",
      "Processing batch 7381/11884 - Loss: 30.1565\n",
      "Processing batch 7382/11884 - Loss: 30.8996\n",
      "Processing batch 7383/11884 - Loss: 30.1631\n",
      "Processing batch 7384/11884 - Loss: 31.8215\n",
      "Processing batch 7385/11884 - Loss: 30.7938\n",
      "Processing batch 7386/11884 - Loss: 30.7078\n",
      "Processing batch 7387/11884 - Loss: 30.6107\n",
      "Processing batch 7388/11884 - Loss: 30.3371\n",
      "Processing batch 7389/11884 - Loss: 29.0228\n",
      "Processing batch 7390/11884 - Loss: 29.7538\n",
      "Processing batch 7391/11884 - Loss: 29.5741\n",
      "Processing batch 7392/11884 - Loss: 29.8872\n",
      "Processing batch 7393/11884 - Loss: 29.5506\n",
      "Processing batch 7394/11884 - Loss: 29.6971\n",
      "Processing batch 7395/11884 - Loss: 31.2217\n",
      "Processing batch 7396/11884 - Loss: 30.7118\n",
      "Processing batch 7397/11884 - Loss: 31.7990\n",
      "Processing batch 7398/11884 - Loss: 31.4706\n",
      "Processing batch 7399/11884 - Loss: 30.8131\n",
      "Processing batch 7400/11884 - Loss: 29.8170\n",
      "Processing batch 7401/11884 - Loss: 29.5655\n",
      "Processing batch 7402/11884 - Loss: 30.8876\n",
      "Processing batch 7403/11884 - Loss: 29.9954\n",
      "Processing batch 7404/11884 - Loss: 30.0050\n",
      "Processing batch 7405/11884 - Loss: 30.1845\n",
      "Processing batch 7406/11884 - Loss: 29.6147\n",
      "Processing batch 7407/11884 - Loss: 30.6149\n",
      "Processing batch 7408/11884 - Loss: 30.2778\n",
      "Processing batch 7409/11884 - Loss: 29.5989\n",
      "Processing batch 7410/11884 - Loss: 30.1346\n",
      "Processing batch 7411/11884 - Loss: 31.0084\n",
      "Processing batch 7412/11884 - Loss: 31.4728\n",
      "Processing batch 7413/11884 - Loss: 30.5556\n",
      "Processing batch 7414/11884 - Loss: 30.7261\n",
      "Processing batch 7415/11884 - Loss: 30.6998\n",
      "Processing batch 7416/11884 - Loss: 30.0155\n",
      "Processing batch 7417/11884 - Loss: 31.4429\n",
      "Processing batch 7418/11884 - Loss: 30.6652\n",
      "Processing batch 7419/11884 - Loss: 29.4221\n",
      "Processing batch 7420/11884 - Loss: 30.3153\n",
      "Processing batch 7421/11884 - Loss: 30.8549\n",
      "Processing batch 7422/11884 - Loss: 31.0397\n",
      "Processing batch 7423/11884 - Loss: 29.9411\n",
      "Processing batch 7424/11884 - Loss: 30.6590\n",
      "Processing batch 7425/11884 - Loss: 29.9862\n",
      "Processing batch 7426/11884 - Loss: 30.6574\n",
      "Processing batch 7427/11884 - Loss: 29.4192\n",
      "Processing batch 7428/11884 - Loss: 29.7094\n",
      "Processing batch 7429/11884 - Loss: 29.7995\n",
      "Processing batch 7430/11884 - Loss: 30.2525\n",
      "Processing batch 7431/11884 - Loss: 31.1569\n",
      "Processing batch 7432/11884 - Loss: 29.9299\n",
      "Processing batch 7433/11884 - Loss: 30.5157\n",
      "Processing batch 7434/11884 - Loss: 30.0877\n",
      "Processing batch 7435/11884 - Loss: 31.5737\n",
      "Processing batch 7436/11884 - Loss: 30.6345\n",
      "Processing batch 7437/11884 - Loss: 30.2617\n",
      "Processing batch 7438/11884 - Loss: 29.7019\n",
      "Processing batch 7439/11884 - Loss: 31.8602\n",
      "Processing batch 7440/11884 - Loss: 31.2393\n",
      "Processing batch 7441/11884 - Loss: 30.0110\n",
      "Processing batch 7442/11884 - Loss: 30.0749\n",
      "Processing batch 7443/11884 - Loss: 31.3825\n",
      "Processing batch 7444/11884 - Loss: 31.8833\n",
      "Processing batch 7445/11884 - Loss: 30.7892\n",
      "Processing batch 7446/11884 - Loss: 31.5668\n",
      "Processing batch 7447/11884 - Loss: 30.2752\n",
      "Processing batch 7448/11884 - Loss: 30.4474\n",
      "Processing batch 7449/11884 - Loss: 30.0346\n",
      "Processing batch 7450/11884 - Loss: 29.3670\n",
      "Processing batch 7451/11884 - Loss: 29.7221\n",
      "Processing batch 7452/11884 - Loss: 31.0437\n",
      "Processing batch 7453/11884 - Loss: 29.7667\n",
      "Processing batch 7454/11884 - Loss: 30.1051\n",
      "Processing batch 7455/11884 - Loss: 32.0327\n",
      "Processing batch 7456/11884 - Loss: 29.2107\n",
      "Processing batch 7457/11884 - Loss: 31.2576\n",
      "Processing batch 7458/11884 - Loss: 30.9614\n",
      "Processing batch 7459/11884 - Loss: 29.9728\n",
      "Processing batch 7460/11884 - Loss: 31.5966\n",
      "Processing batch 7461/11884 - Loss: 30.4723\n",
      "Processing batch 7462/11884 - Loss: 31.0255\n",
      "Processing batch 7463/11884 - Loss: 30.1782\n",
      "Processing batch 7464/11884 - Loss: 30.3556\n",
      "Processing batch 7465/11884 - Loss: 30.5585\n",
      "Processing batch 7466/11884 - Loss: 31.2583\n",
      "Processing batch 7467/11884 - Loss: 29.5597\n",
      "Processing batch 7468/11884 - Loss: 30.7119\n",
      "Processing batch 7469/11884 - Loss: 30.1964\n",
      "Processing batch 7470/11884 - Loss: 30.8400\n",
      "Processing batch 7471/11884 - Loss: 32.6482\n",
      "Processing batch 7472/11884 - Loss: 31.0090\n",
      "Processing batch 7473/11884 - Loss: 30.5924\n",
      "Processing batch 7474/11884 - Loss: 30.5929\n",
      "Processing batch 7475/11884 - Loss: 30.7098\n",
      "Processing batch 7476/11884 - Loss: 30.7882\n",
      "Processing batch 7477/11884 - Loss: 30.6745\n",
      "Processing batch 7478/11884 - Loss: 30.7678\n",
      "Processing batch 7479/11884 - Loss: 29.9776\n",
      "Processing batch 7480/11884 - Loss: 30.6756\n",
      "Processing batch 7481/11884 - Loss: 30.9172\n",
      "Processing batch 7482/11884 - Loss: 30.1548\n",
      "Processing batch 7483/11884 - Loss: 29.7486\n",
      "Processing batch 7484/11884 - Loss: 30.0045\n",
      "Processing batch 7485/11884 - Loss: 30.7396\n",
      "Processing batch 7486/11884 - Loss: 30.4085\n",
      "Processing batch 7487/11884 - Loss: 30.3749\n",
      "Processing batch 7488/11884 - Loss: 30.0785\n",
      "Processing batch 7489/11884 - Loss: 30.2568\n",
      "Processing batch 7490/11884 - Loss: 29.5175\n",
      "Processing batch 7491/11884 - Loss: 29.2019\n",
      "Processing batch 7492/11884 - Loss: 30.0514\n",
      "Processing batch 7493/11884 - Loss: 29.0235\n",
      "Processing batch 7494/11884 - Loss: 31.4031\n",
      "Processing batch 7495/11884 - Loss: 29.6764\n",
      "Processing batch 7496/11884 - Loss: 29.4411\n",
      "Processing batch 7497/11884 - Loss: 29.8991\n",
      "Processing batch 7498/11884 - Loss: 30.7239\n",
      "Processing batch 7499/11884 - Loss: 29.4422\n",
      "Processing batch 7500/11884 - Loss: 31.1538\n",
      "Processing batch 7501/11884 - Loss: 31.0175\n",
      "Processing batch 7502/11884 - Loss: 30.6804\n",
      "Processing batch 7503/11884 - Loss: 30.6838\n",
      "Processing batch 7504/11884 - Loss: 30.1656\n",
      "Processing batch 7505/11884 - Loss: 29.4844\n",
      "Processing batch 7506/11884 - Loss: 30.5688\n",
      "Processing batch 7507/11884 - Loss: 30.6173\n",
      "Processing batch 7508/11884 - Loss: 29.4993\n",
      "Processing batch 7509/11884 - Loss: 29.4484\n",
      "Processing batch 7510/11884 - Loss: 30.2871\n",
      "Processing batch 7511/11884 - Loss: 30.8539\n",
      "Processing batch 7512/11884 - Loss: 29.7370\n",
      "Processing batch 7513/11884 - Loss: 30.6566\n",
      "Processing batch 7514/11884 - Loss: 29.0949\n",
      "Processing batch 7515/11884 - Loss: 30.3009\n",
      "Processing batch 7516/11884 - Loss: 31.0149\n",
      "Processing batch 7517/11884 - Loss: 30.9641\n",
      "Processing batch 7518/11884 - Loss: 30.1915\n",
      "Processing batch 7519/11884 - Loss: 30.5825\n",
      "Processing batch 7520/11884 - Loss: 32.3019\n",
      "Processing batch 7521/11884 - Loss: 31.4272\n",
      "Processing batch 7522/11884 - Loss: 31.1446\n",
      "Processing batch 7523/11884 - Loss: 30.3084\n",
      "Processing batch 7524/11884 - Loss: 30.8026\n",
      "Processing batch 7525/11884 - Loss: 30.9662\n",
      "Processing batch 7526/11884 - Loss: 29.8670\n",
      "Processing batch 7527/11884 - Loss: 30.9425\n",
      "Processing batch 7528/11884 - Loss: 29.8981\n",
      "Processing batch 7529/11884 - Loss: 29.7081\n",
      "Processing batch 7530/11884 - Loss: 30.2415\n",
      "Processing batch 7531/11884 - Loss: 31.5823\n",
      "Processing batch 7532/11884 - Loss: 31.1054\n",
      "Processing batch 7533/11884 - Loss: 29.1912\n",
      "Processing batch 7534/11884 - Loss: 30.8231\n",
      "Processing batch 7535/11884 - Loss: 31.3606\n",
      "Processing batch 7536/11884 - Loss: 29.4076\n",
      "Processing batch 7537/11884 - Loss: 30.1036\n",
      "Processing batch 7538/11884 - Loss: 30.8252\n",
      "Processing batch 7539/11884 - Loss: 29.1753\n",
      "Processing batch 7540/11884 - Loss: 29.5849\n",
      "Processing batch 7541/11884 - Loss: 29.8686\n",
      "Processing batch 7542/11884 - Loss: 30.8062\n",
      "Processing batch 7543/11884 - Loss: 30.9600\n",
      "Processing batch 7544/11884 - Loss: 30.1250\n",
      "Processing batch 7545/11884 - Loss: 31.2733\n",
      "Processing batch 7546/11884 - Loss: 30.3242\n",
      "Processing batch 7547/11884 - Loss: 30.4568\n",
      "Processing batch 7548/11884 - Loss: 30.0746\n",
      "Processing batch 7549/11884 - Loss: 29.0005\n",
      "Processing batch 7550/11884 - Loss: 30.3325\n",
      "Processing batch 7551/11884 - Loss: 30.9320\n",
      "Processing batch 7552/11884 - Loss: 30.1015\n",
      "Processing batch 7553/11884 - Loss: 31.2166\n",
      "Processing batch 7554/11884 - Loss: 30.3060\n",
      "Processing batch 7555/11884 - Loss: 29.9170\n",
      "Processing batch 7556/11884 - Loss: 30.4633\n",
      "Processing batch 7557/11884 - Loss: 31.6297\n",
      "Processing batch 7558/11884 - Loss: 30.1309\n",
      "Processing batch 7559/11884 - Loss: 32.0013\n",
      "Processing batch 7560/11884 - Loss: 29.9313\n",
      "Processing batch 7561/11884 - Loss: 29.0700\n",
      "Processing batch 7562/11884 - Loss: 30.7208\n",
      "Processing batch 7563/11884 - Loss: 31.8656\n",
      "Processing batch 7564/11884 - Loss: 30.3161\n",
      "Processing batch 7565/11884 - Loss: 29.7798\n",
      "Processing batch 7566/11884 - Loss: 30.9840\n",
      "Processing batch 7567/11884 - Loss: 29.9029\n",
      "Processing batch 7568/11884 - Loss: 31.2305\n",
      "Processing batch 7569/11884 - Loss: 31.0351\n",
      "Processing batch 7570/11884 - Loss: 31.1890\n",
      "Processing batch 7571/11884 - Loss: 30.9150\n",
      "Processing batch 7572/11884 - Loss: 31.3386\n",
      "Processing batch 7573/11884 - Loss: 30.6105\n",
      "Processing batch 7574/11884 - Loss: 30.7383\n",
      "Processing batch 7575/11884 - Loss: 31.4183\n",
      "Processing batch 7576/11884 - Loss: 30.9015\n",
      "Processing batch 7577/11884 - Loss: 31.8568\n",
      "Processing batch 7578/11884 - Loss: 31.3138\n",
      "Processing batch 7579/11884 - Loss: 30.7681\n",
      "Processing batch 7580/11884 - Loss: 30.8068\n",
      "Processing batch 7581/11884 - Loss: 31.1673\n",
      "Processing batch 7582/11884 - Loss: 31.1679\n",
      "Processing batch 7583/11884 - Loss: 28.9297\n",
      "Processing batch 7584/11884 - Loss: 29.1827\n",
      "Processing batch 7585/11884 - Loss: 30.3471\n",
      "Processing batch 7586/11884 - Loss: 29.7674\n",
      "Processing batch 7587/11884 - Loss: 31.0167\n",
      "Processing batch 7588/11884 - Loss: 30.0347\n",
      "Processing batch 7589/11884 - Loss: 29.9456\n",
      "Processing batch 7590/11884 - Loss: 29.0656\n",
      "Processing batch 7591/11884 - Loss: 29.2589\n",
      "Processing batch 7592/11884 - Loss: 29.7317\n",
      "Processing batch 7593/11884 - Loss: 28.8433\n",
      "Processing batch 7594/11884 - Loss: 30.6159\n",
      "Processing batch 7595/11884 - Loss: 29.7273\n",
      "Processing batch 7596/11884 - Loss: 31.0034\n",
      "Processing batch 7597/11884 - Loss: 29.7759\n",
      "Processing batch 7598/11884 - Loss: 31.3169\n",
      "Processing batch 7599/11884 - Loss: 29.1377\n",
      "Processing batch 7600/11884 - Loss: 30.4651\n",
      "Processing batch 7601/11884 - Loss: 30.0059\n",
      "Processing batch 7602/11884 - Loss: 29.0285\n",
      "Processing batch 7603/11884 - Loss: 31.0630\n",
      "Processing batch 7604/11884 - Loss: 30.2611\n",
      "Processing batch 7605/11884 - Loss: 29.8479\n",
      "Processing batch 7606/11884 - Loss: 31.3736\n",
      "Processing batch 7607/11884 - Loss: 30.1102\n",
      "Processing batch 7608/11884 - Loss: 29.6025\n",
      "Processing batch 7609/11884 - Loss: 31.4743\n",
      "Processing batch 7610/11884 - Loss: 30.0807\n",
      "Processing batch 7611/11884 - Loss: 30.0389\n",
      "Processing batch 7612/11884 - Loss: 29.8678\n",
      "Processing batch 7613/11884 - Loss: 30.1219\n",
      "Processing batch 7614/11884 - Loss: 30.1503\n",
      "Processing batch 7615/11884 - Loss: 30.6397\n",
      "Processing batch 7616/11884 - Loss: 31.4813\n",
      "Processing batch 7617/11884 - Loss: 31.0679\n",
      "Processing batch 7618/11884 - Loss: 31.2594\n",
      "Processing batch 7619/11884 - Loss: 30.9538\n",
      "Processing batch 7620/11884 - Loss: 29.9944\n",
      "Processing batch 7621/11884 - Loss: 31.5995\n",
      "Processing batch 7622/11884 - Loss: 30.1275\n",
      "Processing batch 7623/11884 - Loss: 29.9815\n",
      "Processing batch 7624/11884 - Loss: 30.1427\n",
      "Processing batch 7625/11884 - Loss: 31.3098\n",
      "Processing batch 7626/11884 - Loss: 31.9938\n",
      "Processing batch 7627/11884 - Loss: 32.1347\n",
      "Processing batch 7628/11884 - Loss: 30.5335\n",
      "Processing batch 7629/11884 - Loss: 30.3192\n",
      "Processing batch 7630/11884 - Loss: 30.4581\n",
      "Processing batch 7631/11884 - Loss: 29.3792\n",
      "Processing batch 7632/11884 - Loss: 31.5624\n",
      "Processing batch 7633/11884 - Loss: 31.3074\n",
      "Processing batch 7634/11884 - Loss: 31.0072\n",
      "Processing batch 7635/11884 - Loss: 30.7406\n",
      "Processing batch 7636/11884 - Loss: 31.3398\n",
      "Processing batch 7637/11884 - Loss: 30.1825\n",
      "Processing batch 7638/11884 - Loss: 31.2151\n",
      "Processing batch 7639/11884 - Loss: 31.1224\n",
      "Processing batch 7640/11884 - Loss: 30.5185\n",
      "Processing batch 7641/11884 - Loss: 31.2985\n",
      "Processing batch 7642/11884 - Loss: 30.0590\n",
      "Processing batch 7643/11884 - Loss: 31.0902\n",
      "Processing batch 7644/11884 - Loss: 29.7270\n",
      "Processing batch 7645/11884 - Loss: 29.2436\n",
      "Processing batch 7646/11884 - Loss: 30.1145\n",
      "Processing batch 7647/11884 - Loss: 30.2981\n",
      "Processing batch 7648/11884 - Loss: 30.7338\n",
      "Processing batch 7649/11884 - Loss: 31.2804\n",
      "Processing batch 7650/11884 - Loss: 30.5084\n",
      "Processing batch 7651/11884 - Loss: 29.1303\n",
      "Processing batch 7652/11884 - Loss: 31.0241\n",
      "Processing batch 7653/11884 - Loss: 30.8532\n",
      "Processing batch 7654/11884 - Loss: 30.6344\n",
      "Processing batch 7655/11884 - Loss: 29.1312\n",
      "Processing batch 7656/11884 - Loss: 31.4605\n",
      "Processing batch 7657/11884 - Loss: 30.8541\n",
      "Processing batch 7658/11884 - Loss: 30.9640\n",
      "Processing batch 7659/11884 - Loss: 30.2151\n",
      "Processing batch 7660/11884 - Loss: 29.3000\n",
      "Processing batch 7661/11884 - Loss: 29.3630\n",
      "Processing batch 7662/11884 - Loss: 29.6948\n",
      "Processing batch 7663/11884 - Loss: 30.7475\n",
      "Processing batch 7664/11884 - Loss: 31.6699\n",
      "Processing batch 7665/11884 - Loss: 31.1300\n",
      "Processing batch 7666/11884 - Loss: 30.0546\n",
      "Processing batch 7667/11884 - Loss: 31.6854\n",
      "Processing batch 7668/11884 - Loss: 30.4080\n",
      "Processing batch 7669/11884 - Loss: 30.9299\n",
      "Processing batch 7670/11884 - Loss: 30.1733\n",
      "Processing batch 7671/11884 - Loss: 30.3527\n",
      "Processing batch 7672/11884 - Loss: 29.7182\n",
      "Processing batch 7673/11884 - Loss: 30.9993\n",
      "Processing batch 7674/11884 - Loss: 30.7894\n",
      "Processing batch 7675/11884 - Loss: 29.8896\n",
      "Processing batch 7676/11884 - Loss: 30.5371\n",
      "Processing batch 7677/11884 - Loss: 30.1832\n",
      "Processing batch 7678/11884 - Loss: 29.5683\n",
      "Processing batch 7679/11884 - Loss: 30.4595\n",
      "Processing batch 7680/11884 - Loss: 29.7843\n",
      "Processing batch 7681/11884 - Loss: 30.8591\n",
      "Processing batch 7682/11884 - Loss: 29.7513\n",
      "Processing batch 7683/11884 - Loss: 30.8700\n",
      "Processing batch 7684/11884 - Loss: 29.4562\n",
      "Processing batch 7685/11884 - Loss: 29.4838\n",
      "Processing batch 7686/11884 - Loss: 30.2871\n",
      "Processing batch 7687/11884 - Loss: 30.8732\n",
      "Processing batch 7688/11884 - Loss: 31.8198\n",
      "Processing batch 7689/11884 - Loss: 31.9881\n",
      "Processing batch 7690/11884 - Loss: 32.3503\n",
      "Processing batch 7691/11884 - Loss: 30.5880\n",
      "Processing batch 7692/11884 - Loss: 30.1432\n",
      "Processing batch 7693/11884 - Loss: 29.1076\n",
      "Processing batch 7694/11884 - Loss: 30.0956\n",
      "Processing batch 7695/11884 - Loss: 30.5320\n",
      "Processing batch 7696/11884 - Loss: 31.6337\n",
      "Processing batch 7697/11884 - Loss: 29.4434\n",
      "Processing batch 7698/11884 - Loss: 30.6227\n",
      "Processing batch 7699/11884 - Loss: 31.8427\n",
      "Processing batch 7700/11884 - Loss: 30.8180\n",
      "Processing batch 7701/11884 - Loss: 29.7830\n",
      "Processing batch 7702/11884 - Loss: 31.0949\n",
      "Processing batch 7703/11884 - Loss: 29.4601\n",
      "Processing batch 7704/11884 - Loss: 29.7113\n",
      "Processing batch 7705/11884 - Loss: 31.5081\n",
      "Processing batch 7706/11884 - Loss: 28.9875\n",
      "Processing batch 7707/11884 - Loss: 30.8471\n",
      "Processing batch 7708/11884 - Loss: 29.6093\n",
      "Processing batch 7709/11884 - Loss: 31.9951\n",
      "Processing batch 7710/11884 - Loss: 30.9140\n",
      "Processing batch 7711/11884 - Loss: 30.5748\n",
      "Processing batch 7712/11884 - Loss: 29.9615\n",
      "Processing batch 7713/11884 - Loss: 30.5307\n",
      "Processing batch 7714/11884 - Loss: 30.6500\n",
      "Processing batch 7715/11884 - Loss: 30.8060\n",
      "Processing batch 7716/11884 - Loss: 30.3018\n",
      "Processing batch 7717/11884 - Loss: 30.5627\n",
      "Processing batch 7718/11884 - Loss: 29.8753\n",
      "Processing batch 7719/11884 - Loss: 29.9243\n",
      "Processing batch 7720/11884 - Loss: 30.1923\n",
      "Processing batch 7721/11884 - Loss: 29.8664\n",
      "Processing batch 7722/11884 - Loss: 30.0418\n",
      "Processing batch 7723/11884 - Loss: 28.2354\n",
      "Processing batch 7724/11884 - Loss: 29.8424\n",
      "Processing batch 7725/11884 - Loss: 30.3976\n",
      "Processing batch 7726/11884 - Loss: 30.5284\n",
      "Processing batch 7727/11884 - Loss: 30.3568\n",
      "Processing batch 7728/11884 - Loss: 31.1419\n",
      "Processing batch 7729/11884 - Loss: 31.4317\n",
      "Processing batch 7730/11884 - Loss: 29.0898\n",
      "Processing batch 7731/11884 - Loss: 31.5077\n",
      "Processing batch 7732/11884 - Loss: 30.5947\n",
      "Processing batch 7733/11884 - Loss: 31.6103\n",
      "Processing batch 7734/11884 - Loss: 29.8750\n",
      "Processing batch 7735/11884 - Loss: 31.2895\n",
      "Processing batch 7736/11884 - Loss: 31.0421\n",
      "Processing batch 7737/11884 - Loss: 30.3984\n",
      "Processing batch 7738/11884 - Loss: 31.0189\n",
      "Processing batch 7739/11884 - Loss: 31.0615\n",
      "Processing batch 7740/11884 - Loss: 29.3346\n",
      "Processing batch 7741/11884 - Loss: 29.7703\n",
      "Processing batch 7742/11884 - Loss: 30.7839\n",
      "Processing batch 7743/11884 - Loss: 29.2032\n",
      "Processing batch 7744/11884 - Loss: 30.5859\n",
      "Processing batch 7745/11884 - Loss: 29.6717\n",
      "Processing batch 7746/11884 - Loss: 31.6545\n",
      "Processing batch 7747/11884 - Loss: 30.9593\n",
      "Processing batch 7748/11884 - Loss: 29.8954\n",
      "Processing batch 7749/11884 - Loss: 30.5312\n",
      "Processing batch 7750/11884 - Loss: 31.2936\n",
      "Processing batch 7751/11884 - Loss: 28.7357\n",
      "Processing batch 7752/11884 - Loss: 28.0683\n",
      "Processing batch 7753/11884 - Loss: 29.4105\n",
      "Processing batch 7754/11884 - Loss: 29.8848\n",
      "Processing batch 7755/11884 - Loss: 32.4230\n",
      "Processing batch 7756/11884 - Loss: 30.9114\n",
      "Processing batch 7757/11884 - Loss: 28.7070\n",
      "Processing batch 7758/11884 - Loss: 30.4165\n",
      "Processing batch 7759/11884 - Loss: 30.6153\n",
      "Processing batch 7760/11884 - Loss: 30.6367\n",
      "Processing batch 7761/11884 - Loss: 30.7023\n",
      "Processing batch 7762/11884 - Loss: 29.7312\n",
      "Processing batch 7763/11884 - Loss: 29.1346\n",
      "Processing batch 7764/11884 - Loss: 31.0963\n",
      "Processing batch 7765/11884 - Loss: 28.6110\n",
      "Processing batch 7766/11884 - Loss: 30.8350\n",
      "Processing batch 7767/11884 - Loss: 31.0222\n",
      "Processing batch 7768/11884 - Loss: 31.2165\n",
      "Processing batch 7769/11884 - Loss: 29.2795\n",
      "Processing batch 7770/11884 - Loss: 30.8429\n",
      "Processing batch 7771/11884 - Loss: 29.5253\n",
      "Processing batch 7772/11884 - Loss: 30.7148\n",
      "Processing batch 7773/11884 - Loss: 31.0776\n",
      "Processing batch 7774/11884 - Loss: 30.3898\n",
      "Processing batch 7775/11884 - Loss: 30.2746\n",
      "Processing batch 7776/11884 - Loss: 31.1495\n",
      "Processing batch 7777/11884 - Loss: 30.6721\n",
      "Processing batch 7778/11884 - Loss: 30.7718\n",
      "Processing batch 7779/11884 - Loss: 30.9631\n",
      "Processing batch 7780/11884 - Loss: 31.8920\n",
      "Processing batch 7781/11884 - Loss: 30.2251\n",
      "Processing batch 7782/11884 - Loss: 30.4677\n",
      "Processing batch 7783/11884 - Loss: 30.4452\n",
      "Processing batch 7784/11884 - Loss: 29.3975\n",
      "Processing batch 7785/11884 - Loss: 30.4982\n",
      "Processing batch 7786/11884 - Loss: 30.5458\n",
      "Processing batch 7787/11884 - Loss: 31.7058\n",
      "Processing batch 7788/11884 - Loss: 28.4995\n",
      "Processing batch 7789/11884 - Loss: 29.4466\n",
      "Processing batch 7790/11884 - Loss: 30.4750\n",
      "Processing batch 7791/11884 - Loss: 29.5923\n",
      "Processing batch 7792/11884 - Loss: 31.0113\n",
      "Processing batch 7793/11884 - Loss: 30.6674\n",
      "Processing batch 7794/11884 - Loss: 30.8778\n",
      "Processing batch 7795/11884 - Loss: 29.4216\n",
      "Processing batch 7796/11884 - Loss: 30.2402\n",
      "Processing batch 7797/11884 - Loss: 31.1224\n",
      "Processing batch 7798/11884 - Loss: 30.7871\n",
      "Processing batch 7799/11884 - Loss: 29.8252\n",
      "Processing batch 7800/11884 - Loss: 30.2615\n",
      "Processing batch 7801/11884 - Loss: 31.2514\n",
      "Processing batch 7802/11884 - Loss: 29.6005\n",
      "Processing batch 7803/11884 - Loss: 32.2648\n",
      "Processing batch 7804/11884 - Loss: 29.2476\n",
      "Processing batch 7805/11884 - Loss: 29.5054\n",
      "Processing batch 7806/11884 - Loss: 31.5389\n",
      "Processing batch 7807/11884 - Loss: 30.7695\n",
      "Processing batch 7808/11884 - Loss: 30.2297\n",
      "Processing batch 7809/11884 - Loss: 29.7226\n",
      "Processing batch 7810/11884 - Loss: 29.8949\n",
      "Processing batch 7811/11884 - Loss: 31.4739\n",
      "Processing batch 7812/11884 - Loss: 31.4184\n",
      "Processing batch 7813/11884 - Loss: 30.5549\n",
      "Processing batch 7814/11884 - Loss: 29.6399\n",
      "Processing batch 7815/11884 - Loss: 30.9051\n",
      "Processing batch 7816/11884 - Loss: 28.9434\n",
      "Processing batch 7817/11884 - Loss: 31.2249\n",
      "Processing batch 7818/11884 - Loss: 31.2801\n",
      "Processing batch 7819/11884 - Loss: 29.7978\n",
      "Processing batch 7820/11884 - Loss: 30.4745\n",
      "Processing batch 7821/11884 - Loss: 30.1835\n",
      "Processing batch 7822/11884 - Loss: 29.1406\n",
      "Processing batch 7823/11884 - Loss: 29.6295\n",
      "Processing batch 7824/11884 - Loss: 29.5121\n",
      "Processing batch 7825/11884 - Loss: 29.0476\n",
      "Processing batch 7826/11884 - Loss: 30.0487\n",
      "Processing batch 7827/11884 - Loss: 30.8083\n",
      "Processing batch 7828/11884 - Loss: 29.7331\n",
      "Processing batch 7829/11884 - Loss: 30.8073\n",
      "Processing batch 7830/11884 - Loss: 29.8600\n",
      "Processing batch 7831/11884 - Loss: 30.6591\n",
      "Processing batch 7832/11884 - Loss: 31.0969\n",
      "Processing batch 7833/11884 - Loss: 30.1436\n",
      "Processing batch 7834/11884 - Loss: 29.7182\n",
      "Processing batch 7835/11884 - Loss: 30.9383\n",
      "Processing batch 7836/11884 - Loss: 30.1104\n",
      "Processing batch 7837/11884 - Loss: 29.9161\n",
      "Processing batch 7838/11884 - Loss: 30.2312\n",
      "Processing batch 7839/11884 - Loss: 32.6691\n",
      "Processing batch 7840/11884 - Loss: 30.8211\n",
      "Processing batch 7841/11884 - Loss: 30.5061\n",
      "Processing batch 7842/11884 - Loss: 30.4880\n",
      "Processing batch 7843/11884 - Loss: 30.6710\n",
      "Processing batch 7844/11884 - Loss: 30.6861\n",
      "Processing batch 7845/11884 - Loss: 30.8474\n",
      "Processing batch 7846/11884 - Loss: 29.5609\n",
      "Processing batch 7847/11884 - Loss: 30.5005\n",
      "Processing batch 7848/11884 - Loss: 30.6052\n",
      "Processing batch 7849/11884 - Loss: 30.0633\n",
      "Processing batch 7850/11884 - Loss: 29.4108\n",
      "Processing batch 7851/11884 - Loss: 30.9155\n",
      "Processing batch 7852/11884 - Loss: 29.6817\n",
      "Processing batch 7853/11884 - Loss: 29.7857\n",
      "Processing batch 7854/11884 - Loss: 32.0211\n",
      "Processing batch 7855/11884 - Loss: 30.6348\n",
      "Processing batch 7856/11884 - Loss: 30.4946\n",
      "Processing batch 7857/11884 - Loss: 31.6743\n",
      "Processing batch 7858/11884 - Loss: 29.8790\n",
      "Processing batch 7859/11884 - Loss: 29.8567\n",
      "Processing batch 7860/11884 - Loss: 30.2914\n",
      "Processing batch 7861/11884 - Loss: 30.3819\n",
      "Processing batch 7862/11884 - Loss: 30.3373\n",
      "Processing batch 7863/11884 - Loss: 31.2986\n",
      "Processing batch 7864/11884 - Loss: 31.3153\n",
      "Processing batch 7865/11884 - Loss: 30.2235\n",
      "Processing batch 7866/11884 - Loss: 30.5185\n",
      "Processing batch 7867/11884 - Loss: 29.9195\n",
      "Processing batch 7868/11884 - Loss: 30.3421\n",
      "Processing batch 7869/11884 - Loss: 30.2732\n",
      "Processing batch 7870/11884 - Loss: 30.8861\n",
      "Processing batch 7871/11884 - Loss: 30.5789\n",
      "Processing batch 7872/11884 - Loss: 30.1701\n",
      "Processing batch 7873/11884 - Loss: 30.2890\n",
      "Processing batch 7874/11884 - Loss: 32.3518\n",
      "Processing batch 7875/11884 - Loss: 30.1340\n",
      "Processing batch 7876/11884 - Loss: 30.2359\n",
      "Processing batch 7877/11884 - Loss: 32.0717\n",
      "Processing batch 7878/11884 - Loss: 30.7398\n",
      "Processing batch 7879/11884 - Loss: 30.8637\n",
      "Processing batch 7880/11884 - Loss: 31.2422\n",
      "Processing batch 7881/11884 - Loss: 31.0455\n",
      "Processing batch 7882/11884 - Loss: 30.6816\n",
      "Processing batch 7883/11884 - Loss: 31.8458\n",
      "Processing batch 7884/11884 - Loss: 30.7306\n",
      "Processing batch 7885/11884 - Loss: 30.2179\n",
      "Processing batch 7886/11884 - Loss: 30.5959\n",
      "Processing batch 7887/11884 - Loss: 29.7406\n",
      "Processing batch 7888/11884 - Loss: 29.7550\n",
      "Processing batch 7889/11884 - Loss: 29.9277\n",
      "Processing batch 7890/11884 - Loss: 29.5049\n",
      "Processing batch 7891/11884 - Loss: 30.4661\n",
      "Processing batch 7892/11884 - Loss: 30.4325\n",
      "Processing batch 7893/11884 - Loss: 30.3028\n",
      "Processing batch 7894/11884 - Loss: 29.4053\n",
      "Processing batch 7895/11884 - Loss: 29.6348\n",
      "Processing batch 7896/11884 - Loss: 29.7293\n",
      "Processing batch 7897/11884 - Loss: 31.9275\n",
      "Processing batch 7898/11884 - Loss: 31.5771\n",
      "Processing batch 7899/11884 - Loss: 29.4277\n",
      "Processing batch 7900/11884 - Loss: 31.1205\n",
      "Processing batch 7901/11884 - Loss: 30.0221\n",
      "Processing batch 7902/11884 - Loss: 29.5175\n",
      "Processing batch 7903/11884 - Loss: 30.3696\n",
      "Processing batch 7904/11884 - Loss: 30.6534\n",
      "Processing batch 7905/11884 - Loss: 29.2868\n",
      "Processing batch 7906/11884 - Loss: 31.2231\n",
      "Processing batch 7907/11884 - Loss: 30.1108\n",
      "Processing batch 7908/11884 - Loss: 30.5276\n",
      "Processing batch 7909/11884 - Loss: 31.7933\n",
      "Processing batch 7910/11884 - Loss: 30.3731\n",
      "Processing batch 7911/11884 - Loss: 30.6323\n",
      "Processing batch 7912/11884 - Loss: 30.2454\n",
      "Processing batch 7913/11884 - Loss: 30.5467\n",
      "Processing batch 7914/11884 - Loss: 31.5965\n",
      "Processing batch 7915/11884 - Loss: 31.0075\n",
      "Processing batch 7916/11884 - Loss: 30.9161\n",
      "Processing batch 7917/11884 - Loss: 29.2364\n",
      "Processing batch 7918/11884 - Loss: 30.1022\n",
      "Processing batch 7919/11884 - Loss: 30.9001\n",
      "Processing batch 7920/11884 - Loss: 29.8868\n",
      "Processing batch 7921/11884 - Loss: 30.3738\n",
      "Processing batch 7922/11884 - Loss: 31.4164\n",
      "Processing batch 7923/11884 - Loss: 31.6460\n",
      "Processing batch 7924/11884 - Loss: 30.5330\n",
      "Processing batch 7925/11884 - Loss: 29.5416\n",
      "Processing batch 7926/11884 - Loss: 30.4297\n",
      "Processing batch 7927/11884 - Loss: 31.0076\n",
      "Processing batch 7928/11884 - Loss: 31.3543\n",
      "Processing batch 7929/11884 - Loss: 31.0399\n",
      "Processing batch 7930/11884 - Loss: 31.2563\n",
      "Processing batch 7931/11884 - Loss: 31.4317\n",
      "Processing batch 7932/11884 - Loss: 29.6173\n",
      "Processing batch 7933/11884 - Loss: 29.5107\n",
      "Processing batch 7934/11884 - Loss: 31.3933\n",
      "Processing batch 7935/11884 - Loss: 30.6800\n",
      "Processing batch 7936/11884 - Loss: 30.9128\n",
      "Processing batch 7937/11884 - Loss: 29.2055\n",
      "Processing batch 7938/11884 - Loss: 31.1596\n",
      "Processing batch 7939/11884 - Loss: 29.3133\n",
      "Processing batch 7940/11884 - Loss: 29.5841\n",
      "Processing batch 7941/11884 - Loss: 30.8171\n",
      "Processing batch 7942/11884 - Loss: 29.7458\n",
      "Processing batch 7943/11884 - Loss: 30.5215\n",
      "Processing batch 7944/11884 - Loss: 31.2178\n",
      "Processing batch 7945/11884 - Loss: 30.1763\n",
      "Processing batch 7946/11884 - Loss: 30.5721\n",
      "Processing batch 7947/11884 - Loss: 30.4953\n",
      "Processing batch 7948/11884 - Loss: 28.8620\n",
      "Processing batch 7949/11884 - Loss: 30.9794\n",
      "Processing batch 7950/11884 - Loss: 30.3232\n",
      "Processing batch 7951/11884 - Loss: 28.8928\n",
      "Processing batch 7952/11884 - Loss: 30.3925\n",
      "Processing batch 7953/11884 - Loss: 30.3510\n",
      "Processing batch 7954/11884 - Loss: 29.5727\n",
      "Processing batch 7955/11884 - Loss: 31.4446\n",
      "Processing batch 7956/11884 - Loss: 30.4741\n",
      "Processing batch 7957/11884 - Loss: 29.4523\n",
      "Processing batch 7958/11884 - Loss: 29.0962\n",
      "Processing batch 7959/11884 - Loss: 30.0960\n",
      "Processing batch 7960/11884 - Loss: 30.7042\n",
      "Processing batch 7961/11884 - Loss: 30.6179\n",
      "Processing batch 7962/11884 - Loss: 29.4618\n",
      "Processing batch 7963/11884 - Loss: 29.4125\n",
      "Processing batch 7964/11884 - Loss: 31.3366\n",
      "Processing batch 7965/11884 - Loss: 30.5771\n",
      "Processing batch 7966/11884 - Loss: 31.2775\n",
      "Processing batch 7967/11884 - Loss: 29.7005\n",
      "Processing batch 7968/11884 - Loss: 28.4820\n",
      "Processing batch 7969/11884 - Loss: 31.5898\n",
      "Processing batch 7970/11884 - Loss: 30.0435\n",
      "Processing batch 7971/11884 - Loss: 30.2960\n",
      "Processing batch 7972/11884 - Loss: 30.4913\n",
      "Processing batch 7973/11884 - Loss: 30.9651\n",
      "Processing batch 7974/11884 - Loss: 30.2037\n",
      "Processing batch 7975/11884 - Loss: 29.5928\n",
      "Processing batch 7976/11884 - Loss: 29.7196\n",
      "Processing batch 7977/11884 - Loss: 30.1236\n",
      "Processing batch 7978/11884 - Loss: 30.0005\n",
      "Processing batch 7979/11884 - Loss: 29.6330\n",
      "Processing batch 7980/11884 - Loss: 31.2085\n",
      "Processing batch 7981/11884 - Loss: 29.8301\n",
      "Processing batch 7982/11884 - Loss: 30.2028\n",
      "Processing batch 7983/11884 - Loss: 30.8395\n",
      "Processing batch 7984/11884 - Loss: 31.2074\n",
      "Processing batch 7985/11884 - Loss: 30.4892\n",
      "Processing batch 7986/11884 - Loss: 30.1645\n",
      "Processing batch 7987/11884 - Loss: 31.5198\n",
      "Processing batch 7988/11884 - Loss: 32.2956\n",
      "Processing batch 7989/11884 - Loss: 30.2335\n",
      "Processing batch 7990/11884 - Loss: 30.8066\n",
      "Processing batch 7991/11884 - Loss: 30.9873\n",
      "Processing batch 7992/11884 - Loss: 30.4746\n",
      "Processing batch 7993/11884 - Loss: 29.9533\n",
      "Processing batch 7994/11884 - Loss: 29.5624\n",
      "Processing batch 7995/11884 - Loss: 30.4377\n",
      "Processing batch 7996/11884 - Loss: 29.6805\n",
      "Processing batch 7997/11884 - Loss: 30.0689\n",
      "Processing batch 7998/11884 - Loss: 30.6578\n",
      "Processing batch 7999/11884 - Loss: 32.2185\n",
      "Processing batch 8000/11884 - Loss: 32.1801\n",
      "Processing batch 8001/11884 - Loss: 30.5913\n",
      "Processing batch 8002/11884 - Loss: 30.0566\n",
      "Processing batch 8003/11884 - Loss: 29.7119\n",
      "Processing batch 8004/11884 - Loss: 30.9955\n",
      "Processing batch 8005/11884 - Loss: 31.4471\n",
      "Processing batch 8006/11884 - Loss: 30.1599\n",
      "Processing batch 8007/11884 - Loss: 29.4874\n",
      "Processing batch 8008/11884 - Loss: 29.2073\n",
      "Processing batch 8009/11884 - Loss: 30.5313\n",
      "Processing batch 8010/11884 - Loss: 31.1925\n",
      "Processing batch 8011/11884 - Loss: 30.2082\n",
      "Processing batch 8012/11884 - Loss: 32.5916\n",
      "Processing batch 8013/11884 - Loss: 30.0095\n",
      "Processing batch 8014/11884 - Loss: 30.4270\n",
      "Processing batch 8015/11884 - Loss: 31.2821\n",
      "Processing batch 8016/11884 - Loss: 31.2838\n",
      "Processing batch 8017/11884 - Loss: 28.8575\n",
      "Processing batch 8018/11884 - Loss: 29.6665\n",
      "Processing batch 8019/11884 - Loss: 29.7762\n",
      "Processing batch 8020/11884 - Loss: 30.6149\n",
      "Processing batch 8021/11884 - Loss: 29.7377\n",
      "Processing batch 8022/11884 - Loss: 29.9352\n",
      "Processing batch 8023/11884 - Loss: 28.6391\n",
      "Processing batch 8024/11884 - Loss: 30.1197\n",
      "Processing batch 8025/11884 - Loss: 30.9083\n",
      "Processing batch 8026/11884 - Loss: 30.6413\n",
      "Processing batch 8027/11884 - Loss: 29.9867\n",
      "Processing batch 8028/11884 - Loss: 29.5251\n",
      "Processing batch 8029/11884 - Loss: 29.3609\n",
      "Processing batch 8030/11884 - Loss: 30.7032\n",
      "Processing batch 8031/11884 - Loss: 29.1592\n",
      "Processing batch 8032/11884 - Loss: 29.0584\n",
      "Processing batch 8033/11884 - Loss: 30.9135\n",
      "Processing batch 8034/11884 - Loss: 28.9039\n",
      "Processing batch 8035/11884 - Loss: 30.6665\n",
      "Processing batch 8036/11884 - Loss: 29.8755\n",
      "Processing batch 8037/11884 - Loss: 30.1649\n",
      "Processing batch 8038/11884 - Loss: 30.2969\n",
      "Processing batch 8039/11884 - Loss: 30.4319\n",
      "Processing batch 8040/11884 - Loss: 28.7693\n",
      "Processing batch 8041/11884 - Loss: 29.8031\n",
      "Processing batch 8042/11884 - Loss: 29.2096\n",
      "Processing batch 8043/11884 - Loss: 32.5042\n",
      "Processing batch 8044/11884 - Loss: 31.4257\n",
      "Processing batch 8045/11884 - Loss: 30.8797\n",
      "Processing batch 8046/11884 - Loss: 30.6629\n",
      "Processing batch 8047/11884 - Loss: 30.5859\n",
      "Processing batch 8048/11884 - Loss: 29.1057\n",
      "Processing batch 8049/11884 - Loss: 30.9454\n",
      "Processing batch 8050/11884 - Loss: 31.3514\n",
      "Processing batch 8051/11884 - Loss: 29.9143\n",
      "Processing batch 8052/11884 - Loss: 29.6048\n",
      "Processing batch 8053/11884 - Loss: 30.9599\n",
      "Processing batch 8054/11884 - Loss: 30.0118\n",
      "Processing batch 8055/11884 - Loss: 31.4695\n",
      "Processing batch 8056/11884 - Loss: 30.8795\n",
      "Processing batch 8057/11884 - Loss: 29.7391\n",
      "Processing batch 8058/11884 - Loss: 30.7948\n",
      "Processing batch 8059/11884 - Loss: 29.4924\n",
      "Processing batch 8060/11884 - Loss: 30.0123\n",
      "Processing batch 8061/11884 - Loss: 29.8743\n",
      "Processing batch 8062/11884 - Loss: 30.2273\n",
      "Processing batch 8063/11884 - Loss: 30.2554\n",
      "Processing batch 8064/11884 - Loss: 30.6888\n",
      "Processing batch 8065/11884 - Loss: 30.6614\n",
      "Processing batch 8066/11884 - Loss: 30.4209\n",
      "Processing batch 8067/11884 - Loss: 31.9792\n",
      "Processing batch 8068/11884 - Loss: 30.4928\n",
      "Processing batch 8069/11884 - Loss: 30.8835\n",
      "Processing batch 8070/11884 - Loss: 30.2468\n",
      "Processing batch 8071/11884 - Loss: 30.9914\n",
      "Processing batch 8072/11884 - Loss: 30.4459\n",
      "Processing batch 8073/11884 - Loss: 31.4093\n",
      "Processing batch 8074/11884 - Loss: 30.7550\n",
      "Processing batch 8075/11884 - Loss: 30.5672\n",
      "Processing batch 8076/11884 - Loss: 31.2662\n",
      "Processing batch 8077/11884 - Loss: 30.6142\n",
      "Processing batch 8078/11884 - Loss: 31.4308\n",
      "Processing batch 8079/11884 - Loss: 29.3333\n",
      "Processing batch 8080/11884 - Loss: 29.5392\n",
      "Processing batch 8081/11884 - Loss: 29.6122\n",
      "Processing batch 8082/11884 - Loss: 30.9881\n",
      "Processing batch 8083/11884 - Loss: 30.2707\n",
      "Processing batch 8084/11884 - Loss: 29.9834\n",
      "Processing batch 8085/11884 - Loss: 29.3010\n",
      "Processing batch 8086/11884 - Loss: 29.2862\n",
      "Processing batch 8087/11884 - Loss: 31.2178\n",
      "Processing batch 8088/11884 - Loss: 30.7478\n",
      "Processing batch 8089/11884 - Loss: 32.1796\n",
      "Processing batch 8090/11884 - Loss: 29.6470\n",
      "Processing batch 8091/11884 - Loss: 30.7630\n",
      "Processing batch 8092/11884 - Loss: 30.4583\n",
      "Processing batch 8093/11884 - Loss: 29.5147\n",
      "Processing batch 8094/11884 - Loss: 29.9897\n",
      "Processing batch 8095/11884 - Loss: 30.3069\n",
      "Processing batch 8096/11884 - Loss: 29.4842\n",
      "Processing batch 8097/11884 - Loss: 31.1001\n",
      "Processing batch 8098/11884 - Loss: 29.4767\n",
      "Processing batch 8099/11884 - Loss: 31.1585\n",
      "Processing batch 8100/11884 - Loss: 30.6901\n",
      "Processing batch 8101/11884 - Loss: 30.7755\n",
      "Processing batch 8102/11884 - Loss: 32.0877\n",
      "Processing batch 8103/11884 - Loss: 30.2799\n",
      "Processing batch 8104/11884 - Loss: 30.6150\n",
      "Processing batch 8105/11884 - Loss: 30.2654\n",
      "Processing batch 8106/11884 - Loss: 31.6774\n",
      "Processing batch 8107/11884 - Loss: 31.0350\n",
      "Processing batch 8108/11884 - Loss: 30.4862\n",
      "Processing batch 8109/11884 - Loss: 30.9335\n",
      "Processing batch 8110/11884 - Loss: 29.9157\n",
      "Processing batch 8111/11884 - Loss: 30.8316\n",
      "Processing batch 8112/11884 - Loss: 30.3202\n",
      "Processing batch 8113/11884 - Loss: 31.0181\n",
      "Processing batch 8114/11884 - Loss: 29.0723\n",
      "Processing batch 8115/11884 - Loss: 30.7723\n",
      "Processing batch 8116/11884 - Loss: 30.4651\n",
      "Processing batch 8117/11884 - Loss: 29.9330\n",
      "Processing batch 8118/11884 - Loss: 29.5116\n",
      "Processing batch 8119/11884 - Loss: 30.3053\n",
      "Processing batch 8120/11884 - Loss: 29.4556\n",
      "Processing batch 8121/11884 - Loss: 30.0593\n",
      "Processing batch 8122/11884 - Loss: 30.2971\n",
      "Processing batch 8123/11884 - Loss: 29.9038\n",
      "Processing batch 8124/11884 - Loss: 29.3306\n",
      "Processing batch 8125/11884 - Loss: 30.6800\n",
      "Processing batch 8126/11884 - Loss: 29.9344\n",
      "Processing batch 8127/11884 - Loss: 30.5272\n",
      "Processing batch 8128/11884 - Loss: 31.5798\n",
      "Processing batch 8129/11884 - Loss: 30.7399\n",
      "Processing batch 8130/11884 - Loss: 30.0199\n",
      "Processing batch 8131/11884 - Loss: 30.2404\n",
      "Processing batch 8132/11884 - Loss: 30.7834\n",
      "Processing batch 8133/11884 - Loss: 29.4909\n",
      "Processing batch 8134/11884 - Loss: 32.1867\n",
      "Processing batch 8135/11884 - Loss: 31.6133\n",
      "Processing batch 8136/11884 - Loss: 30.2590\n",
      "Processing batch 8137/11884 - Loss: 30.0197\n",
      "Processing batch 8138/11884 - Loss: 29.6296\n",
      "Processing batch 8139/11884 - Loss: 29.2504\n",
      "Processing batch 8140/11884 - Loss: 30.8233\n",
      "Processing batch 8141/11884 - Loss: 30.7653\n",
      "Processing batch 8142/11884 - Loss: 29.9351\n",
      "Processing batch 8143/11884 - Loss: 31.2656\n",
      "Processing batch 8144/11884 - Loss: 31.6824\n",
      "Processing batch 8145/11884 - Loss: 30.8032\n",
      "Processing batch 8146/11884 - Loss: 29.5447\n",
      "Processing batch 8147/11884 - Loss: 29.8251\n",
      "Processing batch 8148/11884 - Loss: 30.8735\n",
      "Processing batch 8149/11884 - Loss: 29.8187\n",
      "Processing batch 8150/11884 - Loss: 30.7662\n",
      "Processing batch 8151/11884 - Loss: 30.2781\n",
      "Processing batch 8152/11884 - Loss: 30.6670\n",
      "Processing batch 8153/11884 - Loss: 29.0897\n",
      "Processing batch 8154/11884 - Loss: 32.0205\n",
      "Processing batch 8155/11884 - Loss: 30.2646\n",
      "Processing batch 8156/11884 - Loss: 29.9455\n",
      "Processing batch 8157/11884 - Loss: 29.6394\n",
      "Processing batch 8158/11884 - Loss: 29.3716\n",
      "Processing batch 8159/11884 - Loss: 30.9041\n",
      "Processing batch 8160/11884 - Loss: 32.1072\n",
      "Processing batch 8161/11884 - Loss: 29.1843\n",
      "Processing batch 8162/11884 - Loss: 30.5077\n",
      "Processing batch 8163/11884 - Loss: 30.7748\n",
      "Processing batch 8164/11884 - Loss: 30.7000\n",
      "Processing batch 8165/11884 - Loss: 31.4813\n",
      "Processing batch 8166/11884 - Loss: 30.3888\n",
      "Processing batch 8167/11884 - Loss: 29.7546\n",
      "Processing batch 8168/11884 - Loss: 31.0914\n",
      "Processing batch 8169/11884 - Loss: 29.9801\n",
      "Processing batch 8170/11884 - Loss: 31.4500\n",
      "Processing batch 8171/11884 - Loss: 31.2970\n",
      "Processing batch 8172/11884 - Loss: 30.2270\n",
      "Processing batch 8173/11884 - Loss: 30.8529\n",
      "Processing batch 8174/11884 - Loss: 29.8917\n",
      "Processing batch 8175/11884 - Loss: 29.9272\n",
      "Processing batch 8176/11884 - Loss: 29.5131\n",
      "Processing batch 8177/11884 - Loss: 29.4702\n",
      "Processing batch 8178/11884 - Loss: 29.3479\n",
      "Processing batch 8179/11884 - Loss: 29.2072\n",
      "Processing batch 8180/11884 - Loss: 30.3537\n",
      "Processing batch 8181/11884 - Loss: 30.5664\n",
      "Processing batch 8182/11884 - Loss: 31.0171\n",
      "Processing batch 8183/11884 - Loss: 30.2455\n",
      "Processing batch 8184/11884 - Loss: 29.7562\n",
      "Processing batch 8185/11884 - Loss: 29.6486\n",
      "Processing batch 8186/11884 - Loss: 29.3359\n",
      "Processing batch 8187/11884 - Loss: 30.3259\n",
      "Processing batch 8188/11884 - Loss: 31.3295\n",
      "Processing batch 8189/11884 - Loss: 30.5088\n",
      "Processing batch 8190/11884 - Loss: 30.1862\n",
      "Processing batch 8191/11884 - Loss: 30.6723\n",
      "Processing batch 8192/11884 - Loss: 30.4894\n",
      "Processing batch 8193/11884 - Loss: 30.7462\n",
      "Processing batch 8194/11884 - Loss: 30.9957\n",
      "Processing batch 8195/11884 - Loss: 30.0390\n",
      "Processing batch 8196/11884 - Loss: 31.2103\n",
      "Processing batch 8197/11884 - Loss: 29.6302\n",
      "Processing batch 8198/11884 - Loss: 31.5864\n",
      "Processing batch 8199/11884 - Loss: 30.4760\n",
      "Processing batch 8200/11884 - Loss: 31.5078\n",
      "Processing batch 8201/11884 - Loss: 29.8345\n",
      "Processing batch 8202/11884 - Loss: 29.7922\n",
      "Processing batch 8203/11884 - Loss: 30.9870\n",
      "Processing batch 8204/11884 - Loss: 30.0007\n",
      "Processing batch 8205/11884 - Loss: 31.2580\n",
      "Processing batch 8206/11884 - Loss: 29.7878\n",
      "Processing batch 8207/11884 - Loss: 29.9367\n",
      "Processing batch 8208/11884 - Loss: 29.8516\n",
      "Processing batch 8209/11884 - Loss: 30.6990\n",
      "Processing batch 8210/11884 - Loss: 28.4566\n",
      "Processing batch 8211/11884 - Loss: 30.8500\n",
      "Processing batch 8212/11884 - Loss: 29.3103\n",
      "Processing batch 8213/11884 - Loss: 30.2037\n",
      "Processing batch 8214/11884 - Loss: 29.9233\n",
      "Processing batch 8215/11884 - Loss: 30.4165\n",
      "Processing batch 8216/11884 - Loss: 30.0052\n",
      "Processing batch 8217/11884 - Loss: 32.2190\n",
      "Processing batch 8218/11884 - Loss: 29.8421\n",
      "Processing batch 8219/11884 - Loss: 31.0885\n",
      "Processing batch 8220/11884 - Loss: 30.9850\n",
      "Processing batch 8221/11884 - Loss: 30.9756\n",
      "Processing batch 8222/11884 - Loss: 31.4856\n",
      "Processing batch 8223/11884 - Loss: 30.3417\n",
      "Processing batch 8224/11884 - Loss: 29.9861\n",
      "Processing batch 8225/11884 - Loss: 30.8687\n",
      "Processing batch 8226/11884 - Loss: 31.0570\n",
      "Processing batch 8227/11884 - Loss: 30.4500\n",
      "Processing batch 8228/11884 - Loss: 29.9846\n",
      "Processing batch 8229/11884 - Loss: 30.5959\n",
      "Processing batch 8230/11884 - Loss: 30.3905\n",
      "Processing batch 8231/11884 - Loss: 30.7734\n",
      "Processing batch 8232/11884 - Loss: 28.8343\n",
      "Processing batch 8233/11884 - Loss: 30.5049\n",
      "Processing batch 8234/11884 - Loss: 30.0820\n",
      "Processing batch 8235/11884 - Loss: 30.8899\n",
      "Processing batch 8236/11884 - Loss: 29.6200\n",
      "Processing batch 8237/11884 - Loss: 30.3548\n",
      "Processing batch 8238/11884 - Loss: 30.4285\n",
      "Processing batch 8239/11884 - Loss: 29.9709\n",
      "Processing batch 8240/11884 - Loss: 31.8299\n",
      "Processing batch 8241/11884 - Loss: 29.5990\n",
      "Processing batch 8242/11884 - Loss: 29.7290\n",
      "Processing batch 8243/11884 - Loss: 30.6879\n",
      "Processing batch 8244/11884 - Loss: 31.5270\n",
      "Processing batch 8245/11884 - Loss: 31.7714\n",
      "Processing batch 8246/11884 - Loss: 30.8900\n",
      "Processing batch 8247/11884 - Loss: 28.9910\n",
      "Processing batch 8248/11884 - Loss: 29.6319\n",
      "Processing batch 8249/11884 - Loss: 28.5333\n",
      "Processing batch 8250/11884 - Loss: 30.4142\n",
      "Processing batch 8251/11884 - Loss: 30.5147\n",
      "Processing batch 8252/11884 - Loss: 30.3421\n",
      "Processing batch 8253/11884 - Loss: 31.3299\n",
      "Processing batch 8254/11884 - Loss: 31.2817\n",
      "Processing batch 8255/11884 - Loss: 29.7120\n",
      "Processing batch 8256/11884 - Loss: 29.8241\n",
      "Processing batch 8257/11884 - Loss: 31.3205\n",
      "Processing batch 8258/11884 - Loss: 30.4410\n",
      "Processing batch 8259/11884 - Loss: 30.3862\n",
      "Processing batch 8260/11884 - Loss: 29.4950\n",
      "Processing batch 8261/11884 - Loss: 30.2977\n",
      "Processing batch 8262/11884 - Loss: 30.5565\n",
      "Processing batch 8263/11884 - Loss: 30.6306\n",
      "Processing batch 8264/11884 - Loss: 32.1678\n",
      "Processing batch 8265/11884 - Loss: 30.7163\n",
      "Processing batch 8266/11884 - Loss: 30.6527\n",
      "Processing batch 8267/11884 - Loss: 30.7664\n",
      "Processing batch 8268/11884 - Loss: 29.8699\n",
      "Processing batch 8269/11884 - Loss: 30.4475\n",
      "Processing batch 8270/11884 - Loss: 29.5268\n",
      "Processing batch 8271/11884 - Loss: 30.7324\n",
      "Processing batch 8272/11884 - Loss: 30.3465\n",
      "Processing batch 8273/11884 - Loss: 31.1088\n",
      "Processing batch 8274/11884 - Loss: 30.5555\n",
      "Processing batch 8275/11884 - Loss: 30.6873\n",
      "Processing batch 8276/11884 - Loss: 30.0083\n",
      "Processing batch 8277/11884 - Loss: 31.1202\n",
      "Processing batch 8278/11884 - Loss: 30.8170\n",
      "Processing batch 8279/11884 - Loss: 31.1259\n",
      "Processing batch 8280/11884 - Loss: 29.4896\n",
      "Processing batch 8281/11884 - Loss: 31.0576\n",
      "Processing batch 8282/11884 - Loss: 30.6595\n",
      "Processing batch 8283/11884 - Loss: 29.8036\n",
      "Processing batch 8284/11884 - Loss: 29.0986\n",
      "Processing batch 8285/11884 - Loss: 31.1442\n",
      "Processing batch 8286/11884 - Loss: 30.1422\n",
      "Processing batch 8287/11884 - Loss: 30.6625\n",
      "Processing batch 8288/11884 - Loss: 30.5723\n",
      "Processing batch 8289/11884 - Loss: 31.0399\n",
      "Processing batch 8290/11884 - Loss: 29.9397\n",
      "Processing batch 8291/11884 - Loss: 29.3606\n",
      "Processing batch 8292/11884 - Loss: 30.4856\n",
      "Processing batch 8293/11884 - Loss: 29.2072\n",
      "Processing batch 8294/11884 - Loss: 30.5348\n",
      "Processing batch 8295/11884 - Loss: 31.0675\n",
      "Processing batch 8296/11884 - Loss: 29.6822\n",
      "Processing batch 8297/11884 - Loss: 30.6537\n",
      "Processing batch 8298/11884 - Loss: 31.7014\n",
      "Processing batch 8299/11884 - Loss: 31.3777\n",
      "Processing batch 8300/11884 - Loss: 30.8034\n",
      "Processing batch 8301/11884 - Loss: 31.4566\n",
      "Processing batch 8302/11884 - Loss: 31.5968\n",
      "Processing batch 8303/11884 - Loss: 30.4377\n",
      "Processing batch 8304/11884 - Loss: 30.0482\n",
      "Processing batch 8305/11884 - Loss: 29.6817\n",
      "Processing batch 8306/11884 - Loss: 31.0501\n",
      "Processing batch 8307/11884 - Loss: 31.8676\n",
      "Processing batch 8308/11884 - Loss: 31.0697\n",
      "Processing batch 8309/11884 - Loss: 30.7274\n",
      "Processing batch 8310/11884 - Loss: 29.9006\n",
      "Processing batch 8311/11884 - Loss: 30.9044\n",
      "Processing batch 8312/11884 - Loss: 30.8567\n",
      "Processing batch 8313/11884 - Loss: 31.2752\n",
      "Processing batch 8314/11884 - Loss: 29.4350\n",
      "Processing batch 8315/11884 - Loss: 31.1612\n",
      "Processing batch 8316/11884 - Loss: 29.4318\n",
      "Processing batch 8317/11884 - Loss: 30.1237\n",
      "Processing batch 8318/11884 - Loss: 29.2193\n",
      "Processing batch 8319/11884 - Loss: 31.1886\n",
      "Processing batch 8320/11884 - Loss: 29.8769\n",
      "Processing batch 8321/11884 - Loss: 30.6836\n",
      "Processing batch 8322/11884 - Loss: 30.7169\n",
      "Processing batch 8323/11884 - Loss: 29.1985\n",
      "Processing batch 8324/11884 - Loss: 31.1314\n",
      "Processing batch 8325/11884 - Loss: 30.1852\n",
      "Processing batch 8326/11884 - Loss: 29.9725\n",
      "Processing batch 8327/11884 - Loss: 30.7223\n",
      "Processing batch 8328/11884 - Loss: 29.6040\n",
      "Processing batch 8329/11884 - Loss: 30.6655\n",
      "Processing batch 8330/11884 - Loss: 29.8553\n",
      "Processing batch 8331/11884 - Loss: 31.7026\n",
      "Processing batch 8332/11884 - Loss: 30.5710\n",
      "Processing batch 8333/11884 - Loss: 30.4998\n",
      "Processing batch 8334/11884 - Loss: 29.6165\n",
      "Processing batch 8335/11884 - Loss: 29.2858\n",
      "Processing batch 8336/11884 - Loss: 31.0137\n",
      "Processing batch 8337/11884 - Loss: 30.8895\n",
      "Processing batch 8338/11884 - Loss: 30.4687\n",
      "Processing batch 8339/11884 - Loss: 29.8962\n",
      "Processing batch 8340/11884 - Loss: 29.6890\n",
      "Processing batch 8341/11884 - Loss: 30.1454\n",
      "Processing batch 8342/11884 - Loss: 30.4773\n",
      "Processing batch 8343/11884 - Loss: 29.8853\n",
      "Processing batch 8344/11884 - Loss: 30.2913\n",
      "Processing batch 8345/11884 - Loss: 29.8409\n",
      "Processing batch 8346/11884 - Loss: 29.9390\n",
      "Processing batch 8347/11884 - Loss: 30.0539\n",
      "Processing batch 8348/11884 - Loss: 30.3059\n",
      "Processing batch 8349/11884 - Loss: 30.4116\n",
      "Processing batch 8350/11884 - Loss: 31.0242\n",
      "Processing batch 8351/11884 - Loss: 30.9574\n",
      "Processing batch 8352/11884 - Loss: 29.4984\n",
      "Processing batch 8353/11884 - Loss: 30.8496\n",
      "Processing batch 8354/11884 - Loss: 30.6627\n",
      "Processing batch 8355/11884 - Loss: 29.7875\n",
      "Processing batch 8356/11884 - Loss: 29.7519\n",
      "Processing batch 8357/11884 - Loss: 30.2608\n",
      "Processing batch 8358/11884 - Loss: 30.1638\n",
      "Processing batch 8359/11884 - Loss: 29.7905\n",
      "Processing batch 8360/11884 - Loss: 29.2695\n",
      "Processing batch 8361/11884 - Loss: 30.4720\n",
      "Processing batch 8362/11884 - Loss: 31.8029\n",
      "Processing batch 8363/11884 - Loss: 30.1457\n",
      "Processing batch 8364/11884 - Loss: 30.2869\n",
      "Processing batch 8365/11884 - Loss: 31.1722\n",
      "Processing batch 8366/11884 - Loss: 30.5427\n",
      "Processing batch 8367/11884 - Loss: 30.0458\n",
      "Processing batch 8368/11884 - Loss: 31.1646\n",
      "Processing batch 8369/11884 - Loss: 30.2436\n",
      "Processing batch 8370/11884 - Loss: 30.2797\n",
      "Processing batch 8371/11884 - Loss: 30.0463\n",
      "Processing batch 8372/11884 - Loss: 30.7319\n",
      "Processing batch 8373/11884 - Loss: 31.2307\n",
      "Processing batch 8374/11884 - Loss: 30.3482\n",
      "Processing batch 8375/11884 - Loss: 30.9657\n",
      "Processing batch 8376/11884 - Loss: 31.5529\n",
      "Processing batch 8377/11884 - Loss: 29.8212\n",
      "Processing batch 8378/11884 - Loss: 30.5953\n",
      "Processing batch 8379/11884 - Loss: 30.9074\n",
      "Processing batch 8380/11884 - Loss: 30.6657\n",
      "Processing batch 8381/11884 - Loss: 30.1565\n",
      "Processing batch 8382/11884 - Loss: 30.7709\n",
      "Processing batch 8383/11884 - Loss: 30.3570\n",
      "Processing batch 8384/11884 - Loss: 31.4370\n",
      "Processing batch 8385/11884 - Loss: 30.2887\n",
      "Processing batch 8386/11884 - Loss: 29.7532\n",
      "Processing batch 8387/11884 - Loss: 30.4864\n",
      "Processing batch 8388/11884 - Loss: 29.1087\n",
      "Processing batch 8389/11884 - Loss: 29.6946\n",
      "Processing batch 8390/11884 - Loss: 29.3784\n",
      "Processing batch 8391/11884 - Loss: 30.8184\n",
      "Processing batch 8392/11884 - Loss: 30.0590\n",
      "Processing batch 8393/11884 - Loss: 30.7653\n",
      "Processing batch 8394/11884 - Loss: 30.9561\n",
      "Processing batch 8395/11884 - Loss: 30.8157\n",
      "Processing batch 8396/11884 - Loss: 30.2955\n",
      "Processing batch 8397/11884 - Loss: 29.9913\n",
      "Processing batch 8398/11884 - Loss: 30.5235\n",
      "Processing batch 8399/11884 - Loss: 30.6996\n",
      "Processing batch 8400/11884 - Loss: 31.0635\n",
      "Processing batch 8401/11884 - Loss: 31.3363\n",
      "Processing batch 8402/11884 - Loss: 31.5174\n",
      "Processing batch 8403/11884 - Loss: 29.7821\n",
      "Processing batch 8404/11884 - Loss: 30.4455\n",
      "Processing batch 8405/11884 - Loss: 30.6194\n",
      "Processing batch 8406/11884 - Loss: 30.5468\n",
      "Processing batch 8407/11884 - Loss: 29.1740\n",
      "Processing batch 8408/11884 - Loss: 30.7780\n",
      "Processing batch 8409/11884 - Loss: 31.4740\n",
      "Processing batch 8410/11884 - Loss: 30.5863\n",
      "Processing batch 8411/11884 - Loss: 30.0134\n",
      "Processing batch 8412/11884 - Loss: 30.5084\n",
      "Processing batch 8413/11884 - Loss: 30.2536\n",
      "Processing batch 8414/11884 - Loss: 30.9693\n",
      "Processing batch 8415/11884 - Loss: 30.7745\n",
      "Processing batch 8416/11884 - Loss: 30.9644\n",
      "Processing batch 8417/11884 - Loss: 31.5887\n",
      "Processing batch 8418/11884 - Loss: 28.2862\n",
      "Processing batch 8419/11884 - Loss: 29.9850\n",
      "Processing batch 8420/11884 - Loss: 30.1542\n",
      "Processing batch 8421/11884 - Loss: 30.0566\n",
      "Processing batch 8422/11884 - Loss: 29.8639\n",
      "Processing batch 8423/11884 - Loss: 31.1628\n",
      "Processing batch 8424/11884 - Loss: 30.4964\n",
      "Processing batch 8425/11884 - Loss: 30.4180\n",
      "Processing batch 8426/11884 - Loss: 31.1124\n",
      "Processing batch 8427/11884 - Loss: 31.1612\n",
      "Processing batch 8428/11884 - Loss: 30.6420\n",
      "Processing batch 8429/11884 - Loss: 29.4894\n",
      "Processing batch 8430/11884 - Loss: 31.5788\n",
      "Processing batch 8431/11884 - Loss: 29.9811\n",
      "Processing batch 8432/11884 - Loss: 30.2610\n",
      "Processing batch 8433/11884 - Loss: 30.6863\n",
      "Processing batch 8434/11884 - Loss: 30.9428\n",
      "Processing batch 8435/11884 - Loss: 29.8816\n",
      "Processing batch 8436/11884 - Loss: 30.8587\n",
      "Processing batch 8437/11884 - Loss: 30.3082\n",
      "Processing batch 8438/11884 - Loss: 29.8850\n",
      "Processing batch 8439/11884 - Loss: 31.6123\n",
      "Processing batch 8440/11884 - Loss: 29.6399\n",
      "Processing batch 8441/11884 - Loss: 30.8901\n",
      "Processing batch 8442/11884 - Loss: 30.1962\n",
      "Processing batch 8443/11884 - Loss: 30.1199\n",
      "Processing batch 8444/11884 - Loss: 30.3778\n",
      "Processing batch 8445/11884 - Loss: 30.5598\n",
      "Processing batch 8446/11884 - Loss: 28.9463\n",
      "Processing batch 8447/11884 - Loss: 30.2911\n",
      "Processing batch 8448/11884 - Loss: 30.3775\n",
      "Processing batch 8449/11884 - Loss: 30.2281\n",
      "Processing batch 8450/11884 - Loss: 30.8327\n",
      "Processing batch 8451/11884 - Loss: 29.6182\n",
      "Processing batch 8452/11884 - Loss: 31.4257\n",
      "Processing batch 8453/11884 - Loss: 30.0816\n",
      "Processing batch 8454/11884 - Loss: 31.6842\n",
      "Processing batch 8455/11884 - Loss: 29.8715\n",
      "Processing batch 8456/11884 - Loss: 30.0432\n",
      "Processing batch 8457/11884 - Loss: 29.3439\n",
      "Processing batch 8458/11884 - Loss: 30.1361\n",
      "Processing batch 8459/11884 - Loss: 29.5556\n",
      "Processing batch 8460/11884 - Loss: 30.8329\n",
      "Processing batch 8461/11884 - Loss: 29.2719\n",
      "Processing batch 8462/11884 - Loss: 30.4408\n",
      "Processing batch 8463/11884 - Loss: 29.8634\n",
      "Processing batch 8464/11884 - Loss: 31.3360\n",
      "Processing batch 8465/11884 - Loss: 30.0933\n",
      "Processing batch 8466/11884 - Loss: 30.1096\n",
      "Processing batch 8467/11884 - Loss: 29.4292\n",
      "Processing batch 8468/11884 - Loss: 30.3504\n",
      "Processing batch 8469/11884 - Loss: 30.7694\n",
      "Processing batch 8470/11884 - Loss: 30.2998\n",
      "Processing batch 8471/11884 - Loss: 30.7247\n",
      "Processing batch 8472/11884 - Loss: 30.1530\n",
      "Processing batch 8473/11884 - Loss: 30.9801\n",
      "Processing batch 8474/11884 - Loss: 30.9393\n",
      "Processing batch 8475/11884 - Loss: 29.7587\n",
      "Processing batch 8476/11884 - Loss: 29.9132\n",
      "Processing batch 8477/11884 - Loss: 30.8175\n",
      "Processing batch 8478/11884 - Loss: 30.7925\n",
      "Processing batch 8479/11884 - Loss: 31.2907\n",
      "Processing batch 8480/11884 - Loss: 29.2037\n",
      "Processing batch 8481/11884 - Loss: 30.7558\n",
      "Processing batch 8482/11884 - Loss: 29.4934\n",
      "Processing batch 8483/11884 - Loss: 30.0070\n",
      "Processing batch 8484/11884 - Loss: 29.9404\n",
      "Processing batch 8485/11884 - Loss: 31.4910\n",
      "Processing batch 8486/11884 - Loss: 29.2132\n",
      "Processing batch 8487/11884 - Loss: 29.6510\n",
      "Processing batch 8488/11884 - Loss: 31.2741\n",
      "Processing batch 8489/11884 - Loss: 30.2290\n",
      "Processing batch 8490/11884 - Loss: 29.6270\n",
      "Processing batch 8491/11884 - Loss: 29.5013\n",
      "Processing batch 8492/11884 - Loss: 30.4333\n",
      "Processing batch 8493/11884 - Loss: 31.3650\n",
      "Processing batch 8494/11884 - Loss: 30.4005\n",
      "Processing batch 8495/11884 - Loss: 31.8569\n",
      "Processing batch 8496/11884 - Loss: 29.8860\n",
      "Processing batch 8497/11884 - Loss: 31.3533\n",
      "Processing batch 8498/11884 - Loss: 31.5586\n",
      "Processing batch 8499/11884 - Loss: 30.1008\n",
      "Processing batch 8500/11884 - Loss: 31.0371\n",
      "Processing batch 8501/11884 - Loss: 30.4921\n",
      "Processing batch 8502/11884 - Loss: 30.2965\n",
      "Processing batch 8503/11884 - Loss: 30.3530\n",
      "Processing batch 8504/11884 - Loss: 31.0532\n",
      "Processing batch 8505/11884 - Loss: 29.7924\n",
      "Processing batch 8506/11884 - Loss: 30.6177\n",
      "Processing batch 8507/11884 - Loss: 29.9021\n",
      "Processing batch 8508/11884 - Loss: 30.4230\n",
      "Processing batch 8509/11884 - Loss: 30.7081\n",
      "Processing batch 8510/11884 - Loss: 29.9329\n",
      "Processing batch 8511/11884 - Loss: 30.3709\n",
      "Processing batch 8512/11884 - Loss: 31.6733\n",
      "Processing batch 8513/11884 - Loss: 30.4138\n",
      "Processing batch 8514/11884 - Loss: 28.7640\n",
      "Processing batch 8515/11884 - Loss: 31.8519\n",
      "Processing batch 8516/11884 - Loss: 30.2405\n",
      "Processing batch 8517/11884 - Loss: 30.1419\n",
      "Processing batch 8518/11884 - Loss: 31.3553\n",
      "Processing batch 8519/11884 - Loss: 31.3937\n",
      "Processing batch 8520/11884 - Loss: 31.3722\n",
      "Processing batch 8521/11884 - Loss: 31.1274\n",
      "Processing batch 8522/11884 - Loss: 30.3642\n",
      "Processing batch 8523/11884 - Loss: 30.3561\n",
      "Processing batch 8524/11884 - Loss: 29.6481\n",
      "Processing batch 8525/11884 - Loss: 29.6634\n",
      "Processing batch 8526/11884 - Loss: 31.8386\n",
      "Processing batch 8527/11884 - Loss: 29.8639\n",
      "Processing batch 8528/11884 - Loss: 32.3146\n",
      "Processing batch 8529/11884 - Loss: 29.7551\n",
      "Processing batch 8530/11884 - Loss: 29.3463\n",
      "Processing batch 8531/11884 - Loss: 31.0037\n",
      "Processing batch 8532/11884 - Loss: 30.6970\n",
      "Processing batch 8533/11884 - Loss: 29.6061\n",
      "Processing batch 8534/11884 - Loss: 29.9000\n",
      "Processing batch 8535/11884 - Loss: 30.0946\n",
      "Processing batch 8536/11884 - Loss: 32.2846\n",
      "Processing batch 8537/11884 - Loss: 30.1205\n",
      "Processing batch 8538/11884 - Loss: 30.9763\n",
      "Processing batch 8539/11884 - Loss: 30.8598\n",
      "Processing batch 8540/11884 - Loss: 30.6822\n",
      "Processing batch 8541/11884 - Loss: 29.7579\n",
      "Processing batch 8542/11884 - Loss: 29.9508\n",
      "Processing batch 8543/11884 - Loss: 31.0834\n",
      "Processing batch 8544/11884 - Loss: 30.2740\n",
      "Processing batch 8545/11884 - Loss: 29.9101\n",
      "Processing batch 8546/11884 - Loss: 31.5520\n",
      "Processing batch 8547/11884 - Loss: 28.7673\n",
      "Processing batch 8548/11884 - Loss: 31.4514\n",
      "Processing batch 8549/11884 - Loss: 29.3559\n",
      "Processing batch 8550/11884 - Loss: 32.3470\n",
      "Processing batch 8551/11884 - Loss: 30.4754\n",
      "Processing batch 8552/11884 - Loss: 30.0170\n",
      "Processing batch 8553/11884 - Loss: 30.9996\n",
      "Processing batch 8554/11884 - Loss: 31.3911\n",
      "Processing batch 8555/11884 - Loss: 30.4120\n",
      "Processing batch 8556/11884 - Loss: 30.8739\n",
      "Processing batch 8557/11884 - Loss: 30.5079\n",
      "Processing batch 8558/11884 - Loss: 29.5838\n",
      "Processing batch 8559/11884 - Loss: 30.4171\n",
      "Processing batch 8560/11884 - Loss: 31.7931\n",
      "Processing batch 8561/11884 - Loss: 30.1099\n",
      "Processing batch 8562/11884 - Loss: 31.1887\n",
      "Processing batch 8563/11884 - Loss: 31.0856\n",
      "Processing batch 8564/11884 - Loss: 31.5756\n",
      "Processing batch 8565/11884 - Loss: 30.2756\n",
      "Processing batch 8566/11884 - Loss: 30.6073\n",
      "Processing batch 8567/11884 - Loss: 31.3272\n",
      "Processing batch 8568/11884 - Loss: 29.4620\n",
      "Processing batch 8569/11884 - Loss: 30.0428\n",
      "Processing batch 8570/11884 - Loss: 31.9334\n",
      "Processing batch 8571/11884 - Loss: 30.6852\n",
      "Processing batch 8572/11884 - Loss: 31.7794\n",
      "Processing batch 8573/11884 - Loss: 30.7770\n",
      "Processing batch 8574/11884 - Loss: 32.4313\n",
      "Processing batch 8575/11884 - Loss: 29.4611\n",
      "Processing batch 8576/11884 - Loss: 30.2691\n",
      "Processing batch 8577/11884 - Loss: 29.8367\n",
      "Processing batch 8578/11884 - Loss: 31.3425\n",
      "Processing batch 8579/11884 - Loss: 30.5466\n",
      "Processing batch 8580/11884 - Loss: 31.3219\n",
      "Processing batch 8581/11884 - Loss: 31.0819\n",
      "Processing batch 8582/11884 - Loss: 29.2089\n",
      "Processing batch 8583/11884 - Loss: 30.8675\n",
      "Processing batch 8584/11884 - Loss: 29.8949\n",
      "Processing batch 8585/11884 - Loss: 29.7606\n",
      "Processing batch 8586/11884 - Loss: 29.3008\n",
      "Processing batch 8587/11884 - Loss: 30.3904\n",
      "Processing batch 8588/11884 - Loss: 30.5707\n",
      "Processing batch 8589/11884 - Loss: 29.3340\n",
      "Processing batch 8590/11884 - Loss: 31.1489\n",
      "Processing batch 8591/11884 - Loss: 31.0497\n",
      "Processing batch 8592/11884 - Loss: 28.8849\n",
      "Processing batch 8593/11884 - Loss: 30.7221\n",
      "Processing batch 8594/11884 - Loss: 30.6156\n",
      "Processing batch 8595/11884 - Loss: 30.3434\n",
      "Processing batch 8596/11884 - Loss: 29.4444\n",
      "Processing batch 8597/11884 - Loss: 31.0020\n",
      "Processing batch 8598/11884 - Loss: 30.7575\n",
      "Processing batch 8599/11884 - Loss: 29.0206\n",
      "Processing batch 8600/11884 - Loss: 29.3332\n",
      "Processing batch 8601/11884 - Loss: 30.5887\n",
      "Processing batch 8602/11884 - Loss: 29.9353\n",
      "Processing batch 8603/11884 - Loss: 28.4150\n",
      "Processing batch 8604/11884 - Loss: 31.1640\n",
      "Processing batch 8605/11884 - Loss: 30.3863\n",
      "Processing batch 8606/11884 - Loss: 30.6495\n",
      "Processing batch 8607/11884 - Loss: 31.0933\n",
      "Processing batch 8608/11884 - Loss: 31.0191\n",
      "Processing batch 8609/11884 - Loss: 30.7473\n",
      "Processing batch 8610/11884 - Loss: 30.1297\n",
      "Processing batch 8611/11884 - Loss: 28.9858\n",
      "Processing batch 8612/11884 - Loss: 29.9334\n",
      "Processing batch 8613/11884 - Loss: 30.5309\n",
      "Processing batch 8614/11884 - Loss: 31.0977\n",
      "Processing batch 8615/11884 - Loss: 30.9583\n",
      "Processing batch 8616/11884 - Loss: 30.6649\n",
      "Processing batch 8617/11884 - Loss: 29.9340\n",
      "Processing batch 8618/11884 - Loss: 30.7528\n",
      "Processing batch 8619/11884 - Loss: 31.2482\n",
      "Processing batch 8620/11884 - Loss: 31.0198\n",
      "Processing batch 8621/11884 - Loss: 30.1810\n",
      "Processing batch 8622/11884 - Loss: 30.6989\n",
      "Processing batch 8623/11884 - Loss: 30.6459\n",
      "Processing batch 8624/11884 - Loss: 30.1524\n",
      "Processing batch 8625/11884 - Loss: 30.1299\n",
      "Processing batch 8626/11884 - Loss: 30.2796\n",
      "Processing batch 8627/11884 - Loss: 29.1942\n",
      "Processing batch 8628/11884 - Loss: 31.3501\n",
      "Processing batch 8629/11884 - Loss: 29.5087\n",
      "Processing batch 8630/11884 - Loss: 31.5430\n",
      "Processing batch 8631/11884 - Loss: 28.8515\n",
      "Processing batch 8632/11884 - Loss: 30.7915\n",
      "Processing batch 8633/11884 - Loss: 29.7400\n",
      "Processing batch 8634/11884 - Loss: 30.4164\n",
      "Processing batch 8635/11884 - Loss: 29.0558\n",
      "Processing batch 8636/11884 - Loss: 29.1768\n",
      "Processing batch 8637/11884 - Loss: 30.5066\n",
      "Processing batch 8638/11884 - Loss: 31.0732\n",
      "Processing batch 8639/11884 - Loss: 29.6700\n",
      "Processing batch 8640/11884 - Loss: 30.4820\n",
      "Processing batch 8641/11884 - Loss: 31.4741\n",
      "Processing batch 8642/11884 - Loss: 30.3332\n",
      "Processing batch 8643/11884 - Loss: 30.8563\n",
      "Processing batch 8644/11884 - Loss: 30.1165\n",
      "Processing batch 8645/11884 - Loss: 30.3526\n",
      "Processing batch 8646/11884 - Loss: 31.3543\n",
      "Processing batch 8647/11884 - Loss: 30.8993\n",
      "Processing batch 8648/11884 - Loss: 30.4653\n",
      "Processing batch 8649/11884 - Loss: 30.6008\n",
      "Processing batch 8650/11884 - Loss: 31.0876\n",
      "Processing batch 8651/11884 - Loss: 29.0902\n",
      "Processing batch 8652/11884 - Loss: 29.8723\n",
      "Processing batch 8653/11884 - Loss: 30.2952\n",
      "Processing batch 8654/11884 - Loss: 30.2468\n",
      "Processing batch 8655/11884 - Loss: 29.3316\n",
      "Processing batch 8656/11884 - Loss: 30.8061\n",
      "Processing batch 8657/11884 - Loss: 28.9854\n",
      "Processing batch 8658/11884 - Loss: 30.2187\n",
      "Processing batch 8659/11884 - Loss: 31.3033\n",
      "Processing batch 8660/11884 - Loss: 31.2810\n",
      "Processing batch 8661/11884 - Loss: 30.8848\n",
      "Processing batch 8662/11884 - Loss: 30.5519\n",
      "Processing batch 8663/11884 - Loss: 31.2182\n",
      "Processing batch 8664/11884 - Loss: 30.6321\n",
      "Processing batch 8665/11884 - Loss: 30.5734\n",
      "Processing batch 8666/11884 - Loss: 30.1389\n",
      "Processing batch 8667/11884 - Loss: 31.3346\n",
      "Processing batch 8668/11884 - Loss: 30.7032\n",
      "Processing batch 8669/11884 - Loss: 30.0198\n",
      "Processing batch 8670/11884 - Loss: 30.6714\n",
      "Processing batch 8671/11884 - Loss: 30.2424\n",
      "Processing batch 8672/11884 - Loss: 29.8733\n",
      "Processing batch 8673/11884 - Loss: 31.3924\n",
      "Processing batch 8674/11884 - Loss: 29.8753\n",
      "Processing batch 8675/11884 - Loss: 29.9472\n",
      "Processing batch 8676/11884 - Loss: 29.5651\n",
      "Processing batch 8677/11884 - Loss: 29.8128\n",
      "Processing batch 8678/11884 - Loss: 29.9098\n",
      "Processing batch 8679/11884 - Loss: 29.6832\n",
      "Processing batch 8680/11884 - Loss: 30.0508\n",
      "Processing batch 8681/11884 - Loss: 31.6201\n",
      "Processing batch 8682/11884 - Loss: 30.2883\n",
      "Processing batch 8683/11884 - Loss: 29.5155\n",
      "Processing batch 8684/11884 - Loss: 30.3266\n",
      "Processing batch 8685/11884 - Loss: 31.9483\n",
      "Processing batch 8686/11884 - Loss: 30.2919\n",
      "Processing batch 8687/11884 - Loss: 30.4627\n",
      "Processing batch 8688/11884 - Loss: 30.5065\n",
      "Processing batch 8689/11884 - Loss: 31.5719\n",
      "Processing batch 8690/11884 - Loss: 30.8156\n",
      "Processing batch 8691/11884 - Loss: 30.8878\n",
      "Processing batch 8692/11884 - Loss: 30.4516\n",
      "Processing batch 8693/11884 - Loss: 29.0533\n",
      "Processing batch 8694/11884 - Loss: 31.5808\n",
      "Processing batch 8695/11884 - Loss: 30.7550\n",
      "Processing batch 8696/11884 - Loss: 30.9219\n",
      "Processing batch 8697/11884 - Loss: 30.6189\n",
      "Processing batch 8698/11884 - Loss: 30.9501\n",
      "Processing batch 8699/11884 - Loss: 30.9792\n",
      "Processing batch 8700/11884 - Loss: 30.2064\n",
      "Processing batch 8701/11884 - Loss: 29.6878\n",
      "Processing batch 8702/11884 - Loss: 31.4486\n",
      "Processing batch 8703/11884 - Loss: 31.5016\n",
      "Processing batch 8704/11884 - Loss: 30.9128\n",
      "Processing batch 8705/11884 - Loss: 29.1155\n",
      "Processing batch 8706/11884 - Loss: 30.8882\n",
      "Processing batch 8707/11884 - Loss: 29.4110\n",
      "Processing batch 8708/11884 - Loss: 30.2133\n",
      "Processing batch 8709/11884 - Loss: 31.0195\n",
      "Processing batch 8710/11884 - Loss: 30.8567\n",
      "Processing batch 8711/11884 - Loss: 30.1921\n",
      "Processing batch 8712/11884 - Loss: 30.4563\n",
      "Processing batch 8713/11884 - Loss: 29.6931\n",
      "Processing batch 8714/11884 - Loss: 30.3248\n",
      "Processing batch 8715/11884 - Loss: 32.0769\n",
      "Processing batch 8716/11884 - Loss: 30.8916\n",
      "Processing batch 8717/11884 - Loss: 28.9874\n",
      "Processing batch 8718/11884 - Loss: 30.8262\n",
      "Processing batch 8719/11884 - Loss: 29.9361\n",
      "Processing batch 8720/11884 - Loss: 29.2526\n",
      "Processing batch 8721/11884 - Loss: 30.1016\n",
      "Processing batch 8722/11884 - Loss: 30.0476\n",
      "Processing batch 8723/11884 - Loss: 29.8545\n",
      "Processing batch 8724/11884 - Loss: 31.1098\n",
      "Processing batch 8725/11884 - Loss: 30.9218\n",
      "Processing batch 8726/11884 - Loss: 31.0710\n",
      "Processing batch 8727/11884 - Loss: 29.9987\n",
      "Processing batch 8728/11884 - Loss: 30.9241\n",
      "Processing batch 8729/11884 - Loss: 30.5622\n",
      "Processing batch 8730/11884 - Loss: 30.1909\n",
      "Processing batch 8731/11884 - Loss: 30.9698\n",
      "Processing batch 8732/11884 - Loss: 30.5877\n",
      "Processing batch 8733/11884 - Loss: 29.8432\n",
      "Processing batch 8734/11884 - Loss: 28.9932\n",
      "Processing batch 8735/11884 - Loss: 29.6115\n",
      "Processing batch 8736/11884 - Loss: 31.7903\n",
      "Processing batch 8737/11884 - Loss: 30.0796\n",
      "Processing batch 8738/11884 - Loss: 30.4480\n",
      "Processing batch 8739/11884 - Loss: 29.5973\n",
      "Processing batch 8740/11884 - Loss: 30.6113\n",
      "Processing batch 8741/11884 - Loss: 29.6632\n",
      "Processing batch 8742/11884 - Loss: 30.3559\n",
      "Processing batch 8743/11884 - Loss: 31.1226\n",
      "Processing batch 8744/11884 - Loss: 30.7957\n",
      "Processing batch 8745/11884 - Loss: 30.0909\n",
      "Processing batch 8746/11884 - Loss: 30.0145\n",
      "Processing batch 8747/11884 - Loss: 29.8972\n",
      "Processing batch 8748/11884 - Loss: 30.0190\n",
      "Processing batch 8749/11884 - Loss: 31.5431\n",
      "Processing batch 8750/11884 - Loss: 31.8433\n",
      "Processing batch 8751/11884 - Loss: 30.5514\n",
      "Processing batch 8752/11884 - Loss: 29.5456\n",
      "Processing batch 8753/11884 - Loss: 30.1861\n",
      "Processing batch 8754/11884 - Loss: 30.9759\n",
      "Processing batch 8755/11884 - Loss: 31.0477\n",
      "Processing batch 8756/11884 - Loss: 29.9573\n",
      "Processing batch 8757/11884 - Loss: 30.4067\n",
      "Processing batch 8758/11884 - Loss: 29.9131\n",
      "Processing batch 8759/11884 - Loss: 30.7134\n",
      "Processing batch 8760/11884 - Loss: 30.0856\n",
      "Processing batch 8761/11884 - Loss: 28.9470\n",
      "Processing batch 8762/11884 - Loss: 29.9284\n",
      "Processing batch 8763/11884 - Loss: 30.3082\n",
      "Processing batch 8764/11884 - Loss: 30.0954\n",
      "Processing batch 8765/11884 - Loss: 30.9725\n",
      "Processing batch 8766/11884 - Loss: 30.8007\n",
      "Processing batch 8767/11884 - Loss: 30.5451\n",
      "Processing batch 8768/11884 - Loss: 31.3304\n",
      "Processing batch 8769/11884 - Loss: 30.3021\n",
      "Processing batch 8770/11884 - Loss: 30.8916\n",
      "Processing batch 8771/11884 - Loss: 30.9610\n",
      "Processing batch 8772/11884 - Loss: 30.5149\n",
      "Processing batch 8773/11884 - Loss: 30.5676\n",
      "Processing batch 8774/11884 - Loss: 30.9764\n",
      "Processing batch 8775/11884 - Loss: 31.3033\n",
      "Processing batch 8776/11884 - Loss: 31.7628\n",
      "Processing batch 8777/11884 - Loss: 30.8829\n",
      "Processing batch 8778/11884 - Loss: 31.4485\n",
      "Processing batch 8779/11884 - Loss: 30.7012\n",
      "Processing batch 8780/11884 - Loss: 29.9289\n",
      "Processing batch 8781/11884 - Loss: 31.1852\n",
      "Processing batch 8782/11884 - Loss: 30.9671\n",
      "Processing batch 8783/11884 - Loss: 30.9328\n",
      "Processing batch 8784/11884 - Loss: 30.1997\n",
      "Processing batch 8785/11884 - Loss: 29.1660\n",
      "Processing batch 8786/11884 - Loss: 30.4202\n",
      "Processing batch 8787/11884 - Loss: 30.5908\n",
      "Processing batch 8788/11884 - Loss: 30.7691\n",
      "Processing batch 8789/11884 - Loss: 30.0245\n",
      "Processing batch 8790/11884 - Loss: 31.2802\n",
      "Processing batch 8791/11884 - Loss: 29.2738\n",
      "Processing batch 8792/11884 - Loss: 29.4865\n",
      "Processing batch 8793/11884 - Loss: 28.2404\n",
      "Processing batch 8794/11884 - Loss: 29.9434\n",
      "Processing batch 8795/11884 - Loss: 29.7606\n",
      "Processing batch 8796/11884 - Loss: 29.7738\n",
      "Processing batch 8797/11884 - Loss: 30.6759\n",
      "Processing batch 8798/11884 - Loss: 30.2937\n",
      "Processing batch 8799/11884 - Loss: 30.4912\n",
      "Processing batch 8800/11884 - Loss: 33.1369\n",
      "Processing batch 8801/11884 - Loss: 30.2096\n",
      "Processing batch 8802/11884 - Loss: 30.0828\n",
      "Processing batch 8803/11884 - Loss: 30.7957\n",
      "Processing batch 8804/11884 - Loss: 31.3792\n",
      "Processing batch 8805/11884 - Loss: 31.0123\n",
      "Processing batch 8806/11884 - Loss: 30.1870\n",
      "Processing batch 8807/11884 - Loss: 30.2984\n",
      "Processing batch 8808/11884 - Loss: 29.9407\n",
      "Processing batch 8809/11884 - Loss: 30.5989\n",
      "Processing batch 8810/11884 - Loss: 30.6649\n",
      "Processing batch 8811/11884 - Loss: 29.2614\n",
      "Processing batch 8812/11884 - Loss: 31.1058\n",
      "Processing batch 8813/11884 - Loss: 29.9561\n",
      "Processing batch 8814/11884 - Loss: 28.5973\n",
      "Processing batch 8815/11884 - Loss: 31.1818\n",
      "Processing batch 8816/11884 - Loss: 30.2007\n",
      "Processing batch 8817/11884 - Loss: 30.3760\n",
      "Processing batch 8818/11884 - Loss: 29.2613\n",
      "Processing batch 8819/11884 - Loss: 29.5676\n",
      "Processing batch 8820/11884 - Loss: 31.1242\n",
      "Processing batch 8821/11884 - Loss: 31.7997\n",
      "Processing batch 8822/11884 - Loss: 31.1498\n",
      "Processing batch 8823/11884 - Loss: 30.4596\n",
      "Processing batch 8824/11884 - Loss: 31.0412\n",
      "Processing batch 8825/11884 - Loss: 30.0553\n",
      "Processing batch 8826/11884 - Loss: 29.4024\n",
      "Processing batch 8827/11884 - Loss: 31.3008\n",
      "Processing batch 8828/11884 - Loss: 31.2871\n",
      "Processing batch 8829/11884 - Loss: 30.4545\n",
      "Processing batch 8830/11884 - Loss: 30.1902\n",
      "Processing batch 8831/11884 - Loss: 29.9675\n",
      "Processing batch 8832/11884 - Loss: 30.6081\n",
      "Processing batch 8833/11884 - Loss: 31.1535\n",
      "Processing batch 8834/11884 - Loss: 30.9935\n",
      "Processing batch 8835/11884 - Loss: 29.8262\n",
      "Processing batch 8836/11884 - Loss: 30.4469\n",
      "Processing batch 8837/11884 - Loss: 30.5227\n",
      "Processing batch 8838/11884 - Loss: 29.9385\n",
      "Processing batch 8839/11884 - Loss: 30.2739\n",
      "Processing batch 8840/11884 - Loss: 29.0436\n",
      "Processing batch 8841/11884 - Loss: 31.0449\n",
      "Processing batch 8842/11884 - Loss: 29.3504\n",
      "Processing batch 8843/11884 - Loss: 29.1127\n",
      "Processing batch 8844/11884 - Loss: 30.2123\n",
      "Processing batch 8845/11884 - Loss: 29.9101\n",
      "Processing batch 8846/11884 - Loss: 29.2764\n",
      "Processing batch 8847/11884 - Loss: 31.2552\n",
      "Processing batch 8848/11884 - Loss: 30.3359\n",
      "Processing batch 8849/11884 - Loss: 30.2299\n",
      "Processing batch 8850/11884 - Loss: 30.5212\n",
      "Processing batch 8851/11884 - Loss: 30.3825\n",
      "Processing batch 8852/11884 - Loss: 30.1328\n",
      "Processing batch 8853/11884 - Loss: 30.6642\n",
      "Processing batch 8854/11884 - Loss: 31.9114\n",
      "Processing batch 8855/11884 - Loss: 30.4260\n",
      "Processing batch 8856/11884 - Loss: 28.6129\n",
      "Processing batch 8857/11884 - Loss: 31.0368\n",
      "Processing batch 8858/11884 - Loss: 30.8695\n",
      "Processing batch 8859/11884 - Loss: 30.3788\n",
      "Processing batch 8860/11884 - Loss: 29.3842\n",
      "Processing batch 8861/11884 - Loss: 30.8854\n",
      "Processing batch 8862/11884 - Loss: 30.5292\n",
      "Processing batch 8863/11884 - Loss: 30.4949\n",
      "Processing batch 8864/11884 - Loss: 29.6417\n",
      "Processing batch 8865/11884 - Loss: 30.2751\n",
      "Processing batch 8866/11884 - Loss: 30.1329\n",
      "Processing batch 8867/11884 - Loss: 29.4615\n",
      "Processing batch 8868/11884 - Loss: 30.4854\n",
      "Processing batch 8869/11884 - Loss: 30.4077\n",
      "Processing batch 8870/11884 - Loss: 28.6313\n",
      "Processing batch 8871/11884 - Loss: 30.3873\n",
      "Processing batch 8872/11884 - Loss: 31.4008\n",
      "Processing batch 8873/11884 - Loss: 30.6367\n",
      "Processing batch 8874/11884 - Loss: 30.7463\n",
      "Processing batch 8875/11884 - Loss: 31.2297\n",
      "Processing batch 8876/11884 - Loss: 30.6165\n",
      "Processing batch 8877/11884 - Loss: 31.0180\n",
      "Processing batch 8878/11884 - Loss: 29.6106\n",
      "Processing batch 8879/11884 - Loss: 30.1627\n",
      "Processing batch 8880/11884 - Loss: 30.9957\n",
      "Processing batch 8881/11884 - Loss: 28.6786\n",
      "Processing batch 8882/11884 - Loss: 30.4149\n",
      "Processing batch 8883/11884 - Loss: 31.2719\n",
      "Processing batch 8884/11884 - Loss: 31.3554\n",
      "Processing batch 8885/11884 - Loss: 29.9708\n",
      "Processing batch 8886/11884 - Loss: 30.4210\n",
      "Processing batch 8887/11884 - Loss: 30.4297\n",
      "Processing batch 8888/11884 - Loss: 29.9167\n",
      "Processing batch 8889/11884 - Loss: 29.8962\n",
      "Processing batch 8890/11884 - Loss: 31.4029\n",
      "Processing batch 8891/11884 - Loss: 30.6477\n",
      "Processing batch 8892/11884 - Loss: 30.7967\n",
      "Processing batch 8893/11884 - Loss: 30.2654\n",
      "Processing batch 8894/11884 - Loss: 29.8284\n",
      "Processing batch 8895/11884 - Loss: 31.1508\n",
      "Processing batch 8896/11884 - Loss: 32.1449\n",
      "Processing batch 8897/11884 - Loss: 31.3873\n",
      "Processing batch 8898/11884 - Loss: 30.3959\n",
      "Processing batch 8899/11884 - Loss: 30.8618\n",
      "Processing batch 8900/11884 - Loss: 30.9959\n",
      "Processing batch 8901/11884 - Loss: 31.9471\n",
      "Processing batch 8902/11884 - Loss: 31.2563\n",
      "Processing batch 8903/11884 - Loss: 30.8546\n",
      "Processing batch 8904/11884 - Loss: 30.4279\n",
      "Processing batch 8905/11884 - Loss: 29.6579\n",
      "Processing batch 8906/11884 - Loss: 29.9489\n",
      "Processing batch 8907/11884 - Loss: 29.9148\n",
      "Processing batch 8908/11884 - Loss: 31.6862\n",
      "Processing batch 8909/11884 - Loss: 30.3283\n",
      "Processing batch 8910/11884 - Loss: 30.1964\n",
      "Processing batch 8911/11884 - Loss: 30.5922\n",
      "Processing batch 8912/11884 - Loss: 30.7091\n",
      "Processing batch 8913/11884 - Loss: 29.7781\n",
      "Processing batch 8914/11884 - Loss: 29.9051\n",
      "Processing batch 8915/11884 - Loss: 30.8606\n",
      "Processing batch 8916/11884 - Loss: 29.5395\n",
      "Processing batch 8917/11884 - Loss: 30.6291\n",
      "Processing batch 8918/11884 - Loss: 29.4122\n",
      "Processing batch 8919/11884 - Loss: 29.4188\n",
      "Processing batch 8920/11884 - Loss: 30.3475\n",
      "Processing batch 8921/11884 - Loss: 32.2420\n",
      "Processing batch 8922/11884 - Loss: 31.1426\n",
      "Processing batch 8923/11884 - Loss: 30.1136\n",
      "Processing batch 8924/11884 - Loss: 29.4584\n",
      "Processing batch 8925/11884 - Loss: 30.6227\n",
      "Processing batch 8926/11884 - Loss: 30.7554\n",
      "Processing batch 8927/11884 - Loss: 30.8925\n",
      "Processing batch 8928/11884 - Loss: 31.1500\n",
      "Processing batch 8929/11884 - Loss: 29.9760\n",
      "Processing batch 8930/11884 - Loss: 29.6143\n",
      "Processing batch 8931/11884 - Loss: 29.6555\n",
      "Processing batch 8932/11884 - Loss: 31.1482\n",
      "Processing batch 8933/11884 - Loss: 30.9423\n",
      "Processing batch 8934/11884 - Loss: 31.4550\n",
      "Processing batch 8935/11884 - Loss: 32.0332\n",
      "Processing batch 8936/11884 - Loss: 29.8050\n",
      "Processing batch 8937/11884 - Loss: 30.8147\n",
      "Processing batch 8938/11884 - Loss: 30.8191\n",
      "Processing batch 8939/11884 - Loss: 31.1159\n",
      "Processing batch 8940/11884 - Loss: 30.0636\n",
      "Processing batch 8941/11884 - Loss: 29.9712\n",
      "Processing batch 8942/11884 - Loss: 28.7141\n",
      "Processing batch 8943/11884 - Loss: 29.6719\n",
      "Processing batch 8944/11884 - Loss: 30.4105\n",
      "Processing batch 8945/11884 - Loss: 30.2107\n",
      "Processing batch 8946/11884 - Loss: 30.2739\n",
      "Processing batch 8947/11884 - Loss: 29.5768\n",
      "Processing batch 8948/11884 - Loss: 30.5319\n",
      "Processing batch 8949/11884 - Loss: 30.0924\n",
      "Processing batch 8950/11884 - Loss: 30.3578\n",
      "Processing batch 8951/11884 - Loss: 30.8243\n",
      "Processing batch 8952/11884 - Loss: 31.9352\n",
      "Processing batch 8953/11884 - Loss: 30.5828\n",
      "Processing batch 8954/11884 - Loss: 30.8369\n",
      "Processing batch 8955/11884 - Loss: 30.3682\n",
      "Processing batch 8956/11884 - Loss: 30.7919\n",
      "Processing batch 8957/11884 - Loss: 29.2584\n",
      "Processing batch 8958/11884 - Loss: 31.6991\n",
      "Processing batch 8959/11884 - Loss: 29.2272\n",
      "Processing batch 8960/11884 - Loss: 30.9407\n",
      "Processing batch 8961/11884 - Loss: 30.8933\n",
      "Processing batch 8962/11884 - Loss: 30.4233\n",
      "Processing batch 8963/11884 - Loss: 30.5403\n",
      "Processing batch 8964/11884 - Loss: 30.0199\n",
      "Processing batch 8965/11884 - Loss: 31.2122\n",
      "Processing batch 8966/11884 - Loss: 30.7615\n",
      "Processing batch 8967/11884 - Loss: 29.4429\n",
      "Processing batch 8968/11884 - Loss: 29.6698\n",
      "Processing batch 8969/11884 - Loss: 29.7968\n",
      "Processing batch 8970/11884 - Loss: 29.6030\n",
      "Processing batch 8971/11884 - Loss: 28.8610\n",
      "Processing batch 8972/11884 - Loss: 30.9854\n",
      "Processing batch 8973/11884 - Loss: 31.5609\n",
      "Processing batch 8974/11884 - Loss: 30.3198\n",
      "Processing batch 8975/11884 - Loss: 30.5909\n",
      "Processing batch 8976/11884 - Loss: 30.2345\n",
      "Processing batch 8977/11884 - Loss: 29.9095\n",
      "Processing batch 8978/11884 - Loss: 29.3398\n",
      "Processing batch 8979/11884 - Loss: 31.4236\n",
      "Processing batch 8980/11884 - Loss: 31.1446\n",
      "Processing batch 8981/11884 - Loss: 32.1463\n",
      "Processing batch 8982/11884 - Loss: 30.2805\n",
      "Processing batch 8983/11884 - Loss: 30.6990\n",
      "Processing batch 8984/11884 - Loss: 29.8260\n",
      "Processing batch 8985/11884 - Loss: 28.8117\n",
      "Processing batch 8986/11884 - Loss: 31.2039\n",
      "Processing batch 8987/11884 - Loss: 29.7622\n",
      "Processing batch 8988/11884 - Loss: 29.9583\n",
      "Processing batch 8989/11884 - Loss: 29.4267\n",
      "Processing batch 8990/11884 - Loss: 29.0955\n",
      "Processing batch 8991/11884 - Loss: 30.6301\n",
      "Processing batch 8992/11884 - Loss: 31.0339\n",
      "Processing batch 8993/11884 - Loss: 29.2387\n",
      "Processing batch 8994/11884 - Loss: 30.0822\n",
      "Processing batch 8995/11884 - Loss: 30.2225\n",
      "Processing batch 8996/11884 - Loss: 30.6083\n",
      "Processing batch 8997/11884 - Loss: 29.5793\n",
      "Processing batch 8998/11884 - Loss: 32.2672\n",
      "Processing batch 8999/11884 - Loss: 30.6274\n",
      "Processing batch 9000/11884 - Loss: 29.0864\n",
      "Processing batch 9001/11884 - Loss: 29.6633\n",
      "Processing batch 9002/11884 - Loss: 29.9323\n",
      "Processing batch 9003/11884 - Loss: 30.3415\n",
      "Processing batch 9004/11884 - Loss: 30.4249\n",
      "Processing batch 9005/11884 - Loss: 30.8565\n",
      "Processing batch 9006/11884 - Loss: 28.5470\n",
      "Processing batch 9007/11884 - Loss: 31.0793\n",
      "Processing batch 9008/11884 - Loss: 31.0236\n",
      "Processing batch 9009/11884 - Loss: 30.5311\n",
      "Processing batch 9010/11884 - Loss: 30.0968\n",
      "Processing batch 9011/11884 - Loss: 30.6698\n",
      "Processing batch 9012/11884 - Loss: 30.1806\n",
      "Processing batch 9013/11884 - Loss: 29.0345\n",
      "Processing batch 9014/11884 - Loss: 30.2399\n",
      "Processing batch 9015/11884 - Loss: 30.9716\n",
      "Processing batch 9016/11884 - Loss: 30.9769\n",
      "Processing batch 9017/11884 - Loss: 29.8633\n",
      "Processing batch 9018/11884 - Loss: 30.8070\n",
      "Processing batch 9019/11884 - Loss: 30.9048\n",
      "Processing batch 9020/11884 - Loss: 29.7908\n",
      "Processing batch 9021/11884 - Loss: 29.4117\n",
      "Processing batch 9022/11884 - Loss: 29.0889\n",
      "Processing batch 9023/11884 - Loss: 29.6418\n",
      "Processing batch 9024/11884 - Loss: 30.6394\n",
      "Processing batch 9025/11884 - Loss: 30.2096\n",
      "Processing batch 9026/11884 - Loss: 29.7094\n",
      "Processing batch 9027/11884 - Loss: 29.8538\n",
      "Processing batch 9028/11884 - Loss: 31.8284\n",
      "Processing batch 9029/11884 - Loss: 29.9589\n",
      "Processing batch 9030/11884 - Loss: 29.6722\n",
      "Processing batch 9031/11884 - Loss: 31.1041\n",
      "Processing batch 9032/11884 - Loss: 30.5453\n",
      "Processing batch 9033/11884 - Loss: 30.5680\n",
      "Processing batch 9034/11884 - Loss: 30.4310\n",
      "Processing batch 9035/11884 - Loss: 29.1570\n",
      "Processing batch 9036/11884 - Loss: 30.5764\n",
      "Processing batch 9037/11884 - Loss: 30.1120\n",
      "Processing batch 9038/11884 - Loss: 30.0083\n",
      "Processing batch 9039/11884 - Loss: 30.5172\n",
      "Processing batch 9040/11884 - Loss: 29.5610\n",
      "Processing batch 9041/11884 - Loss: 31.0616\n",
      "Processing batch 9042/11884 - Loss: 29.7726\n",
      "Processing batch 9043/11884 - Loss: 29.9296\n",
      "Processing batch 9044/11884 - Loss: 29.6682\n",
      "Processing batch 9045/11884 - Loss: 31.2024\n",
      "Processing batch 9046/11884 - Loss: 30.9243\n",
      "Processing batch 9047/11884 - Loss: 30.3228\n",
      "Processing batch 9048/11884 - Loss: 31.3955\n",
      "Processing batch 9049/11884 - Loss: 30.8423\n",
      "Processing batch 9050/11884 - Loss: 28.0477\n",
      "Processing batch 9051/11884 - Loss: 29.4158\n",
      "Processing batch 9052/11884 - Loss: 30.3511\n",
      "Processing batch 9053/11884 - Loss: 30.7542\n",
      "Processing batch 9054/11884 - Loss: 29.9347\n",
      "Processing batch 9055/11884 - Loss: 30.1529\n",
      "Processing batch 9056/11884 - Loss: 28.8668\n",
      "Processing batch 9057/11884 - Loss: 29.9662\n",
      "Processing batch 9058/11884 - Loss: 30.0618\n",
      "Processing batch 9059/11884 - Loss: 31.9531\n",
      "Processing batch 9060/11884 - Loss: 29.2830\n",
      "Processing batch 9061/11884 - Loss: 30.3662\n",
      "Processing batch 9062/11884 - Loss: 32.1538\n",
      "Processing batch 9063/11884 - Loss: 30.4997\n",
      "Processing batch 9064/11884 - Loss: 29.8751\n",
      "Processing batch 9065/11884 - Loss: 31.0641\n",
      "Processing batch 9066/11884 - Loss: 30.3495\n",
      "Processing batch 9067/11884 - Loss: 30.3222\n",
      "Processing batch 9068/11884 - Loss: 29.4840\n",
      "Processing batch 9069/11884 - Loss: 30.0417\n",
      "Processing batch 9070/11884 - Loss: 30.3111\n",
      "Processing batch 9071/11884 - Loss: 31.4759\n",
      "Processing batch 9072/11884 - Loss: 30.6735\n",
      "Processing batch 9073/11884 - Loss: 30.8442\n",
      "Processing batch 9074/11884 - Loss: 30.2805\n",
      "Processing batch 9075/11884 - Loss: 28.7024\n",
      "Processing batch 9076/11884 - Loss: 28.8610\n",
      "Processing batch 9077/11884 - Loss: 30.8469\n",
      "Processing batch 9078/11884 - Loss: 30.3818\n",
      "Processing batch 9079/11884 - Loss: 29.9094\n",
      "Processing batch 9080/11884 - Loss: 31.0923\n",
      "Processing batch 9081/11884 - Loss: 32.2947\n",
      "Processing batch 9082/11884 - Loss: 30.8209\n",
      "Processing batch 9083/11884 - Loss: 31.5985\n",
      "Processing batch 9084/11884 - Loss: 30.0298\n",
      "Processing batch 9085/11884 - Loss: 30.7119\n",
      "Processing batch 9086/11884 - Loss: 28.6198\n",
      "Processing batch 9087/11884 - Loss: 31.4424\n",
      "Processing batch 9088/11884 - Loss: 30.5790\n",
      "Processing batch 9089/11884 - Loss: 30.7869\n",
      "Processing batch 9090/11884 - Loss: 31.0204\n",
      "Processing batch 9091/11884 - Loss: 29.1104\n",
      "Processing batch 9092/11884 - Loss: 31.6345\n",
      "Processing batch 9093/11884 - Loss: 30.6829\n",
      "Processing batch 9094/11884 - Loss: 27.8012\n",
      "Processing batch 9095/11884 - Loss: 29.2466\n",
      "Processing batch 9096/11884 - Loss: 31.1224\n",
      "Processing batch 9097/11884 - Loss: 29.9535\n",
      "Processing batch 9098/11884 - Loss: 31.6198\n",
      "Processing batch 9099/11884 - Loss: 30.4127\n",
      "Processing batch 9100/11884 - Loss: 30.1098\n",
      "Processing batch 9101/11884 - Loss: 29.8296\n",
      "Processing batch 9102/11884 - Loss: 28.6543\n",
      "Processing batch 9103/11884 - Loss: 30.4833\n",
      "Processing batch 9104/11884 - Loss: 31.0140\n",
      "Processing batch 9105/11884 - Loss: 30.2990\n",
      "Processing batch 9106/11884 - Loss: 29.3261\n",
      "Processing batch 9107/11884 - Loss: 30.0898\n",
      "Processing batch 9108/11884 - Loss: 30.6039\n",
      "Processing batch 9109/11884 - Loss: 30.8529\n",
      "Processing batch 9110/11884 - Loss: 29.5413\n",
      "Processing batch 9111/11884 - Loss: 31.1586\n",
      "Processing batch 9112/11884 - Loss: 31.1073\n",
      "Processing batch 9113/11884 - Loss: 30.4869\n",
      "Processing batch 9114/11884 - Loss: 31.2128\n",
      "Processing batch 9115/11884 - Loss: 30.4942\n",
      "Processing batch 9116/11884 - Loss: 29.6790\n",
      "Processing batch 9117/11884 - Loss: 30.0688\n",
      "Processing batch 9118/11884 - Loss: 29.4149\n",
      "Processing batch 9119/11884 - Loss: 30.8311\n",
      "Processing batch 9120/11884 - Loss: 29.7622\n",
      "Processing batch 9121/11884 - Loss: 30.4443\n",
      "Processing batch 9122/11884 - Loss: 31.5706\n",
      "Processing batch 9123/11884 - Loss: 30.0498\n",
      "Processing batch 9124/11884 - Loss: 30.8515\n",
      "Processing batch 9125/11884 - Loss: 31.3279\n",
      "Processing batch 9126/11884 - Loss: 31.0554\n",
      "Processing batch 9127/11884 - Loss: 30.5241\n",
      "Processing batch 9128/11884 - Loss: 31.4570\n",
      "Processing batch 9129/11884 - Loss: 31.3733\n",
      "Processing batch 9130/11884 - Loss: 30.2983\n",
      "Processing batch 9131/11884 - Loss: 29.9673\n",
      "Processing batch 9132/11884 - Loss: 30.7937\n",
      "Processing batch 9133/11884 - Loss: 29.3878\n",
      "Processing batch 9134/11884 - Loss: 30.3483\n",
      "Processing batch 9135/11884 - Loss: 28.6863\n",
      "Processing batch 9136/11884 - Loss: 27.9236\n",
      "Processing batch 9137/11884 - Loss: 29.7510\n",
      "Processing batch 9138/11884 - Loss: 30.3439\n",
      "Processing batch 9139/11884 - Loss: 30.4693\n",
      "Processing batch 9140/11884 - Loss: 32.0921\n",
      "Processing batch 9141/11884 - Loss: 31.8984\n",
      "Processing batch 9142/11884 - Loss: 29.8122\n",
      "Processing batch 9143/11884 - Loss: 31.2795\n",
      "Processing batch 9144/11884 - Loss: 30.5618\n",
      "Processing batch 9145/11884 - Loss: 31.2595\n",
      "Processing batch 9146/11884 - Loss: 31.1260\n",
      "Processing batch 9147/11884 - Loss: 29.6902\n",
      "Processing batch 9148/11884 - Loss: 30.2731\n",
      "Processing batch 9149/11884 - Loss: 29.7978\n",
      "Processing batch 9150/11884 - Loss: 29.4796\n",
      "Processing batch 9151/11884 - Loss: 30.1017\n",
      "Processing batch 9152/11884 - Loss: 30.9271\n",
      "Processing batch 9153/11884 - Loss: 29.2225\n",
      "Processing batch 9154/11884 - Loss: 30.3783\n",
      "Processing batch 9155/11884 - Loss: 30.4484\n",
      "Processing batch 9156/11884 - Loss: 29.4345\n",
      "Processing batch 9157/11884 - Loss: 30.6883\n",
      "Processing batch 9158/11884 - Loss: 30.1934\n",
      "Processing batch 9159/11884 - Loss: 30.9396\n",
      "Processing batch 9160/11884 - Loss: 30.1712\n",
      "Processing batch 9161/11884 - Loss: 30.3822\n",
      "Processing batch 9162/11884 - Loss: 29.7670\n",
      "Processing batch 9163/11884 - Loss: 31.2113\n",
      "Processing batch 9164/11884 - Loss: 31.6191\n",
      "Processing batch 9165/11884 - Loss: 31.1333\n",
      "Processing batch 9166/11884 - Loss: 29.7083\n",
      "Processing batch 9167/11884 - Loss: 30.7407\n",
      "Processing batch 9168/11884 - Loss: 30.7640\n",
      "Processing batch 9169/11884 - Loss: 29.6889\n",
      "Processing batch 9170/11884 - Loss: 31.3114\n",
      "Processing batch 9171/11884 - Loss: 31.0857\n",
      "Processing batch 9172/11884 - Loss: 30.6955\n",
      "Processing batch 9173/11884 - Loss: 30.3471\n",
      "Processing batch 9174/11884 - Loss: 29.9723\n",
      "Processing batch 9175/11884 - Loss: 30.9182\n",
      "Processing batch 9176/11884 - Loss: 30.2433\n",
      "Processing batch 9177/11884 - Loss: 30.6572\n",
      "Processing batch 9178/11884 - Loss: 29.7688\n",
      "Processing batch 9179/11884 - Loss: 30.6218\n",
      "Processing batch 9180/11884 - Loss: 30.1072\n",
      "Processing batch 9181/11884 - Loss: 30.0238\n",
      "Processing batch 9182/11884 - Loss: 31.1620\n",
      "Processing batch 9183/11884 - Loss: 30.2282\n",
      "Processing batch 9184/11884 - Loss: 30.2402\n",
      "Processing batch 9185/11884 - Loss: 30.1403\n",
      "Processing batch 9186/11884 - Loss: 31.3805\n",
      "Processing batch 9187/11884 - Loss: 30.0581\n",
      "Processing batch 9188/11884 - Loss: 29.5717\n",
      "Processing batch 9189/11884 - Loss: 29.4079\n",
      "Processing batch 9190/11884 - Loss: 30.6108\n",
      "Processing batch 9191/11884 - Loss: 30.5082\n",
      "Processing batch 9192/11884 - Loss: 30.2662\n",
      "Processing batch 9193/11884 - Loss: 30.3128\n",
      "Processing batch 9194/11884 - Loss: 29.4012\n",
      "Processing batch 9195/11884 - Loss: 29.8249\n",
      "Processing batch 9196/11884 - Loss: 30.1155\n",
      "Processing batch 9197/11884 - Loss: 31.0558\n",
      "Processing batch 9198/11884 - Loss: 31.3921\n",
      "Processing batch 9199/11884 - Loss: 31.8770\n",
      "Processing batch 9200/11884 - Loss: 29.0742\n",
      "Processing batch 9201/11884 - Loss: 30.1702\n",
      "Processing batch 9202/11884 - Loss: 30.1095\n",
      "Processing batch 9203/11884 - Loss: 30.1388\n",
      "Processing batch 9204/11884 - Loss: 30.1869\n",
      "Processing batch 9205/11884 - Loss: 29.0373\n",
      "Processing batch 9206/11884 - Loss: 30.5034\n",
      "Processing batch 9207/11884 - Loss: 29.7963\n",
      "Processing batch 9208/11884 - Loss: 29.5124\n",
      "Processing batch 9209/11884 - Loss: 30.9808\n",
      "Processing batch 9210/11884 - Loss: 31.1381\n",
      "Processing batch 9211/11884 - Loss: 29.6041\n",
      "Processing batch 9212/11884 - Loss: 30.0907\n",
      "Processing batch 9213/11884 - Loss: 31.6163\n",
      "Processing batch 9214/11884 - Loss: 29.9927\n",
      "Processing batch 9215/11884 - Loss: 29.6948\n",
      "Processing batch 9216/11884 - Loss: 30.2424\n",
      "Processing batch 9217/11884 - Loss: 29.3085\n",
      "Processing batch 9218/11884 - Loss: 30.7930\n",
      "Processing batch 9219/11884 - Loss: 30.5603\n",
      "Processing batch 9220/11884 - Loss: 29.2931\n",
      "Processing batch 9221/11884 - Loss: 31.2713\n",
      "Processing batch 9222/11884 - Loss: 31.2086\n",
      "Processing batch 9223/11884 - Loss: 30.8756\n",
      "Processing batch 9224/11884 - Loss: 29.2396\n",
      "Processing batch 9225/11884 - Loss: 30.6066\n",
      "Processing batch 9226/11884 - Loss: 29.7006\n",
      "Processing batch 9227/11884 - Loss: 31.3737\n",
      "Processing batch 9228/11884 - Loss: 29.0035\n",
      "Processing batch 9229/11884 - Loss: 30.0052\n",
      "Processing batch 9230/11884 - Loss: 30.8784\n",
      "Processing batch 9231/11884 - Loss: 30.0673\n",
      "Processing batch 9232/11884 - Loss: 32.4698\n",
      "Processing batch 9233/11884 - Loss: 29.9668\n",
      "Processing batch 9234/11884 - Loss: 31.2930\n",
      "Processing batch 9235/11884 - Loss: 30.5449\n",
      "Processing batch 9236/11884 - Loss: 30.7291\n",
      "Processing batch 9237/11884 - Loss: 31.5120\n",
      "Processing batch 9238/11884 - Loss: 28.9029\n",
      "Processing batch 9239/11884 - Loss: 30.5330\n",
      "Processing batch 9240/11884 - Loss: 31.2287\n",
      "Processing batch 9241/11884 - Loss: 30.3485\n",
      "Processing batch 9242/11884 - Loss: 30.3440\n",
      "Processing batch 9243/11884 - Loss: 30.0801\n",
      "Processing batch 9244/11884 - Loss: 30.7468\n",
      "Processing batch 9245/11884 - Loss: 31.0222\n",
      "Processing batch 9246/11884 - Loss: 29.7754\n",
      "Processing batch 9247/11884 - Loss: 30.3836\n",
      "Processing batch 9248/11884 - Loss: 30.3860\n",
      "Processing batch 9249/11884 - Loss: 31.5604\n",
      "Processing batch 9250/11884 - Loss: 31.9610\n",
      "Processing batch 9251/11884 - Loss: 30.7538\n",
      "Processing batch 9252/11884 - Loss: 29.8358\n",
      "Processing batch 9253/11884 - Loss: 29.9394\n",
      "Processing batch 9254/11884 - Loss: 31.5874\n",
      "Processing batch 9255/11884 - Loss: 30.2437\n",
      "Processing batch 9256/11884 - Loss: 30.6994\n",
      "Processing batch 9257/11884 - Loss: 29.0290\n",
      "Processing batch 9258/11884 - Loss: 30.3180\n",
      "Processing batch 9259/11884 - Loss: 30.9506\n",
      "Processing batch 9260/11884 - Loss: 30.2706\n",
      "Processing batch 9261/11884 - Loss: 31.5066\n",
      "Processing batch 9262/11884 - Loss: 29.8053\n",
      "Processing batch 9263/11884 - Loss: 31.0620\n",
      "Processing batch 9264/11884 - Loss: 31.0443\n",
      "Processing batch 9265/11884 - Loss: 30.0436\n",
      "Processing batch 9266/11884 - Loss: 31.4526\n",
      "Processing batch 9267/11884 - Loss: 31.1339\n",
      "Processing batch 9268/11884 - Loss: 31.7207\n",
      "Processing batch 9269/11884 - Loss: 30.6436\n",
      "Processing batch 9270/11884 - Loss: 29.2405\n",
      "Processing batch 9271/11884 - Loss: 30.2527\n",
      "Processing batch 9272/11884 - Loss: 30.3606\n",
      "Processing batch 9273/11884 - Loss: 30.0616\n",
      "Processing batch 9274/11884 - Loss: 30.2730\n",
      "Processing batch 9275/11884 - Loss: 30.3775\n",
      "Processing batch 9276/11884 - Loss: 30.1155\n",
      "Processing batch 9277/11884 - Loss: 30.8021\n",
      "Processing batch 9278/11884 - Loss: 30.3158\n",
      "Processing batch 9279/11884 - Loss: 31.0248\n",
      "Processing batch 9280/11884 - Loss: 30.5535\n",
      "Processing batch 9281/11884 - Loss: 30.8584\n",
      "Processing batch 9282/11884 - Loss: 29.7926\n",
      "Processing batch 9283/11884 - Loss: 30.8984\n",
      "Processing batch 9284/11884 - Loss: 30.8919\n",
      "Processing batch 9285/11884 - Loss: 30.9742\n",
      "Processing batch 9286/11884 - Loss: 29.8241\n",
      "Processing batch 9287/11884 - Loss: 30.9021\n",
      "Processing batch 9288/11884 - Loss: 29.7286\n",
      "Processing batch 9289/11884 - Loss: 31.5062\n",
      "Processing batch 9290/11884 - Loss: 30.0424\n",
      "Processing batch 9291/11884 - Loss: 32.1387\n",
      "Processing batch 9292/11884 - Loss: 29.0644\n",
      "Processing batch 9293/11884 - Loss: 30.5787\n",
      "Processing batch 9294/11884 - Loss: 30.4170\n",
      "Processing batch 9295/11884 - Loss: 30.6227\n",
      "Processing batch 9296/11884 - Loss: 30.2343\n",
      "Processing batch 9297/11884 - Loss: 29.4939\n",
      "Processing batch 9298/11884 - Loss: 30.2682\n",
      "Processing batch 9299/11884 - Loss: 29.9645\n",
      "Processing batch 9300/11884 - Loss: 30.2075\n",
      "Processing batch 9301/11884 - Loss: 30.7207\n",
      "Processing batch 9302/11884 - Loss: 31.1506\n",
      "Processing batch 9303/11884 - Loss: 31.2649\n",
      "Processing batch 9304/11884 - Loss: 29.9716\n",
      "Processing batch 9305/11884 - Loss: 31.3169\n",
      "Processing batch 9306/11884 - Loss: 31.9846\n",
      "Processing batch 9307/11884 - Loss: 29.4771\n",
      "Processing batch 9308/11884 - Loss: 30.0578\n",
      "Processing batch 9309/11884 - Loss: 31.3221\n",
      "Processing batch 9310/11884 - Loss: 29.5666\n",
      "Processing batch 9311/11884 - Loss: 29.8449\n",
      "Processing batch 9312/11884 - Loss: 29.7555\n",
      "Processing batch 9313/11884 - Loss: 29.4727\n",
      "Processing batch 9314/11884 - Loss: 31.0710\n",
      "Processing batch 9315/11884 - Loss: 31.4908\n",
      "Processing batch 9316/11884 - Loss: 30.6599\n",
      "Processing batch 9317/11884 - Loss: 30.8848\n",
      "Processing batch 9318/11884 - Loss: 30.9733\n",
      "Processing batch 9319/11884 - Loss: 29.9129\n",
      "Processing batch 9320/11884 - Loss: 29.2452\n",
      "Processing batch 9321/11884 - Loss: 29.6855\n",
      "Processing batch 9322/11884 - Loss: 30.1722\n",
      "Processing batch 9323/11884 - Loss: 29.8305\n",
      "Processing batch 9324/11884 - Loss: 31.4314\n",
      "Processing batch 9325/11884 - Loss: 28.8360\n",
      "Processing batch 9326/11884 - Loss: 29.1554\n",
      "Processing batch 9327/11884 - Loss: 30.1897\n",
      "Processing batch 9328/11884 - Loss: 30.2688\n",
      "Processing batch 9329/11884 - Loss: 32.2639\n",
      "Processing batch 9330/11884 - Loss: 30.3977\n",
      "Processing batch 9331/11884 - Loss: 29.9921\n",
      "Processing batch 9332/11884 - Loss: 29.8228\n",
      "Processing batch 9333/11884 - Loss: 29.7929\n",
      "Processing batch 9334/11884 - Loss: 30.9707\n",
      "Processing batch 9335/11884 - Loss: 29.9397\n",
      "Processing batch 9336/11884 - Loss: 30.6792\n",
      "Processing batch 9337/11884 - Loss: 30.4394\n",
      "Processing batch 9338/11884 - Loss: 29.2723\n",
      "Processing batch 9339/11884 - Loss: 29.9140\n",
      "Processing batch 9340/11884 - Loss: 30.0358\n",
      "Processing batch 9341/11884 - Loss: 30.8364\n",
      "Processing batch 9342/11884 - Loss: 30.2552\n",
      "Processing batch 9343/11884 - Loss: 30.6721\n",
      "Processing batch 9344/11884 - Loss: 30.5847\n",
      "Processing batch 9345/11884 - Loss: 29.6664\n",
      "Processing batch 9346/11884 - Loss: 30.2370\n",
      "Processing batch 9347/11884 - Loss: 29.6401\n",
      "Processing batch 9348/11884 - Loss: 31.1718\n",
      "Processing batch 9349/11884 - Loss: 30.7790\n",
      "Processing batch 9350/11884 - Loss: 28.6956\n",
      "Processing batch 9351/11884 - Loss: 31.0103\n",
      "Processing batch 9352/11884 - Loss: 29.2746\n",
      "Processing batch 9353/11884 - Loss: 30.2287\n",
      "Processing batch 9354/11884 - Loss: 30.5470\n",
      "Processing batch 9355/11884 - Loss: 31.1336\n",
      "Processing batch 9356/11884 - Loss: 30.4488\n",
      "Processing batch 9357/11884 - Loss: 30.4256\n",
      "Processing batch 9358/11884 - Loss: 30.2678\n",
      "Processing batch 9359/11884 - Loss: 30.5793\n",
      "Processing batch 9360/11884 - Loss: 31.6861\n",
      "Processing batch 9361/11884 - Loss: 30.5476\n",
      "Processing batch 9362/11884 - Loss: 28.6336\n",
      "Processing batch 9363/11884 - Loss: 31.0752\n",
      "Processing batch 9364/11884 - Loss: 29.8763\n",
      "Processing batch 9365/11884 - Loss: 30.9662\n",
      "Processing batch 9366/11884 - Loss: 30.1905\n",
      "Processing batch 9367/11884 - Loss: 31.8876\n",
      "Processing batch 9368/11884 - Loss: 29.0597\n",
      "Processing batch 9369/11884 - Loss: 30.9004\n",
      "Processing batch 9370/11884 - Loss: 30.2855\n",
      "Processing batch 9371/11884 - Loss: 30.0518\n",
      "Processing batch 9372/11884 - Loss: 29.8573\n",
      "Processing batch 9373/11884 - Loss: 31.3732\n",
      "Processing batch 9374/11884 - Loss: 29.4269\n",
      "Processing batch 9375/11884 - Loss: 31.1114\n",
      "Processing batch 9376/11884 - Loss: 31.5762\n",
      "Processing batch 9377/11884 - Loss: 30.4413\n",
      "Processing batch 9378/11884 - Loss: 29.6998\n",
      "Processing batch 9379/11884 - Loss: 30.7223\n",
      "Processing batch 9380/11884 - Loss: 30.1212\n",
      "Processing batch 9381/11884 - Loss: 30.9266\n",
      "Processing batch 9382/11884 - Loss: 30.8410\n",
      "Processing batch 9383/11884 - Loss: 31.3387\n",
      "Processing batch 9384/11884 - Loss: 30.2072\n",
      "Processing batch 9385/11884 - Loss: 29.2608\n",
      "Processing batch 9386/11884 - Loss: 29.4460\n",
      "Processing batch 9387/11884 - Loss: 29.3351\n",
      "Processing batch 9388/11884 - Loss: 29.2698\n",
      "Processing batch 9389/11884 - Loss: 31.4421\n",
      "Processing batch 9390/11884 - Loss: 29.0419\n",
      "Processing batch 9391/11884 - Loss: 30.6761\n",
      "Processing batch 9392/11884 - Loss: 30.9522\n",
      "Processing batch 9393/11884 - Loss: 31.1985\n",
      "Processing batch 9394/11884 - Loss: 29.9200\n",
      "Processing batch 9395/11884 - Loss: 29.9557\n",
      "Processing batch 9396/11884 - Loss: 31.9505\n",
      "Processing batch 9397/11884 - Loss: 28.5160\n",
      "Processing batch 9398/11884 - Loss: 30.3781\n",
      "Processing batch 9399/11884 - Loss: 30.1412\n",
      "Processing batch 9400/11884 - Loss: 31.1770\n",
      "Processing batch 9401/11884 - Loss: 30.5217\n",
      "Processing batch 9402/11884 - Loss: 29.9522\n",
      "Processing batch 9403/11884 - Loss: 30.7435\n",
      "Processing batch 9404/11884 - Loss: 31.6175\n",
      "Processing batch 9405/11884 - Loss: 29.3489\n",
      "Processing batch 9406/11884 - Loss: 31.6247\n",
      "Processing batch 9407/11884 - Loss: 31.2257\n",
      "Processing batch 9408/11884 - Loss: 30.5412\n",
      "Processing batch 9409/11884 - Loss: 30.0546\n",
      "Processing batch 9410/11884 - Loss: 30.6485\n",
      "Processing batch 9411/11884 - Loss: 30.2637\n",
      "Processing batch 9412/11884 - Loss: 31.5251\n",
      "Processing batch 9413/11884 - Loss: 30.8881\n",
      "Processing batch 9414/11884 - Loss: 28.9421\n",
      "Processing batch 9415/11884 - Loss: 31.1348\n",
      "Processing batch 9416/11884 - Loss: 30.1808\n",
      "Processing batch 9417/11884 - Loss: 30.4022\n",
      "Processing batch 9418/11884 - Loss: 30.7055\n",
      "Processing batch 9419/11884 - Loss: 30.5571\n",
      "Processing batch 9420/11884 - Loss: 29.9523\n",
      "Processing batch 9421/11884 - Loss: 31.6511\n",
      "Processing batch 9422/11884 - Loss: 29.8220\n",
      "Processing batch 9423/11884 - Loss: 28.7536\n",
      "Processing batch 9424/11884 - Loss: 29.7867\n",
      "Processing batch 9425/11884 - Loss: 29.8012\n",
      "Processing batch 9426/11884 - Loss: 29.4079\n",
      "Processing batch 9427/11884 - Loss: 30.1308\n",
      "Processing batch 9428/11884 - Loss: 30.1801\n",
      "Processing batch 9429/11884 - Loss: 30.2909\n",
      "Processing batch 9430/11884 - Loss: 30.7587\n",
      "Processing batch 9431/11884 - Loss: 29.9025\n",
      "Processing batch 9432/11884 - Loss: 29.8126\n",
      "Processing batch 9433/11884 - Loss: 30.4426\n",
      "Processing batch 9434/11884 - Loss: 30.4358\n",
      "Processing batch 9435/11884 - Loss: 30.4211\n",
      "Processing batch 9436/11884 - Loss: 29.7719\n",
      "Processing batch 9437/11884 - Loss: 29.2913\n",
      "Processing batch 9438/11884 - Loss: 30.3181\n",
      "Processing batch 9439/11884 - Loss: 29.5086\n",
      "Processing batch 9440/11884 - Loss: 31.7751\n",
      "Processing batch 9441/11884 - Loss: 29.5861\n",
      "Processing batch 9442/11884 - Loss: 29.3779\n",
      "Processing batch 9443/11884 - Loss: 31.3400\n",
      "Processing batch 9444/11884 - Loss: 30.5369\n",
      "Processing batch 9445/11884 - Loss: 32.8052\n",
      "Processing batch 9446/11884 - Loss: 30.2289\n",
      "Processing batch 9447/11884 - Loss: 30.8644\n",
      "Processing batch 9448/11884 - Loss: 31.4440\n",
      "Processing batch 9449/11884 - Loss: 31.6780\n",
      "Processing batch 9450/11884 - Loss: 29.9455\n",
      "Processing batch 9451/11884 - Loss: 30.0999\n",
      "Processing batch 9452/11884 - Loss: 29.7676\n",
      "Processing batch 9453/11884 - Loss: 30.6623\n",
      "Processing batch 9454/11884 - Loss: 30.7184\n",
      "Processing batch 9455/11884 - Loss: 30.9241\n",
      "Processing batch 9456/11884 - Loss: 31.1294\n",
      "Processing batch 9457/11884 - Loss: 30.0844\n",
      "Processing batch 9458/11884 - Loss: 29.8698\n",
      "Processing batch 9459/11884 - Loss: 31.2091\n",
      "Processing batch 9460/11884 - Loss: 29.3482\n",
      "Processing batch 9461/11884 - Loss: 30.4844\n",
      "Processing batch 9462/11884 - Loss: 29.9288\n",
      "Processing batch 9463/11884 - Loss: 30.8471\n",
      "Processing batch 9464/11884 - Loss: 31.2437\n",
      "Processing batch 9465/11884 - Loss: 30.8424\n",
      "Processing batch 9466/11884 - Loss: 30.0223\n",
      "Processing batch 9467/11884 - Loss: 31.8169\n",
      "Processing batch 9468/11884 - Loss: 30.2004\n",
      "Processing batch 9469/11884 - Loss: 29.4094\n",
      "Processing batch 9470/11884 - Loss: 29.2460\n",
      "Processing batch 9471/11884 - Loss: 30.0423\n",
      "Processing batch 9472/11884 - Loss: 29.9532\n",
      "Processing batch 9473/11884 - Loss: 30.5069\n",
      "Processing batch 9474/11884 - Loss: 30.6503\n",
      "Processing batch 9475/11884 - Loss: 28.5491\n",
      "Processing batch 9476/11884 - Loss: 30.3696\n",
      "Processing batch 9477/11884 - Loss: 29.4990\n",
      "Processing batch 9478/11884 - Loss: 30.8477\n",
      "Processing batch 9479/11884 - Loss: 29.8921\n",
      "Processing batch 9480/11884 - Loss: 29.4075\n",
      "Processing batch 9481/11884 - Loss: 31.0956\n",
      "Processing batch 9482/11884 - Loss: 30.6664\n",
      "Processing batch 9483/11884 - Loss: 31.0321\n",
      "Processing batch 9484/11884 - Loss: 30.1249\n",
      "Processing batch 9485/11884 - Loss: 30.5412\n",
      "Processing batch 9486/11884 - Loss: 30.4057\n",
      "Processing batch 9487/11884 - Loss: 31.9158\n",
      "Processing batch 9488/11884 - Loss: 30.3041\n",
      "Processing batch 9489/11884 - Loss: 28.3511\n",
      "Processing batch 9490/11884 - Loss: 29.8517\n",
      "Processing batch 9491/11884 - Loss: 29.9070\n",
      "Processing batch 9492/11884 - Loss: 30.0501\n",
      "Processing batch 9493/11884 - Loss: 30.4720\n",
      "Processing batch 9494/11884 - Loss: 30.5064\n",
      "Processing batch 9495/11884 - Loss: 29.0160\n",
      "Processing batch 9496/11884 - Loss: 31.0053\n",
      "Processing batch 9497/11884 - Loss: 31.0613\n",
      "Processing batch 9498/11884 - Loss: 31.0419\n",
      "Processing batch 9499/11884 - Loss: 30.5570\n",
      "Processing batch 9500/11884 - Loss: 29.9627\n",
      "Processing batch 9501/11884 - Loss: 29.0744\n",
      "Processing batch 9502/11884 - Loss: 29.4405\n",
      "Processing batch 9503/11884 - Loss: 28.4356\n",
      "Processing batch 9504/11884 - Loss: 29.9296\n",
      "Processing batch 9505/11884 - Loss: 28.4022\n",
      "Processing batch 9506/11884 - Loss: 29.9251\n",
      "Processing batch 9507/11884 - Loss: 30.4684\n",
      "Processing batch 9508/11884 - Loss: 29.3363\n",
      "Processing batch 9509/11884 - Loss: 29.1938\n",
      "Processing batch 9510/11884 - Loss: 30.2977\n",
      "Processing batch 9511/11884 - Loss: 30.7788\n",
      "Processing batch 9512/11884 - Loss: 29.6856\n",
      "Processing batch 9513/11884 - Loss: 30.4943\n",
      "Processing batch 9514/11884 - Loss: 30.7532\n",
      "Processing batch 9515/11884 - Loss: 29.0789\n",
      "Processing batch 9516/11884 - Loss: 30.4079\n",
      "Processing batch 9517/11884 - Loss: 31.7988\n",
      "Processing batch 9518/11884 - Loss: 29.9104\n",
      "Processing batch 9519/11884 - Loss: 30.4990\n",
      "Processing batch 9520/11884 - Loss: 29.1291\n",
      "Processing batch 9521/11884 - Loss: 29.7933\n",
      "Processing batch 9522/11884 - Loss: 29.5895\n",
      "Processing batch 9523/11884 - Loss: 30.8760\n",
      "Processing batch 9524/11884 - Loss: 31.3078\n",
      "Processing batch 9525/11884 - Loss: 31.0171\n",
      "Processing batch 9526/11884 - Loss: 30.3495\n",
      "Processing batch 9527/11884 - Loss: 30.0476\n",
      "Processing batch 9528/11884 - Loss: 30.5131\n",
      "Processing batch 9529/11884 - Loss: 32.8035\n",
      "Processing batch 9530/11884 - Loss: 30.7304\n",
      "Processing batch 9531/11884 - Loss: 30.0751\n",
      "Processing batch 9532/11884 - Loss: 32.2826\n",
      "Processing batch 9533/11884 - Loss: 31.0592\n",
      "Processing batch 9534/11884 - Loss: 29.9461\n",
      "Processing batch 9535/11884 - Loss: 28.9502\n",
      "Processing batch 9536/11884 - Loss: 30.1514\n",
      "Processing batch 9537/11884 - Loss: 31.1138\n",
      "Processing batch 9538/11884 - Loss: 29.0721\n",
      "Processing batch 9539/11884 - Loss: 30.5386\n",
      "Processing batch 9540/11884 - Loss: 30.2937\n",
      "Processing batch 9541/11884 - Loss: 29.7900\n",
      "Processing batch 9542/11884 - Loss: 29.8059\n",
      "Processing batch 9543/11884 - Loss: 28.4176\n",
      "Processing batch 9544/11884 - Loss: 31.4222\n",
      "Processing batch 9545/11884 - Loss: 29.6506\n",
      "Processing batch 9546/11884 - Loss: 30.3384\n",
      "Processing batch 9547/11884 - Loss: 28.9486\n",
      "Processing batch 9548/11884 - Loss: 31.0690\n",
      "Processing batch 9549/11884 - Loss: 31.1842\n",
      "Processing batch 9550/11884 - Loss: 29.3780\n",
      "Processing batch 9551/11884 - Loss: 29.9210\n",
      "Processing batch 9552/11884 - Loss: 30.8850\n",
      "Processing batch 9553/11884 - Loss: 30.0449\n",
      "Processing batch 9554/11884 - Loss: 30.6564\n",
      "Processing batch 9555/11884 - Loss: 31.4726\n",
      "Processing batch 9556/11884 - Loss: 28.3557\n",
      "Processing batch 9557/11884 - Loss: 30.7014\n",
      "Processing batch 9558/11884 - Loss: 30.5187\n",
      "Processing batch 9559/11884 - Loss: 30.0661\n",
      "Processing batch 9560/11884 - Loss: 29.0883\n",
      "Processing batch 9561/11884 - Loss: 30.8091\n",
      "Processing batch 9562/11884 - Loss: 31.0175\n",
      "Processing batch 9563/11884 - Loss: 31.5479\n",
      "Processing batch 9564/11884 - Loss: 29.3013\n",
      "Processing batch 9565/11884 - Loss: 29.8209\n",
      "Processing batch 9566/11884 - Loss: 30.6408\n",
      "Processing batch 9567/11884 - Loss: 29.2780\n",
      "Processing batch 9568/11884 - Loss: 31.5169\n",
      "Processing batch 9569/11884 - Loss: 29.9398\n",
      "Processing batch 9570/11884 - Loss: 30.4970\n",
      "Processing batch 9571/11884 - Loss: 31.1761\n",
      "Processing batch 9572/11884 - Loss: 30.5463\n",
      "Processing batch 9573/11884 - Loss: 29.6716\n",
      "Processing batch 9574/11884 - Loss: 30.7042\n",
      "Processing batch 9575/11884 - Loss: 31.2785\n",
      "Processing batch 9576/11884 - Loss: 30.0091\n",
      "Processing batch 9577/11884 - Loss: 30.8063\n",
      "Processing batch 9578/11884 - Loss: 30.5259\n",
      "Processing batch 9579/11884 - Loss: 30.3075\n",
      "Processing batch 9580/11884 - Loss: 29.9504\n",
      "Processing batch 9581/11884 - Loss: 30.7276\n",
      "Processing batch 9582/11884 - Loss: 30.7539\n",
      "Processing batch 9583/11884 - Loss: 29.8949\n",
      "Processing batch 9584/11884 - Loss: 30.3498\n",
      "Processing batch 9585/11884 - Loss: 31.3029\n",
      "Processing batch 9586/11884 - Loss: 29.6553\n",
      "Processing batch 9587/11884 - Loss: 30.6717\n",
      "Processing batch 9588/11884 - Loss: 31.0533\n",
      "Processing batch 9589/11884 - Loss: 30.9664\n",
      "Processing batch 9590/11884 - Loss: 29.2769\n",
      "Processing batch 9591/11884 - Loss: 28.2621\n",
      "Processing batch 9592/11884 - Loss: 30.6583\n",
      "Processing batch 9593/11884 - Loss: 29.7361\n",
      "Processing batch 9594/11884 - Loss: 30.7046\n",
      "Processing batch 9595/11884 - Loss: 30.4080\n",
      "Processing batch 9596/11884 - Loss: 30.7702\n",
      "Processing batch 9597/11884 - Loss: 31.5084\n",
      "Processing batch 9598/11884 - Loss: 30.7030\n",
      "Processing batch 9599/11884 - Loss: 30.6909\n",
      "Processing batch 9600/11884 - Loss: 31.5973\n",
      "Processing batch 9601/11884 - Loss: 31.6352\n",
      "Processing batch 9602/11884 - Loss: 31.1927\n",
      "Processing batch 9603/11884 - Loss: 31.0472\n",
      "Processing batch 9604/11884 - Loss: 31.2676\n",
      "Processing batch 9605/11884 - Loss: 30.7841\n",
      "Processing batch 9606/11884 - Loss: 30.2232\n",
      "Processing batch 9607/11884 - Loss: 31.6248\n",
      "Processing batch 9608/11884 - Loss: 30.6482\n",
      "Processing batch 9609/11884 - Loss: 29.5594\n",
      "Processing batch 9610/11884 - Loss: 31.5840\n",
      "Processing batch 9611/11884 - Loss: 30.4673\n",
      "Processing batch 9612/11884 - Loss: 31.9840\n",
      "Processing batch 9613/11884 - Loss: 29.6720\n",
      "Processing batch 9614/11884 - Loss: 30.5598\n",
      "Processing batch 9615/11884 - Loss: 31.3369\n",
      "Processing batch 9616/11884 - Loss: 30.7167\n",
      "Processing batch 9617/11884 - Loss: 31.1881\n",
      "Processing batch 9618/11884 - Loss: 29.8250\n",
      "Processing batch 9619/11884 - Loss: 29.8497\n",
      "Processing batch 9620/11884 - Loss: 29.5925\n",
      "Processing batch 9621/11884 - Loss: 31.2935\n",
      "Processing batch 9622/11884 - Loss: 30.9887\n",
      "Processing batch 9623/11884 - Loss: 30.0342\n",
      "Processing batch 9624/11884 - Loss: 30.6022\n",
      "Processing batch 9625/11884 - Loss: 29.5301\n",
      "Processing batch 9626/11884 - Loss: 31.4336\n",
      "Processing batch 9627/11884 - Loss: 30.8587\n",
      "Processing batch 9628/11884 - Loss: 29.4120\n",
      "Processing batch 9629/11884 - Loss: 29.7337\n",
      "Processing batch 9630/11884 - Loss: 30.4020\n",
      "Processing batch 9631/11884 - Loss: 29.3622\n",
      "Processing batch 9632/11884 - Loss: 31.2389\n",
      "Processing batch 9633/11884 - Loss: 30.7042\n",
      "Processing batch 9634/11884 - Loss: 30.8842\n",
      "Processing batch 9635/11884 - Loss: 28.7753\n",
      "Processing batch 9636/11884 - Loss: 29.9745\n",
      "Processing batch 9637/11884 - Loss: 30.7904\n",
      "Processing batch 9638/11884 - Loss: 31.0125\n",
      "Processing batch 9639/11884 - Loss: 30.0501\n",
      "Processing batch 9640/11884 - Loss: 30.7683\n",
      "Processing batch 9641/11884 - Loss: 30.3037\n",
      "Processing batch 9642/11884 - Loss: 29.6292\n",
      "Processing batch 9643/11884 - Loss: 30.3301\n",
      "Processing batch 9644/11884 - Loss: 31.4082\n",
      "Processing batch 9645/11884 - Loss: 29.4533\n",
      "Processing batch 9646/11884 - Loss: 30.1322\n",
      "Processing batch 9647/11884 - Loss: 29.8794\n",
      "Processing batch 9648/11884 - Loss: 28.7181\n",
      "Processing batch 9649/11884 - Loss: 30.0282\n",
      "Processing batch 9650/11884 - Loss: 29.7206\n",
      "Processing batch 9651/11884 - Loss: 30.4313\n",
      "Processing batch 9652/11884 - Loss: 31.0216\n",
      "Processing batch 9653/11884 - Loss: 30.2798\n",
      "Processing batch 9654/11884 - Loss: 30.4132\n",
      "Processing batch 9655/11884 - Loss: 30.5012\n",
      "Processing batch 9656/11884 - Loss: 30.6001\n",
      "Processing batch 9657/11884 - Loss: 30.8105\n",
      "Processing batch 9658/11884 - Loss: 30.8829\n",
      "Processing batch 9659/11884 - Loss: 29.6194\n",
      "Processing batch 9660/11884 - Loss: 31.3703\n",
      "Processing batch 9661/11884 - Loss: 31.1166\n",
      "Processing batch 9662/11884 - Loss: 30.2729\n",
      "Processing batch 9663/11884 - Loss: 29.7572\n",
      "Processing batch 9664/11884 - Loss: 30.6744\n",
      "Processing batch 9665/11884 - Loss: 29.4065\n",
      "Processing batch 9666/11884 - Loss: 30.4712\n",
      "Processing batch 9667/11884 - Loss: 29.8803\n",
      "Processing batch 9668/11884 - Loss: 29.8148\n",
      "Processing batch 9669/11884 - Loss: 32.0752\n",
      "Processing batch 9670/11884 - Loss: 30.5403\n",
      "Processing batch 9671/11884 - Loss: 30.1282\n",
      "Processing batch 9672/11884 - Loss: 31.0962\n",
      "Processing batch 9673/11884 - Loss: 30.7908\n",
      "Processing batch 9674/11884 - Loss: 29.6476\n",
      "Processing batch 9675/11884 - Loss: 30.9876\n",
      "Processing batch 9676/11884 - Loss: 29.2684\n",
      "Processing batch 9677/11884 - Loss: 29.6399\n",
      "Processing batch 9678/11884 - Loss: 30.7886\n",
      "Processing batch 9679/11884 - Loss: 30.2809\n",
      "Processing batch 9680/11884 - Loss: 30.3494\n",
      "Processing batch 9681/11884 - Loss: 31.2868\n",
      "Processing batch 9682/11884 - Loss: 31.0434\n",
      "Processing batch 9683/11884 - Loss: 30.2192\n",
      "Processing batch 9684/11884 - Loss: 31.6851\n",
      "Processing batch 9685/11884 - Loss: 29.6906\n",
      "Processing batch 9686/11884 - Loss: 30.5179\n",
      "Processing batch 9687/11884 - Loss: 31.0564\n",
      "Processing batch 9688/11884 - Loss: 30.6436\n",
      "Processing batch 9689/11884 - Loss: 30.9501\n",
      "Processing batch 9690/11884 - Loss: 30.8590\n",
      "Processing batch 9691/11884 - Loss: 30.0053\n",
      "Processing batch 9692/11884 - Loss: 30.3424\n",
      "Processing batch 9693/11884 - Loss: 31.9344\n",
      "Processing batch 9694/11884 - Loss: 31.5849\n",
      "Processing batch 9695/11884 - Loss: 30.2541\n",
      "Processing batch 9696/11884 - Loss: 30.6205\n",
      "Processing batch 9697/11884 - Loss: 28.9788\n",
      "Processing batch 9698/11884 - Loss: 30.5287\n",
      "Processing batch 9699/11884 - Loss: 30.4123\n",
      "Processing batch 9700/11884 - Loss: 29.0824\n",
      "Processing batch 9701/11884 - Loss: 29.3154\n",
      "Processing batch 9702/11884 - Loss: 31.4324\n",
      "Processing batch 9703/11884 - Loss: 30.6164\n",
      "Processing batch 9704/11884 - Loss: 30.0529\n",
      "Processing batch 9705/11884 - Loss: 30.9395\n",
      "Processing batch 9706/11884 - Loss: 31.3481\n",
      "Processing batch 9707/11884 - Loss: 29.6393\n",
      "Processing batch 9708/11884 - Loss: 30.6194\n",
      "Processing batch 9709/11884 - Loss: 30.6992\n",
      "Processing batch 9710/11884 - Loss: 30.3403\n",
      "Processing batch 9711/11884 - Loss: 31.3864\n",
      "Processing batch 9712/11884 - Loss: 30.1808\n",
      "Processing batch 9713/11884 - Loss: 30.1966\n",
      "Processing batch 9714/11884 - Loss: 30.1890\n",
      "Processing batch 9715/11884 - Loss: 30.9350\n",
      "Processing batch 9716/11884 - Loss: 30.9750\n",
      "Processing batch 9717/11884 - Loss: 30.5369\n",
      "Processing batch 9718/11884 - Loss: 31.3720\n",
      "Processing batch 9719/11884 - Loss: 30.2880\n",
      "Processing batch 9720/11884 - Loss: 29.9306\n",
      "Processing batch 9721/11884 - Loss: 29.7988\n",
      "Processing batch 9722/11884 - Loss: 30.4454\n",
      "Processing batch 9723/11884 - Loss: 30.1787\n",
      "Processing batch 9724/11884 - Loss: 30.2164\n",
      "Processing batch 9725/11884 - Loss: 29.8501\n",
      "Processing batch 9726/11884 - Loss: 29.8538\n",
      "Processing batch 9727/11884 - Loss: 30.5961\n",
      "Processing batch 9728/11884 - Loss: 30.3336\n",
      "Processing batch 9729/11884 - Loss: 29.3488\n",
      "Processing batch 9730/11884 - Loss: 29.8571\n",
      "Processing batch 9731/11884 - Loss: 28.9572\n",
      "Processing batch 9732/11884 - Loss: 30.9148\n",
      "Processing batch 9733/11884 - Loss: 31.1879\n",
      "Processing batch 9734/11884 - Loss: 30.9497\n",
      "Processing batch 9735/11884 - Loss: 30.9810\n",
      "Processing batch 9736/11884 - Loss: 30.6258\n",
      "Processing batch 9737/11884 - Loss: 30.9753\n",
      "Processing batch 9738/11884 - Loss: 29.7062\n",
      "Processing batch 9739/11884 - Loss: 30.8127\n",
      "Processing batch 9740/11884 - Loss: 29.4857\n",
      "Processing batch 9741/11884 - Loss: 29.8199\n",
      "Processing batch 9742/11884 - Loss: 30.0985\n",
      "Processing batch 9743/11884 - Loss: 31.6675\n",
      "Processing batch 9744/11884 - Loss: 29.0133\n",
      "Processing batch 9745/11884 - Loss: 30.1031\n",
      "Processing batch 9746/11884 - Loss: 30.2104\n",
      "Processing batch 9747/11884 - Loss: 30.3207\n",
      "Processing batch 9748/11884 - Loss: 30.9744\n",
      "Processing batch 9749/11884 - Loss: 31.2722\n",
      "Processing batch 9750/11884 - Loss: 31.1323\n",
      "Processing batch 9751/11884 - Loss: 31.3039\n",
      "Processing batch 9752/11884 - Loss: 30.0854\n",
      "Processing batch 9753/11884 - Loss: 30.4215\n",
      "Processing batch 9754/11884 - Loss: 30.0136\n",
      "Processing batch 9755/11884 - Loss: 31.5850\n",
      "Processing batch 9756/11884 - Loss: 31.4801\n",
      "Processing batch 9757/11884 - Loss: 31.7404\n",
      "Processing batch 9758/11884 - Loss: 31.2562\n",
      "Processing batch 9759/11884 - Loss: 31.1363\n",
      "Processing batch 9760/11884 - Loss: 31.2496\n",
      "Processing batch 9761/11884 - Loss: 31.9001\n",
      "Processing batch 9762/11884 - Loss: 30.7156\n",
      "Processing batch 9763/11884 - Loss: 30.5625\n",
      "Processing batch 9764/11884 - Loss: 30.5865\n",
      "Processing batch 9765/11884 - Loss: 29.7075\n",
      "Processing batch 9766/11884 - Loss: 30.4990\n",
      "Processing batch 9767/11884 - Loss: 30.2699\n",
      "Processing batch 9768/11884 - Loss: 30.1286\n",
      "Processing batch 9769/11884 - Loss: 30.8047\n",
      "Processing batch 9770/11884 - Loss: 30.9890\n",
      "Processing batch 9771/11884 - Loss: 29.3436\n",
      "Processing batch 9772/11884 - Loss: 29.9934\n",
      "Processing batch 9773/11884 - Loss: 30.3770\n",
      "Processing batch 9774/11884 - Loss: 30.1952\n",
      "Processing batch 9775/11884 - Loss: 31.3277\n",
      "Processing batch 9776/11884 - Loss: 30.4793\n",
      "Processing batch 9777/11884 - Loss: 30.4264\n",
      "Processing batch 9778/11884 - Loss: 31.5086\n",
      "Processing batch 9779/11884 - Loss: 31.8838\n",
      "Processing batch 9780/11884 - Loss: 29.7366\n",
      "Processing batch 9781/11884 - Loss: 29.8878\n",
      "Processing batch 9782/11884 - Loss: 29.1480\n",
      "Processing batch 9783/11884 - Loss: 28.9867\n",
      "Processing batch 9784/11884 - Loss: 29.2650\n",
      "Processing batch 9785/11884 - Loss: 30.7356\n",
      "Processing batch 9786/11884 - Loss: 30.6320\n",
      "Processing batch 9787/11884 - Loss: 30.2599\n",
      "Processing batch 9788/11884 - Loss: 28.9062\n",
      "Processing batch 9789/11884 - Loss: 30.4090\n",
      "Processing batch 9790/11884 - Loss: 31.1290\n",
      "Processing batch 9791/11884 - Loss: 29.8187\n",
      "Processing batch 9792/11884 - Loss: 30.5514\n",
      "Processing batch 9793/11884 - Loss: 31.1235\n",
      "Processing batch 9794/11884 - Loss: 31.0588\n",
      "Processing batch 9795/11884 - Loss: 29.5040\n",
      "Processing batch 9796/11884 - Loss: 29.0345\n",
      "Processing batch 9797/11884 - Loss: 29.3130\n",
      "Processing batch 9798/11884 - Loss: 31.2239\n",
      "Processing batch 9799/11884 - Loss: 29.7492\n",
      "Processing batch 9800/11884 - Loss: 30.6511\n",
      "Processing batch 9801/11884 - Loss: 30.9263\n",
      "Processing batch 9802/11884 - Loss: 31.4903\n",
      "Processing batch 9803/11884 - Loss: 30.0715\n",
      "Processing batch 9804/11884 - Loss: 30.3557\n",
      "Processing batch 9805/11884 - Loss: 30.0555\n",
      "Processing batch 9806/11884 - Loss: 29.6742\n",
      "Processing batch 9807/11884 - Loss: 30.6273\n",
      "Processing batch 9808/11884 - Loss: 31.0070\n",
      "Processing batch 9809/11884 - Loss: 30.7711\n",
      "Processing batch 9810/11884 - Loss: 29.7477\n",
      "Processing batch 9811/11884 - Loss: 30.1410\n",
      "Processing batch 9812/11884 - Loss: 31.2149\n",
      "Processing batch 9813/11884 - Loss: 31.3984\n",
      "Processing batch 9814/11884 - Loss: 29.5638\n",
      "Processing batch 9815/11884 - Loss: 31.6924\n",
      "Processing batch 9816/11884 - Loss: 30.0257\n",
      "Processing batch 9817/11884 - Loss: 31.8314\n",
      "Processing batch 9818/11884 - Loss: 29.4906\n",
      "Processing batch 9819/11884 - Loss: 29.7349\n",
      "Processing batch 9820/11884 - Loss: 28.6197\n",
      "Processing batch 9821/11884 - Loss: 30.9807\n",
      "Processing batch 9822/11884 - Loss: 31.7012\n",
      "Processing batch 9823/11884 - Loss: 30.0197\n",
      "Processing batch 9824/11884 - Loss: 31.4686\n",
      "Processing batch 9825/11884 - Loss: 30.7428\n",
      "Processing batch 9826/11884 - Loss: 29.5007\n",
      "Processing batch 9827/11884 - Loss: 29.9885\n",
      "Processing batch 9828/11884 - Loss: 30.1204\n",
      "Processing batch 9829/11884 - Loss: 29.3766\n",
      "Processing batch 9830/11884 - Loss: 30.8945\n",
      "Processing batch 9831/11884 - Loss: 31.3117\n",
      "Processing batch 9832/11884 - Loss: 29.9910\n",
      "Processing batch 9833/11884 - Loss: 29.2510\n",
      "Processing batch 9834/11884 - Loss: 28.1658\n",
      "Processing batch 9835/11884 - Loss: 31.0595\n",
      "Processing batch 9836/11884 - Loss: 30.9330\n",
      "Processing batch 9837/11884 - Loss: 29.8530\n",
      "Processing batch 9838/11884 - Loss: 32.2261\n",
      "Processing batch 9839/11884 - Loss: 30.7145\n",
      "Processing batch 9840/11884 - Loss: 31.4389\n",
      "Processing batch 9841/11884 - Loss: 30.1935\n",
      "Processing batch 9842/11884 - Loss: 29.5855\n",
      "Processing batch 9843/11884 - Loss: 29.8165\n",
      "Processing batch 9844/11884 - Loss: 29.8112\n",
      "Processing batch 9845/11884 - Loss: 28.7176\n",
      "Processing batch 9846/11884 - Loss: 30.9434\n",
      "Processing batch 9847/11884 - Loss: 31.6037\n",
      "Processing batch 9848/11884 - Loss: 31.4685\n",
      "Processing batch 9849/11884 - Loss: 30.9256\n",
      "Processing batch 9850/11884 - Loss: 30.3818\n",
      "Processing batch 9851/11884 - Loss: 31.2808\n",
      "Processing batch 9852/11884 - Loss: 30.3324\n",
      "Processing batch 9853/11884 - Loss: 29.7669\n",
      "Processing batch 9854/11884 - Loss: 30.4229\n",
      "Processing batch 9855/11884 - Loss: 31.7658\n",
      "Processing batch 9856/11884 - Loss: 29.6471\n",
      "Processing batch 9857/11884 - Loss: 30.8881\n",
      "Processing batch 9858/11884 - Loss: 31.1493\n",
      "Processing batch 9859/11884 - Loss: 29.5632\n",
      "Processing batch 9860/11884 - Loss: 29.2000\n",
      "Processing batch 9861/11884 - Loss: 29.3220\n",
      "Processing batch 9862/11884 - Loss: 29.1734\n",
      "Processing batch 9863/11884 - Loss: 29.7505\n",
      "Processing batch 9864/11884 - Loss: 30.3683\n",
      "Processing batch 9865/11884 - Loss: 31.4274\n",
      "Processing batch 9866/11884 - Loss: 29.9080\n",
      "Processing batch 9867/11884 - Loss: 30.1951\n",
      "Processing batch 9868/11884 - Loss: 31.6788\n",
      "Processing batch 9869/11884 - Loss: 30.1427\n",
      "Processing batch 9870/11884 - Loss: 30.0294\n",
      "Processing batch 9871/11884 - Loss: 31.1489\n",
      "Processing batch 9872/11884 - Loss: 29.5650\n",
      "Processing batch 9873/11884 - Loss: 30.1956\n",
      "Processing batch 9874/11884 - Loss: 29.9333\n",
      "Processing batch 9875/11884 - Loss: 31.4450\n",
      "Processing batch 9876/11884 - Loss: 30.6947\n",
      "Processing batch 9877/11884 - Loss: 30.0080\n",
      "Processing batch 9878/11884 - Loss: 31.0270\n",
      "Processing batch 9879/11884 - Loss: 29.5102\n",
      "Processing batch 9880/11884 - Loss: 30.3821\n",
      "Processing batch 9881/11884 - Loss: 29.5002\n",
      "Processing batch 9882/11884 - Loss: 29.6944\n",
      "Processing batch 9883/11884 - Loss: 32.0676\n",
      "Processing batch 9884/11884 - Loss: 31.0524\n",
      "Processing batch 9885/11884 - Loss: 29.9013\n",
      "Processing batch 9886/11884 - Loss: 28.9445\n",
      "Processing batch 9887/11884 - Loss: 30.4819\n",
      "Processing batch 9888/11884 - Loss: 29.9294\n",
      "Processing batch 9889/11884 - Loss: 28.6731\n",
      "Processing batch 9890/11884 - Loss: 29.5428\n",
      "Processing batch 9891/11884 - Loss: 30.7122\n",
      "Processing batch 9892/11884 - Loss: 29.5272\n",
      "Processing batch 9893/11884 - Loss: 31.4590\n",
      "Processing batch 9894/11884 - Loss: 30.8204\n",
      "Processing batch 9895/11884 - Loss: 30.2444\n",
      "Processing batch 9896/11884 - Loss: 30.3299\n",
      "Processing batch 9897/11884 - Loss: 29.6512\n",
      "Processing batch 9898/11884 - Loss: 30.2501\n",
      "Processing batch 9899/11884 - Loss: 29.7397\n",
      "Processing batch 9900/11884 - Loss: 30.3798\n",
      "Processing batch 9901/11884 - Loss: 30.2498\n",
      "Processing batch 9902/11884 - Loss: 30.5624\n",
      "Processing batch 9903/11884 - Loss: 31.2447\n",
      "Processing batch 9904/11884 - Loss: 29.6614\n",
      "Processing batch 9905/11884 - Loss: 31.0016\n",
      "Processing batch 9906/11884 - Loss: 30.7675\n",
      "Processing batch 9907/11884 - Loss: 30.5655\n",
      "Processing batch 9908/11884 - Loss: 30.6198\n",
      "Processing batch 9909/11884 - Loss: 29.1682\n",
      "Processing batch 9910/11884 - Loss: 30.2843\n",
      "Processing batch 9911/11884 - Loss: 30.1999\n",
      "Processing batch 9912/11884 - Loss: 31.7765\n",
      "Processing batch 9913/11884 - Loss: 30.6125\n",
      "Processing batch 9914/11884 - Loss: 29.3823\n",
      "Processing batch 9915/11884 - Loss: 29.9600\n",
      "Processing batch 9916/11884 - Loss: 30.3014\n",
      "Processing batch 9917/11884 - Loss: 29.9018\n",
      "Processing batch 9918/11884 - Loss: 30.8380\n",
      "Processing batch 9919/11884 - Loss: 30.8667\n",
      "Processing batch 9920/11884 - Loss: 31.0847\n",
      "Processing batch 9921/11884 - Loss: 30.9347\n",
      "Processing batch 9922/11884 - Loss: 31.1395\n",
      "Processing batch 9923/11884 - Loss: 30.5998\n",
      "Processing batch 9924/11884 - Loss: 30.7784\n",
      "Processing batch 9925/11884 - Loss: 29.5967\n",
      "Processing batch 9926/11884 - Loss: 30.0417\n",
      "Processing batch 9927/11884 - Loss: 30.0804\n",
      "Processing batch 9928/11884 - Loss: 30.0347\n",
      "Processing batch 9929/11884 - Loss: 29.5696\n",
      "Processing batch 9930/11884 - Loss: 29.7882\n",
      "Processing batch 9931/11884 - Loss: 30.4089\n",
      "Processing batch 9932/11884 - Loss: 29.0538\n",
      "Processing batch 9933/11884 - Loss: 28.7702\n",
      "Processing batch 9934/11884 - Loss: 30.7199\n",
      "Processing batch 9935/11884 - Loss: 31.1362\n",
      "Processing batch 9936/11884 - Loss: 30.2719\n",
      "Processing batch 9937/11884 - Loss: 29.5149\n",
      "Processing batch 9938/11884 - Loss: 29.0983\n",
      "Processing batch 9939/11884 - Loss: 29.6873\n",
      "Processing batch 9940/11884 - Loss: 29.2020\n",
      "Processing batch 9941/11884 - Loss: 31.1554\n",
      "Processing batch 9942/11884 - Loss: 30.8391\n",
      "Processing batch 9943/11884 - Loss: 30.3091\n",
      "Processing batch 9944/11884 - Loss: 30.0916\n",
      "Processing batch 9945/11884 - Loss: 30.2918\n",
      "Processing batch 9946/11884 - Loss: 29.3594\n",
      "Processing batch 9947/11884 - Loss: 30.7963\n",
      "Processing batch 9948/11884 - Loss: 30.4319\n",
      "Processing batch 9949/11884 - Loss: 31.0139\n",
      "Processing batch 9950/11884 - Loss: 31.1137\n",
      "Processing batch 9951/11884 - Loss: 30.1826\n",
      "Processing batch 9952/11884 - Loss: 29.8082\n",
      "Processing batch 9953/11884 - Loss: 30.7444\n",
      "Processing batch 9954/11884 - Loss: 29.5644\n",
      "Processing batch 9955/11884 - Loss: 29.8867\n",
      "Processing batch 9956/11884 - Loss: 29.4227\n",
      "Processing batch 9957/11884 - Loss: 32.0672\n",
      "Processing batch 9958/11884 - Loss: 30.9807\n",
      "Processing batch 9959/11884 - Loss: 29.8618\n",
      "Processing batch 9960/11884 - Loss: 27.4816\n",
      "Processing batch 9961/11884 - Loss: 30.5177\n",
      "Processing batch 9962/11884 - Loss: 30.3940\n",
      "Processing batch 9963/11884 - Loss: 30.5145\n",
      "Processing batch 9964/11884 - Loss: 30.6723\n",
      "Processing batch 9965/11884 - Loss: 30.4635\n",
      "Processing batch 9966/11884 - Loss: 29.5732\n",
      "Processing batch 9967/11884 - Loss: 30.7231\n",
      "Processing batch 9968/11884 - Loss: 31.5437\n",
      "Processing batch 9969/11884 - Loss: 30.4032\n",
      "Processing batch 9970/11884 - Loss: 30.0532\n",
      "Processing batch 9971/11884 - Loss: 29.6266\n",
      "Processing batch 9972/11884 - Loss: 30.0386\n",
      "Processing batch 9973/11884 - Loss: 28.5688\n",
      "Processing batch 9974/11884 - Loss: 29.5198\n",
      "Processing batch 9975/11884 - Loss: 30.1116\n",
      "Processing batch 9976/11884 - Loss: 30.8227\n",
      "Processing batch 9977/11884 - Loss: 29.8369\n",
      "Processing batch 9978/11884 - Loss: 30.2431\n",
      "Processing batch 9979/11884 - Loss: 30.6785\n",
      "Processing batch 9980/11884 - Loss: 30.7468\n",
      "Processing batch 9981/11884 - Loss: 29.4608\n",
      "Processing batch 9982/11884 - Loss: 31.7239\n",
      "Processing batch 9983/11884 - Loss: 31.4426\n",
      "Processing batch 9984/11884 - Loss: 29.3198\n",
      "Processing batch 9985/11884 - Loss: 30.2383\n",
      "Processing batch 9986/11884 - Loss: 30.4656\n",
      "Processing batch 9987/11884 - Loss: 30.9422\n",
      "Processing batch 9988/11884 - Loss: 29.3110\n",
      "Processing batch 9989/11884 - Loss: 30.8241\n",
      "Processing batch 9990/11884 - Loss: 30.0998\n",
      "Processing batch 9991/11884 - Loss: 30.2052\n",
      "Processing batch 9992/11884 - Loss: 29.8697\n",
      "Processing batch 9993/11884 - Loss: 31.4646\n",
      "Processing batch 9994/11884 - Loss: 30.0568\n",
      "Processing batch 9995/11884 - Loss: 31.8808\n",
      "Processing batch 9996/11884 - Loss: 29.9756\n",
      "Processing batch 9997/11884 - Loss: 29.5358\n",
      "Processing batch 9998/11884 - Loss: 30.8010\n",
      "Processing batch 9999/11884 - Loss: 29.9338\n",
      "Processing batch 10000/11884 - Loss: 29.9930\n",
      "Processing batch 10001/11884 - Loss: 29.5341\n",
      "Processing batch 10002/11884 - Loss: 30.2559\n",
      "Processing batch 10003/11884 - Loss: 31.1756\n",
      "Processing batch 10004/11884 - Loss: 30.0446\n",
      "Processing batch 10005/11884 - Loss: 29.8374\n",
      "Processing batch 10006/11884 - Loss: 29.1625\n",
      "Processing batch 10007/11884 - Loss: 28.7966\n",
      "Processing batch 10008/11884 - Loss: 31.0330\n",
      "Processing batch 10009/11884 - Loss: 30.7380\n",
      "Processing batch 10010/11884 - Loss: 30.1793\n",
      "Processing batch 10011/11884 - Loss: 29.5277\n",
      "Processing batch 10012/11884 - Loss: 30.6949\n",
      "Processing batch 10013/11884 - Loss: 29.9679\n",
      "Processing batch 10014/11884 - Loss: 29.7658\n",
      "Processing batch 10015/11884 - Loss: 30.2312\n",
      "Processing batch 10016/11884 - Loss: 29.6827\n",
      "Processing batch 10017/11884 - Loss: 30.5396\n",
      "Processing batch 10018/11884 - Loss: 31.6445\n",
      "Processing batch 10019/11884 - Loss: 30.5641\n",
      "Processing batch 10020/11884 - Loss: 30.1921\n",
      "Processing batch 10021/11884 - Loss: 31.0779\n",
      "Processing batch 10022/11884 - Loss: 31.0981\n",
      "Processing batch 10023/11884 - Loss: 30.1200\n",
      "Processing batch 10024/11884 - Loss: 29.6379\n",
      "Processing batch 10025/11884 - Loss: 30.0151\n",
      "Processing batch 10026/11884 - Loss: 32.5369\n",
      "Processing batch 10027/11884 - Loss: 30.0532\n",
      "Processing batch 10028/11884 - Loss: 29.5758\n",
      "Processing batch 10029/11884 - Loss: 29.7170\n",
      "Processing batch 10030/11884 - Loss: 30.4126\n",
      "Processing batch 10031/11884 - Loss: 30.1452\n",
      "Processing batch 10032/11884 - Loss: 29.1985\n",
      "Processing batch 10033/11884 - Loss: 29.2085\n",
      "Processing batch 10034/11884 - Loss: 31.6546\n",
      "Processing batch 10035/11884 - Loss: 30.2515\n",
      "Processing batch 10036/11884 - Loss: 31.0377\n",
      "Processing batch 10037/11884 - Loss: 30.9313\n",
      "Processing batch 10038/11884 - Loss: 30.3615\n",
      "Processing batch 10039/11884 - Loss: 30.6565\n",
      "Processing batch 10040/11884 - Loss: 32.3328\n",
      "Processing batch 10041/11884 - Loss: 29.3839\n",
      "Processing batch 10042/11884 - Loss: 29.5305\n",
      "Processing batch 10043/11884 - Loss: 30.3506\n",
      "Processing batch 10044/11884 - Loss: 30.5242\n",
      "Processing batch 10045/11884 - Loss: 30.9516\n",
      "Processing batch 10046/11884 - Loss: 31.1917\n",
      "Processing batch 10047/11884 - Loss: 29.8664\n",
      "Processing batch 10048/11884 - Loss: 28.8844\n",
      "Processing batch 10049/11884 - Loss: 29.8519\n",
      "Processing batch 10050/11884 - Loss: 29.1352\n",
      "Processing batch 10051/11884 - Loss: 29.2471\n",
      "Processing batch 10052/11884 - Loss: 30.0594\n",
      "Processing batch 10053/11884 - Loss: 31.0742\n",
      "Processing batch 10054/11884 - Loss: 30.2980\n",
      "Processing batch 10055/11884 - Loss: 30.6841\n",
      "Processing batch 10056/11884 - Loss: 30.6409\n",
      "Processing batch 10057/11884 - Loss: 29.8784\n",
      "Processing batch 10058/11884 - Loss: 29.6101\n",
      "Processing batch 10059/11884 - Loss: 30.6740\n",
      "Processing batch 10060/11884 - Loss: 30.0306\n",
      "Processing batch 10061/11884 - Loss: 30.9649\n",
      "Processing batch 10062/11884 - Loss: 31.3175\n",
      "Processing batch 10063/11884 - Loss: 29.5601\n",
      "Processing batch 10064/11884 - Loss: 30.3559\n",
      "Processing batch 10065/11884 - Loss: 30.8029\n",
      "Processing batch 10066/11884 - Loss: 30.0981\n",
      "Processing batch 10067/11884 - Loss: 30.4439\n",
      "Processing batch 10068/11884 - Loss: 30.8628\n",
      "Processing batch 10069/11884 - Loss: 30.9274\n",
      "Processing batch 10070/11884 - Loss: 30.2732\n",
      "Processing batch 10071/11884 - Loss: 29.3832\n",
      "Processing batch 10072/11884 - Loss: 29.3640\n",
      "Processing batch 10073/11884 - Loss: 30.4614\n",
      "Processing batch 10074/11884 - Loss: 31.2612\n",
      "Processing batch 10075/11884 - Loss: 30.0025\n",
      "Processing batch 10076/11884 - Loss: 29.7350\n",
      "Processing batch 10077/11884 - Loss: 31.6606\n",
      "Processing batch 10078/11884 - Loss: 29.1803\n",
      "Processing batch 10079/11884 - Loss: 31.1256\n",
      "Processing batch 10080/11884 - Loss: 29.0180\n",
      "Processing batch 10081/11884 - Loss: 31.2711\n",
      "Processing batch 10082/11884 - Loss: 30.1053\n",
      "Processing batch 10083/11884 - Loss: 29.4503\n",
      "Processing batch 10084/11884 - Loss: 29.2422\n",
      "Processing batch 10085/11884 - Loss: 30.2575\n",
      "Processing batch 10086/11884 - Loss: 31.8767\n",
      "Processing batch 10087/11884 - Loss: 29.8932\n",
      "Processing batch 10088/11884 - Loss: 29.5992\n",
      "Processing batch 10089/11884 - Loss: 29.4841\n",
      "Processing batch 10090/11884 - Loss: 30.5998\n",
      "Processing batch 10091/11884 - Loss: 31.1926\n",
      "Processing batch 10092/11884 - Loss: 29.3928\n",
      "Processing batch 10093/11884 - Loss: 30.7991\n",
      "Processing batch 10094/11884 - Loss: 28.8620\n",
      "Processing batch 10095/11884 - Loss: 29.9055\n",
      "Processing batch 10096/11884 - Loss: 31.0577\n",
      "Processing batch 10097/11884 - Loss: 31.3754\n",
      "Processing batch 10098/11884 - Loss: 30.0668\n",
      "Processing batch 10099/11884 - Loss: 29.2482\n",
      "Processing batch 10100/11884 - Loss: 30.0868\n",
      "Processing batch 10101/11884 - Loss: 30.1039\n",
      "Processing batch 10102/11884 - Loss: 29.3108\n",
      "Processing batch 10103/11884 - Loss: 30.4923\n",
      "Processing batch 10104/11884 - Loss: 31.4150\n",
      "Processing batch 10105/11884 - Loss: 30.5077\n",
      "Processing batch 10106/11884 - Loss: 30.3980\n",
      "Processing batch 10107/11884 - Loss: 29.8406\n",
      "Processing batch 10108/11884 - Loss: 31.0767\n",
      "Processing batch 10109/11884 - Loss: 30.3643\n",
      "Processing batch 10110/11884 - Loss: 30.8802\n",
      "Processing batch 10111/11884 - Loss: 29.4672\n",
      "Processing batch 10112/11884 - Loss: 30.3341\n",
      "Processing batch 10113/11884 - Loss: 30.2838\n",
      "Processing batch 10114/11884 - Loss: 30.4915\n",
      "Processing batch 10115/11884 - Loss: 30.1033\n",
      "Processing batch 10116/11884 - Loss: 30.2745\n",
      "Processing batch 10117/11884 - Loss: 30.7999\n",
      "Processing batch 10118/11884 - Loss: 29.9897\n",
      "Processing batch 10119/11884 - Loss: 30.7773\n",
      "Processing batch 10120/11884 - Loss: 31.2420\n",
      "Processing batch 10121/11884 - Loss: 29.7678\n",
      "Processing batch 10122/11884 - Loss: 30.2907\n",
      "Processing batch 10123/11884 - Loss: 30.5644\n",
      "Processing batch 10124/11884 - Loss: 30.2781\n",
      "Processing batch 10125/11884 - Loss: 30.4196\n",
      "Processing batch 10126/11884 - Loss: 30.2610\n",
      "Processing batch 10127/11884 - Loss: 29.8194\n",
      "Processing batch 10128/11884 - Loss: 30.5764\n",
      "Processing batch 10129/11884 - Loss: 30.9436\n",
      "Processing batch 10130/11884 - Loss: 29.8533\n",
      "Processing batch 10131/11884 - Loss: 30.9665\n",
      "Processing batch 10132/11884 - Loss: 30.0246\n",
      "Processing batch 10133/11884 - Loss: 29.7301\n",
      "Processing batch 10134/11884 - Loss: 31.4686\n",
      "Processing batch 10135/11884 - Loss: 30.3084\n",
      "Processing batch 10136/11884 - Loss: 30.2032\n",
      "Processing batch 10137/11884 - Loss: 29.3633\n",
      "Processing batch 10138/11884 - Loss: 30.7093\n",
      "Processing batch 10139/11884 - Loss: 30.6243\n",
      "Processing batch 10140/11884 - Loss: 30.3444\n",
      "Processing batch 10141/11884 - Loss: 30.3745\n",
      "Processing batch 10142/11884 - Loss: 30.9104\n",
      "Processing batch 10143/11884 - Loss: 31.7905\n",
      "Processing batch 10144/11884 - Loss: 30.1845\n",
      "Processing batch 10145/11884 - Loss: 30.5051\n",
      "Processing batch 10146/11884 - Loss: 30.4743\n",
      "Processing batch 10147/11884 - Loss: 30.5333\n",
      "Processing batch 10148/11884 - Loss: 30.5074\n",
      "Processing batch 10149/11884 - Loss: 29.8661\n",
      "Processing batch 10150/11884 - Loss: 29.0592\n",
      "Processing batch 10151/11884 - Loss: 29.1104\n",
      "Processing batch 10152/11884 - Loss: 29.1562\n",
      "Processing batch 10153/11884 - Loss: 29.9684\n",
      "Processing batch 10154/11884 - Loss: 31.7563\n",
      "Processing batch 10155/11884 - Loss: 29.6854\n",
      "Processing batch 10156/11884 - Loss: 29.9114\n",
      "Processing batch 10157/11884 - Loss: 28.9620\n",
      "Processing batch 10158/11884 - Loss: 29.9035\n",
      "Processing batch 10159/11884 - Loss: 32.1356\n",
      "Processing batch 10160/11884 - Loss: 31.0731\n",
      "Processing batch 10161/11884 - Loss: 30.9671\n",
      "Processing batch 10162/11884 - Loss: 29.0502\n",
      "Processing batch 10163/11884 - Loss: 29.7100\n",
      "Processing batch 10164/11884 - Loss: 32.2407\n",
      "Processing batch 10165/11884 - Loss: 31.2962\n",
      "Processing batch 10166/11884 - Loss: 31.4378\n",
      "Processing batch 10167/11884 - Loss: 31.1749\n",
      "Processing batch 10168/11884 - Loss: 30.4169\n",
      "Processing batch 10169/11884 - Loss: 30.2684\n",
      "Processing batch 10170/11884 - Loss: 29.5501\n",
      "Processing batch 10171/11884 - Loss: 30.2708\n",
      "Processing batch 10172/11884 - Loss: 29.7254\n",
      "Processing batch 10173/11884 - Loss: 31.0118\n",
      "Processing batch 10174/11884 - Loss: 30.9083\n",
      "Processing batch 10175/11884 - Loss: 29.0215\n",
      "Processing batch 10176/11884 - Loss: 30.0949\n",
      "Processing batch 10177/11884 - Loss: 29.6348\n",
      "Processing batch 10178/11884 - Loss: 29.5719\n",
      "Processing batch 10179/11884 - Loss: 30.9434\n",
      "Processing batch 10180/11884 - Loss: 29.4066\n",
      "Processing batch 10181/11884 - Loss: 31.0942\n",
      "Processing batch 10182/11884 - Loss: 30.4510\n",
      "Processing batch 10183/11884 - Loss: 30.5335\n",
      "Processing batch 10184/11884 - Loss: 30.0303\n",
      "Processing batch 10185/11884 - Loss: 30.6357\n",
      "Processing batch 10186/11884 - Loss: 31.1535\n",
      "Processing batch 10187/11884 - Loss: 30.1906\n",
      "Processing batch 10188/11884 - Loss: 29.5921\n",
      "Processing batch 10189/11884 - Loss: 30.1695\n",
      "Processing batch 10190/11884 - Loss: 30.6152\n",
      "Processing batch 10191/11884 - Loss: 30.2435\n",
      "Processing batch 10192/11884 - Loss: 29.4872\n",
      "Processing batch 10193/11884 - Loss: 29.9817\n",
      "Processing batch 10194/11884 - Loss: 31.7974\n",
      "Processing batch 10195/11884 - Loss: 31.1956\n",
      "Processing batch 10196/11884 - Loss: 30.0059\n",
      "Processing batch 10197/11884 - Loss: 29.7029\n",
      "Processing batch 10198/11884 - Loss: 31.5670\n",
      "Processing batch 10199/11884 - Loss: 30.5934\n",
      "Processing batch 10200/11884 - Loss: 29.0199\n",
      "Processing batch 10201/11884 - Loss: 30.7517\n",
      "Processing batch 10202/11884 - Loss: 29.9098\n",
      "Processing batch 10203/11884 - Loss: 31.7811\n",
      "Processing batch 10204/11884 - Loss: 28.4803\n",
      "Processing batch 10205/11884 - Loss: 31.0945\n",
      "Processing batch 10206/11884 - Loss: 30.8458\n",
      "Processing batch 10207/11884 - Loss: 29.3464\n",
      "Processing batch 10208/11884 - Loss: 30.5157\n",
      "Processing batch 10209/11884 - Loss: 30.0342\n",
      "Processing batch 10210/11884 - Loss: 30.4475\n",
      "Processing batch 10211/11884 - Loss: 30.8098\n",
      "Processing batch 10212/11884 - Loss: 30.7700\n",
      "Processing batch 10213/11884 - Loss: 31.6018\n",
      "Processing batch 10214/11884 - Loss: 30.0317\n",
      "Processing batch 10215/11884 - Loss: 30.4946\n",
      "Processing batch 10216/11884 - Loss: 31.2243\n",
      "Processing batch 10217/11884 - Loss: 31.0161\n",
      "Processing batch 10218/11884 - Loss: 30.9271\n",
      "Processing batch 10219/11884 - Loss: 30.4123\n",
      "Processing batch 10220/11884 - Loss: 31.4645\n",
      "Processing batch 10221/11884 - Loss: 31.2338\n",
      "Processing batch 10222/11884 - Loss: 30.1640\n",
      "Processing batch 10223/11884 - Loss: 31.1312\n",
      "Processing batch 10224/11884 - Loss: 30.4973\n",
      "Processing batch 10225/11884 - Loss: 30.1240\n",
      "Processing batch 10226/11884 - Loss: 30.9061\n",
      "Processing batch 10227/11884 - Loss: 29.9226\n",
      "Processing batch 10228/11884 - Loss: 30.1050\n",
      "Processing batch 10229/11884 - Loss: 30.8219\n",
      "Processing batch 10230/11884 - Loss: 29.3010\n",
      "Processing batch 10231/11884 - Loss: 30.7306\n",
      "Processing batch 10232/11884 - Loss: 32.4857\n",
      "Processing batch 10233/11884 - Loss: 31.3159\n",
      "Processing batch 10234/11884 - Loss: 29.8105\n",
      "Processing batch 10235/11884 - Loss: 31.1079\n",
      "Processing batch 10236/11884 - Loss: 29.7811\n",
      "Processing batch 10237/11884 - Loss: 29.0732\n",
      "Processing batch 10238/11884 - Loss: 30.0161\n",
      "Processing batch 10239/11884 - Loss: 29.8123\n",
      "Processing batch 10240/11884 - Loss: 30.9263\n",
      "Processing batch 10241/11884 - Loss: 30.4880\n",
      "Processing batch 10242/11884 - Loss: 31.3440\n",
      "Processing batch 10243/11884 - Loss: 29.9984\n",
      "Processing batch 10244/11884 - Loss: 29.8798\n",
      "Processing batch 10245/11884 - Loss: 29.0609\n",
      "Processing batch 10246/11884 - Loss: 29.3379\n",
      "Processing batch 10247/11884 - Loss: 31.9556\n",
      "Processing batch 10248/11884 - Loss: 29.6860\n",
      "Processing batch 10249/11884 - Loss: 29.6046\n",
      "Processing batch 10250/11884 - Loss: 28.4041\n",
      "Processing batch 10251/11884 - Loss: 29.3760\n",
      "Processing batch 10252/11884 - Loss: 31.6586\n",
      "Processing batch 10253/11884 - Loss: 30.8725\n",
      "Processing batch 10254/11884 - Loss: 30.4627\n",
      "Processing batch 10255/11884 - Loss: 30.0832\n",
      "Processing batch 10256/11884 - Loss: 31.1823\n",
      "Processing batch 10257/11884 - Loss: 29.6429\n",
      "Processing batch 10258/11884 - Loss: 30.4505\n",
      "Processing batch 10259/11884 - Loss: 30.5097\n",
      "Processing batch 10260/11884 - Loss: 29.0959\n",
      "Processing batch 10261/11884 - Loss: 29.5303\n",
      "Processing batch 10262/11884 - Loss: 29.6150\n",
      "Processing batch 10263/11884 - Loss: 30.1339\n",
      "Processing batch 10264/11884 - Loss: 29.9635\n",
      "Processing batch 10265/11884 - Loss: 29.8541\n",
      "Processing batch 10266/11884 - Loss: 29.1377\n",
      "Processing batch 10267/11884 - Loss: 32.5057\n",
      "Processing batch 10268/11884 - Loss: 30.6149\n",
      "Processing batch 10269/11884 - Loss: 29.6628\n",
      "Processing batch 10270/11884 - Loss: 31.5556\n",
      "Processing batch 10271/11884 - Loss: 29.9658\n",
      "Processing batch 10272/11884 - Loss: 30.6205\n",
      "Processing batch 10273/11884 - Loss: 29.5210\n",
      "Processing batch 10274/11884 - Loss: 30.5412\n",
      "Processing batch 10275/11884 - Loss: 30.1407\n",
      "Processing batch 10276/11884 - Loss: 29.2475\n",
      "Processing batch 10277/11884 - Loss: 30.5559\n",
      "Processing batch 10278/11884 - Loss: 31.5729\n",
      "Processing batch 10279/11884 - Loss: 29.6851\n",
      "Processing batch 10280/11884 - Loss: 30.8887\n",
      "Processing batch 10281/11884 - Loss: 30.8847\n",
      "Processing batch 10282/11884 - Loss: 29.7644\n",
      "Processing batch 10283/11884 - Loss: 30.3864\n",
      "Processing batch 10284/11884 - Loss: 30.6473\n",
      "Processing batch 10285/11884 - Loss: 30.5916\n",
      "Processing batch 10286/11884 - Loss: 30.2095\n",
      "Processing batch 10287/11884 - Loss: 30.6812\n",
      "Processing batch 10288/11884 - Loss: 30.1054\n",
      "Processing batch 10289/11884 - Loss: 29.5827\n",
      "Processing batch 10290/11884 - Loss: 31.0855\n",
      "Processing batch 10291/11884 - Loss: 29.8824\n",
      "Processing batch 10292/11884 - Loss: 30.7799\n",
      "Processing batch 10293/11884 - Loss: 31.6547\n",
      "Processing batch 10294/11884 - Loss: 28.7562\n",
      "Processing batch 10295/11884 - Loss: 31.4007\n",
      "Processing batch 10296/11884 - Loss: 30.4440\n",
      "Processing batch 10297/11884 - Loss: 29.4800\n",
      "Processing batch 10298/11884 - Loss: 30.7275\n",
      "Processing batch 10299/11884 - Loss: 31.9636\n",
      "Processing batch 10300/11884 - Loss: 31.1999\n",
      "Processing batch 10301/11884 - Loss: 30.2760\n",
      "Processing batch 10302/11884 - Loss: 29.3351\n",
      "Processing batch 10303/11884 - Loss: 30.7912\n",
      "Processing batch 10304/11884 - Loss: 29.3738\n",
      "Processing batch 10305/11884 - Loss: 29.6593\n",
      "Processing batch 10306/11884 - Loss: 30.8934\n",
      "Processing batch 10307/11884 - Loss: 29.7658\n",
      "Processing batch 10308/11884 - Loss: 30.9017\n",
      "Processing batch 10309/11884 - Loss: 29.6252\n",
      "Processing batch 10310/11884 - Loss: 29.8587\n",
      "Processing batch 10311/11884 - Loss: 29.7111\n",
      "Processing batch 10312/11884 - Loss: 30.6527\n",
      "Processing batch 10313/11884 - Loss: 29.8613\n",
      "Processing batch 10314/11884 - Loss: 30.3066\n",
      "Processing batch 10315/11884 - Loss: 29.2404\n",
      "Processing batch 10316/11884 - Loss: 29.8583\n",
      "Processing batch 10317/11884 - Loss: 30.5754\n",
      "Processing batch 10318/11884 - Loss: 31.7545\n",
      "Processing batch 10319/11884 - Loss: 30.5210\n",
      "Processing batch 10320/11884 - Loss: 30.6671\n",
      "Processing batch 10321/11884 - Loss: 30.6521\n",
      "Processing batch 10322/11884 - Loss: 30.3527\n",
      "Processing batch 10323/11884 - Loss: 29.9260\n",
      "Processing batch 10324/11884 - Loss: 30.1300\n",
      "Processing batch 10325/11884 - Loss: 30.2787\n",
      "Processing batch 10326/11884 - Loss: 30.5155\n",
      "Processing batch 10327/11884 - Loss: 30.4661\n",
      "Processing batch 10328/11884 - Loss: 30.9900\n",
      "Processing batch 10329/11884 - Loss: 29.2885\n",
      "Processing batch 10330/11884 - Loss: 29.9279\n",
      "Processing batch 10331/11884 - Loss: 30.1460\n",
      "Processing batch 10332/11884 - Loss: 30.9975\n",
      "Processing batch 10333/11884 - Loss: 30.5537\n",
      "Processing batch 10334/11884 - Loss: 29.3155\n",
      "Processing batch 10335/11884 - Loss: 30.6864\n",
      "Processing batch 10336/11884 - Loss: 30.9480\n",
      "Processing batch 10337/11884 - Loss: 31.0831\n",
      "Processing batch 10338/11884 - Loss: 30.2226\n",
      "Processing batch 10339/11884 - Loss: 29.3915\n",
      "Processing batch 10340/11884 - Loss: 31.0919\n",
      "Processing batch 10341/11884 - Loss: 32.2387\n",
      "Processing batch 10342/11884 - Loss: 30.1170\n",
      "Processing batch 10343/11884 - Loss: 30.1129\n",
      "Processing batch 10344/11884 - Loss: 29.8078\n",
      "Processing batch 10345/11884 - Loss: 29.0654\n",
      "Processing batch 10346/11884 - Loss: 29.0482\n",
      "Processing batch 10347/11884 - Loss: 32.1080\n",
      "Processing batch 10348/11884 - Loss: 30.7347\n",
      "Processing batch 10349/11884 - Loss: 31.6551\n",
      "Processing batch 10350/11884 - Loss: 30.7326\n",
      "Processing batch 10351/11884 - Loss: 30.8416\n",
      "Processing batch 10352/11884 - Loss: 31.5617\n",
      "Processing batch 10353/11884 - Loss: 30.1993\n",
      "Processing batch 10354/11884 - Loss: 30.0109\n",
      "Processing batch 10355/11884 - Loss: 30.7951\n",
      "Processing batch 10356/11884 - Loss: 30.1942\n",
      "Processing batch 10357/11884 - Loss: 29.7524\n",
      "Processing batch 10358/11884 - Loss: 30.4716\n",
      "Processing batch 10359/11884 - Loss: 31.4495\n",
      "Processing batch 10360/11884 - Loss: 30.2643\n",
      "Processing batch 10361/11884 - Loss: 30.4843\n",
      "Processing batch 10362/11884 - Loss: 28.4128\n",
      "Processing batch 10363/11884 - Loss: 30.6883\n",
      "Processing batch 10364/11884 - Loss: 30.8708\n",
      "Processing batch 10365/11884 - Loss: 30.4688\n",
      "Processing batch 10366/11884 - Loss: 30.0487\n",
      "Processing batch 10367/11884 - Loss: 29.0252\n",
      "Processing batch 10368/11884 - Loss: 30.1444\n",
      "Processing batch 10369/11884 - Loss: 30.3857\n",
      "Processing batch 10370/11884 - Loss: 29.7150\n",
      "Processing batch 10371/11884 - Loss: 30.9455\n",
      "Processing batch 10372/11884 - Loss: 31.8917\n",
      "Processing batch 10373/11884 - Loss: 30.6044\n",
      "Processing batch 10374/11884 - Loss: 32.3379\n",
      "Processing batch 10375/11884 - Loss: 31.1647\n",
      "Processing batch 10376/11884 - Loss: 29.3978\n",
      "Processing batch 10377/11884 - Loss: 29.9575\n",
      "Processing batch 10378/11884 - Loss: 30.7301\n",
      "Processing batch 10379/11884 - Loss: 29.6106\n",
      "Processing batch 10380/11884 - Loss: 31.1181\n",
      "Processing batch 10381/11884 - Loss: 29.8939\n",
      "Processing batch 10382/11884 - Loss: 31.6286\n",
      "Processing batch 10383/11884 - Loss: 30.7701\n",
      "Processing batch 10384/11884 - Loss: 30.9242\n",
      "Processing batch 10385/11884 - Loss: 30.6715\n",
      "Processing batch 10386/11884 - Loss: 29.4972\n",
      "Processing batch 10387/11884 - Loss: 30.3117\n",
      "Processing batch 10388/11884 - Loss: 30.9975\n",
      "Processing batch 10389/11884 - Loss: 29.8370\n",
      "Processing batch 10390/11884 - Loss: 31.0854\n",
      "Processing batch 10391/11884 - Loss: 29.8209\n",
      "Processing batch 10392/11884 - Loss: 28.5019\n",
      "Processing batch 10393/11884 - Loss: 30.5366\n",
      "Processing batch 10394/11884 - Loss: 30.1607\n",
      "Processing batch 10395/11884 - Loss: 30.6066\n",
      "Processing batch 10396/11884 - Loss: 30.1517\n",
      "Processing batch 10397/11884 - Loss: 30.1659\n",
      "Processing batch 10398/11884 - Loss: 30.1632\n",
      "Processing batch 10399/11884 - Loss: 30.6156\n",
      "Processing batch 10400/11884 - Loss: 30.3183\n",
      "Processing batch 10401/11884 - Loss: 31.6405\n",
      "Processing batch 10402/11884 - Loss: 29.9513\n",
      "Processing batch 10403/11884 - Loss: 31.8317\n",
      "Processing batch 10404/11884 - Loss: 30.9729\n",
      "Processing batch 10405/11884 - Loss: 30.7418\n",
      "Processing batch 10406/11884 - Loss: 30.5876\n",
      "Processing batch 10407/11884 - Loss: 29.9376\n",
      "Processing batch 10408/11884 - Loss: 30.2275\n",
      "Processing batch 10409/11884 - Loss: 29.3165\n",
      "Processing batch 10410/11884 - Loss: 29.1130\n",
      "Processing batch 10411/11884 - Loss: 30.1541\n",
      "Processing batch 10412/11884 - Loss: 30.6304\n",
      "Processing batch 10413/11884 - Loss: 30.3154\n",
      "Processing batch 10414/11884 - Loss: 30.0246\n",
      "Processing batch 10415/11884 - Loss: 30.0431\n",
      "Processing batch 10416/11884 - Loss: 30.6681\n",
      "Processing batch 10417/11884 - Loss: 31.2709\n",
      "Processing batch 10418/11884 - Loss: 31.2238\n",
      "Processing batch 10419/11884 - Loss: 30.2010\n",
      "Processing batch 10420/11884 - Loss: 31.1707\n",
      "Processing batch 10421/11884 - Loss: 29.5680\n",
      "Processing batch 10422/11884 - Loss: 30.1907\n",
      "Processing batch 10423/11884 - Loss: 31.0290\n",
      "Processing batch 10424/11884 - Loss: 30.4340\n",
      "Processing batch 10425/11884 - Loss: 31.2594\n",
      "Processing batch 10426/11884 - Loss: 31.1257\n",
      "Processing batch 10427/11884 - Loss: 30.3284\n",
      "Processing batch 10428/11884 - Loss: 29.9209\n",
      "Processing batch 10429/11884 - Loss: 29.5928\n",
      "Processing batch 10430/11884 - Loss: 30.5514\n",
      "Processing batch 10431/11884 - Loss: 28.8326\n",
      "Processing batch 10432/11884 - Loss: 31.0555\n",
      "Processing batch 10433/11884 - Loss: 30.3085\n",
      "Processing batch 10434/11884 - Loss: 30.4835\n",
      "Processing batch 10435/11884 - Loss: 29.6754\n",
      "Processing batch 10436/11884 - Loss: 29.6305\n",
      "Processing batch 10437/11884 - Loss: 30.0300\n",
      "Processing batch 10438/11884 - Loss: 29.8495\n",
      "Processing batch 10439/11884 - Loss: 30.3326\n",
      "Processing batch 10440/11884 - Loss: 29.2914\n",
      "Processing batch 10441/11884 - Loss: 29.9584\n",
      "Processing batch 10442/11884 - Loss: 30.0557\n",
      "Processing batch 10443/11884 - Loss: 28.4647\n",
      "Processing batch 10444/11884 - Loss: 30.9935\n",
      "Processing batch 10445/11884 - Loss: 31.4501\n",
      "Processing batch 10446/11884 - Loss: 30.1379\n",
      "Processing batch 10447/11884 - Loss: 30.3617\n",
      "Processing batch 10448/11884 - Loss: 29.9247\n",
      "Processing batch 10449/11884 - Loss: 29.4996\n",
      "Processing batch 10450/11884 - Loss: 29.5235\n",
      "Processing batch 10451/11884 - Loss: 29.9188\n",
      "Processing batch 10452/11884 - Loss: 31.1080\n",
      "Processing batch 10453/11884 - Loss: 30.6784\n",
      "Processing batch 10454/11884 - Loss: 30.2374\n",
      "Processing batch 10455/11884 - Loss: 29.2774\n",
      "Processing batch 10456/11884 - Loss: 30.4977\n",
      "Processing batch 10457/11884 - Loss: 31.7824\n",
      "Processing batch 10458/11884 - Loss: 31.1166\n",
      "Processing batch 10459/11884 - Loss: 30.4226\n",
      "Processing batch 10460/11884 - Loss: 31.1136\n",
      "Processing batch 10461/11884 - Loss: 32.0217\n",
      "Processing batch 10462/11884 - Loss: 29.7075\n",
      "Processing batch 10463/11884 - Loss: 31.1825\n",
      "Processing batch 10464/11884 - Loss: 30.4643\n",
      "Processing batch 10465/11884 - Loss: 30.0212\n",
      "Processing batch 10466/11884 - Loss: 31.4386\n",
      "Processing batch 10467/11884 - Loss: 30.2693\n",
      "Processing batch 10468/11884 - Loss: 30.1585\n",
      "Processing batch 10469/11884 - Loss: 29.6906\n",
      "Processing batch 10470/11884 - Loss: 29.9660\n",
      "Processing batch 10471/11884 - Loss: 29.8175\n",
      "Processing batch 10472/11884 - Loss: 29.0252\n",
      "Processing batch 10473/11884 - Loss: 30.6031\n",
      "Processing batch 10474/11884 - Loss: 30.8207\n",
      "Processing batch 10475/11884 - Loss: 30.5026\n",
      "Processing batch 10476/11884 - Loss: 29.1396\n",
      "Processing batch 10477/11884 - Loss: 30.5854\n",
      "Processing batch 10478/11884 - Loss: 29.7988\n",
      "Processing batch 10479/11884 - Loss: 30.2654\n",
      "Processing batch 10480/11884 - Loss: 30.1351\n",
      "Processing batch 10481/11884 - Loss: 32.0779\n",
      "Processing batch 10482/11884 - Loss: 29.9610\n",
      "Processing batch 10483/11884 - Loss: 29.9979\n",
      "Processing batch 10484/11884 - Loss: 29.2039\n",
      "Processing batch 10485/11884 - Loss: 31.2056\n",
      "Processing batch 10486/11884 - Loss: 32.3343\n",
      "Processing batch 10487/11884 - Loss: 30.5109\n",
      "Processing batch 10488/11884 - Loss: 31.2802\n",
      "Processing batch 10489/11884 - Loss: 29.2543\n",
      "Processing batch 10490/11884 - Loss: 29.4049\n",
      "Processing batch 10491/11884 - Loss: 30.1439\n",
      "Processing batch 10492/11884 - Loss: 30.4789\n",
      "Processing batch 10493/11884 - Loss: 29.1999\n",
      "Processing batch 10494/11884 - Loss: 31.2019\n",
      "Processing batch 10495/11884 - Loss: 30.1104\n",
      "Processing batch 10496/11884 - Loss: 29.2659\n",
      "Processing batch 10497/11884 - Loss: 29.9665\n",
      "Processing batch 10498/11884 - Loss: 29.3671\n",
      "Processing batch 10499/11884 - Loss: 30.3256\n",
      "Processing batch 10500/11884 - Loss: 30.2349\n",
      "Processing batch 10501/11884 - Loss: 30.9396\n",
      "Processing batch 10502/11884 - Loss: 30.5928\n",
      "Processing batch 10503/11884 - Loss: 30.6931\n",
      "Processing batch 10504/11884 - Loss: 31.0211\n",
      "Processing batch 10505/11884 - Loss: 30.4088\n",
      "Processing batch 10506/11884 - Loss: 29.6313\n",
      "Processing batch 10507/11884 - Loss: 29.4625\n",
      "Processing batch 10508/11884 - Loss: 30.6558\n",
      "Processing batch 10509/11884 - Loss: 30.4464\n",
      "Processing batch 10510/11884 - Loss: 29.0408\n",
      "Processing batch 10511/11884 - Loss: 30.0789\n",
      "Processing batch 10512/11884 - Loss: 31.5012\n",
      "Processing batch 10513/11884 - Loss: 31.2018\n",
      "Processing batch 10514/11884 - Loss: 30.7679\n",
      "Processing batch 10515/11884 - Loss: 30.3401\n",
      "Processing batch 10516/11884 - Loss: 31.3614\n",
      "Processing batch 10517/11884 - Loss: 30.0078\n",
      "Processing batch 10518/11884 - Loss: 30.2348\n",
      "Processing batch 10519/11884 - Loss: 29.9323\n",
      "Processing batch 10520/11884 - Loss: 30.6145\n",
      "Processing batch 10521/11884 - Loss: 29.2174\n",
      "Processing batch 10522/11884 - Loss: 30.5758\n",
      "Processing batch 10523/11884 - Loss: 30.0488\n",
      "Processing batch 10524/11884 - Loss: 31.2630\n",
      "Processing batch 10525/11884 - Loss: 31.0484\n",
      "Processing batch 10526/11884 - Loss: 30.7976\n",
      "Processing batch 10527/11884 - Loss: 30.4825\n",
      "Processing batch 10528/11884 - Loss: 30.7384\n",
      "Processing batch 10529/11884 - Loss: 30.2296\n",
      "Processing batch 10530/11884 - Loss: 30.0977\n",
      "Processing batch 10531/11884 - Loss: 30.1629\n",
      "Processing batch 10532/11884 - Loss: 30.6331\n",
      "Processing batch 10533/11884 - Loss: 30.2277\n",
      "Processing batch 10534/11884 - Loss: 29.9238\n",
      "Processing batch 10535/11884 - Loss: 30.0655\n",
      "Processing batch 10536/11884 - Loss: 30.0482\n",
      "Processing batch 10537/11884 - Loss: 30.9802\n",
      "Processing batch 10538/11884 - Loss: 31.0117\n",
      "Processing batch 10539/11884 - Loss: 29.4828\n",
      "Processing batch 10540/11884 - Loss: 30.9378\n",
      "Processing batch 10541/11884 - Loss: 30.8756\n",
      "Processing batch 10542/11884 - Loss: 30.1872\n",
      "Processing batch 10543/11884 - Loss: 30.3366\n",
      "Processing batch 10544/11884 - Loss: 30.3757\n",
      "Processing batch 10545/11884 - Loss: 30.4824\n",
      "Processing batch 10546/11884 - Loss: 31.0337\n",
      "Processing batch 10547/11884 - Loss: 29.9082\n",
      "Processing batch 10548/11884 - Loss: 30.8487\n",
      "Processing batch 10549/11884 - Loss: 30.9423\n",
      "Processing batch 10550/11884 - Loss: 31.4576\n",
      "Processing batch 10551/11884 - Loss: 29.4053\n",
      "Processing batch 10552/11884 - Loss: 30.2696\n",
      "Processing batch 10553/11884 - Loss: 29.6860\n",
      "Processing batch 10554/11884 - Loss: 29.6657\n",
      "Processing batch 10555/11884 - Loss: 31.3883\n",
      "Processing batch 10556/11884 - Loss: 31.7970\n",
      "Processing batch 10557/11884 - Loss: 29.9298\n",
      "Processing batch 10558/11884 - Loss: 29.9317\n",
      "Processing batch 10559/11884 - Loss: 29.5433\n",
      "Processing batch 10560/11884 - Loss: 30.6897\n",
      "Processing batch 10561/11884 - Loss: 29.1556\n",
      "Processing batch 10562/11884 - Loss: 29.9481\n",
      "Processing batch 10563/11884 - Loss: 30.8955\n",
      "Processing batch 10564/11884 - Loss: 30.4249\n",
      "Processing batch 10565/11884 - Loss: 30.1693\n",
      "Processing batch 10566/11884 - Loss: 31.5238\n",
      "Processing batch 10567/11884 - Loss: 30.1460\n",
      "Processing batch 10568/11884 - Loss: 29.1053\n",
      "Processing batch 10569/11884 - Loss: 29.9325\n",
      "Processing batch 10570/11884 - Loss: 29.6577\n",
      "Processing batch 10571/11884 - Loss: 30.2101\n",
      "Processing batch 10572/11884 - Loss: 29.1903\n",
      "Processing batch 10573/11884 - Loss: 28.9337\n",
      "Processing batch 10574/11884 - Loss: 30.0671\n",
      "Processing batch 10575/11884 - Loss: 31.3140\n",
      "Processing batch 10576/11884 - Loss: 30.4009\n",
      "Processing batch 10577/11884 - Loss: 30.0385\n",
      "Processing batch 10578/11884 - Loss: 32.5655\n",
      "Processing batch 10579/11884 - Loss: 31.9784\n",
      "Processing batch 10580/11884 - Loss: 31.0495\n",
      "Processing batch 10581/11884 - Loss: 30.2361\n",
      "Processing batch 10582/11884 - Loss: 30.8035\n",
      "Processing batch 10583/11884 - Loss: 30.5820\n",
      "Processing batch 10584/11884 - Loss: 31.0757\n",
      "Processing batch 10585/11884 - Loss: 30.2371\n",
      "Processing batch 10586/11884 - Loss: 31.4214\n",
      "Processing batch 10587/11884 - Loss: 30.9927\n",
      "Processing batch 10588/11884 - Loss: 29.7125\n",
      "Processing batch 10589/11884 - Loss: 30.8907\n",
      "Processing batch 10590/11884 - Loss: 30.8148\n",
      "Processing batch 10591/11884 - Loss: 29.6668\n",
      "Processing batch 10592/11884 - Loss: 30.1739\n",
      "Processing batch 10593/11884 - Loss: 31.2056\n",
      "Processing batch 10594/11884 - Loss: 29.6970\n",
      "Processing batch 10595/11884 - Loss: 29.6292\n",
      "Processing batch 10596/11884 - Loss: 30.3123\n",
      "Processing batch 10597/11884 - Loss: 31.3031\n",
      "Processing batch 10598/11884 - Loss: 29.5261\n",
      "Processing batch 10599/11884 - Loss: 30.1314\n",
      "Processing batch 10600/11884 - Loss: 30.8497\n",
      "Processing batch 10601/11884 - Loss: 30.6559\n",
      "Processing batch 10602/11884 - Loss: 30.1512\n",
      "Processing batch 10603/11884 - Loss: 30.4381\n",
      "Processing batch 10604/11884 - Loss: 30.2162\n",
      "Processing batch 10605/11884 - Loss: 30.5532\n",
      "Processing batch 10606/11884 - Loss: 29.6877\n",
      "Processing batch 10607/11884 - Loss: 30.1896\n",
      "Processing batch 10608/11884 - Loss: 30.9368\n",
      "Processing batch 10609/11884 - Loss: 31.6393\n",
      "Processing batch 10610/11884 - Loss: 30.4289\n",
      "Processing batch 10611/11884 - Loss: 30.9431\n",
      "Processing batch 10612/11884 - Loss: 30.6127\n",
      "Processing batch 10613/11884 - Loss: 30.5269\n",
      "Processing batch 10614/11884 - Loss: 31.2563\n",
      "Processing batch 10615/11884 - Loss: 31.5445\n",
      "Processing batch 10616/11884 - Loss: 31.0818\n",
      "Processing batch 10617/11884 - Loss: 29.6134\n",
      "Processing batch 10618/11884 - Loss: 30.2705\n",
      "Processing batch 10619/11884 - Loss: 30.3961\n",
      "Processing batch 10620/11884 - Loss: 30.2515\n",
      "Processing batch 10621/11884 - Loss: 29.8326\n",
      "Processing batch 10622/11884 - Loss: 30.3277\n",
      "Processing batch 10623/11884 - Loss: 28.9809\n",
      "Processing batch 10624/11884 - Loss: 29.7268\n",
      "Processing batch 10625/11884 - Loss: 30.4876\n",
      "Processing batch 10626/11884 - Loss: 30.7943\n",
      "Processing batch 10627/11884 - Loss: 31.8035\n",
      "Processing batch 10628/11884 - Loss: 30.5911\n",
      "Processing batch 10629/11884 - Loss: 31.1652\n",
      "Processing batch 10630/11884 - Loss: 30.5301\n",
      "Processing batch 10631/11884 - Loss: 30.4923\n",
      "Processing batch 10632/11884 - Loss: 30.6869\n",
      "Processing batch 10633/11884 - Loss: 30.2305\n",
      "Processing batch 10634/11884 - Loss: 32.7139\n",
      "Processing batch 10635/11884 - Loss: 30.0705\n",
      "Processing batch 10636/11884 - Loss: 31.2958\n",
      "Processing batch 10637/11884 - Loss: 29.5802\n",
      "Processing batch 10638/11884 - Loss: 30.1735\n",
      "Processing batch 10639/11884 - Loss: 30.1564\n",
      "Processing batch 10640/11884 - Loss: 30.3681\n",
      "Processing batch 10641/11884 - Loss: 31.3875\n",
      "Processing batch 10642/11884 - Loss: 30.4597\n",
      "Processing batch 10643/11884 - Loss: 29.1385\n",
      "Processing batch 10644/11884 - Loss: 30.5116\n",
      "Processing batch 10645/11884 - Loss: 31.1136\n",
      "Processing batch 10646/11884 - Loss: 30.8472\n",
      "Processing batch 10647/11884 - Loss: 30.7549\n",
      "Processing batch 10648/11884 - Loss: 30.7643\n",
      "Processing batch 10649/11884 - Loss: 31.0933\n",
      "Processing batch 10650/11884 - Loss: 31.3317\n",
      "Processing batch 10651/11884 - Loss: 31.1443\n",
      "Processing batch 10652/11884 - Loss: 30.5155\n",
      "Processing batch 10653/11884 - Loss: 31.4879\n",
      "Processing batch 10654/11884 - Loss: 30.5366\n",
      "Processing batch 10655/11884 - Loss: 30.1243\n",
      "Processing batch 10656/11884 - Loss: 30.1723\n",
      "Processing batch 10657/11884 - Loss: 29.2080\n",
      "Processing batch 10658/11884 - Loss: 30.4467\n",
      "Processing batch 10659/11884 - Loss: 30.8001\n",
      "Processing batch 10660/11884 - Loss: 31.2374\n",
      "Processing batch 10661/11884 - Loss: 31.1002\n",
      "Processing batch 10662/11884 - Loss: 31.6683\n",
      "Processing batch 10663/11884 - Loss: 29.7633\n",
      "Processing batch 10664/11884 - Loss: 31.5076\n",
      "Processing batch 10665/11884 - Loss: 30.4731\n",
      "Processing batch 10666/11884 - Loss: 30.8949\n",
      "Processing batch 10667/11884 - Loss: 31.5125\n",
      "Processing batch 10668/11884 - Loss: 29.9998\n",
      "Processing batch 10669/11884 - Loss: 31.7700\n",
      "Processing batch 10670/11884 - Loss: 32.0206\n",
      "Processing batch 10671/11884 - Loss: 29.6317\n",
      "Processing batch 10672/11884 - Loss: 29.8961\n",
      "Processing batch 10673/11884 - Loss: 30.1895\n",
      "Processing batch 10674/11884 - Loss: 30.9544\n",
      "Processing batch 10675/11884 - Loss: 29.7327\n",
      "Processing batch 10676/11884 - Loss: 29.6088\n",
      "Processing batch 10677/11884 - Loss: 30.0601\n",
      "Processing batch 10678/11884 - Loss: 30.4536\n",
      "Processing batch 10679/11884 - Loss: 29.8372\n",
      "Processing batch 10680/11884 - Loss: 29.4289\n",
      "Processing batch 10681/11884 - Loss: 30.3734\n",
      "Processing batch 10682/11884 - Loss: 30.6807\n",
      "Processing batch 10683/11884 - Loss: 31.0038\n",
      "Processing batch 10684/11884 - Loss: 30.8059\n",
      "Processing batch 10685/11884 - Loss: 31.9862\n",
      "Processing batch 10686/11884 - Loss: 29.3076\n",
      "Processing batch 10687/11884 - Loss: 28.8183\n",
      "Processing batch 10688/11884 - Loss: 29.9924\n",
      "Processing batch 10689/11884 - Loss: 29.7655\n",
      "Processing batch 10690/11884 - Loss: 30.5207\n",
      "Processing batch 10691/11884 - Loss: 28.7863\n",
      "Processing batch 10692/11884 - Loss: 28.9490\n",
      "Processing batch 10693/11884 - Loss: 31.0150\n",
      "Processing batch 10694/11884 - Loss: 30.0941\n",
      "Processing batch 10695/11884 - Loss: 30.6640\n",
      "Processing batch 10696/11884 - Loss: 29.3469\n",
      "Processing batch 10697/11884 - Loss: 30.7919\n",
      "Processing batch 10698/11884 - Loss: 30.8838\n",
      "Processing batch 10699/11884 - Loss: 29.8734\n",
      "Processing batch 10700/11884 - Loss: 30.5009\n",
      "Processing batch 10701/11884 - Loss: 30.7811\n",
      "Processing batch 10702/11884 - Loss: 30.5602\n",
      "Processing batch 10703/11884 - Loss: 28.9743\n",
      "Processing batch 10704/11884 - Loss: 30.6437\n",
      "Processing batch 10705/11884 - Loss: 30.3753\n",
      "Processing batch 10706/11884 - Loss: 30.6765\n",
      "Processing batch 10707/11884 - Loss: 29.1843\n",
      "Processing batch 10708/11884 - Loss: 30.5012\n",
      "Processing batch 10709/11884 - Loss: 30.3698\n",
      "Processing batch 10710/11884 - Loss: 29.3277\n",
      "Processing batch 10711/11884 - Loss: 29.7595\n",
      "Processing batch 10712/11884 - Loss: 30.3400\n",
      "Processing batch 10713/11884 - Loss: 30.6052\n",
      "Processing batch 10714/11884 - Loss: 32.5760\n",
      "Processing batch 10715/11884 - Loss: 30.9582\n",
      "Processing batch 10716/11884 - Loss: 30.8741\n",
      "Processing batch 10717/11884 - Loss: 30.4767\n",
      "Processing batch 10718/11884 - Loss: 29.4688\n",
      "Processing batch 10719/11884 - Loss: 32.2073\n",
      "Processing batch 10720/11884 - Loss: 30.8289\n",
      "Processing batch 10721/11884 - Loss: 29.6782\n",
      "Processing batch 10722/11884 - Loss: 30.4038\n",
      "Processing batch 10723/11884 - Loss: 32.0199\n",
      "Processing batch 10724/11884 - Loss: 31.0631\n",
      "Processing batch 10725/11884 - Loss: 28.6290\n",
      "Processing batch 10726/11884 - Loss: 30.2561\n",
      "Processing batch 10727/11884 - Loss: 30.8087\n",
      "Processing batch 10728/11884 - Loss: 31.0519\n",
      "Processing batch 10729/11884 - Loss: 30.4069\n",
      "Processing batch 10730/11884 - Loss: 30.2761\n",
      "Processing batch 10731/11884 - Loss: 29.9897\n",
      "Processing batch 10732/11884 - Loss: 30.3845\n",
      "Processing batch 10733/11884 - Loss: 30.4526\n",
      "Processing batch 10734/11884 - Loss: 30.5633\n",
      "Processing batch 10735/11884 - Loss: 31.8956\n",
      "Processing batch 10736/11884 - Loss: 30.0699\n",
      "Processing batch 10737/11884 - Loss: 31.0036\n",
      "Processing batch 10738/11884 - Loss: 28.8157\n",
      "Processing batch 10739/11884 - Loss: 28.5983\n",
      "Processing batch 10740/11884 - Loss: 29.7113\n",
      "Processing batch 10741/11884 - Loss: 28.5877\n",
      "Processing batch 10742/11884 - Loss: 30.1325\n",
      "Processing batch 10743/11884 - Loss: 29.6715\n",
      "Processing batch 10744/11884 - Loss: 30.4552\n",
      "Processing batch 10745/11884 - Loss: 29.6295\n",
      "Processing batch 10746/11884 - Loss: 30.6951\n",
      "Processing batch 10747/11884 - Loss: 29.3595\n",
      "Processing batch 10748/11884 - Loss: 30.4998\n",
      "Processing batch 10749/11884 - Loss: 30.2872\n",
      "Processing batch 10750/11884 - Loss: 30.7473\n",
      "Processing batch 10751/11884 - Loss: 30.9266\n",
      "Processing batch 10752/11884 - Loss: 30.1307\n",
      "Processing batch 10753/11884 - Loss: 31.2624\n",
      "Processing batch 10754/11884 - Loss: 29.8947\n",
      "Processing batch 10755/11884 - Loss: 29.3215\n",
      "Processing batch 10756/11884 - Loss: 29.6206\n",
      "Processing batch 10757/11884 - Loss: 29.4023\n",
      "Processing batch 10758/11884 - Loss: 29.5155\n",
      "Processing batch 10759/11884 - Loss: 30.3258\n",
      "Processing batch 10760/11884 - Loss: 31.8283\n",
      "Processing batch 10761/11884 - Loss: 29.9878\n",
      "Processing batch 10762/11884 - Loss: 30.1151\n",
      "Processing batch 10763/11884 - Loss: 30.4885\n",
      "Processing batch 10764/11884 - Loss: 29.9353\n",
      "Processing batch 10765/11884 - Loss: 30.7439\n",
      "Processing batch 10766/11884 - Loss: 30.3479\n",
      "Processing batch 10767/11884 - Loss: 29.2992\n",
      "Processing batch 10768/11884 - Loss: 30.4550\n",
      "Processing batch 10769/11884 - Loss: 31.1300\n",
      "Processing batch 10770/11884 - Loss: 29.3105\n",
      "Processing batch 10771/11884 - Loss: 30.2877\n",
      "Processing batch 10772/11884 - Loss: 30.7699\n",
      "Processing batch 10773/11884 - Loss: 29.1377\n",
      "Processing batch 10774/11884 - Loss: 30.8925\n",
      "Processing batch 10775/11884 - Loss: 29.6267\n",
      "Processing batch 10776/11884 - Loss: 29.6176\n",
      "Processing batch 10777/11884 - Loss: 29.4878\n",
      "Processing batch 10778/11884 - Loss: 30.8068\n",
      "Processing batch 10779/11884 - Loss: 30.2334\n",
      "Processing batch 10780/11884 - Loss: 30.1669\n",
      "Processing batch 10781/11884 - Loss: 31.0845\n",
      "Processing batch 10782/11884 - Loss: 31.6233\n",
      "Processing batch 10783/11884 - Loss: 29.5185\n",
      "Processing batch 10784/11884 - Loss: 31.1409\n",
      "Processing batch 10785/11884 - Loss: 29.5325\n",
      "Processing batch 10786/11884 - Loss: 29.1465\n",
      "Processing batch 10787/11884 - Loss: 29.9527\n",
      "Processing batch 10788/11884 - Loss: 30.1894\n",
      "Processing batch 10789/11884 - Loss: 30.6052\n",
      "Processing batch 10790/11884 - Loss: 30.7448\n",
      "Processing batch 10791/11884 - Loss: 28.3631\n",
      "Processing batch 10792/11884 - Loss: 31.3284\n",
      "Processing batch 10793/11884 - Loss: 30.0828\n",
      "Processing batch 10794/11884 - Loss: 29.5513\n",
      "Processing batch 10795/11884 - Loss: 31.1058\n",
      "Processing batch 10796/11884 - Loss: 29.5958\n",
      "Processing batch 10797/11884 - Loss: 30.8474\n",
      "Processing batch 10798/11884 - Loss: 29.9999\n",
      "Processing batch 10799/11884 - Loss: 31.3307\n",
      "Processing batch 10800/11884 - Loss: 31.7824\n",
      "Processing batch 10801/11884 - Loss: 29.0759\n",
      "Processing batch 10802/11884 - Loss: 29.1361\n",
      "Processing batch 10803/11884 - Loss: 30.0032\n",
      "Processing batch 10804/11884 - Loss: 31.2583\n",
      "Processing batch 10805/11884 - Loss: 29.9195\n",
      "Processing batch 10806/11884 - Loss: 30.6126\n",
      "Processing batch 10807/11884 - Loss: 29.4368\n",
      "Processing batch 10808/11884 - Loss: 30.6620\n",
      "Processing batch 10809/11884 - Loss: 29.6856\n",
      "Processing batch 10810/11884 - Loss: 30.0286\n",
      "Processing batch 10811/11884 - Loss: 30.6579\n",
      "Processing batch 10812/11884 - Loss: 29.1916\n",
      "Processing batch 10813/11884 - Loss: 29.3144\n",
      "Processing batch 10814/11884 - Loss: 30.3935\n",
      "Processing batch 10815/11884 - Loss: 30.8085\n",
      "Processing batch 10816/11884 - Loss: 28.7150\n",
      "Processing batch 10817/11884 - Loss: 30.9471\n",
      "Processing batch 10818/11884 - Loss: 30.4684\n",
      "Processing batch 10819/11884 - Loss: 31.6936\n",
      "Processing batch 10820/11884 - Loss: 32.6626\n",
      "Processing batch 10821/11884 - Loss: 29.8741\n",
      "Processing batch 10822/11884 - Loss: 30.2446\n",
      "Processing batch 10823/11884 - Loss: 31.7139\n",
      "Processing batch 10824/11884 - Loss: 30.4632\n",
      "Processing batch 10825/11884 - Loss: 30.2582\n",
      "Processing batch 10826/11884 - Loss: 29.3322\n",
      "Processing batch 10827/11884 - Loss: 30.6600\n",
      "Processing batch 10828/11884 - Loss: 29.6073\n",
      "Processing batch 10829/11884 - Loss: 31.0717\n",
      "Processing batch 10830/11884 - Loss: 30.2650\n",
      "Processing batch 10831/11884 - Loss: 32.1399\n",
      "Processing batch 10832/11884 - Loss: 31.3724\n",
      "Processing batch 10833/11884 - Loss: 29.4457\n",
      "Processing batch 10834/11884 - Loss: 29.9310\n",
      "Processing batch 10835/11884 - Loss: 32.2165\n",
      "Processing batch 10836/11884 - Loss: 29.7172\n",
      "Processing batch 10837/11884 - Loss: 31.0427\n",
      "Processing batch 10838/11884 - Loss: 30.7529\n",
      "Processing batch 10839/11884 - Loss: 31.0583\n",
      "Processing batch 10840/11884 - Loss: 30.9080\n",
      "Processing batch 10841/11884 - Loss: 30.5867\n",
      "Processing batch 10842/11884 - Loss: 31.4528\n",
      "Processing batch 10843/11884 - Loss: 30.7869\n",
      "Processing batch 10844/11884 - Loss: 32.2747\n",
      "Processing batch 10845/11884 - Loss: 30.8756\n",
      "Processing batch 10846/11884 - Loss: 30.6897\n",
      "Processing batch 10847/11884 - Loss: 30.5026\n",
      "Processing batch 10848/11884 - Loss: 31.4195\n",
      "Processing batch 10849/11884 - Loss: 29.4422\n",
      "Processing batch 10850/11884 - Loss: 29.5683\n",
      "Processing batch 10851/11884 - Loss: 30.5292\n",
      "Processing batch 10852/11884 - Loss: 29.8610\n",
      "Processing batch 10853/11884 - Loss: 29.8200\n",
      "Processing batch 10854/11884 - Loss: 30.6184\n",
      "Processing batch 10855/11884 - Loss: 30.7641\n",
      "Processing batch 10856/11884 - Loss: 30.2266\n",
      "Processing batch 10857/11884 - Loss: 30.6840\n",
      "Processing batch 10858/11884 - Loss: 30.4000\n",
      "Processing batch 10859/11884 - Loss: 31.7081\n",
      "Processing batch 10860/11884 - Loss: 29.8460\n",
      "Processing batch 10861/11884 - Loss: 31.3664\n",
      "Processing batch 10862/11884 - Loss: 29.9230\n",
      "Processing batch 10863/11884 - Loss: 30.6520\n",
      "Processing batch 10864/11884 - Loss: 31.1584\n",
      "Processing batch 10865/11884 - Loss: 30.6915\n",
      "Processing batch 10866/11884 - Loss: 30.4453\n",
      "Processing batch 10867/11884 - Loss: 29.8919\n",
      "Processing batch 10868/11884 - Loss: 30.6422\n",
      "Processing batch 10869/11884 - Loss: 29.6770\n",
      "Processing batch 10870/11884 - Loss: 30.1796\n",
      "Processing batch 10871/11884 - Loss: 30.4265\n",
      "Processing batch 10872/11884 - Loss: 30.4442\n",
      "Processing batch 10873/11884 - Loss: 29.1710\n",
      "Processing batch 10874/11884 - Loss: 30.8409\n",
      "Processing batch 10875/11884 - Loss: 30.1028\n",
      "Processing batch 10876/11884 - Loss: 28.5692\n",
      "Processing batch 10877/11884 - Loss: 29.9332\n",
      "Processing batch 10878/11884 - Loss: 30.9120\n",
      "Processing batch 10879/11884 - Loss: 30.9018\n",
      "Processing batch 10880/11884 - Loss: 29.6325\n",
      "Processing batch 10881/11884 - Loss: 30.8302\n",
      "Processing batch 10882/11884 - Loss: 29.0525\n",
      "Processing batch 10883/11884 - Loss: 30.6530\n",
      "Processing batch 10884/11884 - Loss: 29.9482\n",
      "Processing batch 10885/11884 - Loss: 30.1486\n",
      "Processing batch 10886/11884 - Loss: 30.7949\n",
      "Processing batch 10887/11884 - Loss: 31.2222\n",
      "Processing batch 10888/11884 - Loss: 30.3420\n",
      "Processing batch 10889/11884 - Loss: 31.1119\n",
      "Processing batch 10890/11884 - Loss: 30.9140\n",
      "Processing batch 10891/11884 - Loss: 30.9265\n",
      "Processing batch 10892/11884 - Loss: 30.3496\n",
      "Processing batch 10893/11884 - Loss: 32.3405\n",
      "Processing batch 10894/11884 - Loss: 29.6742\n",
      "Processing batch 10895/11884 - Loss: 31.0976\n",
      "Processing batch 10896/11884 - Loss: 29.4888\n",
      "Processing batch 10897/11884 - Loss: 28.6845\n",
      "Processing batch 10898/11884 - Loss: 30.9593\n",
      "Processing batch 10899/11884 - Loss: 29.3322\n",
      "Processing batch 10900/11884 - Loss: 31.5229\n",
      "Processing batch 10901/11884 - Loss: 30.2669\n",
      "Processing batch 10902/11884 - Loss: 31.9809\n",
      "Processing batch 10903/11884 - Loss: 31.2586\n",
      "Processing batch 10904/11884 - Loss: 28.6949\n",
      "Processing batch 10905/11884 - Loss: 29.6810\n",
      "Processing batch 10906/11884 - Loss: 31.1330\n",
      "Processing batch 10907/11884 - Loss: 28.8547\n",
      "Processing batch 10908/11884 - Loss: 30.8221\n",
      "Processing batch 10909/11884 - Loss: 28.6586\n",
      "Processing batch 10910/11884 - Loss: 29.5718\n",
      "Processing batch 10911/11884 - Loss: 30.8309\n",
      "Processing batch 10912/11884 - Loss: 31.4791\n",
      "Processing batch 10913/11884 - Loss: 30.0255\n",
      "Processing batch 10914/11884 - Loss: 30.4668\n",
      "Processing batch 10915/11884 - Loss: 30.2167\n",
      "Processing batch 10916/11884 - Loss: 31.5683\n",
      "Processing batch 10917/11884 - Loss: 29.4801\n",
      "Processing batch 10918/11884 - Loss: 30.1245\n",
      "Processing batch 10919/11884 - Loss: 31.4915\n",
      "Processing batch 10920/11884 - Loss: 29.4620\n",
      "Processing batch 10921/11884 - Loss: 30.0018\n",
      "Processing batch 10922/11884 - Loss: 30.2917\n",
      "Processing batch 10923/11884 - Loss: 30.0924\n",
      "Processing batch 10924/11884 - Loss: 30.7571\n",
      "Processing batch 10925/11884 - Loss: 29.4485\n",
      "Processing batch 10926/11884 - Loss: 29.8102\n",
      "Processing batch 10927/11884 - Loss: 30.3577\n",
      "Processing batch 10928/11884 - Loss: 32.0736\n",
      "Processing batch 10929/11884 - Loss: 31.1136\n",
      "Processing batch 10930/11884 - Loss: 30.6211\n",
      "Processing batch 10931/11884 - Loss: 29.0484\n",
      "Processing batch 10932/11884 - Loss: 29.7608\n",
      "Processing batch 10933/11884 - Loss: 29.4545\n",
      "Processing batch 10934/11884 - Loss: 30.2266\n",
      "Processing batch 10935/11884 - Loss: 30.4884\n",
      "Processing batch 10936/11884 - Loss: 29.3599\n",
      "Processing batch 10937/11884 - Loss: 30.0825\n",
      "Processing batch 10938/11884 - Loss: 31.0336\n",
      "Processing batch 10939/11884 - Loss: 28.8794\n",
      "Processing batch 10940/11884 - Loss: 29.7958\n",
      "Processing batch 10941/11884 - Loss: 30.8637\n",
      "Processing batch 10942/11884 - Loss: 30.4712\n",
      "Processing batch 10943/11884 - Loss: 31.8062\n",
      "Processing batch 10944/11884 - Loss: 30.0574\n",
      "Processing batch 10945/11884 - Loss: 31.0200\n",
      "Processing batch 10946/11884 - Loss: 31.4997\n",
      "Processing batch 10947/11884 - Loss: 29.1022\n",
      "Processing batch 10948/11884 - Loss: 30.3667\n",
      "Processing batch 10949/11884 - Loss: 31.3185\n",
      "Processing batch 10950/11884 - Loss: 29.9894\n",
      "Processing batch 10951/11884 - Loss: 29.7999\n",
      "Processing batch 10952/11884 - Loss: 31.1454\n",
      "Processing batch 10953/11884 - Loss: 30.4165\n",
      "Processing batch 10954/11884 - Loss: 30.5594\n",
      "Processing batch 10955/11884 - Loss: 30.1457\n",
      "Processing batch 10956/11884 - Loss: 30.4349\n",
      "Processing batch 10957/11884 - Loss: 29.4457\n",
      "Processing batch 10958/11884 - Loss: 29.1635\n",
      "Processing batch 10959/11884 - Loss: 29.6525\n",
      "Processing batch 10960/11884 - Loss: 30.2316\n",
      "Processing batch 10961/11884 - Loss: 30.3943\n",
      "Processing batch 10962/11884 - Loss: 30.3507\n",
      "Processing batch 10963/11884 - Loss: 30.1506\n",
      "Processing batch 10964/11884 - Loss: 29.4609\n",
      "Processing batch 10965/11884 - Loss: 28.1231\n",
      "Processing batch 10966/11884 - Loss: 29.7593\n",
      "Processing batch 10967/11884 - Loss: 29.6552\n",
      "Processing batch 10968/11884 - Loss: 30.0791\n",
      "Processing batch 10969/11884 - Loss: 29.9892\n",
      "Processing batch 10970/11884 - Loss: 31.1418\n",
      "Processing batch 10971/11884 - Loss: 29.4035\n",
      "Processing batch 10972/11884 - Loss: 29.8917\n",
      "Processing batch 10973/11884 - Loss: 30.2240\n",
      "Processing batch 10974/11884 - Loss: 29.9873\n",
      "Processing batch 10975/11884 - Loss: 30.7801\n",
      "Processing batch 10976/11884 - Loss: 30.4748\n",
      "Processing batch 10977/11884 - Loss: 30.9074\n",
      "Processing batch 10978/11884 - Loss: 31.0444\n",
      "Processing batch 10979/11884 - Loss: 29.6897\n",
      "Processing batch 10980/11884 - Loss: 30.5993\n",
      "Processing batch 10981/11884 - Loss: 30.3440\n",
      "Processing batch 10982/11884 - Loss: 30.1812\n",
      "Processing batch 10983/11884 - Loss: 31.5711\n",
      "Processing batch 10984/11884 - Loss: 30.2857\n",
      "Processing batch 10985/11884 - Loss: 29.1176\n",
      "Processing batch 10986/11884 - Loss: 31.4844\n",
      "Processing batch 10987/11884 - Loss: 29.1725\n",
      "Processing batch 10988/11884 - Loss: 29.6250\n",
      "Processing batch 10989/11884 - Loss: 29.3221\n",
      "Processing batch 10990/11884 - Loss: 30.7404\n",
      "Processing batch 10991/11884 - Loss: 30.2326\n",
      "Processing batch 10992/11884 - Loss: 30.1509\n",
      "Processing batch 10993/11884 - Loss: 30.1670\n",
      "Processing batch 10994/11884 - Loss: 31.1309\n",
      "Processing batch 10995/11884 - Loss: 30.3895\n",
      "Processing batch 10996/11884 - Loss: 29.8575\n",
      "Processing batch 10997/11884 - Loss: 30.0649\n",
      "Processing batch 10998/11884 - Loss: 29.6797\n",
      "Processing batch 10999/11884 - Loss: 30.9550\n",
      "Processing batch 11000/11884 - Loss: 31.3420\n",
      "Processing batch 11001/11884 - Loss: 30.7269\n",
      "Processing batch 11002/11884 - Loss: 29.8485\n",
      "Processing batch 11003/11884 - Loss: 30.8069\n",
      "Processing batch 11004/11884 - Loss: 30.3804\n",
      "Processing batch 11005/11884 - Loss: 29.2675\n",
      "Processing batch 11006/11884 - Loss: 29.0135\n",
      "Processing batch 11007/11884 - Loss: 29.5862\n",
      "Processing batch 11008/11884 - Loss: 31.2145\n",
      "Processing batch 11009/11884 - Loss: 28.2348\n",
      "Processing batch 11010/11884 - Loss: 29.6392\n",
      "Processing batch 11011/11884 - Loss: 30.4346\n",
      "Processing batch 11012/11884 - Loss: 29.9468\n",
      "Processing batch 11013/11884 - Loss: 29.2474\n",
      "Processing batch 11014/11884 - Loss: 30.7308\n",
      "Processing batch 11015/11884 - Loss: 31.1730\n",
      "Processing batch 11016/11884 - Loss: 28.4021\n",
      "Processing batch 11017/11884 - Loss: 29.0951\n",
      "Processing batch 11018/11884 - Loss: 30.5860\n",
      "Processing batch 11019/11884 - Loss: 30.4749\n",
      "Processing batch 11020/11884 - Loss: 31.0651\n",
      "Processing batch 11021/11884 - Loss: 31.7966\n",
      "Processing batch 11022/11884 - Loss: 29.9895\n",
      "Processing batch 11023/11884 - Loss: 30.3366\n",
      "Processing batch 11024/11884 - Loss: 30.2491\n",
      "Processing batch 11025/11884 - Loss: 30.0687\n",
      "Processing batch 11026/11884 - Loss: 31.1478\n",
      "Processing batch 11027/11884 - Loss: 30.7810\n",
      "Processing batch 11028/11884 - Loss: 30.8310\n",
      "Processing batch 11029/11884 - Loss: 30.8254\n",
      "Processing batch 11030/11884 - Loss: 29.9719\n",
      "Processing batch 11031/11884 - Loss: 30.2309\n",
      "Processing batch 11032/11884 - Loss: 31.0311\n",
      "Processing batch 11033/11884 - Loss: 30.0714\n",
      "Processing batch 11034/11884 - Loss: 28.3843\n",
      "Processing batch 11035/11884 - Loss: 30.2735\n",
      "Processing batch 11036/11884 - Loss: 30.6195\n",
      "Processing batch 11037/11884 - Loss: 31.2692\n",
      "Processing batch 11038/11884 - Loss: 30.6021\n",
      "Processing batch 11039/11884 - Loss: 30.1792\n",
      "Processing batch 11040/11884 - Loss: 30.3754\n",
      "Processing batch 11041/11884 - Loss: 30.3918\n",
      "Processing batch 11042/11884 - Loss: 31.0558\n",
      "Processing batch 11043/11884 - Loss: 30.3519\n",
      "Processing batch 11044/11884 - Loss: 30.9037\n",
      "Processing batch 11045/11884 - Loss: 30.0067\n",
      "Processing batch 11046/11884 - Loss: 29.1713\n",
      "Processing batch 11047/11884 - Loss: 31.6896\n",
      "Processing batch 11048/11884 - Loss: 31.1860\n",
      "Processing batch 11049/11884 - Loss: 29.9238\n",
      "Processing batch 11050/11884 - Loss: 29.1779\n",
      "Processing batch 11051/11884 - Loss: 29.8850\n",
      "Processing batch 11052/11884 - Loss: 30.4858\n",
      "Processing batch 11053/11884 - Loss: 30.3113\n",
      "Processing batch 11054/11884 - Loss: 30.6442\n",
      "Processing batch 11055/11884 - Loss: 31.2128\n",
      "Processing batch 11056/11884 - Loss: 30.7206\n",
      "Processing batch 11057/11884 - Loss: 29.5645\n",
      "Processing batch 11058/11884 - Loss: 31.1276\n",
      "Processing batch 11059/11884 - Loss: 30.1342\n",
      "Processing batch 11060/11884 - Loss: 28.6187\n",
      "Processing batch 11061/11884 - Loss: 30.4231\n",
      "Processing batch 11062/11884 - Loss: 30.6158\n",
      "Processing batch 11063/11884 - Loss: 29.0702\n",
      "Processing batch 11064/11884 - Loss: 31.5093\n",
      "Processing batch 11065/11884 - Loss: 30.2868\n",
      "Processing batch 11066/11884 - Loss: 28.7083\n",
      "Processing batch 11067/11884 - Loss: 30.1733\n",
      "Processing batch 11068/11884 - Loss: 30.6531\n",
      "Processing batch 11069/11884 - Loss: 30.1937\n",
      "Processing batch 11070/11884 - Loss: 30.8717\n",
      "Processing batch 11071/11884 - Loss: 29.4788\n",
      "Processing batch 11072/11884 - Loss: 30.3140\n",
      "Processing batch 11073/11884 - Loss: 28.9838\n",
      "Processing batch 11074/11884 - Loss: 30.5037\n",
      "Processing batch 11075/11884 - Loss: 32.0650\n",
      "Processing batch 11076/11884 - Loss: 30.7405\n",
      "Processing batch 11077/11884 - Loss: 29.6861\n",
      "Processing batch 11078/11884 - Loss: 30.3287\n",
      "Processing batch 11079/11884 - Loss: 29.9050\n",
      "Processing batch 11080/11884 - Loss: 30.2688\n",
      "Processing batch 11081/11884 - Loss: 30.4234\n",
      "Processing batch 11082/11884 - Loss: 29.7619\n",
      "Processing batch 11083/11884 - Loss: 31.3833\n",
      "Processing batch 11084/11884 - Loss: 29.6405\n",
      "Processing batch 11085/11884 - Loss: 29.5466\n",
      "Processing batch 11086/11884 - Loss: 30.9986\n",
      "Processing batch 11087/11884 - Loss: 31.2372\n",
      "Processing batch 11088/11884 - Loss: 30.3891\n",
      "Processing batch 11089/11884 - Loss: 31.7858\n",
      "Processing batch 11090/11884 - Loss: 30.5277\n",
      "Processing batch 11091/11884 - Loss: 30.2216\n",
      "Processing batch 11092/11884 - Loss: 30.2538\n",
      "Processing batch 11093/11884 - Loss: 29.3578\n",
      "Processing batch 11094/11884 - Loss: 30.0770\n",
      "Processing batch 11095/11884 - Loss: 30.0247\n",
      "Processing batch 11096/11884 - Loss: 31.4155\n",
      "Processing batch 11097/11884 - Loss: 29.8433\n",
      "Processing batch 11098/11884 - Loss: 29.7267\n",
      "Processing batch 11099/11884 - Loss: 29.1997\n",
      "Processing batch 11100/11884 - Loss: 31.2853\n",
      "Processing batch 11101/11884 - Loss: 30.2926\n",
      "Processing batch 11102/11884 - Loss: 31.0138\n",
      "Processing batch 11103/11884 - Loss: 30.7540\n",
      "Processing batch 11104/11884 - Loss: 29.2399\n",
      "Processing batch 11105/11884 - Loss: 30.5322\n",
      "Processing batch 11106/11884 - Loss: 30.2747\n",
      "Processing batch 11107/11884 - Loss: 30.8625\n",
      "Processing batch 11108/11884 - Loss: 30.2426\n",
      "Processing batch 11109/11884 - Loss: 29.7913\n",
      "Processing batch 11110/11884 - Loss: 30.7309\n",
      "Processing batch 11111/11884 - Loss: 29.5089\n",
      "Processing batch 11112/11884 - Loss: 30.9614\n",
      "Processing batch 11113/11884 - Loss: 30.9890\n",
      "Processing batch 11114/11884 - Loss: 30.2841\n",
      "Processing batch 11115/11884 - Loss: 30.4194\n",
      "Processing batch 11116/11884 - Loss: 30.8732\n",
      "Processing batch 11117/11884 - Loss: 30.1320\n",
      "Processing batch 11118/11884 - Loss: 29.7279\n",
      "Processing batch 11119/11884 - Loss: 30.4335\n",
      "Processing batch 11120/11884 - Loss: 30.6749\n",
      "Processing batch 11121/11884 - Loss: 29.3428\n",
      "Processing batch 11122/11884 - Loss: 31.3968\n",
      "Processing batch 11123/11884 - Loss: 31.2159\n",
      "Processing batch 11124/11884 - Loss: 30.8100\n",
      "Processing batch 11125/11884 - Loss: 32.2938\n",
      "Processing batch 11126/11884 - Loss: 30.9521\n",
      "Processing batch 11127/11884 - Loss: 31.6510\n",
      "Processing batch 11128/11884 - Loss: 30.3659\n",
      "Processing batch 11129/11884 - Loss: 30.5293\n",
      "Processing batch 11130/11884 - Loss: 31.0196\n",
      "Processing batch 11131/11884 - Loss: 29.6801\n",
      "Processing batch 11132/11884 - Loss: 29.9315\n",
      "Processing batch 11133/11884 - Loss: 30.3986\n",
      "Processing batch 11134/11884 - Loss: 30.2775\n",
      "Processing batch 11135/11884 - Loss: 31.2122\n",
      "Processing batch 11136/11884 - Loss: 29.8163\n",
      "Processing batch 11137/11884 - Loss: 30.4154\n",
      "Processing batch 11138/11884 - Loss: 28.3161\n",
      "Processing batch 11139/11884 - Loss: 30.3519\n",
      "Processing batch 11140/11884 - Loss: 30.0374\n",
      "Processing batch 11141/11884 - Loss: 29.4492\n",
      "Processing batch 11142/11884 - Loss: 29.4472\n",
      "Processing batch 11143/11884 - Loss: 30.5256\n",
      "Processing batch 11144/11884 - Loss: 30.1664\n",
      "Processing batch 11145/11884 - Loss: 30.9354\n",
      "Processing batch 11146/11884 - Loss: 30.3715\n",
      "Processing batch 11147/11884 - Loss: 30.4853\n",
      "Processing batch 11148/11884 - Loss: 29.7425\n",
      "Processing batch 11149/11884 - Loss: 29.1541\n",
      "Processing batch 11150/11884 - Loss: 29.4181\n",
      "Processing batch 11151/11884 - Loss: 30.1856\n",
      "Processing batch 11152/11884 - Loss: 28.6655\n",
      "Processing batch 11153/11884 - Loss: 31.6389\n",
      "Processing batch 11154/11884 - Loss: 29.8287\n",
      "Processing batch 11155/11884 - Loss: 29.7106\n",
      "Processing batch 11156/11884 - Loss: 30.4396\n",
      "Processing batch 11157/11884 - Loss: 29.5256\n",
      "Processing batch 11158/11884 - Loss: 31.7303\n",
      "Processing batch 11159/11884 - Loss: 31.3020\n",
      "Processing batch 11160/11884 - Loss: 31.1780\n",
      "Processing batch 11161/11884 - Loss: 30.2106\n",
      "Processing batch 11162/11884 - Loss: 31.2352\n",
      "Processing batch 11163/11884 - Loss: 30.1085\n",
      "Processing batch 11164/11884 - Loss: 30.4431\n",
      "Processing batch 11165/11884 - Loss: 31.0874\n",
      "Processing batch 11166/11884 - Loss: 30.8500\n",
      "Processing batch 11167/11884 - Loss: 31.1375\n",
      "Processing batch 11168/11884 - Loss: 30.3834\n",
      "Processing batch 11169/11884 - Loss: 29.8532\n",
      "Processing batch 11170/11884 - Loss: 28.8691\n",
      "Processing batch 11171/11884 - Loss: 30.2428\n",
      "Processing batch 11172/11884 - Loss: 31.4633\n",
      "Processing batch 11173/11884 - Loss: 29.7355\n",
      "Processing batch 11174/11884 - Loss: 30.3216\n",
      "Processing batch 11175/11884 - Loss: 30.3252\n",
      "Processing batch 11176/11884 - Loss: 29.4084\n",
      "Processing batch 11177/11884 - Loss: 30.5829\n",
      "Processing batch 11178/11884 - Loss: 29.9938\n",
      "Processing batch 11179/11884 - Loss: 29.8100\n",
      "Processing batch 11180/11884 - Loss: 30.5585\n",
      "Processing batch 11181/11884 - Loss: 31.5037\n",
      "Processing batch 11182/11884 - Loss: 30.8289\n",
      "Processing batch 11183/11884 - Loss: 30.1573\n",
      "Processing batch 11184/11884 - Loss: 31.2152\n",
      "Processing batch 11185/11884 - Loss: 29.2283\n",
      "Processing batch 11186/11884 - Loss: 30.9532\n",
      "Processing batch 11187/11884 - Loss: 32.3613\n",
      "Processing batch 11188/11884 - Loss: 30.4177\n",
      "Processing batch 11189/11884 - Loss: 30.9351\n",
      "Processing batch 11190/11884 - Loss: 30.1411\n",
      "Processing batch 11191/11884 - Loss: 31.4090\n",
      "Processing batch 11192/11884 - Loss: 30.9753\n",
      "Processing batch 11193/11884 - Loss: 30.9008\n",
      "Processing batch 11194/11884 - Loss: 29.5306\n",
      "Processing batch 11195/11884 - Loss: 30.6859\n",
      "Processing batch 11196/11884 - Loss: 30.7011\n",
      "Processing batch 11197/11884 - Loss: 31.1208\n",
      "Processing batch 11198/11884 - Loss: 30.2011\n",
      "Processing batch 11199/11884 - Loss: 30.3087\n",
      "Processing batch 11200/11884 - Loss: 29.3358\n",
      "Processing batch 11201/11884 - Loss: 28.9641\n",
      "Processing batch 11202/11884 - Loss: 31.3096\n",
      "Processing batch 11203/11884 - Loss: 31.9004\n",
      "Processing batch 11204/11884 - Loss: 30.2287\n",
      "Processing batch 11205/11884 - Loss: 29.9244\n",
      "Processing batch 11206/11884 - Loss: 30.5009\n",
      "Processing batch 11207/11884 - Loss: 31.1703\n",
      "Processing batch 11208/11884 - Loss: 30.1533\n",
      "Processing batch 11209/11884 - Loss: 29.0652\n",
      "Processing batch 11210/11884 - Loss: 30.8477\n",
      "Processing batch 11211/11884 - Loss: 30.7764\n",
      "Processing batch 11212/11884 - Loss: 29.3575\n",
      "Processing batch 11213/11884 - Loss: 29.7296\n",
      "Processing batch 11214/11884 - Loss: 29.3817\n",
      "Processing batch 11215/11884 - Loss: 30.3679\n",
      "Processing batch 11216/11884 - Loss: 30.6261\n",
      "Processing batch 11217/11884 - Loss: 30.3146\n",
      "Processing batch 11218/11884 - Loss: 28.9732\n",
      "Processing batch 11219/11884 - Loss: 29.2396\n",
      "Processing batch 11220/11884 - Loss: 29.7961\n",
      "Processing batch 11221/11884 - Loss: 31.2256\n",
      "Processing batch 11222/11884 - Loss: 29.4589\n",
      "Processing batch 11223/11884 - Loss: 30.5653\n",
      "Processing batch 11224/11884 - Loss: 30.0947\n",
      "Processing batch 11225/11884 - Loss: 31.8035\n",
      "Processing batch 11226/11884 - Loss: 31.1840\n",
      "Processing batch 11227/11884 - Loss: 30.3656\n",
      "Processing batch 11228/11884 - Loss: 31.5744\n",
      "Processing batch 11229/11884 - Loss: 30.7926\n",
      "Processing batch 11230/11884 - Loss: 28.8479\n",
      "Processing batch 11231/11884 - Loss: 29.6123\n",
      "Processing batch 11232/11884 - Loss: 29.3866\n",
      "Processing batch 11233/11884 - Loss: 31.2302\n",
      "Processing batch 11234/11884 - Loss: 30.1871\n",
      "Processing batch 11235/11884 - Loss: 31.0865\n",
      "Processing batch 11236/11884 - Loss: 31.3602\n",
      "Processing batch 11237/11884 - Loss: 31.2387\n",
      "Processing batch 11238/11884 - Loss: 29.4235\n",
      "Processing batch 11239/11884 - Loss: 30.3753\n",
      "Processing batch 11240/11884 - Loss: 29.7391\n",
      "Processing batch 11241/11884 - Loss: 30.7409\n",
      "Processing batch 11242/11884 - Loss: 30.4498\n",
      "Processing batch 11243/11884 - Loss: 30.9749\n",
      "Processing batch 11244/11884 - Loss: 29.2792\n",
      "Processing batch 11245/11884 - Loss: 30.1037\n",
      "Processing batch 11246/11884 - Loss: 30.8750\n",
      "Processing batch 11247/11884 - Loss: 30.4999\n",
      "Processing batch 11248/11884 - Loss: 30.0403\n",
      "Processing batch 11249/11884 - Loss: 29.6645\n",
      "Processing batch 11250/11884 - Loss: 29.6974\n",
      "Processing batch 11251/11884 - Loss: 30.6024\n",
      "Processing batch 11252/11884 - Loss: 29.9017\n",
      "Processing batch 11253/11884 - Loss: 29.1473\n",
      "Processing batch 11254/11884 - Loss: 31.0494\n",
      "Processing batch 11255/11884 - Loss: 31.2338\n",
      "Processing batch 11256/11884 - Loss: 30.2373\n",
      "Processing batch 11257/11884 - Loss: 30.3755\n",
      "Processing batch 11258/11884 - Loss: 30.7467\n",
      "Processing batch 11259/11884 - Loss: 30.6726\n",
      "Processing batch 11260/11884 - Loss: 30.5331\n",
      "Processing batch 11261/11884 - Loss: 30.1135\n",
      "Processing batch 11262/11884 - Loss: 30.6241\n",
      "Processing batch 11263/11884 - Loss: 31.2886\n",
      "Processing batch 11264/11884 - Loss: 31.0811\n",
      "Processing batch 11265/11884 - Loss: 29.8136\n",
      "Processing batch 11266/11884 - Loss: 29.4524\n",
      "Processing batch 11267/11884 - Loss: 29.0152\n",
      "Processing batch 11268/11884 - Loss: 29.7105\n",
      "Processing batch 11269/11884 - Loss: 28.8271\n",
      "Processing batch 11270/11884 - Loss: 29.9060\n",
      "Processing batch 11271/11884 - Loss: 30.4674\n",
      "Processing batch 11272/11884 - Loss: 31.0928\n",
      "Processing batch 11273/11884 - Loss: 29.8926\n",
      "Processing batch 11274/11884 - Loss: 30.6460\n",
      "Processing batch 11275/11884 - Loss: 30.8284\n",
      "Processing batch 11276/11884 - Loss: 29.9492\n",
      "Processing batch 11277/11884 - Loss: 28.6455\n",
      "Processing batch 11278/11884 - Loss: 31.8518\n",
      "Processing batch 11279/11884 - Loss: 28.9139\n",
      "Processing batch 11280/11884 - Loss: 30.1888\n",
      "Processing batch 11281/11884 - Loss: 30.7436\n",
      "Processing batch 11282/11884 - Loss: 30.1187\n",
      "Processing batch 11283/11884 - Loss: 30.6296\n",
      "Processing batch 11284/11884 - Loss: 31.8574\n",
      "Processing batch 11285/11884 - Loss: 29.9928\n",
      "Processing batch 11286/11884 - Loss: 30.3799\n",
      "Processing batch 11287/11884 - Loss: 29.9313\n",
      "Processing batch 11288/11884 - Loss: 30.6055\n",
      "Processing batch 11289/11884 - Loss: 29.6915\n",
      "Processing batch 11290/11884 - Loss: 30.6113\n",
      "Processing batch 11291/11884 - Loss: 30.8746\n",
      "Processing batch 11292/11884 - Loss: 32.0320\n",
      "Processing batch 11293/11884 - Loss: 30.2739\n",
      "Processing batch 11294/11884 - Loss: 30.7179\n",
      "Processing batch 11295/11884 - Loss: 32.4189\n",
      "Processing batch 11296/11884 - Loss: 30.4560\n",
      "Processing batch 11297/11884 - Loss: 30.4658\n",
      "Processing batch 11298/11884 - Loss: 30.9715\n",
      "Processing batch 11299/11884 - Loss: 31.2944\n",
      "Processing batch 11300/11884 - Loss: 30.6648\n",
      "Processing batch 11301/11884 - Loss: 30.8855\n",
      "Processing batch 11302/11884 - Loss: 30.4955\n",
      "Processing batch 11303/11884 - Loss: 30.8313\n",
      "Processing batch 11304/11884 - Loss: 30.8153\n",
      "Processing batch 11305/11884 - Loss: 30.9839\n",
      "Processing batch 11306/11884 - Loss: 30.4320\n",
      "Processing batch 11307/11884 - Loss: 30.5324\n",
      "Processing batch 11308/11884 - Loss: 30.0531\n",
      "Processing batch 11309/11884 - Loss: 30.2673\n",
      "Processing batch 11310/11884 - Loss: 29.8875\n",
      "Processing batch 11311/11884 - Loss: 30.5122\n",
      "Processing batch 11312/11884 - Loss: 31.1157\n",
      "Processing batch 11313/11884 - Loss: 30.7112\n",
      "Processing batch 11314/11884 - Loss: 30.8263\n",
      "Processing batch 11315/11884 - Loss: 31.6433\n",
      "Processing batch 11316/11884 - Loss: 30.3858\n",
      "Processing batch 11317/11884 - Loss: 30.2622\n",
      "Processing batch 11318/11884 - Loss: 29.6102\n",
      "Processing batch 11319/11884 - Loss: 30.3584\n",
      "Processing batch 11320/11884 - Loss: 29.5134\n",
      "Processing batch 11321/11884 - Loss: 31.1330\n",
      "Processing batch 11322/11884 - Loss: 30.9724\n",
      "Processing batch 11323/11884 - Loss: 30.8263\n",
      "Processing batch 11324/11884 - Loss: 30.5118\n",
      "Processing batch 11325/11884 - Loss: 29.9414\n",
      "Processing batch 11326/11884 - Loss: 28.9738\n",
      "Processing batch 11327/11884 - Loss: 30.4819\n",
      "Processing batch 11328/11884 - Loss: 30.5163\n",
      "Processing batch 11329/11884 - Loss: 29.4879\n",
      "Processing batch 11330/11884 - Loss: 31.2334\n",
      "Processing batch 11331/11884 - Loss: 31.0661\n",
      "Processing batch 11332/11884 - Loss: 29.8350\n",
      "Processing batch 11333/11884 - Loss: 30.8175\n",
      "Processing batch 11334/11884 - Loss: 30.4613\n",
      "Processing batch 11335/11884 - Loss: 30.9212\n",
      "Processing batch 11336/11884 - Loss: 31.2484\n",
      "Processing batch 11337/11884 - Loss: 29.4862\n",
      "Processing batch 11338/11884 - Loss: 29.9332\n",
      "Processing batch 11339/11884 - Loss: 29.8595\n",
      "Processing batch 11340/11884 - Loss: 30.2006\n",
      "Processing batch 11341/11884 - Loss: 29.4252\n",
      "Processing batch 11342/11884 - Loss: 29.5270\n",
      "Processing batch 11343/11884 - Loss: 30.3190\n",
      "Processing batch 11344/11884 - Loss: 30.7484\n",
      "Processing batch 11345/11884 - Loss: 31.6732\n",
      "Processing batch 11346/11884 - Loss: 30.5654\n",
      "Processing batch 11347/11884 - Loss: 29.9181\n",
      "Processing batch 11348/11884 - Loss: 29.9677\n",
      "Processing batch 11349/11884 - Loss: 31.2536\n",
      "Processing batch 11350/11884 - Loss: 29.7016\n",
      "Processing batch 11351/11884 - Loss: 30.7286\n",
      "Processing batch 11352/11884 - Loss: 31.3455\n",
      "Processing batch 11353/11884 - Loss: 30.8923\n",
      "Processing batch 11354/11884 - Loss: 31.3222\n",
      "Processing batch 11355/11884 - Loss: 30.1835\n",
      "Processing batch 11356/11884 - Loss: 30.0477\n",
      "Processing batch 11357/11884 - Loss: 30.8099\n",
      "Processing batch 11358/11884 - Loss: 29.9840\n",
      "Processing batch 11359/11884 - Loss: 30.9026\n",
      "Processing batch 11360/11884 - Loss: 30.1110\n",
      "Processing batch 11361/11884 - Loss: 29.7806\n",
      "Processing batch 11362/11884 - Loss: 30.1950\n",
      "Processing batch 11363/11884 - Loss: 29.2876\n",
      "Processing batch 11364/11884 - Loss: 29.2479\n",
      "Processing batch 11365/11884 - Loss: 29.0489\n",
      "Processing batch 11366/11884 - Loss: 30.4462\n",
      "Processing batch 11367/11884 - Loss: 29.8173\n",
      "Processing batch 11368/11884 - Loss: 30.8949\n",
      "Processing batch 11369/11884 - Loss: 29.5752\n",
      "Processing batch 11370/11884 - Loss: 31.6139\n",
      "Processing batch 11371/11884 - Loss: 29.2440\n",
      "Processing batch 11372/11884 - Loss: 29.9181\n",
      "Processing batch 11373/11884 - Loss: 29.7662\n",
      "Processing batch 11374/11884 - Loss: 30.7738\n",
      "Processing batch 11375/11884 - Loss: 30.2177\n",
      "Processing batch 11376/11884 - Loss: 30.4386\n",
      "Processing batch 11377/11884 - Loss: 30.7796\n",
      "Processing batch 11378/11884 - Loss: 31.1626\n",
      "Processing batch 11379/11884 - Loss: 30.8617\n",
      "Processing batch 11380/11884 - Loss: 31.5985\n",
      "Processing batch 11381/11884 - Loss: 30.4626\n",
      "Processing batch 11382/11884 - Loss: 31.2449\n",
      "Processing batch 11383/11884 - Loss: 29.6424\n",
      "Processing batch 11384/11884 - Loss: 30.8245\n",
      "Processing batch 11385/11884 - Loss: 30.8272\n",
      "Processing batch 11386/11884 - Loss: 32.7137\n",
      "Processing batch 11387/11884 - Loss: 29.4629\n",
      "Processing batch 11388/11884 - Loss: 31.3290\n",
      "Processing batch 11389/11884 - Loss: 30.0358\n",
      "Processing batch 11390/11884 - Loss: 29.7973\n",
      "Processing batch 11391/11884 - Loss: 30.9737\n",
      "Processing batch 11392/11884 - Loss: 29.4296\n",
      "Processing batch 11393/11884 - Loss: 30.1245\n",
      "Processing batch 11394/11884 - Loss: 29.9195\n",
      "Processing batch 11395/11884 - Loss: 29.5848\n",
      "Processing batch 11396/11884 - Loss: 29.9598\n",
      "Processing batch 11397/11884 - Loss: 31.4377\n",
      "Processing batch 11398/11884 - Loss: 30.2897\n",
      "Processing batch 11399/11884 - Loss: 31.5245\n",
      "Processing batch 11400/11884 - Loss: 31.2246\n",
      "Processing batch 11401/11884 - Loss: 31.1184\n",
      "Processing batch 11402/11884 - Loss: 30.6416\n",
      "Processing batch 11403/11884 - Loss: 31.0447\n",
      "Processing batch 11404/11884 - Loss: 29.7461\n",
      "Processing batch 11405/11884 - Loss: 29.9648\n",
      "Processing batch 11406/11884 - Loss: 30.5368\n",
      "Processing batch 11407/11884 - Loss: 29.9476\n",
      "Processing batch 11408/11884 - Loss: 30.8165\n",
      "Processing batch 11409/11884 - Loss: 30.6171\n",
      "Processing batch 11410/11884 - Loss: 29.9357\n",
      "Processing batch 11411/11884 - Loss: 29.9912\n",
      "Processing batch 11412/11884 - Loss: 28.3475\n",
      "Processing batch 11413/11884 - Loss: 29.2729\n",
      "Processing batch 11414/11884 - Loss: 30.1851\n",
      "Processing batch 11415/11884 - Loss: 29.6327\n",
      "Processing batch 11416/11884 - Loss: 29.4798\n",
      "Processing batch 11417/11884 - Loss: 30.1435\n",
      "Processing batch 11418/11884 - Loss: 30.8324\n",
      "Processing batch 11419/11884 - Loss: 30.9517\n",
      "Processing batch 11420/11884 - Loss: 29.2538\n",
      "Processing batch 11421/11884 - Loss: 30.9826\n",
      "Processing batch 11422/11884 - Loss: 31.0551\n",
      "Processing batch 11423/11884 - Loss: 29.1352\n",
      "Processing batch 11424/11884 - Loss: 29.6864\n",
      "Processing batch 11425/11884 - Loss: 29.8576\n",
      "Processing batch 11426/11884 - Loss: 30.7046\n",
      "Processing batch 11427/11884 - Loss: 30.4806\n",
      "Processing batch 11428/11884 - Loss: 28.8003\n",
      "Processing batch 11429/11884 - Loss: 30.6379\n",
      "Processing batch 11430/11884 - Loss: 29.4384\n",
      "Processing batch 11431/11884 - Loss: 30.8021\n",
      "Processing batch 11432/11884 - Loss: 30.4035\n",
      "Processing batch 11433/11884 - Loss: 30.6311\n",
      "Processing batch 11434/11884 - Loss: 29.7563\n",
      "Processing batch 11435/11884 - Loss: 29.6193\n",
      "Processing batch 11436/11884 - Loss: 30.4202\n",
      "Processing batch 11437/11884 - Loss: 31.4572\n",
      "Processing batch 11438/11884 - Loss: 30.0665\n",
      "Processing batch 11439/11884 - Loss: 29.8672\n",
      "Processing batch 11440/11884 - Loss: 30.1450\n",
      "Processing batch 11441/11884 - Loss: 29.7809\n",
      "Processing batch 11442/11884 - Loss: 29.9974\n",
      "Processing batch 11443/11884 - Loss: 31.8116\n",
      "Processing batch 11444/11884 - Loss: 29.4753\n",
      "Processing batch 11445/11884 - Loss: 30.2108\n",
      "Processing batch 11446/11884 - Loss: 29.5909\n",
      "Processing batch 11447/11884 - Loss: 31.4640\n",
      "Processing batch 11448/11884 - Loss: 30.9369\n",
      "Processing batch 11449/11884 - Loss: 29.7087\n",
      "Processing batch 11450/11884 - Loss: 30.7045\n",
      "Processing batch 11451/11884 - Loss: 29.6673\n",
      "Processing batch 11452/11884 - Loss: 30.4262\n",
      "Processing batch 11453/11884 - Loss: 29.7065\n",
      "Processing batch 11454/11884 - Loss: 30.8571\n",
      "Processing batch 11455/11884 - Loss: 28.9280\n",
      "Processing batch 11456/11884 - Loss: 29.7551\n",
      "Processing batch 11457/11884 - Loss: 29.6237\n",
      "Processing batch 11458/11884 - Loss: 30.1668\n",
      "Processing batch 11459/11884 - Loss: 30.3007\n",
      "Processing batch 11460/11884 - Loss: 30.6700\n",
      "Processing batch 11461/11884 - Loss: 29.6113\n",
      "Processing batch 11462/11884 - Loss: 30.9112\n",
      "Processing batch 11463/11884 - Loss: 30.7717\n",
      "Processing batch 11464/11884 - Loss: 30.9535\n",
      "Processing batch 11465/11884 - Loss: 30.5530\n",
      "Processing batch 11466/11884 - Loss: 30.8737\n",
      "Processing batch 11467/11884 - Loss: 29.8897\n",
      "Processing batch 11468/11884 - Loss: 31.8045\n",
      "Processing batch 11469/11884 - Loss: 29.9662\n",
      "Processing batch 11470/11884 - Loss: 31.4318\n",
      "Processing batch 11471/11884 - Loss: 30.1813\n",
      "Processing batch 11472/11884 - Loss: 28.7273\n",
      "Processing batch 11473/11884 - Loss: 30.9862\n",
      "Processing batch 11474/11884 - Loss: 30.9177\n",
      "Processing batch 11475/11884 - Loss: 31.3600\n",
      "Processing batch 11476/11884 - Loss: 30.7307\n",
      "Processing batch 11477/11884 - Loss: 30.0416\n",
      "Processing batch 11478/11884 - Loss: 30.2111\n",
      "Processing batch 11479/11884 - Loss: 31.2318\n",
      "Processing batch 11480/11884 - Loss: 30.5427\n",
      "Processing batch 11481/11884 - Loss: 30.0211\n",
      "Processing batch 11482/11884 - Loss: 30.8014\n",
      "Processing batch 11483/11884 - Loss: 30.1864\n",
      "Processing batch 11484/11884 - Loss: 31.3273\n",
      "Processing batch 11485/11884 - Loss: 29.6041\n",
      "Processing batch 11486/11884 - Loss: 29.6643\n",
      "Processing batch 11487/11884 - Loss: 30.3215\n",
      "Processing batch 11488/11884 - Loss: 29.7591\n",
      "Processing batch 11489/11884 - Loss: 30.6929\n",
      "Processing batch 11490/11884 - Loss: 29.4546\n",
      "Processing batch 11491/11884 - Loss: 28.7998\n",
      "Processing batch 11492/11884 - Loss: 29.9020\n",
      "Processing batch 11493/11884 - Loss: 30.5229\n",
      "Processing batch 11494/11884 - Loss: 29.7085\n",
      "Processing batch 11495/11884 - Loss: 31.6856\n",
      "Processing batch 11496/11884 - Loss: 28.8632\n",
      "Processing batch 11497/11884 - Loss: 31.1783\n",
      "Processing batch 11498/11884 - Loss: 29.7705\n",
      "Processing batch 11499/11884 - Loss: 30.7242\n",
      "Processing batch 11500/11884 - Loss: 29.9032\n",
      "Processing batch 11501/11884 - Loss: 30.0594\n",
      "Processing batch 11502/11884 - Loss: 30.0543\n",
      "Processing batch 11503/11884 - Loss: 31.3459\n",
      "Processing batch 11504/11884 - Loss: 30.2265\n",
      "Processing batch 11505/11884 - Loss: 30.4284\n",
      "Processing batch 11506/11884 - Loss: 30.2978\n",
      "Processing batch 11507/11884 - Loss: 30.9112\n",
      "Processing batch 11508/11884 - Loss: 31.1722\n",
      "Processing batch 11509/11884 - Loss: 30.4611\n",
      "Processing batch 11510/11884 - Loss: 30.8646\n",
      "Processing batch 11511/11884 - Loss: 29.3728\n",
      "Processing batch 11512/11884 - Loss: 30.4173\n",
      "Processing batch 11513/11884 - Loss: 29.5872\n",
      "Processing batch 11514/11884 - Loss: 30.6829\n",
      "Processing batch 11515/11884 - Loss: 29.5118\n",
      "Processing batch 11516/11884 - Loss: 29.6573\n",
      "Processing batch 11517/11884 - Loss: 31.4930\n",
      "Processing batch 11518/11884 - Loss: 30.4678\n",
      "Processing batch 11519/11884 - Loss: 29.6962\n",
      "Processing batch 11520/11884 - Loss: 31.6114\n",
      "Processing batch 11521/11884 - Loss: 30.9915\n",
      "Processing batch 11522/11884 - Loss: 29.5934\n",
      "Processing batch 11523/11884 - Loss: 30.6219\n",
      "Processing batch 11524/11884 - Loss: 30.1669\n",
      "Processing batch 11525/11884 - Loss: 31.0059\n",
      "Processing batch 11526/11884 - Loss: 29.2447\n",
      "Processing batch 11527/11884 - Loss: 29.9853\n",
      "Processing batch 11528/11884 - Loss: 30.3809\n",
      "Processing batch 11529/11884 - Loss: 28.6627\n",
      "Processing batch 11530/11884 - Loss: 31.1751\n",
      "Processing batch 11531/11884 - Loss: 30.3857\n",
      "Processing batch 11532/11884 - Loss: 30.1201\n",
      "Processing batch 11533/11884 - Loss: 30.0261\n",
      "Processing batch 11534/11884 - Loss: 29.8340\n",
      "Processing batch 11535/11884 - Loss: 28.9260\n",
      "Processing batch 11536/11884 - Loss: 30.5391\n",
      "Processing batch 11537/11884 - Loss: 31.2458\n",
      "Processing batch 11538/11884 - Loss: 30.4170\n",
      "Processing batch 11539/11884 - Loss: 29.5005\n",
      "Processing batch 11540/11884 - Loss: 29.8446\n",
      "Processing batch 11541/11884 - Loss: 31.1754\n",
      "Processing batch 11542/11884 - Loss: 31.1401\n",
      "Processing batch 11543/11884 - Loss: 30.2109\n",
      "Processing batch 11544/11884 - Loss: 30.7705\n",
      "Processing batch 11545/11884 - Loss: 31.0598\n",
      "Processing batch 11546/11884 - Loss: 29.6211\n",
      "Processing batch 11547/11884 - Loss: 29.4773\n",
      "Processing batch 11548/11884 - Loss: 30.5109\n",
      "Processing batch 11549/11884 - Loss: 29.9290\n",
      "Processing batch 11550/11884 - Loss: 30.2197\n",
      "Processing batch 11551/11884 - Loss: 29.5288\n",
      "Processing batch 11552/11884 - Loss: 30.6771\n",
      "Processing batch 11553/11884 - Loss: 29.7329\n",
      "Processing batch 11554/11884 - Loss: 31.0814\n",
      "Processing batch 11555/11884 - Loss: 29.3670\n",
      "Processing batch 11556/11884 - Loss: 31.3302\n",
      "Processing batch 11557/11884 - Loss: 29.2295\n",
      "Processing batch 11558/11884 - Loss: 29.4193\n",
      "Processing batch 11559/11884 - Loss: 30.2010\n",
      "Processing batch 11560/11884 - Loss: 29.5914\n",
      "Processing batch 11561/11884 - Loss: 32.1509\n",
      "Processing batch 11562/11884 - Loss: 31.5431\n",
      "Processing batch 11563/11884 - Loss: 30.4922\n",
      "Processing batch 11564/11884 - Loss: 29.2246\n",
      "Processing batch 11565/11884 - Loss: 31.8688\n",
      "Processing batch 11566/11884 - Loss: 30.6674\n",
      "Processing batch 11567/11884 - Loss: 30.6702\n",
      "Processing batch 11568/11884 - Loss: 29.2890\n",
      "Processing batch 11569/11884 - Loss: 31.7750\n",
      "Processing batch 11570/11884 - Loss: 30.7322\n",
      "Processing batch 11571/11884 - Loss: 30.0438\n",
      "Processing batch 11572/11884 - Loss: 30.9145\n",
      "Processing batch 11573/11884 - Loss: 29.8309\n",
      "Processing batch 11574/11884 - Loss: 29.6934\n",
      "Processing batch 11575/11884 - Loss: 30.9513\n",
      "Processing batch 11576/11884 - Loss: 29.8167\n",
      "Processing batch 11577/11884 - Loss: 32.1155\n",
      "Processing batch 11578/11884 - Loss: 29.7466\n",
      "Processing batch 11579/11884 - Loss: 31.4083\n",
      "Processing batch 11580/11884 - Loss: 30.6097\n",
      "Processing batch 11581/11884 - Loss: 30.8500\n",
      "Processing batch 11582/11884 - Loss: 29.3933\n",
      "Processing batch 11583/11884 - Loss: 29.9693\n",
      "Processing batch 11584/11884 - Loss: 28.5326\n",
      "Processing batch 11585/11884 - Loss: 30.7319\n",
      "Processing batch 11586/11884 - Loss: 29.3475\n",
      "Processing batch 11587/11884 - Loss: 28.2665\n",
      "Processing batch 11588/11884 - Loss: 30.5703\n",
      "Processing batch 11589/11884 - Loss: 30.5839\n",
      "Processing batch 11590/11884 - Loss: 28.8889\n",
      "Processing batch 11591/11884 - Loss: 31.3382\n",
      "Processing batch 11592/11884 - Loss: 30.3137\n",
      "Processing batch 11593/11884 - Loss: 29.9069\n",
      "Processing batch 11594/11884 - Loss: 31.2786\n",
      "Processing batch 11595/11884 - Loss: 30.7141\n",
      "Processing batch 11596/11884 - Loss: 30.8564\n",
      "Processing batch 11597/11884 - Loss: 30.6356\n",
      "Processing batch 11598/11884 - Loss: 30.1823\n",
      "Processing batch 11599/11884 - Loss: 30.4415\n",
      "Processing batch 11600/11884 - Loss: 30.1316\n",
      "Processing batch 11601/11884 - Loss: 28.8334\n",
      "Processing batch 11602/11884 - Loss: 30.1233\n",
      "Processing batch 11603/11884 - Loss: 29.0982\n",
      "Processing batch 11604/11884 - Loss: 29.3789\n",
      "Processing batch 11605/11884 - Loss: 30.9689\n",
      "Processing batch 11606/11884 - Loss: 30.3680\n",
      "Processing batch 11607/11884 - Loss: 30.9547\n",
      "Processing batch 11608/11884 - Loss: 29.9828\n",
      "Processing batch 11609/11884 - Loss: 29.7358\n",
      "Processing batch 11610/11884 - Loss: 30.2300\n",
      "Processing batch 11611/11884 - Loss: 31.0044\n",
      "Processing batch 11612/11884 - Loss: 31.3636\n",
      "Processing batch 11613/11884 - Loss: 30.2654\n",
      "Processing batch 11614/11884 - Loss: 29.0940\n",
      "Processing batch 11615/11884 - Loss: 29.6807\n",
      "Processing batch 11616/11884 - Loss: 30.1291\n",
      "Processing batch 11617/11884 - Loss: 31.0180\n",
      "Processing batch 11618/11884 - Loss: 28.4469\n",
      "Processing batch 11619/11884 - Loss: 29.5240\n",
      "Processing batch 11620/11884 - Loss: 30.6562\n",
      "Processing batch 11621/11884 - Loss: 30.3675\n",
      "Processing batch 11622/11884 - Loss: 31.5621\n",
      "Processing batch 11623/11884 - Loss: 30.4321\n",
      "Processing batch 11624/11884 - Loss: 28.4642\n",
      "Processing batch 11625/11884 - Loss: 29.7099\n",
      "Processing batch 11626/11884 - Loss: 29.0090\n",
      "Processing batch 11627/11884 - Loss: 30.6513\n",
      "Processing batch 11628/11884 - Loss: 30.9413\n",
      "Processing batch 11629/11884 - Loss: 30.7773\n",
      "Processing batch 11630/11884 - Loss: 30.7558\n",
      "Processing batch 11631/11884 - Loss: 29.7764\n",
      "Processing batch 11632/11884 - Loss: 29.5069\n",
      "Processing batch 11633/11884 - Loss: 30.4965\n",
      "Processing batch 11634/11884 - Loss: 30.8461\n",
      "Processing batch 11635/11884 - Loss: 29.8383\n",
      "Processing batch 11636/11884 - Loss: 29.2841\n",
      "Processing batch 11637/11884 - Loss: 31.1210\n",
      "Processing batch 11638/11884 - Loss: 30.1140\n",
      "Processing batch 11639/11884 - Loss: 31.0297\n",
      "Processing batch 11640/11884 - Loss: 29.3619\n",
      "Processing batch 11641/11884 - Loss: 31.7253\n",
      "Processing batch 11642/11884 - Loss: 30.0181\n",
      "Processing batch 11643/11884 - Loss: 30.2245\n",
      "Processing batch 11644/11884 - Loss: 29.1162\n",
      "Processing batch 11645/11884 - Loss: 30.0622\n",
      "Processing batch 11646/11884 - Loss: 31.2462\n",
      "Processing batch 11647/11884 - Loss: 29.9980\n",
      "Processing batch 11648/11884 - Loss: 30.1128\n",
      "Processing batch 11649/11884 - Loss: 30.3870\n",
      "Processing batch 11650/11884 - Loss: 31.0783\n",
      "Processing batch 11651/11884 - Loss: 29.3724\n",
      "Processing batch 11652/11884 - Loss: 29.7509\n",
      "Processing batch 11653/11884 - Loss: 29.0117\n",
      "Processing batch 11654/11884 - Loss: 30.4843\n",
      "Processing batch 11655/11884 - Loss: 31.0451\n",
      "Processing batch 11656/11884 - Loss: 29.9794\n",
      "Processing batch 11657/11884 - Loss: 29.8838\n",
      "Processing batch 11658/11884 - Loss: 30.2222\n",
      "Processing batch 11659/11884 - Loss: 29.3448\n",
      "Processing batch 11660/11884 - Loss: 29.8103\n",
      "Processing batch 11661/11884 - Loss: 30.9676\n",
      "Processing batch 11662/11884 - Loss: 30.2063\n",
      "Processing batch 11663/11884 - Loss: 30.8667\n",
      "Processing batch 11664/11884 - Loss: 29.9894\n",
      "Processing batch 11665/11884 - Loss: 30.2916\n",
      "Processing batch 11666/11884 - Loss: 30.0867\n",
      "Processing batch 11667/11884 - Loss: 29.5601\n",
      "Processing batch 11668/11884 - Loss: 29.8396\n",
      "Processing batch 11669/11884 - Loss: 30.4346\n",
      "Processing batch 11670/11884 - Loss: 29.9106\n",
      "Processing batch 11671/11884 - Loss: 30.4815\n",
      "Processing batch 11672/11884 - Loss: 29.5666\n",
      "Processing batch 11673/11884 - Loss: 30.0912\n",
      "Processing batch 11674/11884 - Loss: 31.6980\n",
      "Processing batch 11675/11884 - Loss: 30.8366\n",
      "Processing batch 11676/11884 - Loss: 29.7148\n",
      "Processing batch 11677/11884 - Loss: 29.8972\n",
      "Processing batch 11678/11884 - Loss: 30.4607\n",
      "Processing batch 11679/11884 - Loss: 29.4120\n",
      "Processing batch 11680/11884 - Loss: 32.1604\n",
      "Processing batch 11681/11884 - Loss: 29.7682\n",
      "Processing batch 11682/11884 - Loss: 30.3279\n",
      "Processing batch 11683/11884 - Loss: 30.9939\n",
      "Processing batch 11684/11884 - Loss: 30.0454\n",
      "Processing batch 11685/11884 - Loss: 29.5098\n",
      "Processing batch 11686/11884 - Loss: 29.5596\n",
      "Processing batch 11687/11884 - Loss: 30.3577\n",
      "Processing batch 11688/11884 - Loss: 29.3913\n",
      "Processing batch 11689/11884 - Loss: 29.1106\n",
      "Processing batch 11690/11884 - Loss: 30.7987\n",
      "Processing batch 11691/11884 - Loss: 30.1121\n",
      "Processing batch 11692/11884 - Loss: 31.0614\n",
      "Processing batch 11693/11884 - Loss: 30.0639\n",
      "Processing batch 11694/11884 - Loss: 32.5471\n",
      "Processing batch 11695/11884 - Loss: 29.9893\n",
      "Processing batch 11696/11884 - Loss: 29.2620\n",
      "Processing batch 11697/11884 - Loss: 29.3870\n",
      "Processing batch 11698/11884 - Loss: 30.9569\n",
      "Processing batch 11699/11884 - Loss: 29.5956\n",
      "Processing batch 11700/11884 - Loss: 30.1499\n",
      "Processing batch 11701/11884 - Loss: 29.4806\n",
      "Processing batch 11702/11884 - Loss: 30.3717\n",
      "Processing batch 11703/11884 - Loss: 29.2091\n",
      "Processing batch 11704/11884 - Loss: 29.4343\n",
      "Processing batch 11705/11884 - Loss: 30.5113\n",
      "Processing batch 11706/11884 - Loss: 30.0728\n",
      "Processing batch 11707/11884 - Loss: 29.6863\n",
      "Processing batch 11708/11884 - Loss: 30.2363\n",
      "Processing batch 11709/11884 - Loss: 30.7273\n",
      "Processing batch 11710/11884 - Loss: 30.8017\n",
      "Processing batch 11711/11884 - Loss: 30.4308\n",
      "Processing batch 11712/11884 - Loss: 31.1620\n",
      "Processing batch 11713/11884 - Loss: 30.2790\n",
      "Processing batch 11714/11884 - Loss: 29.1543\n",
      "Processing batch 11715/11884 - Loss: 30.1596\n",
      "Processing batch 11716/11884 - Loss: 29.1027\n",
      "Processing batch 11717/11884 - Loss: 30.2599\n",
      "Processing batch 11718/11884 - Loss: 30.7218\n",
      "Processing batch 11719/11884 - Loss: 29.8236\n",
      "Processing batch 11720/11884 - Loss: 30.5087\n",
      "Processing batch 11721/11884 - Loss: 30.9027\n",
      "Processing batch 11722/11884 - Loss: 30.0105\n",
      "Processing batch 11723/11884 - Loss: 30.9372\n",
      "Processing batch 11724/11884 - Loss: 31.5545\n",
      "Processing batch 11725/11884 - Loss: 30.7047\n",
      "Processing batch 11726/11884 - Loss: 31.3848\n",
      "Processing batch 11727/11884 - Loss: 30.3965\n",
      "Processing batch 11728/11884 - Loss: 30.0213\n",
      "Processing batch 11729/11884 - Loss: 30.3889\n",
      "Processing batch 11730/11884 - Loss: 29.7859\n",
      "Processing batch 11731/11884 - Loss: 30.1091\n",
      "Processing batch 11732/11884 - Loss: 30.4729\n",
      "Processing batch 11733/11884 - Loss: 29.9003\n",
      "Processing batch 11734/11884 - Loss: 30.8420\n",
      "Processing batch 11735/11884 - Loss: 29.3486\n",
      "Processing batch 11736/11884 - Loss: 30.9347\n",
      "Processing batch 11737/11884 - Loss: 30.7042\n",
      "Processing batch 11738/11884 - Loss: 28.1579\n",
      "Processing batch 11739/11884 - Loss: 30.5286\n",
      "Processing batch 11740/11884 - Loss: 29.6701\n",
      "Processing batch 11741/11884 - Loss: 30.3252\n",
      "Processing batch 11742/11884 - Loss: 29.9171\n",
      "Processing batch 11743/11884 - Loss: 30.4243\n",
      "Processing batch 11744/11884 - Loss: 28.9373\n",
      "Processing batch 11745/11884 - Loss: 30.9165\n",
      "Processing batch 11746/11884 - Loss: 29.8052\n",
      "Processing batch 11747/11884 - Loss: 29.7852\n",
      "Processing batch 11748/11884 - Loss: 31.8321\n",
      "Processing batch 11749/11884 - Loss: 29.9164\n",
      "Processing batch 11750/11884 - Loss: 30.3664\n",
      "Processing batch 11751/11884 - Loss: 30.7319\n",
      "Processing batch 11752/11884 - Loss: 28.7365\n",
      "Processing batch 11753/11884 - Loss: 29.9130\n",
      "Processing batch 11754/11884 - Loss: 30.7516\n",
      "Processing batch 11755/11884 - Loss: 30.3831\n",
      "Processing batch 11756/11884 - Loss: 30.9967\n",
      "Processing batch 11757/11884 - Loss: 32.5236\n",
      "Processing batch 11758/11884 - Loss: 30.4221\n",
      "Processing batch 11759/11884 - Loss: 31.2413\n",
      "Processing batch 11760/11884 - Loss: 29.2962\n",
      "Processing batch 11761/11884 - Loss: 29.8068\n",
      "Processing batch 11762/11884 - Loss: 31.3125\n",
      "Processing batch 11763/11884 - Loss: 31.5090\n",
      "Processing batch 11764/11884 - Loss: 30.9030\n",
      "Processing batch 11765/11884 - Loss: 30.8277\n",
      "Processing batch 11766/11884 - Loss: 29.3123\n",
      "Processing batch 11767/11884 - Loss: 30.4867\n",
      "Processing batch 11768/11884 - Loss: 28.9034\n",
      "Processing batch 11769/11884 - Loss: 29.7372\n",
      "Processing batch 11770/11884 - Loss: 29.8229\n",
      "Processing batch 11771/11884 - Loss: 31.4055\n",
      "Processing batch 11772/11884 - Loss: 29.4221\n",
      "Processing batch 11773/11884 - Loss: 29.9296\n",
      "Processing batch 11774/11884 - Loss: 30.7181\n",
      "Processing batch 11775/11884 - Loss: 29.7900\n",
      "Processing batch 11776/11884 - Loss: 29.4176\n",
      "Processing batch 11777/11884 - Loss: 31.6196\n",
      "Processing batch 11778/11884 - Loss: 29.0824\n",
      "Processing batch 11779/11884 - Loss: 31.4808\n",
      "Processing batch 11780/11884 - Loss: 29.4733\n",
      "Processing batch 11781/11884 - Loss: 30.6140\n",
      "Processing batch 11782/11884 - Loss: 29.8287\n",
      "Processing batch 11783/11884 - Loss: 29.4880\n",
      "Processing batch 11784/11884 - Loss: 29.9751\n",
      "Processing batch 11785/11884 - Loss: 30.8054\n",
      "Processing batch 11786/11884 - Loss: 31.4529\n",
      "Processing batch 11787/11884 - Loss: 31.2963\n",
      "Processing batch 11788/11884 - Loss: 31.1232\n",
      "Processing batch 11789/11884 - Loss: 29.8327\n",
      "Processing batch 11790/11884 - Loss: 30.3593\n",
      "Processing batch 11791/11884 - Loss: 29.0619\n",
      "Processing batch 11792/11884 - Loss: 32.1680\n",
      "Processing batch 11793/11884 - Loss: 30.5092\n",
      "Processing batch 11794/11884 - Loss: 30.2522\n",
      "Processing batch 11795/11884 - Loss: 31.1444\n",
      "Processing batch 11796/11884 - Loss: 31.1330\n",
      "Processing batch 11797/11884 - Loss: 30.5404\n",
      "Processing batch 11798/11884 - Loss: 30.5400\n",
      "Processing batch 11799/11884 - Loss: 31.0044\n",
      "Processing batch 11800/11884 - Loss: 28.6665\n",
      "Processing batch 11801/11884 - Loss: 30.8921\n",
      "Processing batch 11802/11884 - Loss: 28.7150\n",
      "Processing batch 11803/11884 - Loss: 29.2036\n",
      "Processing batch 11804/11884 - Loss: 30.9917\n",
      "Processing batch 11805/11884 - Loss: 30.0700\n",
      "Processing batch 11806/11884 - Loss: 30.1461\n",
      "Processing batch 11807/11884 - Loss: 30.3886\n",
      "Processing batch 11808/11884 - Loss: 29.6210\n",
      "Processing batch 11809/11884 - Loss: 29.8391\n",
      "Processing batch 11810/11884 - Loss: 30.6866\n",
      "Processing batch 11811/11884 - Loss: 29.6319\n",
      "Processing batch 11812/11884 - Loss: 30.5102\n",
      "Processing batch 11813/11884 - Loss: 30.3750\n",
      "Processing batch 11814/11884 - Loss: 30.4428\n",
      "Processing batch 11815/11884 - Loss: 29.4305\n",
      "Processing batch 11816/11884 - Loss: 29.6669\n",
      "Processing batch 11817/11884 - Loss: 30.8612\n",
      "Processing batch 11818/11884 - Loss: 30.7971\n",
      "Processing batch 11819/11884 - Loss: 31.2893\n",
      "Processing batch 11820/11884 - Loss: 30.0305\n",
      "Processing batch 11821/11884 - Loss: 30.4290\n",
      "Processing batch 11822/11884 - Loss: 29.8454\n",
      "Processing batch 11823/11884 - Loss: 29.8026\n",
      "Processing batch 11824/11884 - Loss: 30.4981\n",
      "Processing batch 11825/11884 - Loss: 30.1283\n",
      "Processing batch 11826/11884 - Loss: 31.2460\n",
      "Processing batch 11827/11884 - Loss: 28.9408\n",
      "Processing batch 11828/11884 - Loss: 29.5806\n",
      "Processing batch 11829/11884 - Loss: 30.5424\n",
      "Processing batch 11830/11884 - Loss: 30.1769\n",
      "Processing batch 11831/11884 - Loss: 30.7546\n",
      "Processing batch 11832/11884 - Loss: 29.9274\n",
      "Processing batch 11833/11884 - Loss: 30.5069\n",
      "Processing batch 11834/11884 - Loss: 30.2346\n",
      "Processing batch 11835/11884 - Loss: 30.4740\n",
      "Processing batch 11836/11884 - Loss: 30.4128\n",
      "Processing batch 11837/11884 - Loss: 30.9691\n",
      "Processing batch 11838/11884 - Loss: 28.8717\n",
      "Processing batch 11839/11884 - Loss: 29.4551\n",
      "Processing batch 11840/11884 - Loss: 30.8812\n",
      "Processing batch 11841/11884 - Loss: 29.9976\n",
      "Processing batch 11842/11884 - Loss: 29.4711\n",
      "Processing batch 11843/11884 - Loss: 30.4241\n",
      "Processing batch 11844/11884 - Loss: 32.1033\n",
      "Processing batch 11845/11884 - Loss: 30.0348\n",
      "Processing batch 11846/11884 - Loss: 31.2263\n",
      "Processing batch 11847/11884 - Loss: 31.2723\n",
      "Processing batch 11848/11884 - Loss: 29.8419\n",
      "Processing batch 11849/11884 - Loss: 29.4041\n",
      "Processing batch 11850/11884 - Loss: 30.1120\n",
      "Processing batch 11851/11884 - Loss: 29.6704\n",
      "Processing batch 11852/11884 - Loss: 30.9468\n",
      "Processing batch 11853/11884 - Loss: 29.0098\n",
      "Processing batch 11854/11884 - Loss: 29.3997\n",
      "Processing batch 11855/11884 - Loss: 29.7746\n",
      "Processing batch 11856/11884 - Loss: 30.5551\n",
      "Processing batch 11857/11884 - Loss: 30.7829\n",
      "Processing batch 11858/11884 - Loss: 29.4034\n",
      "Processing batch 11859/11884 - Loss: 31.6454\n",
      "Processing batch 11860/11884 - Loss: 29.8907\n",
      "Processing batch 11861/11884 - Loss: 30.4763\n",
      "Processing batch 11862/11884 - Loss: 32.0075\n",
      "Processing batch 11863/11884 - Loss: 31.1437\n",
      "Processing batch 11864/11884 - Loss: 29.8444\n",
      "Processing batch 11865/11884 - Loss: 32.3402\n",
      "Processing batch 11866/11884 - Loss: 29.3487\n",
      "Processing batch 11867/11884 - Loss: 31.2070\n",
      "Processing batch 11868/11884 - Loss: 28.5843\n",
      "Processing batch 11869/11884 - Loss: 29.9640\n",
      "Processing batch 11870/11884 - Loss: 30.5914\n",
      "Processing batch 11871/11884 - Loss: 31.6298\n",
      "Processing batch 11872/11884 - Loss: 30.6692\n",
      "Processing batch 11873/11884 - Loss: 30.4739\n",
      "Processing batch 11874/11884 - Loss: 29.9190\n",
      "Processing batch 11875/11884 - Loss: 29.7276\n",
      "Processing batch 11876/11884 - Loss: 29.4295\n",
      "Processing batch 11877/11884 - Loss: 30.3855\n",
      "Processing batch 11878/11884 - Loss: 29.3073\n",
      "Processing batch 11879/11884 - Loss: 31.5362\n",
      "Processing batch 11880/11884 - Loss: 31.1112\n",
      "Processing batch 11881/11884 - Loss: 31.8631\n",
      "Processing batch 11882/11884 - Loss: 31.4172\n",
      "Processing batch 11883/11884 - Loss: 30.9578\n",
      "Processing batch 11884/11884 - Loss: 31.6923\n",
      "Processing batch 1/11884 - Val_Loss: 28.9099\n",
      "Processing batch 2/11884 - Val_Loss: 24.3870\n",
      "Processing batch 3/11884 - Val_Loss: 29.3354\n",
      "Processing batch 4/11884 - Val_Loss: 24.7407\n",
      "Processing batch 5/11884 - Val_Loss: 24.7374\n",
      "Processing batch 6/11884 - Val_Loss: 28.5026\n",
      "Processing batch 7/11884 - Val_Loss: 25.9297\n",
      "Processing batch 8/11884 - Val_Loss: 26.0926\n",
      "Processing batch 9/11884 - Val_Loss: 28.1815\n",
      "Processing batch 10/11884 - Val_Loss: 26.8420\n",
      "Processing batch 11/11884 - Val_Loss: 26.3458\n",
      "Processing batch 12/11884 - Val_Loss: 28.9329\n",
      "Processing batch 13/11884 - Val_Loss: 25.4583\n",
      "Processing batch 14/11884 - Val_Loss: 24.3880\n",
      "Processing batch 15/11884 - Val_Loss: 24.4747\n",
      "Processing batch 16/11884 - Val_Loss: 25.1627\n",
      "Processing batch 17/11884 - Val_Loss: 25.2492\n",
      "Processing batch 18/11884 - Val_Loss: 23.0494\n",
      "Processing batch 19/11884 - Val_Loss: 24.8802\n",
      "Processing batch 20/11884 - Val_Loss: 25.5140\n",
      "Processing batch 21/11884 - Val_Loss: 23.2212\n",
      "Processing batch 22/11884 - Val_Loss: 24.5923\n",
      "Processing batch 23/11884 - Val_Loss: 26.4554\n",
      "Processing batch 24/11884 - Val_Loss: 26.0276\n",
      "Processing batch 25/11884 - Val_Loss: 25.5903\n",
      "Processing batch 26/11884 - Val_Loss: 24.3035\n",
      "Processing batch 27/11884 - Val_Loss: 26.1559\n",
      "Processing batch 28/11884 - Val_Loss: 26.0022\n",
      "Processing batch 29/11884 - Val_Loss: 26.2472\n",
      "Processing batch 30/11884 - Val_Loss: 24.8231\n",
      "Processing batch 31/11884 - Val_Loss: 26.2141\n",
      "Processing batch 32/11884 - Val_Loss: 24.5258\n",
      "Processing batch 33/11884 - Val_Loss: 24.7547\n",
      "Processing batch 34/11884 - Val_Loss: 25.8659\n",
      "Processing batch 35/11884 - Val_Loss: 25.0004\n",
      "Processing batch 36/11884 - Val_Loss: 26.2666\n",
      "Processing batch 37/11884 - Val_Loss: 25.1150\n",
      "Processing batch 38/11884 - Val_Loss: 25.3442\n",
      "Processing batch 39/11884 - Val_Loss: 24.5480\n",
      "Processing batch 40/11884 - Val_Loss: 24.2449\n",
      "Processing batch 41/11884 - Val_Loss: 23.9817\n",
      "Processing batch 42/11884 - Val_Loss: 27.4136\n",
      "Processing batch 43/11884 - Val_Loss: 22.6562\n",
      "Processing batch 44/11884 - Val_Loss: 26.3970\n",
      "Processing batch 45/11884 - Val_Loss: 27.5035\n",
      "Processing batch 46/11884 - Val_Loss: 26.7684\n",
      "Processing batch 47/11884 - Val_Loss: 25.4963\n",
      "Processing batch 48/11884 - Val_Loss: 25.3628\n",
      "Processing batch 49/11884 - Val_Loss: 23.1042\n",
      "Processing batch 50/11884 - Val_Loss: 26.6647\n",
      "Processing batch 51/11884 - Val_Loss: 26.7186\n",
      "Processing batch 52/11884 - Val_Loss: 24.7045\n",
      "Processing batch 53/11884 - Val_Loss: 24.0829\n",
      "Processing batch 54/11884 - Val_Loss: 26.3969\n",
      "Processing batch 55/11884 - Val_Loss: 24.5614\n",
      "Processing batch 56/11884 - Val_Loss: 25.2556\n",
      "Processing batch 57/11884 - Val_Loss: 26.8979\n",
      "Processing batch 58/11884 - Val_Loss: 26.8905\n",
      "Processing batch 59/11884 - Val_Loss: 25.1263\n",
      "Processing batch 60/11884 - Val_Loss: 23.8256\n",
      "Processing batch 61/11884 - Val_Loss: 25.5419\n",
      "Processing batch 62/11884 - Val_Loss: 24.1548\n",
      "Processing batch 63/11884 - Val_Loss: 26.7513\n",
      "Processing batch 64/11884 - Val_Loss: 26.2341\n",
      "Processing batch 65/11884 - Val_Loss: 24.1360\n",
      "Processing batch 66/11884 - Val_Loss: 26.7849\n",
      "Processing batch 67/11884 - Val_Loss: 22.8811\n",
      "Processing batch 68/11884 - Val_Loss: 28.6885\n",
      "Processing batch 69/11884 - Val_Loss: 29.6479\n",
      "Processing batch 70/11884 - Val_Loss: 28.0868\n",
      "Processing batch 71/11884 - Val_Loss: 25.4395\n",
      "Processing batch 72/11884 - Val_Loss: 24.3432\n",
      "Processing batch 73/11884 - Val_Loss: 24.9307\n",
      "Processing batch 74/11884 - Val_Loss: 25.5515\n",
      "Processing batch 75/11884 - Val_Loss: 25.2136\n",
      "Processing batch 76/11884 - Val_Loss: 24.8634\n",
      "Processing batch 77/11884 - Val_Loss: 28.5423\n",
      "Processing batch 78/11884 - Val_Loss: 26.3078\n",
      "Processing batch 79/11884 - Val_Loss: 24.7569\n",
      "Processing batch 80/11884 - Val_Loss: 25.0185\n",
      "Processing batch 81/11884 - Val_Loss: 26.9623\n",
      "Processing batch 82/11884 - Val_Loss: 25.3293\n",
      "Processing batch 83/11884 - Val_Loss: 25.7307\n",
      "Processing batch 84/11884 - Val_Loss: 26.0578\n",
      "Processing batch 85/11884 - Val_Loss: 25.8327\n",
      "Processing batch 86/11884 - Val_Loss: 24.9317\n",
      "Processing batch 87/11884 - Val_Loss: 25.0271\n",
      "Processing batch 88/11884 - Val_Loss: 27.7817\n",
      "Processing batch 89/11884 - Val_Loss: 27.9443\n",
      "Processing batch 90/11884 - Val_Loss: 26.5938\n",
      "Processing batch 91/11884 - Val_Loss: 24.5385\n",
      "Processing batch 92/11884 - Val_Loss: 26.8144\n",
      "Processing batch 93/11884 - Val_Loss: 23.1893\n",
      "Processing batch 94/11884 - Val_Loss: 24.2467\n",
      "Processing batch 95/11884 - Val_Loss: 24.5333\n",
      "Processing batch 96/11884 - Val_Loss: 25.6679\n",
      "Processing batch 97/11884 - Val_Loss: 24.9462\n",
      "Processing batch 98/11884 - Val_Loss: 28.3647\n",
      "Processing batch 99/11884 - Val_Loss: 25.9098\n",
      "Processing batch 100/11884 - Val_Loss: 26.8556\n",
      "Processing batch 101/11884 - Val_Loss: 26.6718\n",
      "Processing batch 102/11884 - Val_Loss: 24.7714\n",
      "Processing batch 103/11884 - Val_Loss: 25.2340\n",
      "Processing batch 104/11884 - Val_Loss: 26.6231\n",
      "Processing batch 105/11884 - Val_Loss: 26.1719\n",
      "Processing batch 106/11884 - Val_Loss: 25.8993\n",
      "Processing batch 107/11884 - Val_Loss: 27.2429\n",
      "Processing batch 108/11884 - Val_Loss: 21.5181\n",
      "Processing batch 109/11884 - Val_Loss: 24.3400\n",
      "Processing batch 110/11884 - Val_Loss: 23.8504\n",
      "Processing batch 111/11884 - Val_Loss: 26.9511\n",
      "Processing batch 112/11884 - Val_Loss: 25.1604\n",
      "Processing batch 113/11884 - Val_Loss: 27.6001\n",
      "Processing batch 114/11884 - Val_Loss: 22.8855\n",
      "Processing batch 115/11884 - Val_Loss: 23.7885\n",
      "Processing batch 116/11884 - Val_Loss: 24.8951\n",
      "Processing batch 117/11884 - Val_Loss: 26.2950\n",
      "Processing batch 118/11884 - Val_Loss: 27.3212\n",
      "Processing batch 119/11884 - Val_Loss: 22.7413\n",
      "Processing batch 120/11884 - Val_Loss: 26.0171\n",
      "Processing batch 121/11884 - Val_Loss: 27.1010\n",
      "Processing batch 122/11884 - Val_Loss: 29.3246\n",
      "Processing batch 123/11884 - Val_Loss: 23.1245\n",
      "Processing batch 124/11884 - Val_Loss: 24.7315\n",
      "Processing batch 125/11884 - Val_Loss: 23.2995\n",
      "Processing batch 126/11884 - Val_Loss: 27.8878\n",
      "Processing batch 127/11884 - Val_Loss: 25.7243\n",
      "Processing batch 128/11884 - Val_Loss: 23.9395\n",
      "Processing batch 129/11884 - Val_Loss: 23.7323\n",
      "Processing batch 130/11884 - Val_Loss: 25.5331\n",
      "Processing batch 131/11884 - Val_Loss: 28.5482\n",
      "Processing batch 132/11884 - Val_Loss: 25.9965\n",
      "Processing batch 133/11884 - Val_Loss: 28.0540\n",
      "Processing batch 134/11884 - Val_Loss: 24.6796\n",
      "Processing batch 135/11884 - Val_Loss: 28.7850\n",
      "Processing batch 136/11884 - Val_Loss: 25.7255\n",
      "Processing batch 137/11884 - Val_Loss: 23.7255\n",
      "Processing batch 138/11884 - Val_Loss: 26.6917\n",
      "Processing batch 139/11884 - Val_Loss: 23.6464\n",
      "Processing batch 140/11884 - Val_Loss: 25.3712\n",
      "Processing batch 141/11884 - Val_Loss: 24.1182\n",
      "Processing batch 142/11884 - Val_Loss: 26.0434\n",
      "Processing batch 143/11884 - Val_Loss: 27.0461\n",
      "Processing batch 144/11884 - Val_Loss: 28.2116\n",
      "Processing batch 145/11884 - Val_Loss: 24.1078\n",
      "Processing batch 146/11884 - Val_Loss: 26.8805\n",
      "Processing batch 147/11884 - Val_Loss: 25.9936\n",
      "Processing batch 148/11884 - Val_Loss: 23.2830\n",
      "Processing batch 149/11884 - Val_Loss: 28.1429\n",
      "Processing batch 150/11884 - Val_Loss: 30.9300\n",
      "Processing batch 151/11884 - Val_Loss: 25.9746\n",
      "Processing batch 152/11884 - Val_Loss: 25.6816\n",
      "Processing batch 153/11884 - Val_Loss: 26.7483\n",
      "Processing batch 154/11884 - Val_Loss: 28.1021\n",
      "Processing batch 155/11884 - Val_Loss: 27.3269\n",
      "Processing batch 156/11884 - Val_Loss: 26.8334\n",
      "Processing batch 157/11884 - Val_Loss: 26.1181\n",
      "Processing batch 158/11884 - Val_Loss: 25.4077\n",
      "Processing batch 159/11884 - Val_Loss: 24.8837\n",
      "Processing batch 160/11884 - Val_Loss: 25.0694\n",
      "Processing batch 161/11884 - Val_Loss: 25.3818\n",
      "Processing batch 162/11884 - Val_Loss: 28.0411\n",
      "Processing batch 163/11884 - Val_Loss: 26.5156\n",
      "Processing batch 164/11884 - Val_Loss: 22.0778\n",
      "Processing batch 165/11884 - Val_Loss: 25.1586\n",
      "Processing batch 166/11884 - Val_Loss: 24.1885\n",
      "Processing batch 167/11884 - Val_Loss: 23.1144\n",
      "Processing batch 168/11884 - Val_Loss: 24.9034\n",
      "Processing batch 169/11884 - Val_Loss: 26.0896\n",
      "Processing batch 170/11884 - Val_Loss: 27.2902\n",
      "Processing batch 171/11884 - Val_Loss: 26.2370\n",
      "Processing batch 172/11884 - Val_Loss: 28.3283\n",
      "Processing batch 173/11884 - Val_Loss: 27.1682\n",
      "Processing batch 174/11884 - Val_Loss: 26.0807\n",
      "Processing batch 175/11884 - Val_Loss: 25.8029\n",
      "Processing batch 176/11884 - Val_Loss: 27.6449\n",
      "Processing batch 177/11884 - Val_Loss: 25.2437\n",
      "Processing batch 178/11884 - Val_Loss: 26.5240\n",
      "Processing batch 179/11884 - Val_Loss: 24.6683\n",
      "Processing batch 180/11884 - Val_Loss: 24.4572\n",
      "Processing batch 181/11884 - Val_Loss: 26.4566\n",
      "Processing batch 182/11884 - Val_Loss: 26.2744\n",
      "Processing batch 183/11884 - Val_Loss: 28.5163\n",
      "Processing batch 184/11884 - Val_Loss: 24.6174\n",
      "Processing batch 185/11884 - Val_Loss: 24.8926\n",
      "Processing batch 186/11884 - Val_Loss: 27.9633\n",
      "Processing batch 187/11884 - Val_Loss: 25.5996\n",
      "Processing batch 188/11884 - Val_Loss: 21.5139\n",
      "Processing batch 189/11884 - Val_Loss: 25.3073\n",
      "Processing batch 190/11884 - Val_Loss: 26.3267\n",
      "Processing batch 191/11884 - Val_Loss: 23.3490\n",
      "Processing batch 192/11884 - Val_Loss: 23.5406\n",
      "Processing batch 193/11884 - Val_Loss: 21.6064\n",
      "Processing batch 194/11884 - Val_Loss: 26.4832\n",
      "Processing batch 195/11884 - Val_Loss: 22.9994\n",
      "Processing batch 196/11884 - Val_Loss: 27.1819\n",
      "Processing batch 197/11884 - Val_Loss: 28.1808\n",
      "Processing batch 198/11884 - Val_Loss: 23.2955\n",
      "Processing batch 199/11884 - Val_Loss: 25.4511\n",
      "Processing batch 200/11884 - Val_Loss: 24.8299\n",
      "Processing batch 201/11884 - Val_Loss: 24.9033\n",
      "Processing batch 202/11884 - Val_Loss: 28.8666\n",
      "Processing batch 203/11884 - Val_Loss: 25.6861\n",
      "Processing batch 204/11884 - Val_Loss: 26.7161\n",
      "Processing batch 205/11884 - Val_Loss: 21.0784\n",
      "Processing batch 206/11884 - Val_Loss: 25.2139\n",
      "Processing batch 207/11884 - Val_Loss: 25.5649\n",
      "Processing batch 208/11884 - Val_Loss: 24.7927\n",
      "Processing batch 209/11884 - Val_Loss: 24.4199\n",
      "Processing batch 210/11884 - Val_Loss: 27.9058\n",
      "Processing batch 211/11884 - Val_Loss: 23.8446\n",
      "Processing batch 212/11884 - Val_Loss: 24.2786\n",
      "Processing batch 213/11884 - Val_Loss: 23.0136\n",
      "Processing batch 214/11884 - Val_Loss: 26.0635\n",
      "Processing batch 215/11884 - Val_Loss: 24.8074\n",
      "Processing batch 216/11884 - Val_Loss: 24.6663\n",
      "Processing batch 217/11884 - Val_Loss: 29.3802\n",
      "Processing batch 218/11884 - Val_Loss: 29.3570\n",
      "Processing batch 219/11884 - Val_Loss: 22.5250\n",
      "Processing batch 220/11884 - Val_Loss: 27.3057\n",
      "Processing batch 221/11884 - Val_Loss: 25.0693\n",
      "Processing batch 222/11884 - Val_Loss: 25.2476\n",
      "Processing batch 223/11884 - Val_Loss: 26.6358\n",
      "Processing batch 224/11884 - Val_Loss: 24.2807\n",
      "Processing batch 225/11884 - Val_Loss: 24.8852\n",
      "Processing batch 226/11884 - Val_Loss: 24.9770\n",
      "Processing batch 227/11884 - Val_Loss: 25.7578\n",
      "Processing batch 228/11884 - Val_Loss: 29.3461\n",
      "Processing batch 229/11884 - Val_Loss: 23.4973\n",
      "Processing batch 230/11884 - Val_Loss: 23.6898\n",
      "Processing batch 231/11884 - Val_Loss: 25.6445\n",
      "Processing batch 232/11884 - Val_Loss: 28.2508\n",
      "Processing batch 233/11884 - Val_Loss: 25.4565\n",
      "Processing batch 234/11884 - Val_Loss: 23.5680\n",
      "Processing batch 235/11884 - Val_Loss: 24.1177\n",
      "Processing batch 236/11884 - Val_Loss: 24.9723\n",
      "Processing batch 237/11884 - Val_Loss: 25.2656\n",
      "Processing batch 238/11884 - Val_Loss: 27.2947\n",
      "Processing batch 239/11884 - Val_Loss: 25.1293\n",
      "Processing batch 240/11884 - Val_Loss: 27.0600\n",
      "Processing batch 241/11884 - Val_Loss: 27.7579\n",
      "Processing batch 242/11884 - Val_Loss: 24.9465\n",
      "Processing batch 243/11884 - Val_Loss: 24.0143\n",
      "Processing batch 244/11884 - Val_Loss: 26.6226\n",
      "Processing batch 245/11884 - Val_Loss: 25.7110\n",
      "Processing batch 246/11884 - Val_Loss: 22.7308\n",
      "Processing batch 247/11884 - Val_Loss: 22.2670\n",
      "Processing batch 248/11884 - Val_Loss: 26.9879\n",
      "Processing batch 249/11884 - Val_Loss: 26.7054\n",
      "Processing batch 250/11884 - Val_Loss: 25.6761\n",
      "Processing batch 251/11884 - Val_Loss: 23.6057\n",
      "Processing batch 252/11884 - Val_Loss: 22.3829\n",
      "Processing batch 253/11884 - Val_Loss: 27.1181\n",
      "Processing batch 254/11884 - Val_Loss: 24.8562\n",
      "Processing batch 255/11884 - Val_Loss: 25.7264\n",
      "Processing batch 256/11884 - Val_Loss: 23.4522\n",
      "Processing batch 257/11884 - Val_Loss: 28.2320\n",
      "Processing batch 258/11884 - Val_Loss: 24.7810\n",
      "Processing batch 259/11884 - Val_Loss: 26.8522\n",
      "Processing batch 260/11884 - Val_Loss: 26.8300\n",
      "Processing batch 261/11884 - Val_Loss: 27.0838\n",
      "Processing batch 262/11884 - Val_Loss: 25.8081\n",
      "Processing batch 263/11884 - Val_Loss: 23.5732\n",
      "Processing batch 264/11884 - Val_Loss: 26.1396\n",
      "Processing batch 265/11884 - Val_Loss: 22.7048\n",
      "Processing batch 266/11884 - Val_Loss: 25.1678\n",
      "Processing batch 267/11884 - Val_Loss: 26.2647\n",
      "Processing batch 268/11884 - Val_Loss: 25.4892\n",
      "Processing batch 269/11884 - Val_Loss: 21.7538\n",
      "Processing batch 270/11884 - Val_Loss: 24.8560\n",
      "Processing batch 271/11884 - Val_Loss: 25.1363\n",
      "Processing batch 272/11884 - Val_Loss: 25.0528\n",
      "Processing batch 273/11884 - Val_Loss: 26.2348\n",
      "Processing batch 274/11884 - Val_Loss: 25.7077\n",
      "Processing batch 275/11884 - Val_Loss: 27.1437\n",
      "Processing batch 276/11884 - Val_Loss: 24.6944\n",
      "Processing batch 277/11884 - Val_Loss: 23.4926\n",
      "Processing batch 278/11884 - Val_Loss: 24.6879\n",
      "Processing batch 279/11884 - Val_Loss: 27.3772\n",
      "Processing batch 280/11884 - Val_Loss: 25.5386\n",
      "Processing batch 281/11884 - Val_Loss: 26.8502\n",
      "Processing batch 282/11884 - Val_Loss: 24.5002\n",
      "Processing batch 283/11884 - Val_Loss: 25.2742\n",
      "Processing batch 284/11884 - Val_Loss: 23.8888\n",
      "Processing batch 285/11884 - Val_Loss: 26.5477\n",
      "Processing batch 286/11884 - Val_Loss: 24.9949\n",
      "Processing batch 287/11884 - Val_Loss: 26.5539\n",
      "Processing batch 288/11884 - Val_Loss: 26.4469\n",
      "Processing batch 289/11884 - Val_Loss: 27.3524\n",
      "Processing batch 290/11884 - Val_Loss: 24.1073\n",
      "Processing batch 291/11884 - Val_Loss: 26.4625\n",
      "Processing batch 292/11884 - Val_Loss: 26.8181\n",
      "Processing batch 293/11884 - Val_Loss: 27.0003\n",
      "Processing batch 294/11884 - Val_Loss: 25.5483\n",
      "Processing batch 295/11884 - Val_Loss: 26.0929\n",
      "Processing batch 296/11884 - Val_Loss: 26.0440\n",
      "Processing batch 297/11884 - Val_Loss: 24.8097\n",
      "Processing batch 298/11884 - Val_Loss: 26.0352\n",
      "Processing batch 299/11884 - Val_Loss: 24.6724\n",
      "Processing batch 300/11884 - Val_Loss: 24.6357\n",
      "Processing batch 301/11884 - Val_Loss: 27.3762\n",
      "Processing batch 302/11884 - Val_Loss: 26.8895\n",
      "Processing batch 303/11884 - Val_Loss: 24.7709\n",
      "Processing batch 304/11884 - Val_Loss: 29.7299\n",
      "Processing batch 305/11884 - Val_Loss: 27.0829\n",
      "Processing batch 306/11884 - Val_Loss: 25.4016\n",
      "Processing batch 307/11884 - Val_Loss: 25.4284\n",
      "Processing batch 308/11884 - Val_Loss: 22.4019\n",
      "Processing batch 309/11884 - Val_Loss: 24.1065\n",
      "Processing batch 310/11884 - Val_Loss: 28.8933\n",
      "Processing batch 311/11884 - Val_Loss: 23.9127\n",
      "Processing batch 312/11884 - Val_Loss: 22.2874\n",
      "Processing batch 313/11884 - Val_Loss: 22.8664\n",
      "Processing batch 314/11884 - Val_Loss: 25.8561\n",
      "Processing batch 315/11884 - Val_Loss: 26.2194\n",
      "Processing batch 316/11884 - Val_Loss: 25.0614\n",
      "Processing batch 317/11884 - Val_Loss: 27.8367\n",
      "Processing batch 318/11884 - Val_Loss: 25.1890\n",
      "Processing batch 319/11884 - Val_Loss: 23.4741\n",
      "Processing batch 320/11884 - Val_Loss: 24.2411\n",
      "Processing batch 321/11884 - Val_Loss: 26.1705\n",
      "Processing batch 322/11884 - Val_Loss: 26.8435\n",
      "Processing batch 323/11884 - Val_Loss: 23.7562\n",
      "Processing batch 324/11884 - Val_Loss: 25.8781\n",
      "Processing batch 325/11884 - Val_Loss: 25.3390\n",
      "Processing batch 326/11884 - Val_Loss: 27.3386\n",
      "Processing batch 327/11884 - Val_Loss: 22.8075\n",
      "Processing batch 328/11884 - Val_Loss: 24.1100\n",
      "Processing batch 329/11884 - Val_Loss: 25.4881\n",
      "Processing batch 330/11884 - Val_Loss: 24.1199\n",
      "Processing batch 331/11884 - Val_Loss: 25.2225\n",
      "Processing batch 332/11884 - Val_Loss: 29.2868\n",
      "Processing batch 333/11884 - Val_Loss: 25.4771\n",
      "Processing batch 334/11884 - Val_Loss: 23.7565\n",
      "Processing batch 335/11884 - Val_Loss: 26.5875\n",
      "Processing batch 336/11884 - Val_Loss: 24.8056\n",
      "Processing batch 337/11884 - Val_Loss: 27.2149\n",
      "Processing batch 338/11884 - Val_Loss: 25.3485\n",
      "Processing batch 339/11884 - Val_Loss: 25.6590\n",
      "Processing batch 340/11884 - Val_Loss: 26.8569\n",
      "Processing batch 341/11884 - Val_Loss: 27.5040\n",
      "Processing batch 342/11884 - Val_Loss: 25.8594\n",
      "Processing batch 343/11884 - Val_Loss: 26.5031\n",
      "Processing batch 344/11884 - Val_Loss: 25.1197\n",
      "Processing batch 345/11884 - Val_Loss: 25.9507\n",
      "Processing batch 346/11884 - Val_Loss: 29.0141\n",
      "Processing batch 347/11884 - Val_Loss: 26.1744\n",
      "Processing batch 348/11884 - Val_Loss: 28.8745\n",
      "Processing batch 349/11884 - Val_Loss: 30.1472\n",
      "Processing batch 350/11884 - Val_Loss: 26.3020\n",
      "Processing batch 351/11884 - Val_Loss: 26.1269\n",
      "Processing batch 352/11884 - Val_Loss: 26.7527\n",
      "Processing batch 353/11884 - Val_Loss: 23.7039\n",
      "Processing batch 354/11884 - Val_Loss: 22.4236\n",
      "Processing batch 355/11884 - Val_Loss: 26.3234\n",
      "Processing batch 356/11884 - Val_Loss: 24.8344\n",
      "Processing batch 357/11884 - Val_Loss: 23.9598\n",
      "Processing batch 358/11884 - Val_Loss: 26.0462\n",
      "Processing batch 359/11884 - Val_Loss: 25.9059\n",
      "Processing batch 360/11884 - Val_Loss: 23.1952\n",
      "Processing batch 361/11884 - Val_Loss: 22.7279\n",
      "Processing batch 362/11884 - Val_Loss: 24.7007\n",
      "Processing batch 363/11884 - Val_Loss: 26.1157\n",
      "Processing batch 364/11884 - Val_Loss: 25.3765\n",
      "Processing batch 365/11884 - Val_Loss: 24.2376\n",
      "Processing batch 366/11884 - Val_Loss: 22.6737\n",
      "Processing batch 367/11884 - Val_Loss: 24.8744\n",
      "Processing batch 368/11884 - Val_Loss: 26.9454\n",
      "Processing batch 369/11884 - Val_Loss: 27.0166\n",
      "Processing batch 370/11884 - Val_Loss: 25.6029\n",
      "Processing batch 371/11884 - Val_Loss: 26.4753\n",
      "Processing batch 372/11884 - Val_Loss: 28.3064\n",
      "Processing batch 373/11884 - Val_Loss: 25.9762\n",
      "Processing batch 374/11884 - Val_Loss: 24.2738\n",
      "Processing batch 375/11884 - Val_Loss: 24.0392\n",
      "Processing batch 376/11884 - Val_Loss: 26.9076\n",
      "Processing batch 377/11884 - Val_Loss: 23.7239\n",
      "Processing batch 378/11884 - Val_Loss: 25.7833\n",
      "Processing batch 379/11884 - Val_Loss: 23.6958\n",
      "Processing batch 380/11884 - Val_Loss: 24.2590\n",
      "Processing batch 381/11884 - Val_Loss: 25.8336\n",
      "Processing batch 382/11884 - Val_Loss: 26.1133\n",
      "Processing batch 383/11884 - Val_Loss: 26.9777\n",
      "Processing batch 384/11884 - Val_Loss: 27.1278\n",
      "Processing batch 385/11884 - Val_Loss: 25.7061\n",
      "Processing batch 386/11884 - Val_Loss: 24.9003\n",
      "Processing batch 387/11884 - Val_Loss: 23.0543\n",
      "Processing batch 388/11884 - Val_Loss: 25.8221\n",
      "Processing batch 389/11884 - Val_Loss: 23.8680\n",
      "Processing batch 390/11884 - Val_Loss: 27.5015\n",
      "Processing batch 391/11884 - Val_Loss: 25.7260\n",
      "Processing batch 392/11884 - Val_Loss: 24.3647\n",
      "Processing batch 393/11884 - Val_Loss: 28.4277\n",
      "Processing batch 394/11884 - Val_Loss: 24.1495\n",
      "Processing batch 395/11884 - Val_Loss: 25.0568\n",
      "Processing batch 396/11884 - Val_Loss: 23.0581\n",
      "Processing batch 397/11884 - Val_Loss: 24.8330\n",
      "Processing batch 398/11884 - Val_Loss: 25.4999\n",
      "Processing batch 399/11884 - Val_Loss: 26.7643\n",
      "Processing batch 400/11884 - Val_Loss: 25.0507\n",
      "Processing batch 401/11884 - Val_Loss: 24.4453\n",
      "Processing batch 402/11884 - Val_Loss: 26.2690\n",
      "Processing batch 403/11884 - Val_Loss: 23.5629\n",
      "Processing batch 404/11884 - Val_Loss: 26.7557\n",
      "Processing batch 405/11884 - Val_Loss: 28.1977\n",
      "Processing batch 406/11884 - Val_Loss: 25.0891\n",
      "Processing batch 407/11884 - Val_Loss: 27.9140\n",
      "Processing batch 408/11884 - Val_Loss: 28.9454\n",
      "Processing batch 409/11884 - Val_Loss: 29.5716\n",
      "Processing batch 410/11884 - Val_Loss: 24.4588\n",
      "Processing batch 411/11884 - Val_Loss: 26.0366\n",
      "Processing batch 412/11884 - Val_Loss: 28.8086\n",
      "Processing batch 413/11884 - Val_Loss: 27.4641\n",
      "Processing batch 414/11884 - Val_Loss: 26.7217\n",
      "Processing batch 415/11884 - Val_Loss: 28.8225\n",
      "Processing batch 416/11884 - Val_Loss: 26.2197\n",
      "Processing batch 417/11884 - Val_Loss: 29.9260\n",
      "Processing batch 418/11884 - Val_Loss: 23.9503\n",
      "Processing batch 419/11884 - Val_Loss: 27.2227\n",
      "Processing batch 420/11884 - Val_Loss: 23.8906\n",
      "Processing batch 421/11884 - Val_Loss: 26.2504\n",
      "Processing batch 422/11884 - Val_Loss: 25.2094\n",
      "Processing batch 423/11884 - Val_Loss: 22.1084\n",
      "Processing batch 424/11884 - Val_Loss: 24.7418\n",
      "Processing batch 425/11884 - Val_Loss: 24.4763\n",
      "Processing batch 426/11884 - Val_Loss: 26.9534\n",
      "Processing batch 427/11884 - Val_Loss: 27.5004\n",
      "Processing batch 428/11884 - Val_Loss: 25.1795\n",
      "Processing batch 429/11884 - Val_Loss: 25.6269\n",
      "Processing batch 430/11884 - Val_Loss: 26.6315\n",
      "Processing batch 431/11884 - Val_Loss: 26.5674\n",
      "Processing batch 432/11884 - Val_Loss: 24.5395\n",
      "Processing batch 433/11884 - Val_Loss: 26.1079\n",
      "Processing batch 434/11884 - Val_Loss: 25.0935\n",
      "Processing batch 435/11884 - Val_Loss: 28.8586\n",
      "Processing batch 436/11884 - Val_Loss: 25.2959\n",
      "Processing batch 437/11884 - Val_Loss: 26.5093\n",
      "Processing batch 438/11884 - Val_Loss: 25.8401\n",
      "Processing batch 439/11884 - Val_Loss: 25.3938\n",
      "Processing batch 440/11884 - Val_Loss: 26.8979\n",
      "Processing batch 441/11884 - Val_Loss: 27.7845\n",
      "Processing batch 442/11884 - Val_Loss: 26.0245\n",
      "Processing batch 443/11884 - Val_Loss: 26.0677\n",
      "Processing batch 444/11884 - Val_Loss: 24.7858\n",
      "Processing batch 445/11884 - Val_Loss: 26.7712\n",
      "Processing batch 446/11884 - Val_Loss: 27.7857\n",
      "Processing batch 447/11884 - Val_Loss: 25.4944\n",
      "Processing batch 448/11884 - Val_Loss: 25.6077\n",
      "Processing batch 449/11884 - Val_Loss: 26.0256\n",
      "Processing batch 450/11884 - Val_Loss: 24.5142\n",
      "Processing batch 451/11884 - Val_Loss: 25.8785\n",
      "Processing batch 452/11884 - Val_Loss: 24.8901\n",
      "Processing batch 453/11884 - Val_Loss: 22.5270\n",
      "Processing batch 454/11884 - Val_Loss: 27.7059\n",
      "Processing batch 455/11884 - Val_Loss: 27.0019\n",
      "Processing batch 456/11884 - Val_Loss: 26.1516\n",
      "Processing batch 457/11884 - Val_Loss: 25.8207\n",
      "Processing batch 458/11884 - Val_Loss: 24.9991\n",
      "Processing batch 459/11884 - Val_Loss: 26.2411\n",
      "Processing batch 460/11884 - Val_Loss: 27.5443\n",
      "Processing batch 461/11884 - Val_Loss: 23.4985\n",
      "Processing batch 462/11884 - Val_Loss: 26.6664\n",
      "Processing batch 463/11884 - Val_Loss: 24.8690\n",
      "Processing batch 464/11884 - Val_Loss: 25.3128\n",
      "Processing batch 465/11884 - Val_Loss: 24.2697\n",
      "Processing batch 466/11884 - Val_Loss: 24.5640\n",
      "Processing batch 467/11884 - Val_Loss: 24.9292\n",
      "Processing batch 468/11884 - Val_Loss: 26.9697\n",
      "Processing batch 469/11884 - Val_Loss: 25.0836\n",
      "Processing batch 470/11884 - Val_Loss: 24.6358\n",
      "Processing batch 471/11884 - Val_Loss: 23.9805\n",
      "Processing batch 472/11884 - Val_Loss: 26.3205\n",
      "Processing batch 473/11884 - Val_Loss: 26.0410\n",
      "Processing batch 474/11884 - Val_Loss: 26.1239\n",
      "Processing batch 475/11884 - Val_Loss: 23.4324\n",
      "Processing batch 476/11884 - Val_Loss: 27.2090\n",
      "Processing batch 477/11884 - Val_Loss: 27.6218\n",
      "Processing batch 478/11884 - Val_Loss: 28.2023\n",
      "Processing batch 479/11884 - Val_Loss: 27.8068\n",
      "Processing batch 480/11884 - Val_Loss: 23.9158\n",
      "Processing batch 481/11884 - Val_Loss: 25.9929\n",
      "Processing batch 482/11884 - Val_Loss: 24.3944\n",
      "Processing batch 483/11884 - Val_Loss: 26.3604\n",
      "Processing batch 484/11884 - Val_Loss: 25.3097\n",
      "Processing batch 485/11884 - Val_Loss: 25.7000\n",
      "Processing batch 486/11884 - Val_Loss: 23.8792\n",
      "Processing batch 487/11884 - Val_Loss: 23.5617\n",
      "Processing batch 488/11884 - Val_Loss: 26.4843\n",
      "Processing batch 489/11884 - Val_Loss: 27.1524\n",
      "Processing batch 490/11884 - Val_Loss: 25.4921\n",
      "Processing batch 491/11884 - Val_Loss: 24.6883\n",
      "Processing batch 492/11884 - Val_Loss: 25.9341\n",
      "Processing batch 493/11884 - Val_Loss: 25.9445\n",
      "Processing batch 494/11884 - Val_Loss: 25.3079\n",
      "Processing batch 495/11884 - Val_Loss: 27.5178\n",
      "Processing batch 496/11884 - Val_Loss: 24.4442\n",
      "Processing batch 497/11884 - Val_Loss: 27.3230\n",
      "Processing batch 498/11884 - Val_Loss: 28.3729\n",
      "Processing batch 499/11884 - Val_Loss: 26.9833\n",
      "Processing batch 500/11884 - Val_Loss: 24.9967\n",
      "Processing batch 501/11884 - Val_Loss: 27.2548\n",
      "Processing batch 502/11884 - Val_Loss: 22.7385\n",
      "Processing batch 503/11884 - Val_Loss: 25.1375\n",
      "Processing batch 504/11884 - Val_Loss: 25.1197\n",
      "Processing batch 505/11884 - Val_Loss: 21.6964\n",
      "Processing batch 506/11884 - Val_Loss: 23.9200\n",
      "Processing batch 507/11884 - Val_Loss: 24.1158\n",
      "Processing batch 508/11884 - Val_Loss: 24.9635\n",
      "Processing batch 509/11884 - Val_Loss: 25.5960\n",
      "Processing batch 510/11884 - Val_Loss: 24.5972\n",
      "Processing batch 511/11884 - Val_Loss: 27.3603\n",
      "Processing batch 512/11884 - Val_Loss: 25.5180\n",
      "Processing batch 513/11884 - Val_Loss: 25.5910\n",
      "Processing batch 514/11884 - Val_Loss: 25.8718\n",
      "Processing batch 515/11884 - Val_Loss: 28.3511\n",
      "Processing batch 516/11884 - Val_Loss: 25.9592\n",
      "Processing batch 517/11884 - Val_Loss: 23.6955\n",
      "Processing batch 518/11884 - Val_Loss: 26.1469\n",
      "Processing batch 519/11884 - Val_Loss: 24.6271\n",
      "Processing batch 520/11884 - Val_Loss: 24.1186\n",
      "Processing batch 521/11884 - Val_Loss: 25.1083\n",
      "Processing batch 522/11884 - Val_Loss: 26.6517\n",
      "Processing batch 523/11884 - Val_Loss: 25.6691\n",
      "Processing batch 524/11884 - Val_Loss: 23.9258\n",
      "Processing batch 525/11884 - Val_Loss: 23.7116\n",
      "Processing batch 526/11884 - Val_Loss: 26.2253\n",
      "Processing batch 527/11884 - Val_Loss: 27.9992\n",
      "Processing batch 528/11884 - Val_Loss: 25.6680\n",
      "Processing batch 529/11884 - Val_Loss: 28.7898\n",
      "Processing batch 530/11884 - Val_Loss: 25.7425\n",
      "Processing batch 531/11884 - Val_Loss: 25.7487\n",
      "Processing batch 532/11884 - Val_Loss: 25.2873\n",
      "Processing batch 533/11884 - Val_Loss: 24.9229\n",
      "Processing batch 534/11884 - Val_Loss: 24.9672\n",
      "Processing batch 535/11884 - Val_Loss: 24.8747\n",
      "Processing batch 536/11884 - Val_Loss: 24.9523\n",
      "Processing batch 537/11884 - Val_Loss: 28.2226\n",
      "Processing batch 538/11884 - Val_Loss: 24.5974\n",
      "Processing batch 539/11884 - Val_Loss: 22.6816\n",
      "Processing batch 540/11884 - Val_Loss: 27.9498\n",
      "Processing batch 541/11884 - Val_Loss: 26.1326\n",
      "Processing batch 542/11884 - Val_Loss: 34.9064\n",
      "Processing batch 543/11884 - Val_Loss: 22.0709\n",
      "Processing batch 544/11884 - Val_Loss: 19.6983\n",
      "Processing batch 545/11884 - Val_Loss: 25.9614\n",
      "Processing batch 546/11884 - Val_Loss: 23.4789\n",
      "Processing batch 547/11884 - Val_Loss: 25.9357\n",
      "Processing batch 548/11884 - Val_Loss: 26.0943\n",
      "Processing batch 549/11884 - Val_Loss: 25.4011\n",
      "Processing batch 550/11884 - Val_Loss: 23.8734\n",
      "Processing batch 551/11884 - Val_Loss: 24.1937\n",
      "Processing batch 552/11884 - Val_Loss: 25.6184\n",
      "Processing batch 553/11884 - Val_Loss: 24.6668\n",
      "Processing batch 554/11884 - Val_Loss: 26.2666\n",
      "Processing batch 555/11884 - Val_Loss: 26.2705\n",
      "Processing batch 556/11884 - Val_Loss: 27.1683\n",
      "Processing batch 557/11884 - Val_Loss: 25.8313\n",
      "Processing batch 558/11884 - Val_Loss: 21.9588\n",
      "Processing batch 559/11884 - Val_Loss: 26.4156\n",
      "Processing batch 560/11884 - Val_Loss: 24.4036\n",
      "Processing batch 561/11884 - Val_Loss: 21.3169\n",
      "Processing batch 562/11884 - Val_Loss: 26.4142\n",
      "Processing batch 563/11884 - Val_Loss: 25.5876\n",
      "Processing batch 564/11884 - Val_Loss: 26.7341\n",
      "Processing batch 565/11884 - Val_Loss: 24.7992\n",
      "Processing batch 566/11884 - Val_Loss: 24.9273\n",
      "Processing batch 567/11884 - Val_Loss: 24.4845\n",
      "Processing batch 568/11884 - Val_Loss: 26.3269\n",
      "Processing batch 569/11884 - Val_Loss: 24.1142\n",
      "Processing batch 570/11884 - Val_Loss: 25.4858\n",
      "Processing batch 571/11884 - Val_Loss: 25.4837\n",
      "Processing batch 572/11884 - Val_Loss: 25.0259\n",
      "Processing batch 573/11884 - Val_Loss: 27.5105\n",
      "Processing batch 574/11884 - Val_Loss: 27.9924\n",
      "Processing batch 575/11884 - Val_Loss: 25.1289\n",
      "Processing batch 576/11884 - Val_Loss: 21.4411\n",
      "Processing batch 577/11884 - Val_Loss: 22.3530\n",
      "Processing batch 578/11884 - Val_Loss: 25.9459\n",
      "Processing batch 579/11884 - Val_Loss: 24.3180\n",
      "Processing batch 580/11884 - Val_Loss: 28.0768\n",
      "Processing batch 581/11884 - Val_Loss: 26.7541\n",
      "Processing batch 582/11884 - Val_Loss: 25.6450\n",
      "Processing batch 583/11884 - Val_Loss: 25.3580\n",
      "Processing batch 584/11884 - Val_Loss: 24.2218\n",
      "Processing batch 585/11884 - Val_Loss: 26.7301\n",
      "Processing batch 586/11884 - Val_Loss: 24.2141\n",
      "Processing batch 587/11884 - Val_Loss: 27.2745\n",
      "Processing batch 588/11884 - Val_Loss: 24.3329\n",
      "Processing batch 589/11884 - Val_Loss: 24.3988\n",
      "Processing batch 590/11884 - Val_Loss: 23.8981\n",
      "Processing batch 591/11884 - Val_Loss: 24.1514\n",
      "Processing batch 592/11884 - Val_Loss: 26.9744\n",
      "Processing batch 593/11884 - Val_Loss: 25.6514\n",
      "Processing batch 594/11884 - Val_Loss: 25.4737\n",
      "Processing batch 595/11884 - Val_Loss: 26.9165\n",
      "Processing batch 596/11884 - Val_Loss: 25.9102\n",
      "Processing batch 597/11884 - Val_Loss: 26.3452\n",
      "Processing batch 598/11884 - Val_Loss: 25.5465\n",
      "Processing batch 599/11884 - Val_Loss: 25.7684\n",
      "Processing batch 600/11884 - Val_Loss: 25.0834\n",
      "Processing batch 601/11884 - Val_Loss: 22.4716\n",
      "Processing batch 602/11884 - Val_Loss: 25.5907\n",
      "Processing batch 603/11884 - Val_Loss: 24.4296\n",
      "Processing batch 604/11884 - Val_Loss: 23.9166\n",
      "Processing batch 605/11884 - Val_Loss: 26.1113\n",
      "Processing batch 606/11884 - Val_Loss: 26.2542\n",
      "Processing batch 607/11884 - Val_Loss: 30.6837\n",
      "Processing batch 608/11884 - Val_Loss: 25.7830\n",
      "Processing batch 609/11884 - Val_Loss: 25.0097\n",
      "Processing batch 610/11884 - Val_Loss: 24.7766\n",
      "Processing batch 611/11884 - Val_Loss: 26.4296\n",
      "Processing batch 612/11884 - Val_Loss: 24.3707\n",
      "Processing batch 613/11884 - Val_Loss: 26.2907\n",
      "Processing batch 614/11884 - Val_Loss: 24.8064\n",
      "Processing batch 615/11884 - Val_Loss: 26.6478\n",
      "Processing batch 616/11884 - Val_Loss: 26.6996\n",
      "Processing batch 617/11884 - Val_Loss: 26.6292\n",
      "Processing batch 618/11884 - Val_Loss: 22.6694\n",
      "Processing batch 619/11884 - Val_Loss: 28.9623\n",
      "Processing batch 620/11884 - Val_Loss: 27.0011\n",
      "Processing batch 621/11884 - Val_Loss: 26.3516\n",
      "Processing batch 622/11884 - Val_Loss: 26.6047\n",
      "Processing batch 623/11884 - Val_Loss: 26.9025\n",
      "Processing batch 624/11884 - Val_Loss: 27.8362\n",
      "Processing batch 625/11884 - Val_Loss: 25.0173\n",
      "Processing batch 626/11884 - Val_Loss: 25.4833\n",
      "Processing batch 627/11884 - Val_Loss: 21.5279\n",
      "Processing batch 628/11884 - Val_Loss: 24.7288\n",
      "Processing batch 629/11884 - Val_Loss: 24.5925\n",
      "Processing batch 630/11884 - Val_Loss: 24.2135\n",
      "Processing batch 631/11884 - Val_Loss: 24.5348\n",
      "Processing batch 632/11884 - Val_Loss: 26.1045\n",
      "Processing batch 633/11884 - Val_Loss: 25.8927\n",
      "Processing batch 634/11884 - Val_Loss: 23.8302\n",
      "Processing batch 635/11884 - Val_Loss: 24.3034\n",
      "Processing batch 636/11884 - Val_Loss: 24.8908\n",
      "Processing batch 637/11884 - Val_Loss: 27.8872\n",
      "Processing batch 638/11884 - Val_Loss: 26.3391\n",
      "Processing batch 639/11884 - Val_Loss: 23.2046\n",
      "Processing batch 640/11884 - Val_Loss: 27.6128\n",
      "Processing batch 641/11884 - Val_Loss: 31.4361\n",
      "Processing batch 642/11884 - Val_Loss: 25.8465\n",
      "Processing batch 643/11884 - Val_Loss: 25.5948\n",
      "Processing batch 644/11884 - Val_Loss: 24.4523\n",
      "Processing batch 645/11884 - Val_Loss: 24.3384\n",
      "Processing batch 646/11884 - Val_Loss: 24.9613\n",
      "Processing batch 647/11884 - Val_Loss: 24.8333\n",
      "Processing batch 648/11884 - Val_Loss: 23.7950\n",
      "Processing batch 649/11884 - Val_Loss: 22.8965\n",
      "Processing batch 650/11884 - Val_Loss: 26.9611\n",
      "Processing batch 651/11884 - Val_Loss: 24.0831\n",
      "Processing batch 652/11884 - Val_Loss: 28.7616\n",
      "Processing batch 653/11884 - Val_Loss: 25.8794\n",
      "Processing batch 654/11884 - Val_Loss: 23.1010\n",
      "Processing batch 655/11884 - Val_Loss: 27.2965\n",
      "Processing batch 656/11884 - Val_Loss: 25.3227\n",
      "Processing batch 657/11884 - Val_Loss: 24.2794\n",
      "Processing batch 658/11884 - Val_Loss: 25.7281\n",
      "Processing batch 659/11884 - Val_Loss: 25.7499\n",
      "Processing batch 660/11884 - Val_Loss: 27.2818\n",
      "Processing batch 661/11884 - Val_Loss: 29.1277\n",
      "Processing batch 662/11884 - Val_Loss: 23.1481\n",
      "Processing batch 663/11884 - Val_Loss: 24.9823\n",
      "Processing batch 664/11884 - Val_Loss: 25.3835\n",
      "Processing batch 665/11884 - Val_Loss: 25.7572\n",
      "Processing batch 666/11884 - Val_Loss: 23.8708\n",
      "Processing batch 667/11884 - Val_Loss: 27.1171\n",
      "Processing batch 668/11884 - Val_Loss: 19.7274\n",
      "Processing batch 669/11884 - Val_Loss: 28.7690\n",
      "Processing batch 670/11884 - Val_Loss: 26.3763\n",
      "Processing batch 671/11884 - Val_Loss: 26.0403\n",
      "Processing batch 672/11884 - Val_Loss: 27.3062\n",
      "Processing batch 673/11884 - Val_Loss: 26.9390\n",
      "Processing batch 674/11884 - Val_Loss: 31.1082\n",
      "Processing batch 675/11884 - Val_Loss: 27.2399\n",
      "Processing batch 676/11884 - Val_Loss: 22.9587\n",
      "Processing batch 677/11884 - Val_Loss: 27.7123\n",
      "Processing batch 678/11884 - Val_Loss: 27.5980\n",
      "Processing batch 679/11884 - Val_Loss: 25.4058\n",
      "Processing batch 680/11884 - Val_Loss: 27.6710\n",
      "Processing batch 681/11884 - Val_Loss: 27.0858\n",
      "Processing batch 682/11884 - Val_Loss: 26.5733\n",
      "Processing batch 683/11884 - Val_Loss: 24.1278\n",
      "Processing batch 684/11884 - Val_Loss: 27.7862\n",
      "Processing batch 685/11884 - Val_Loss: 27.0035\n",
      "Processing batch 686/11884 - Val_Loss: 26.8383\n",
      "Processing batch 687/11884 - Val_Loss: 25.8407\n",
      "Processing batch 688/11884 - Val_Loss: 26.2474\n",
      "Processing batch 689/11884 - Val_Loss: 27.1380\n",
      "Processing batch 690/11884 - Val_Loss: 27.2280\n",
      "Processing batch 691/11884 - Val_Loss: 27.0561\n",
      "Processing batch 692/11884 - Val_Loss: 28.7324\n",
      "Processing batch 693/11884 - Val_Loss: 26.1210\n",
      "Processing batch 694/11884 - Val_Loss: 26.8737\n",
      "Processing batch 695/11884 - Val_Loss: 26.9071\n",
      "Processing batch 696/11884 - Val_Loss: 23.8912\n",
      "Processing batch 697/11884 - Val_Loss: 25.8186\n",
      "Processing batch 698/11884 - Val_Loss: 25.4127\n",
      "Processing batch 699/11884 - Val_Loss: 24.7382\n",
      "Processing batch 700/11884 - Val_Loss: 23.6161\n",
      "Processing batch 701/11884 - Val_Loss: 25.8274\n",
      "Processing batch 702/11884 - Val_Loss: 23.6476\n",
      "Processing batch 703/11884 - Val_Loss: 23.1850\n",
      "Processing batch 704/11884 - Val_Loss: 27.3571\n",
      "Processing batch 705/11884 - Val_Loss: 26.0489\n",
      "Processing batch 706/11884 - Val_Loss: 23.6488\n",
      "Processing batch 707/11884 - Val_Loss: 25.2875\n",
      "Processing batch 708/11884 - Val_Loss: 27.4743\n",
      "Processing batch 709/11884 - Val_Loss: 25.2436\n",
      "Processing batch 710/11884 - Val_Loss: 26.1645\n",
      "Processing batch 711/11884 - Val_Loss: 24.7376\n",
      "Processing batch 712/11884 - Val_Loss: 23.6151\n",
      "Processing batch 713/11884 - Val_Loss: 24.8825\n",
      "Processing batch 714/11884 - Val_Loss: 24.0084\n",
      "Processing batch 715/11884 - Val_Loss: 26.4104\n",
      "Processing batch 716/11884 - Val_Loss: 23.6204\n",
      "Processing batch 717/11884 - Val_Loss: 28.5477\n",
      "Processing batch 718/11884 - Val_Loss: 24.4025\n",
      "Processing batch 719/11884 - Val_Loss: 28.8376\n",
      "Processing batch 720/11884 - Val_Loss: 27.8069\n",
      "Processing batch 721/11884 - Val_Loss: 27.7292\n",
      "Processing batch 722/11884 - Val_Loss: 26.7170\n",
      "Processing batch 723/11884 - Val_Loss: 29.4429\n",
      "Processing batch 724/11884 - Val_Loss: 26.7162\n",
      "Processing batch 725/11884 - Val_Loss: 24.8445\n",
      "Processing batch 726/11884 - Val_Loss: 27.6275\n",
      "Processing batch 727/11884 - Val_Loss: 27.0682\n",
      "Processing batch 728/11884 - Val_Loss: 23.8889\n",
      "Processing batch 729/11884 - Val_Loss: 26.4627\n",
      "Processing batch 730/11884 - Val_Loss: 24.4547\n",
      "Processing batch 731/11884 - Val_Loss: 26.1748\n",
      "Processing batch 732/11884 - Val_Loss: 25.5322\n",
      "Processing batch 733/11884 - Val_Loss: 26.2281\n",
      "Processing batch 734/11884 - Val_Loss: 27.9880\n",
      "Processing batch 735/11884 - Val_Loss: 28.8863\n",
      "Processing batch 736/11884 - Val_Loss: 24.4691\n",
      "Processing batch 737/11884 - Val_Loss: 26.2241\n",
      "Processing batch 738/11884 - Val_Loss: 23.6632\n",
      "Processing batch 739/11884 - Val_Loss: 26.0958\n",
      "Processing batch 740/11884 - Val_Loss: 27.9372\n",
      "Processing batch 741/11884 - Val_Loss: 25.5833\n",
      "Processing batch 742/11884 - Val_Loss: 24.6127\n",
      "Processing batch 743/11884 - Val_Loss: 24.9591\n",
      "Processing batch 744/11884 - Val_Loss: 27.3932\n",
      "Processing batch 745/11884 - Val_Loss: 25.3035\n",
      "Processing batch 746/11884 - Val_Loss: 27.0208\n",
      "Processing batch 747/11884 - Val_Loss: 26.8626\n",
      "Processing batch 748/11884 - Val_Loss: 26.9963\n",
      "Processing batch 749/11884 - Val_Loss: 24.9084\n",
      "Processing batch 750/11884 - Val_Loss: 26.2364\n",
      "Processing batch 751/11884 - Val_Loss: 26.2372\n",
      "Processing batch 752/11884 - Val_Loss: 27.4607\n",
      "Processing batch 753/11884 - Val_Loss: 23.9288\n",
      "Processing batch 754/11884 - Val_Loss: 27.8427\n",
      "Processing batch 755/11884 - Val_Loss: 24.2795\n",
      "Processing batch 756/11884 - Val_Loss: 26.4032\n",
      "Processing batch 757/11884 - Val_Loss: 22.2170\n",
      "Processing batch 758/11884 - Val_Loss: 26.2367\n",
      "Processing batch 759/11884 - Val_Loss: 28.9392\n",
      "Processing batch 760/11884 - Val_Loss: 24.8527\n",
      "Processing batch 761/11884 - Val_Loss: 25.7266\n",
      "Processing batch 762/11884 - Val_Loss: 24.5838\n",
      "Processing batch 763/11884 - Val_Loss: 30.0013\n",
      "Processing batch 764/11884 - Val_Loss: 24.5595\n",
      "Processing batch 765/11884 - Val_Loss: 25.9003\n",
      "Processing batch 766/11884 - Val_Loss: 23.7734\n",
      "Processing batch 767/11884 - Val_Loss: 25.9908\n",
      "Processing batch 768/11884 - Val_Loss: 28.3371\n",
      "Processing batch 769/11884 - Val_Loss: 27.3416\n",
      "Processing batch 770/11884 - Val_Loss: 23.9766\n",
      "Processing batch 771/11884 - Val_Loss: 24.0111\n",
      "Processing batch 772/11884 - Val_Loss: 25.4045\n",
      "Processing batch 773/11884 - Val_Loss: 27.4513\n",
      "Processing batch 774/11884 - Val_Loss: 25.4011\n",
      "Processing batch 775/11884 - Val_Loss: 24.9279\n",
      "Processing batch 776/11884 - Val_Loss: 27.0145\n",
      "Processing batch 777/11884 - Val_Loss: 25.7044\n",
      "Processing batch 778/11884 - Val_Loss: 29.8068\n",
      "Processing batch 779/11884 - Val_Loss: 25.9406\n",
      "Processing batch 780/11884 - Val_Loss: 28.8676\n",
      "Processing batch 781/11884 - Val_Loss: 25.2120\n",
      "Processing batch 782/11884 - Val_Loss: 27.1050\n",
      "Processing batch 783/11884 - Val_Loss: 26.3108\n",
      "Processing batch 784/11884 - Val_Loss: 27.1619\n",
      "Processing batch 785/11884 - Val_Loss: 24.7715\n",
      "Processing batch 786/11884 - Val_Loss: 25.7120\n",
      "Processing batch 787/11884 - Val_Loss: 25.3181\n",
      "Processing batch 788/11884 - Val_Loss: 24.1910\n",
      "Processing batch 789/11884 - Val_Loss: 25.8874\n",
      "Processing batch 790/11884 - Val_Loss: 22.8241\n",
      "Processing batch 791/11884 - Val_Loss: 23.6924\n",
      "Processing batch 792/11884 - Val_Loss: 27.1539\n",
      "Processing batch 793/11884 - Val_Loss: 24.5359\n",
      "Processing batch 794/11884 - Val_Loss: 27.0119\n",
      "Processing batch 795/11884 - Val_Loss: 23.2137\n",
      "Processing batch 796/11884 - Val_Loss: 24.9493\n",
      "Processing batch 797/11884 - Val_Loss: 25.4971\n",
      "Processing batch 798/11884 - Val_Loss: 27.9721\n",
      "Processing batch 799/11884 - Val_Loss: 26.4274\n",
      "Processing batch 800/11884 - Val_Loss: 29.6780\n",
      "Processing batch 801/11884 - Val_Loss: 24.5938\n",
      "Processing batch 802/11884 - Val_Loss: 25.4760\n",
      "Processing batch 803/11884 - Val_Loss: 23.9076\n",
      "Processing batch 804/11884 - Val_Loss: 24.3807\n",
      "Processing batch 805/11884 - Val_Loss: 25.3969\n",
      "Processing batch 806/11884 - Val_Loss: 25.3523\n",
      "Processing batch 807/11884 - Val_Loss: 25.3835\n",
      "Processing batch 808/11884 - Val_Loss: 24.1192\n",
      "Processing batch 809/11884 - Val_Loss: 25.6515\n",
      "Processing batch 810/11884 - Val_Loss: 25.4095\n",
      "Processing batch 811/11884 - Val_Loss: 27.9871\n",
      "Processing batch 812/11884 - Val_Loss: 27.6129\n",
      "Processing batch 813/11884 - Val_Loss: 23.9604\n",
      "Processing batch 814/11884 - Val_Loss: 24.9593\n",
      "Processing batch 815/11884 - Val_Loss: 26.1281\n",
      "Processing batch 816/11884 - Val_Loss: 27.2261\n",
      "Processing batch 817/11884 - Val_Loss: 24.0949\n",
      "Processing batch 818/11884 - Val_Loss: 24.2777\n",
      "Processing batch 819/11884 - Val_Loss: 24.7118\n",
      "Processing batch 820/11884 - Val_Loss: 24.0307\n",
      "Processing batch 821/11884 - Val_Loss: 24.6304\n",
      "Processing batch 822/11884 - Val_Loss: 24.8434\n",
      "Processing batch 823/11884 - Val_Loss: 29.0199\n",
      "Processing batch 824/11884 - Val_Loss: 24.7946\n",
      "Processing batch 825/11884 - Val_Loss: 25.6184\n",
      "Processing batch 826/11884 - Val_Loss: 26.0079\n",
      "Processing batch 827/11884 - Val_Loss: 26.4920\n",
      "Processing batch 828/11884 - Val_Loss: 25.7393\n",
      "Processing batch 829/11884 - Val_Loss: 22.0093\n",
      "Processing batch 830/11884 - Val_Loss: 26.1515\n",
      "Processing batch 831/11884 - Val_Loss: 25.6785\n",
      "Processing batch 832/11884 - Val_Loss: 25.2074\n",
      "Processing batch 833/11884 - Val_Loss: 26.7972\n",
      "Processing batch 834/11884 - Val_Loss: 29.4329\n",
      "Processing batch 835/11884 - Val_Loss: 26.2895\n",
      "Processing batch 836/11884 - Val_Loss: 24.8603\n",
      "Processing batch 837/11884 - Val_Loss: 26.8100\n",
      "Processing batch 838/11884 - Val_Loss: 26.9507\n",
      "Processing batch 839/11884 - Val_Loss: 27.8151\n",
      "Processing batch 840/11884 - Val_Loss: 22.1828\n",
      "Processing batch 841/11884 - Val_Loss: 21.6296\n",
      "Processing batch 842/11884 - Val_Loss: 27.1662\n",
      "Processing batch 843/11884 - Val_Loss: 27.8108\n",
      "Processing batch 844/11884 - Val_Loss: 24.3179\n",
      "Processing batch 845/11884 - Val_Loss: 24.7611\n",
      "Processing batch 846/11884 - Val_Loss: 28.5852\n",
      "Processing batch 847/11884 - Val_Loss: 24.3652\n",
      "Processing batch 848/11884 - Val_Loss: 25.9120\n",
      "Processing batch 849/11884 - Val_Loss: 26.1984\n",
      "Processing batch 850/11884 - Val_Loss: 24.0538\n",
      "Processing batch 851/11884 - Val_Loss: 24.2336\n",
      "Processing batch 852/11884 - Val_Loss: 24.3937\n",
      "Processing batch 853/11884 - Val_Loss: 27.8146\n",
      "Processing batch 854/11884 - Val_Loss: 27.0615\n",
      "Processing batch 855/11884 - Val_Loss: 26.5958\n",
      "Processing batch 856/11884 - Val_Loss: 26.6064\n",
      "Processing batch 857/11884 - Val_Loss: 27.6014\n",
      "Processing batch 858/11884 - Val_Loss: 24.9116\n",
      "Processing batch 859/11884 - Val_Loss: 25.8509\n",
      "Processing batch 860/11884 - Val_Loss: 26.2827\n",
      "Processing batch 861/11884 - Val_Loss: 24.8573\n",
      "Processing batch 862/11884 - Val_Loss: 24.5276\n",
      "Processing batch 863/11884 - Val_Loss: 25.0932\n",
      "Processing batch 864/11884 - Val_Loss: 26.1484\n",
      "Processing batch 865/11884 - Val_Loss: 22.0245\n",
      "Processing batch 866/11884 - Val_Loss: 23.3498\n",
      "Processing batch 867/11884 - Val_Loss: 23.4600\n",
      "Processing batch 868/11884 - Val_Loss: 27.5438\n",
      "Processing batch 869/11884 - Val_Loss: 27.7459\n",
      "Processing batch 870/11884 - Val_Loss: 24.8917\n",
      "Processing batch 871/11884 - Val_Loss: 27.4942\n",
      "Processing batch 872/11884 - Val_Loss: 25.9621\n",
      "Processing batch 873/11884 - Val_Loss: 21.7491\n",
      "Processing batch 874/11884 - Val_Loss: 25.5560\n",
      "Processing batch 875/11884 - Val_Loss: 26.7889\n",
      "Processing batch 876/11884 - Val_Loss: 27.6324\n",
      "Processing batch 877/11884 - Val_Loss: 27.9103\n",
      "Processing batch 878/11884 - Val_Loss: 24.6875\n",
      "Processing batch 879/11884 - Val_Loss: 26.7968\n",
      "Processing batch 880/11884 - Val_Loss: 25.3135\n",
      "Processing batch 881/11884 - Val_Loss: 28.9334\n",
      "Processing batch 882/11884 - Val_Loss: 25.4818\n",
      "Processing batch 883/11884 - Val_Loss: 27.5508\n",
      "Processing batch 884/11884 - Val_Loss: 23.3971\n",
      "Processing batch 885/11884 - Val_Loss: 27.4357\n",
      "Processing batch 886/11884 - Val_Loss: 27.3664\n",
      "Processing batch 887/11884 - Val_Loss: 25.2660\n",
      "Processing batch 888/11884 - Val_Loss: 25.1457\n",
      "Processing batch 889/11884 - Val_Loss: 24.8705\n",
      "Processing batch 890/11884 - Val_Loss: 23.6056\n",
      "Processing batch 891/11884 - Val_Loss: 28.9082\n",
      "Processing batch 892/11884 - Val_Loss: 26.0364\n",
      "Processing batch 893/11884 - Val_Loss: 24.7212\n",
      "Processing batch 894/11884 - Val_Loss: 24.3483\n",
      "Processing batch 895/11884 - Val_Loss: 26.3786\n",
      "Processing batch 896/11884 - Val_Loss: 27.2844\n",
      "Processing batch 897/11884 - Val_Loss: 24.8902\n",
      "Processing batch 898/11884 - Val_Loss: 28.5034\n",
      "Processing batch 899/11884 - Val_Loss: 27.7710\n",
      "Processing batch 900/11884 - Val_Loss: 27.4147\n",
      "Processing batch 901/11884 - Val_Loss: 25.1861\n",
      "Processing batch 902/11884 - Val_Loss: 27.3977\n",
      "Processing batch 903/11884 - Val_Loss: 26.7832\n",
      "Processing batch 904/11884 - Val_Loss: 24.3645\n",
      "Processing batch 905/11884 - Val_Loss: 26.4369\n",
      "Processing batch 906/11884 - Val_Loss: 27.8499\n",
      "Processing batch 907/11884 - Val_Loss: 27.3577\n",
      "Processing batch 908/11884 - Val_Loss: 25.3526\n",
      "Processing batch 909/11884 - Val_Loss: 29.8103\n",
      "Processing batch 910/11884 - Val_Loss: 26.9871\n",
      "Processing batch 911/11884 - Val_Loss: 27.2286\n",
      "Processing batch 912/11884 - Val_Loss: 26.2564\n",
      "Processing batch 913/11884 - Val_Loss: 26.8746\n",
      "Processing batch 914/11884 - Val_Loss: 26.2589\n",
      "Processing batch 915/11884 - Val_Loss: 25.9083\n",
      "Processing batch 916/11884 - Val_Loss: 24.6390\n",
      "Processing batch 917/11884 - Val_Loss: 25.6278\n",
      "Processing batch 918/11884 - Val_Loss: 24.7017\n",
      "Processing batch 919/11884 - Val_Loss: 24.7600\n",
      "Processing batch 920/11884 - Val_Loss: 23.4502\n",
      "Processing batch 921/11884 - Val_Loss: 27.1999\n",
      "Processing batch 922/11884 - Val_Loss: 26.5826\n",
      "Processing batch 923/11884 - Val_Loss: 25.4233\n",
      "Processing batch 924/11884 - Val_Loss: 25.6842\n",
      "Processing batch 925/11884 - Val_Loss: 26.6342\n",
      "Processing batch 926/11884 - Val_Loss: 26.1003\n",
      "Processing batch 927/11884 - Val_Loss: 27.5853\n",
      "Processing batch 928/11884 - Val_Loss: 25.7988\n",
      "Processing batch 929/11884 - Val_Loss: 26.6188\n",
      "Processing batch 930/11884 - Val_Loss: 25.4466\n",
      "Processing batch 931/11884 - Val_Loss: 26.3873\n",
      "Processing batch 932/11884 - Val_Loss: 25.3548\n",
      "Processing batch 933/11884 - Val_Loss: 26.7128\n",
      "Processing batch 934/11884 - Val_Loss: 24.4174\n",
      "Processing batch 935/11884 - Val_Loss: 26.1768\n",
      "Processing batch 936/11884 - Val_Loss: 26.0611\n",
      "Processing batch 937/11884 - Val_Loss: 27.5764\n",
      "Processing batch 938/11884 - Val_Loss: 28.5355\n",
      "Processing batch 939/11884 - Val_Loss: 23.1658\n",
      "Processing batch 940/11884 - Val_Loss: 25.5425\n",
      "Processing batch 941/11884 - Val_Loss: 24.6158\n",
      "Processing batch 942/11884 - Val_Loss: 28.6400\n",
      "Processing batch 943/11884 - Val_Loss: 27.2787\n",
      "Processing batch 944/11884 - Val_Loss: 24.6276\n",
      "Processing batch 945/11884 - Val_Loss: 25.7930\n",
      "Processing batch 946/11884 - Val_Loss: 29.0526\n",
      "Processing batch 947/11884 - Val_Loss: 23.6626\n",
      "Processing batch 948/11884 - Val_Loss: 25.2934\n",
      "Processing batch 949/11884 - Val_Loss: 25.7591\n",
      "Processing batch 950/11884 - Val_Loss: 24.3463\n",
      "Processing batch 951/11884 - Val_Loss: 24.6890\n",
      "Processing batch 952/11884 - Val_Loss: 25.5028\n",
      "Processing batch 953/11884 - Val_Loss: 23.9447\n",
      "Processing batch 954/11884 - Val_Loss: 24.3133\n",
      "Processing batch 955/11884 - Val_Loss: 25.4407\n",
      "Processing batch 956/11884 - Val_Loss: 24.4511\n",
      "Processing batch 957/11884 - Val_Loss: 23.4741\n",
      "Processing batch 958/11884 - Val_Loss: 27.0145\n",
      "Processing batch 959/11884 - Val_Loss: 26.0252\n",
      "Processing batch 960/11884 - Val_Loss: 24.5796\n",
      "Processing batch 961/11884 - Val_Loss: 26.0289\n",
      "Processing batch 962/11884 - Val_Loss: 26.2104\n",
      "Processing batch 963/11884 - Val_Loss: 25.9468\n",
      "Processing batch 964/11884 - Val_Loss: 27.3342\n",
      "Processing batch 965/11884 - Val_Loss: 24.5758\n",
      "Processing batch 966/11884 - Val_Loss: 27.5926\n",
      "Processing batch 967/11884 - Val_Loss: 27.6846\n",
      "Processing batch 968/11884 - Val_Loss: 24.3928\n",
      "Processing batch 969/11884 - Val_Loss: 27.1994\n",
      "Processing batch 970/11884 - Val_Loss: 26.6203\n",
      "Processing batch 971/11884 - Val_Loss: 22.8724\n",
      "Processing batch 972/11884 - Val_Loss: 26.5929\n",
      "Processing batch 973/11884 - Val_Loss: 23.4198\n",
      "Processing batch 974/11884 - Val_Loss: 27.3067\n",
      "Processing batch 975/11884 - Val_Loss: 26.8376\n",
      "Processing batch 976/11884 - Val_Loss: 27.8834\n",
      "Processing batch 977/11884 - Val_Loss: 27.0909\n",
      "Processing batch 978/11884 - Val_Loss: 27.1727\n",
      "Processing batch 979/11884 - Val_Loss: 22.2484\n",
      "Processing batch 980/11884 - Val_Loss: 24.0120\n",
      "Processing batch 981/11884 - Val_Loss: 23.4479\n",
      "Processing batch 982/11884 - Val_Loss: 23.9802\n",
      "Processing batch 983/11884 - Val_Loss: 22.4837\n",
      "Processing batch 984/11884 - Val_Loss: 24.3458\n",
      "Processing batch 985/11884 - Val_Loss: 27.8407\n",
      "Processing batch 986/11884 - Val_Loss: 24.4040\n",
      "Processing batch 987/11884 - Val_Loss: 25.0308\n",
      "Processing batch 988/11884 - Val_Loss: 25.8058\n",
      "Processing batch 989/11884 - Val_Loss: 22.5039\n",
      "Processing batch 990/11884 - Val_Loss: 26.9544\n",
      "Processing batch 991/11884 - Val_Loss: 26.5425\n",
      "Processing batch 992/11884 - Val_Loss: 25.5262\n",
      "Processing batch 993/11884 - Val_Loss: 23.7620\n",
      "Processing batch 994/11884 - Val_Loss: 22.3589\n",
      "Processing batch 995/11884 - Val_Loss: 24.4393\n",
      "Processing batch 996/11884 - Val_Loss: 25.1734\n",
      "Processing batch 997/11884 - Val_Loss: 24.6674\n",
      "Processing batch 998/11884 - Val_Loss: 25.8376\n",
      "Processing batch 999/11884 - Val_Loss: 25.8938\n",
      "Processing batch 1000/11884 - Val_Loss: 25.7818\n",
      "Processing batch 1001/11884 - Val_Loss: 26.3054\n",
      "Processing batch 1002/11884 - Val_Loss: 26.0381\n",
      "Processing batch 1003/11884 - Val_Loss: 24.7286\n",
      "Processing batch 1004/11884 - Val_Loss: 26.3175\n",
      "Processing batch 1005/11884 - Val_Loss: 28.5453\n",
      "Processing batch 1006/11884 - Val_Loss: 25.6878\n",
      "Processing batch 1007/11884 - Val_Loss: 25.0942\n",
      "Processing batch 1008/11884 - Val_Loss: 24.9750\n",
      "Processing batch 1009/11884 - Val_Loss: 23.5977\n",
      "Processing batch 1010/11884 - Val_Loss: 26.2861\n",
      "Processing batch 1011/11884 - Val_Loss: 24.1837\n",
      "Processing batch 1012/11884 - Val_Loss: 26.4773\n",
      "Processing batch 1013/11884 - Val_Loss: 25.6243\n",
      "Processing batch 1014/11884 - Val_Loss: 25.1796\n",
      "Processing batch 1015/11884 - Val_Loss: 24.0250\n",
      "Processing batch 1016/11884 - Val_Loss: 27.5667\n",
      "Processing batch 1017/11884 - Val_Loss: 26.8909\n",
      "Processing batch 1018/11884 - Val_Loss: 25.5304\n",
      "Processing batch 1019/11884 - Val_Loss: 25.8246\n",
      "Processing batch 1020/11884 - Val_Loss: 24.7892\n",
      "Processing batch 1021/11884 - Val_Loss: 25.0832\n",
      "Processing batch 1022/11884 - Val_Loss: 26.8393\n",
      "Processing batch 1023/11884 - Val_Loss: 28.8097\n",
      "Processing batch 1024/11884 - Val_Loss: 24.6155\n",
      "Processing batch 1025/11884 - Val_Loss: 25.3365\n",
      "Processing batch 1026/11884 - Val_Loss: 24.7749\n",
      "Processing batch 1027/11884 - Val_Loss: 24.1562\n",
      "Processing batch 1028/11884 - Val_Loss: 25.5389\n",
      "Processing batch 1029/11884 - Val_Loss: 27.5271\n",
      "Processing batch 1030/11884 - Val_Loss: 28.0561\n",
      "Processing batch 1031/11884 - Val_Loss: 25.8630\n",
      "Processing batch 1032/11884 - Val_Loss: 26.6804\n",
      "Processing batch 1033/11884 - Val_Loss: 26.4005\n",
      "Processing batch 1034/11884 - Val_Loss: 24.9146\n",
      "Processing batch 1035/11884 - Val_Loss: 24.5351\n",
      "Processing batch 1036/11884 - Val_Loss: 26.0824\n",
      "Processing batch 1037/11884 - Val_Loss: 26.6485\n",
      "Processing batch 1038/11884 - Val_Loss: 26.7157\n",
      "Processing batch 1039/11884 - Val_Loss: 24.6320\n",
      "Processing batch 1040/11884 - Val_Loss: 25.5487\n",
      "Processing batch 1041/11884 - Val_Loss: 23.9627\n",
      "Processing batch 1042/11884 - Val_Loss: 27.0364\n",
      "Processing batch 1043/11884 - Val_Loss: 26.0958\n",
      "Processing batch 1044/11884 - Val_Loss: 25.2293\n",
      "Processing batch 1045/11884 - Val_Loss: 27.9481\n",
      "Processing batch 1046/11884 - Val_Loss: 27.0584\n",
      "Processing batch 1047/11884 - Val_Loss: 25.7968\n",
      "Processing batch 1048/11884 - Val_Loss: 25.9942\n",
      "Processing batch 1049/11884 - Val_Loss: 28.8424\n",
      "Processing batch 1050/11884 - Val_Loss: 25.1444\n",
      "Processing batch 1051/11884 - Val_Loss: 27.6223\n",
      "Processing batch 1052/11884 - Val_Loss: 26.6240\n",
      "Processing batch 1053/11884 - Val_Loss: 25.9068\n",
      "Processing batch 1054/11884 - Val_Loss: 25.8635\n",
      "Processing batch 1055/11884 - Val_Loss: 24.5384\n",
      "Processing batch 1056/11884 - Val_Loss: 27.1297\n",
      "Processing batch 1057/11884 - Val_Loss: 24.6725\n",
      "Processing batch 1058/11884 - Val_Loss: 24.6129\n",
      "Processing batch 1059/11884 - Val_Loss: 27.6050\n",
      "Processing batch 1060/11884 - Val_Loss: 23.7122\n",
      "Processing batch 1061/11884 - Val_Loss: 25.0349\n",
      "Processing batch 1062/11884 - Val_Loss: 28.9713\n",
      "Processing batch 1063/11884 - Val_Loss: 23.8326\n",
      "Processing batch 1064/11884 - Val_Loss: 26.2152\n",
      "Processing batch 1065/11884 - Val_Loss: 24.6397\n",
      "Processing batch 1066/11884 - Val_Loss: 28.6795\n",
      "Processing batch 1067/11884 - Val_Loss: 24.1737\n",
      "Processing batch 1068/11884 - Val_Loss: 27.0748\n",
      "Processing batch 1069/11884 - Val_Loss: 24.3360\n",
      "Processing batch 1070/11884 - Val_Loss: 25.8917\n",
      "Processing batch 1071/11884 - Val_Loss: 23.1355\n",
      "Processing batch 1072/11884 - Val_Loss: 26.7583\n",
      "Processing batch 1073/11884 - Val_Loss: 22.3016\n",
      "Processing batch 1074/11884 - Val_Loss: 25.1895\n",
      "Processing batch 1075/11884 - Val_Loss: 24.6942\n",
      "Processing batch 1076/11884 - Val_Loss: 26.1818\n",
      "Processing batch 1077/11884 - Val_Loss: 25.5852\n",
      "Processing batch 1078/11884 - Val_Loss: 28.8430\n",
      "Processing batch 1079/11884 - Val_Loss: 25.9682\n",
      "Processing batch 1080/11884 - Val_Loss: 27.0467\n",
      "Processing batch 1081/11884 - Val_Loss: 26.7003\n",
      "Processing batch 1082/11884 - Val_Loss: 24.2972\n",
      "Processing batch 1083/11884 - Val_Loss: 24.5671\n",
      "Processing batch 1084/11884 - Val_Loss: 29.3111\n",
      "Processing batch 1085/11884 - Val_Loss: 22.9446\n",
      "Processing batch 1086/11884 - Val_Loss: 21.4923\n",
      "Processing batch 1087/11884 - Val_Loss: 23.8440\n",
      "Processing batch 1088/11884 - Val_Loss: 24.8072\n",
      "Processing batch 1089/11884 - Val_Loss: 26.2955\n",
      "Processing batch 1090/11884 - Val_Loss: 27.1963\n",
      "Processing batch 1091/11884 - Val_Loss: 29.1152\n",
      "Processing batch 1092/11884 - Val_Loss: 30.3764\n",
      "Processing batch 1093/11884 - Val_Loss: 24.1401\n",
      "Processing batch 1094/11884 - Val_Loss: 25.8962\n",
      "Processing batch 1095/11884 - Val_Loss: 27.7753\n",
      "Processing batch 1096/11884 - Val_Loss: 24.7044\n",
      "Processing batch 1097/11884 - Val_Loss: 27.2584\n",
      "Processing batch 1098/11884 - Val_Loss: 28.3530\n",
      "Processing batch 1099/11884 - Val_Loss: 25.2496\n",
      "Processing batch 1100/11884 - Val_Loss: 22.1685\n",
      "Processing batch 1101/11884 - Val_Loss: 26.2227\n",
      "Processing batch 1102/11884 - Val_Loss: 28.3942\n",
      "Processing batch 1103/11884 - Val_Loss: 25.4776\n",
      "Processing batch 1104/11884 - Val_Loss: 25.6087\n",
      "Processing batch 1105/11884 - Val_Loss: 31.0419\n",
      "Processing batch 1106/11884 - Val_Loss: 26.3296\n",
      "Processing batch 1107/11884 - Val_Loss: 25.8262\n",
      "Processing batch 1108/11884 - Val_Loss: 25.3664\n",
      "Processing batch 1109/11884 - Val_Loss: 25.6336\n",
      "Processing batch 1110/11884 - Val_Loss: 26.0689\n",
      "Processing batch 1111/11884 - Val_Loss: 25.7280\n",
      "Processing batch 1112/11884 - Val_Loss: 26.8154\n",
      "Processing batch 1113/11884 - Val_Loss: 23.6213\n",
      "Processing batch 1114/11884 - Val_Loss: 27.4597\n",
      "Processing batch 1115/11884 - Val_Loss: 26.1217\n",
      "Processing batch 1116/11884 - Val_Loss: 25.8913\n",
      "Processing batch 1117/11884 - Val_Loss: 25.3146\n",
      "Processing batch 1118/11884 - Val_Loss: 26.5572\n",
      "Processing batch 1119/11884 - Val_Loss: 23.6524\n",
      "Processing batch 1120/11884 - Val_Loss: 26.0375\n",
      "Processing batch 1121/11884 - Val_Loss: 22.8929\n",
      "Processing batch 1122/11884 - Val_Loss: 25.5323\n",
      "Processing batch 1123/11884 - Val_Loss: 26.3433\n",
      "Processing batch 1124/11884 - Val_Loss: 28.1756\n",
      "Processing batch 1125/11884 - Val_Loss: 26.5729\n",
      "Processing batch 1126/11884 - Val_Loss: 25.2196\n",
      "Processing batch 1127/11884 - Val_Loss: 25.5577\n",
      "Processing batch 1128/11884 - Val_Loss: 25.9852\n",
      "Processing batch 1129/11884 - Val_Loss: 25.1038\n",
      "Processing batch 1130/11884 - Val_Loss: 24.6462\n",
      "Processing batch 1131/11884 - Val_Loss: 26.9705\n",
      "Processing batch 1132/11884 - Val_Loss: 25.3975\n",
      "Processing batch 1133/11884 - Val_Loss: 26.2539\n",
      "Processing batch 1134/11884 - Val_Loss: 25.6992\n",
      "Processing batch 1135/11884 - Val_Loss: 21.9558\n",
      "Processing batch 1136/11884 - Val_Loss: 24.7681\n",
      "Processing batch 1137/11884 - Val_Loss: 25.1112\n",
      "Processing batch 1138/11884 - Val_Loss: 23.7418\n",
      "Processing batch 1139/11884 - Val_Loss: 25.7742\n",
      "Processing batch 1140/11884 - Val_Loss: 26.2508\n",
      "Processing batch 1141/11884 - Val_Loss: 27.9462\n",
      "Processing batch 1142/11884 - Val_Loss: 24.0705\n",
      "Processing batch 1143/11884 - Val_Loss: 25.0063\n",
      "Processing batch 1144/11884 - Val_Loss: 24.0177\n",
      "Processing batch 1145/11884 - Val_Loss: 25.7734\n",
      "Processing batch 1146/11884 - Val_Loss: 28.1504\n",
      "Processing batch 1147/11884 - Val_Loss: 31.4290\n",
      "Processing batch 1148/11884 - Val_Loss: 28.5803\n",
      "Processing batch 1149/11884 - Val_Loss: 27.4039\n",
      "Processing batch 1150/11884 - Val_Loss: 24.8004\n",
      "Processing batch 1151/11884 - Val_Loss: 24.5571\n",
      "Processing batch 1152/11884 - Val_Loss: 26.4749\n",
      "Processing batch 1153/11884 - Val_Loss: 26.0595\n",
      "Processing batch 1154/11884 - Val_Loss: 29.3994\n",
      "Processing batch 1155/11884 - Val_Loss: 28.3238\n",
      "Processing batch 1156/11884 - Val_Loss: 25.1633\n",
      "Processing batch 1157/11884 - Val_Loss: 24.7858\n",
      "Processing batch 1158/11884 - Val_Loss: 28.7950\n",
      "Processing batch 1159/11884 - Val_Loss: 23.2685\n",
      "Processing batch 1160/11884 - Val_Loss: 25.2067\n",
      "Processing batch 1161/11884 - Val_Loss: 27.1117\n",
      "Processing batch 1162/11884 - Val_Loss: 27.9664\n",
      "Processing batch 1163/11884 - Val_Loss: 24.4920\n",
      "Processing batch 1164/11884 - Val_Loss: 28.5075\n",
      "Processing batch 1165/11884 - Val_Loss: 26.8354\n",
      "Processing batch 1166/11884 - Val_Loss: 28.5848\n",
      "Processing batch 1167/11884 - Val_Loss: 24.0309\n",
      "Processing batch 1168/11884 - Val_Loss: 27.0553\n",
      "Processing batch 1169/11884 - Val_Loss: 25.4403\n",
      "Processing batch 1170/11884 - Val_Loss: 23.3051\n",
      "Processing batch 1171/11884 - Val_Loss: 24.7159\n",
      "Processing batch 1172/11884 - Val_Loss: 25.4128\n",
      "Processing batch 1173/11884 - Val_Loss: 25.6685\n",
      "Processing batch 1174/11884 - Val_Loss: 27.6108\n",
      "Processing batch 1175/11884 - Val_Loss: 24.3614\n",
      "Processing batch 1176/11884 - Val_Loss: 24.8463\n",
      "Processing batch 1177/11884 - Val_Loss: 24.3197\n",
      "Processing batch 1178/11884 - Val_Loss: 25.5627\n",
      "Processing batch 1179/11884 - Val_Loss: 26.5129\n",
      "Processing batch 1180/11884 - Val_Loss: 29.0756\n",
      "Processing batch 1181/11884 - Val_Loss: 26.7814\n",
      "Processing batch 1182/11884 - Val_Loss: 25.3632\n",
      "Processing batch 1183/11884 - Val_Loss: 24.2520\n",
      "Processing batch 1184/11884 - Val_Loss: 23.6405\n",
      "Processing batch 1185/11884 - Val_Loss: 26.7003\n",
      "Processing batch 1186/11884 - Val_Loss: 25.2375\n",
      "Processing batch 1187/11884 - Val_Loss: 28.8756\n",
      "Processing batch 1188/11884 - Val_Loss: 23.4406\n",
      "Processing batch 1189/11884 - Val_Loss: 24.3869\n",
      "Processing batch 1190/11884 - Val_Loss: 25.2391\n",
      "Processing batch 1191/11884 - Val_Loss: 25.0892\n",
      "Processing batch 1192/11884 - Val_Loss: 29.7140\n",
      "Processing batch 1193/11884 - Val_Loss: 27.2935\n",
      "Processing batch 1194/11884 - Val_Loss: 24.7649\n",
      "Processing batch 1195/11884 - Val_Loss: 23.3173\n",
      "Processing batch 1196/11884 - Val_Loss: 29.5072\n",
      "Processing batch 1197/11884 - Val_Loss: 29.3604\n",
      "Processing batch 1198/11884 - Val_Loss: 25.0116\n",
      "Processing batch 1199/11884 - Val_Loss: 24.5184\n",
      "Processing batch 1200/11884 - Val_Loss: 25.0622\n",
      "Processing batch 1201/11884 - Val_Loss: 25.1036\n",
      "Processing batch 1202/11884 - Val_Loss: 23.4687\n",
      "Processing batch 1203/11884 - Val_Loss: 27.2747\n",
      "Processing batch 1204/11884 - Val_Loss: 23.5394\n",
      "Processing batch 1205/11884 - Val_Loss: 25.6548\n",
      "Processing batch 1206/11884 - Val_Loss: 26.1563\n",
      "Processing batch 1207/11884 - Val_Loss: 24.4273\n",
      "Processing batch 1208/11884 - Val_Loss: 27.3512\n",
      "Processing batch 1209/11884 - Val_Loss: 26.5116\n",
      "Processing batch 1210/11884 - Val_Loss: 27.3465\n",
      "Processing batch 1211/11884 - Val_Loss: 23.0855\n",
      "Processing batch 1212/11884 - Val_Loss: 29.1165\n",
      "Processing batch 1213/11884 - Val_Loss: 28.7800\n",
      "Processing batch 1214/11884 - Val_Loss: 27.9712\n",
      "Processing batch 1215/11884 - Val_Loss: 26.1840\n",
      "Processing batch 1216/11884 - Val_Loss: 30.0355\n",
      "Processing batch 1217/11884 - Val_Loss: 26.6326\n",
      "Processing batch 1218/11884 - Val_Loss: 25.3036\n",
      "Processing batch 1219/11884 - Val_Loss: 25.9414\n",
      "Processing batch 1220/11884 - Val_Loss: 31.7284\n",
      "Processing batch 1221/11884 - Val_Loss: 23.4880\n",
      "Processing batch 1222/11884 - Val_Loss: 26.5316\n",
      "Processing batch 1223/11884 - Val_Loss: 24.2663\n",
      "Processing batch 1224/11884 - Val_Loss: 29.2536\n",
      "Processing batch 1225/11884 - Val_Loss: 23.7191\n",
      "Processing batch 1226/11884 - Val_Loss: 25.6690\n",
      "Processing batch 1227/11884 - Val_Loss: 26.8845\n",
      "Processing batch 1228/11884 - Val_Loss: 21.7361\n",
      "Processing batch 1229/11884 - Val_Loss: 24.9787\n",
      "Processing batch 1230/11884 - Val_Loss: 23.7364\n",
      "Processing batch 1231/11884 - Val_Loss: 22.7883\n",
      "Processing batch 1232/11884 - Val_Loss: 24.7305\n",
      "Processing batch 1233/11884 - Val_Loss: 25.3264\n",
      "Processing batch 1234/11884 - Val_Loss: 26.2807\n",
      "Processing batch 1235/11884 - Val_Loss: 28.1576\n",
      "Processing batch 1236/11884 - Val_Loss: 24.0551\n",
      "Processing batch 1237/11884 - Val_Loss: 26.4790\n",
      "Processing batch 1238/11884 - Val_Loss: 25.9132\n",
      "Processing batch 1239/11884 - Val_Loss: 26.2507\n",
      "Processing batch 1240/11884 - Val_Loss: 24.1662\n",
      "Processing batch 1241/11884 - Val_Loss: 26.9817\n",
      "Processing batch 1242/11884 - Val_Loss: 23.8752\n",
      "Processing batch 1243/11884 - Val_Loss: 27.8234\n",
      "Processing batch 1244/11884 - Val_Loss: 25.0735\n",
      "Processing batch 1245/11884 - Val_Loss: 24.8262\n",
      "Processing batch 1246/11884 - Val_Loss: 22.9132\n",
      "Processing batch 1247/11884 - Val_Loss: 26.8775\n",
      "Processing batch 1248/11884 - Val_Loss: 25.6277\n",
      "Processing batch 1249/11884 - Val_Loss: 25.5342\n",
      "Processing batch 1250/11884 - Val_Loss: 24.1714\n",
      "Processing batch 1251/11884 - Val_Loss: 24.8371\n",
      "Processing batch 1252/11884 - Val_Loss: 24.0602\n",
      "Processing batch 1253/11884 - Val_Loss: 24.8196\n",
      "Processing batch 1254/11884 - Val_Loss: 25.1102\n",
      "Processing batch 1255/11884 - Val_Loss: 24.4044\n",
      "Processing batch 1256/11884 - Val_Loss: 23.4661\n",
      "Processing batch 1257/11884 - Val_Loss: 26.6581\n",
      "Processing batch 1258/11884 - Val_Loss: 30.6105\n",
      "Processing batch 1259/11884 - Val_Loss: 24.6983\n",
      "Processing batch 1260/11884 - Val_Loss: 29.5532\n",
      "Processing batch 1261/11884 - Val_Loss: 26.6699\n",
      "Processing batch 1262/11884 - Val_Loss: 25.5361\n",
      "Processing batch 1263/11884 - Val_Loss: 25.4881\n",
      "Processing batch 1264/11884 - Val_Loss: 28.3146\n",
      "Processing batch 1265/11884 - Val_Loss: 25.2747\n",
      "Processing batch 1266/11884 - Val_Loss: 25.0522\n",
      "Processing batch 1267/11884 - Val_Loss: 28.4158\n",
      "Processing batch 1268/11884 - Val_Loss: 26.3748\n",
      "Processing batch 1269/11884 - Val_Loss: 23.1534\n",
      "Processing batch 1270/11884 - Val_Loss: 25.5246\n",
      "Processing batch 1271/11884 - Val_Loss: 23.4927\n",
      "Processing batch 1272/11884 - Val_Loss: 25.3778\n",
      "Processing batch 1273/11884 - Val_Loss: 26.9900\n",
      "Processing batch 1274/11884 - Val_Loss: 30.0495\n",
      "Processing batch 1275/11884 - Val_Loss: 22.8169\n",
      "Processing batch 1276/11884 - Val_Loss: 26.9706\n",
      "Processing batch 1277/11884 - Val_Loss: 25.9798\n",
      "Processing batch 1278/11884 - Val_Loss: 28.2378\n",
      "Processing batch 1279/11884 - Val_Loss: 25.2600\n",
      "Processing batch 1280/11884 - Val_Loss: 23.2851\n",
      "Processing batch 1281/11884 - Val_Loss: 26.6026\n",
      "Processing batch 1282/11884 - Val_Loss: 24.7091\n",
      "Processing batch 1283/11884 - Val_Loss: 26.9690\n",
      "Processing batch 1284/11884 - Val_Loss: 26.3034\n",
      "Processing batch 1285/11884 - Val_Loss: 26.6132\n",
      "Processing batch 1286/11884 - Val_Loss: 26.8126\n",
      "Processing batch 1287/11884 - Val_Loss: 27.2512\n",
      "Processing batch 1288/11884 - Val_Loss: 29.0953\n",
      "Processing batch 1289/11884 - Val_Loss: 24.4234\n",
      "Processing batch 1290/11884 - Val_Loss: 25.9485\n",
      "Processing batch 1291/11884 - Val_Loss: 24.5757\n",
      "Processing batch 1292/11884 - Val_Loss: 25.6851\n",
      "Processing batch 1293/11884 - Val_Loss: 23.6191\n",
      "Processing batch 1294/11884 - Val_Loss: 22.5398\n",
      "Processing batch 1295/11884 - Val_Loss: 23.0694\n",
      "Processing batch 1296/11884 - Val_Loss: 28.0693\n",
      "Processing batch 1297/11884 - Val_Loss: 22.9292\n",
      "Processing batch 1298/11884 - Val_Loss: 25.9569\n",
      "Processing batch 1299/11884 - Val_Loss: 24.2243\n",
      "Processing batch 1300/11884 - Val_Loss: 23.9729\n",
      "Processing batch 1301/11884 - Val_Loss: 23.3880\n",
      "Processing batch 1302/11884 - Val_Loss: 23.3965\n",
      "Processing batch 1303/11884 - Val_Loss: 25.5734\n",
      "Processing batch 1304/11884 - Val_Loss: 27.0374\n",
      "Processing batch 1305/11884 - Val_Loss: 24.8772\n",
      "Processing batch 1306/11884 - Val_Loss: 28.3151\n",
      "Processing batch 1307/11884 - Val_Loss: 24.4663\n",
      "Processing batch 1308/11884 - Val_Loss: 26.4812\n",
      "Processing batch 1309/11884 - Val_Loss: 24.1213\n",
      "Processing batch 1310/11884 - Val_Loss: 22.5377\n",
      "Processing batch 1311/11884 - Val_Loss: 26.7947\n",
      "Processing batch 1312/11884 - Val_Loss: 26.9010\n",
      "Processing batch 1313/11884 - Val_Loss: 26.9169\n",
      "Processing batch 1314/11884 - Val_Loss: 26.7165\n",
      "Processing batch 1315/11884 - Val_Loss: 24.2453\n",
      "Processing batch 1316/11884 - Val_Loss: 31.7141\n",
      "Processing batch 1317/11884 - Val_Loss: 26.3281\n",
      "Processing batch 1318/11884 - Val_Loss: 23.0734\n",
      "Processing batch 1319/11884 - Val_Loss: 28.1084\n",
      "Processing batch 1320/11884 - Val_Loss: 26.0681\n",
      "Processing batch 1321/11884 - Val_Loss: 29.8683\n",
      "Processing batch 1322/11884 - Val_Loss: 25.5620\n",
      "Processing batch 1323/11884 - Val_Loss: 24.9374\n",
      "Processing batch 1324/11884 - Val_Loss: 25.3786\n",
      "Processing batch 1325/11884 - Val_Loss: 27.3860\n",
      "Processing batch 1326/11884 - Val_Loss: 25.8979\n",
      "Processing batch 1327/11884 - Val_Loss: 32.0049\n",
      "Processing batch 1328/11884 - Val_Loss: 27.4395\n",
      "Processing batch 1329/11884 - Val_Loss: 25.8795\n",
      "Processing batch 1330/11884 - Val_Loss: 22.4360\n",
      "Processing batch 1331/11884 - Val_Loss: 22.6521\n",
      "Processing batch 1332/11884 - Val_Loss: 25.6648\n",
      "Processing batch 1333/11884 - Val_Loss: 24.6254\n",
      "Processing batch 1334/11884 - Val_Loss: 25.6115\n",
      "Processing batch 1335/11884 - Val_Loss: 24.0381\n",
      "Processing batch 1336/11884 - Val_Loss: 25.6562\n",
      "Processing batch 1337/11884 - Val_Loss: 24.2457\n",
      "Processing batch 1338/11884 - Val_Loss: 25.8509\n",
      "Processing batch 1339/11884 - Val_Loss: 26.7693\n",
      "Processing batch 1340/11884 - Val_Loss: 28.2407\n",
      "Processing batch 1341/11884 - Val_Loss: 23.7163\n",
      "Processing batch 1342/11884 - Val_Loss: 22.2979\n",
      "Processing batch 1343/11884 - Val_Loss: 23.4122\n",
      "Processing batch 1344/11884 - Val_Loss: 30.7826\n",
      "Processing batch 1345/11884 - Val_Loss: 27.0239\n",
      "Processing batch 1346/11884 - Val_Loss: 26.7433\n",
      "Processing batch 1347/11884 - Val_Loss: 25.6216\n",
      "Processing batch 1348/11884 - Val_Loss: 25.7272\n",
      "Processing batch 1349/11884 - Val_Loss: 25.9564\n",
      "Processing batch 1350/11884 - Val_Loss: 24.8299\n",
      "Processing batch 1351/11884 - Val_Loss: 25.6581\n",
      "Processing batch 1352/11884 - Val_Loss: 24.9012\n",
      "Processing batch 1353/11884 - Val_Loss: 25.4535\n",
      "Processing batch 1354/11884 - Val_Loss: 26.8010\n",
      "Processing batch 1355/11884 - Val_Loss: 29.5639\n",
      "Processing batch 1356/11884 - Val_Loss: 26.4755\n",
      "Processing batch 1357/11884 - Val_Loss: 24.6596\n",
      "Processing batch 1358/11884 - Val_Loss: 22.6995\n",
      "Processing batch 1359/11884 - Val_Loss: 27.6398\n",
      "Processing batch 1360/11884 - Val_Loss: 26.4116\n",
      "Processing batch 1361/11884 - Val_Loss: 24.8148\n",
      "Processing batch 1362/11884 - Val_Loss: 25.5217\n",
      "Processing batch 1363/11884 - Val_Loss: 24.4422\n",
      "Processing batch 1364/11884 - Val_Loss: 24.5194\n",
      "Processing batch 1365/11884 - Val_Loss: 26.2999\n",
      "Processing batch 1366/11884 - Val_Loss: 27.5729\n",
      "Processing batch 1367/11884 - Val_Loss: 24.6934\n",
      "Processing batch 1368/11884 - Val_Loss: 26.0713\n",
      "Processing batch 1369/11884 - Val_Loss: 23.0282\n",
      "Processing batch 1370/11884 - Val_Loss: 24.6774\n",
      "Processing batch 1371/11884 - Val_Loss: 27.4428\n",
      "Processing batch 1372/11884 - Val_Loss: 29.0484\n",
      "Processing batch 1373/11884 - Val_Loss: 24.8971\n",
      "Processing batch 1374/11884 - Val_Loss: 28.1751\n",
      "Processing batch 1375/11884 - Val_Loss: 28.1770\n",
      "Processing batch 1376/11884 - Val_Loss: 26.8984\n",
      "Processing batch 1377/11884 - Val_Loss: 26.7525\n",
      "Processing batch 1378/11884 - Val_Loss: 24.5481\n",
      "Processing batch 1379/11884 - Val_Loss: 23.8440\n",
      "Processing batch 1380/11884 - Val_Loss: 26.6680\n",
      "Processing batch 1381/11884 - Val_Loss: 25.0162\n",
      "Processing batch 1382/11884 - Val_Loss: 29.2170\n",
      "Processing batch 1383/11884 - Val_Loss: 23.3773\n",
      "Processing batch 1384/11884 - Val_Loss: 25.8270\n",
      "Processing batch 1385/11884 - Val_Loss: 22.0105\n",
      "Processing batch 1386/11884 - Val_Loss: 25.5234\n",
      "Processing batch 1387/11884 - Val_Loss: 28.8884\n",
      "Processing batch 1388/11884 - Val_Loss: 28.3388\n",
      "Processing batch 1389/11884 - Val_Loss: 27.8760\n",
      "Processing batch 1390/11884 - Val_Loss: 25.0958\n",
      "Processing batch 1391/11884 - Val_Loss: 25.8176\n",
      "Processing batch 1392/11884 - Val_Loss: 25.6878\n",
      "Processing batch 1393/11884 - Val_Loss: 25.4569\n",
      "Processing batch 1394/11884 - Val_Loss: 24.4461\n",
      "Processing batch 1395/11884 - Val_Loss: 27.1661\n",
      "Processing batch 1396/11884 - Val_Loss: 26.6335\n",
      "Processing batch 1397/11884 - Val_Loss: 27.2280\n",
      "Processing batch 1398/11884 - Val_Loss: 27.0332\n",
      "Processing batch 1399/11884 - Val_Loss: 27.3261\n",
      "Processing batch 1400/11884 - Val_Loss: 23.5647\n",
      "Processing batch 1401/11884 - Val_Loss: 28.5045\n",
      "Processing batch 1402/11884 - Val_Loss: 23.3618\n",
      "Processing batch 1403/11884 - Val_Loss: 26.4338\n",
      "Processing batch 1404/11884 - Val_Loss: 23.2312\n",
      "Processing batch 1405/11884 - Val_Loss: 29.3037\n",
      "Processing batch 1406/11884 - Val_Loss: 25.9219\n",
      "Processing batch 1407/11884 - Val_Loss: 24.4980\n",
      "Processing batch 1408/11884 - Val_Loss: 26.3410\n",
      "Processing batch 1409/11884 - Val_Loss: 24.4728\n",
      "Processing batch 1410/11884 - Val_Loss: 26.1134\n",
      "Processing batch 1411/11884 - Val_Loss: 27.5175\n",
      "Processing batch 1412/11884 - Val_Loss: 24.4003\n",
      "Processing batch 1413/11884 - Val_Loss: 26.6622\n",
      "Processing batch 1414/11884 - Val_Loss: 26.0540\n",
      "Processing batch 1415/11884 - Val_Loss: 24.4494\n",
      "Processing batch 1416/11884 - Val_Loss: 25.8871\n",
      "Processing batch 1417/11884 - Val_Loss: 23.6812\n",
      "Processing batch 1418/11884 - Val_Loss: 25.3813\n",
      "Processing batch 1419/11884 - Val_Loss: 27.6858\n",
      "Processing batch 1420/11884 - Val_Loss: 24.5088\n",
      "Processing batch 1421/11884 - Val_Loss: 26.7289\n",
      "Processing batch 1422/11884 - Val_Loss: 23.0931\n",
      "Processing batch 1423/11884 - Val_Loss: 25.0742\n",
      "Processing batch 1424/11884 - Val_Loss: 27.7449\n",
      "Processing batch 1425/11884 - Val_Loss: 28.5983\n",
      "Processing batch 1426/11884 - Val_Loss: 28.8821\n",
      "Processing batch 1427/11884 - Val_Loss: 24.9731\n",
      "Processing batch 1428/11884 - Val_Loss: 25.6179\n",
      "Processing batch 1429/11884 - Val_Loss: 25.5723\n",
      "Processing batch 1430/11884 - Val_Loss: 25.0980\n",
      "Processing batch 1431/11884 - Val_Loss: 27.9827\n",
      "Processing batch 1432/11884 - Val_Loss: 28.3753\n",
      "Processing batch 1433/11884 - Val_Loss: 26.4265\n",
      "Processing batch 1434/11884 - Val_Loss: 24.0048\n",
      "Processing batch 1435/11884 - Val_Loss: 24.1432\n",
      "Processing batch 1436/11884 - Val_Loss: 24.4056\n",
      "Processing batch 1437/11884 - Val_Loss: 25.7662\n",
      "Processing batch 1438/11884 - Val_Loss: 24.2907\n",
      "Processing batch 1439/11884 - Val_Loss: 29.0587\n",
      "Processing batch 1440/11884 - Val_Loss: 27.3459\n",
      "Processing batch 1441/11884 - Val_Loss: 26.1275\n",
      "Processing batch 1442/11884 - Val_Loss: 26.2973\n",
      "Processing batch 1443/11884 - Val_Loss: 25.0927\n",
      "Processing batch 1444/11884 - Val_Loss: 27.9247\n",
      "Processing batch 1445/11884 - Val_Loss: 28.3043\n",
      "Processing batch 1446/11884 - Val_Loss: 27.7021\n",
      "Processing batch 1447/11884 - Val_Loss: 27.5968\n",
      "Processing batch 1448/11884 - Val_Loss: 24.7530\n",
      "Processing batch 1449/11884 - Val_Loss: 28.5239\n",
      "Processing batch 1450/11884 - Val_Loss: 25.3268\n",
      "Processing batch 1451/11884 - Val_Loss: 24.2728\n",
      "Processing batch 1452/11884 - Val_Loss: 24.1208\n",
      "Processing batch 1453/11884 - Val_Loss: 25.7723\n",
      "Processing batch 1454/11884 - Val_Loss: 26.9825\n",
      "Processing batch 1455/11884 - Val_Loss: 24.8092\n",
      "Processing batch 1456/11884 - Val_Loss: 24.0246\n",
      "Processing batch 1457/11884 - Val_Loss: 25.3270\n",
      "Processing batch 1458/11884 - Val_Loss: 29.0163\n",
      "Processing batch 1459/11884 - Val_Loss: 25.4959\n",
      "Processing batch 1460/11884 - Val_Loss: 27.0493\n",
      "Processing batch 1461/11884 - Val_Loss: 26.0711\n",
      "Processing batch 1462/11884 - Val_Loss: 25.4840\n",
      "Processing batch 1463/11884 - Val_Loss: 24.1952\n",
      "Processing batch 1464/11884 - Val_Loss: 26.1919\n",
      "Processing batch 1465/11884 - Val_Loss: 27.7717\n",
      "Processing batch 1466/11884 - Val_Loss: 25.3360\n",
      "Processing batch 1467/11884 - Val_Loss: 24.9881\n",
      "Processing batch 1468/11884 - Val_Loss: 27.1570\n",
      "Processing batch 1469/11884 - Val_Loss: 28.1411\n",
      "Processing batch 1470/11884 - Val_Loss: 26.0713\n",
      "Processing batch 1471/11884 - Val_Loss: 24.9911\n",
      "Processing batch 1472/11884 - Val_Loss: 26.1816\n",
      "Processing batch 1473/11884 - Val_Loss: 25.0088\n",
      "Processing batch 1474/11884 - Val_Loss: 25.5619\n",
      "Processing batch 1475/11884 - Val_Loss: 26.0418\n",
      "Processing batch 1476/11884 - Val_Loss: 23.0230\n",
      "Processing batch 1477/11884 - Val_Loss: 24.3613\n",
      "Processing batch 1478/11884 - Val_Loss: 24.6474\n",
      "Processing batch 1479/11884 - Val_Loss: 25.9271\n",
      "Processing batch 1480/11884 - Val_Loss: 24.4939\n",
      "Processing batch 1481/11884 - Val_Loss: 24.4166\n",
      "Processing batch 1482/11884 - Val_Loss: 20.5411\n",
      "Processing batch 1483/11884 - Val_Loss: 27.0781\n",
      "Processing batch 1484/11884 - Val_Loss: 26.2411\n",
      "Processing batch 1485/11884 - Val_Loss: 23.9887\n",
      "Processing batch 1486/11884 - Val_Loss: 22.7972\n",
      "Processing batch 1487/11884 - Val_Loss: 25.0790\n",
      "Processing batch 1488/11884 - Val_Loss: 25.5716\n",
      "Processing batch 1489/11884 - Val_Loss: 25.7507\n",
      "Processing batch 1490/11884 - Val_Loss: 28.4287\n",
      "Processing batch 1491/11884 - Val_Loss: 26.9112\n",
      "Processing batch 1492/11884 - Val_Loss: 26.4579\n",
      "Processing batch 1493/11884 - Val_Loss: 27.6123\n",
      "Processing batch 1494/11884 - Val_Loss: 27.8102\n",
      "Processing batch 1495/11884 - Val_Loss: 25.4203\n",
      "Processing batch 1496/11884 - Val_Loss: 25.1030\n",
      "Processing batch 1497/11884 - Val_Loss: 25.6499\n",
      "Processing batch 1498/11884 - Val_Loss: 27.5812\n",
      "Processing batch 1499/11884 - Val_Loss: 26.8853\n",
      "Processing batch 1500/11884 - Val_Loss: 28.2714\n",
      "Processing batch 1501/11884 - Val_Loss: 24.0127\n",
      "Processing batch 1502/11884 - Val_Loss: 25.6230\n",
      "Processing batch 1503/11884 - Val_Loss: 23.8939\n",
      "Processing batch 1504/11884 - Val_Loss: 26.1443\n",
      "Processing batch 1505/11884 - Val_Loss: 25.4288\n",
      "Processing batch 1506/11884 - Val_Loss: 26.0964\n",
      "Processing batch 1507/11884 - Val_Loss: 25.8820\n",
      "Processing batch 1508/11884 - Val_Loss: 25.4279\n",
      "Processing batch 1509/11884 - Val_Loss: 26.9356\n",
      "Processing batch 1510/11884 - Val_Loss: 23.5978\n",
      "Processing batch 1511/11884 - Val_Loss: 30.8300\n",
      "Processing batch 1512/11884 - Val_Loss: 26.1791\n",
      "Processing batch 1513/11884 - Val_Loss: 27.1050\n",
      "Processing batch 1514/11884 - Val_Loss: 26.6743\n",
      "Processing batch 1515/11884 - Val_Loss: 24.4256\n",
      "Processing batch 1516/11884 - Val_Loss: 24.0449\n",
      "Processing batch 1517/11884 - Val_Loss: 26.0758\n",
      "Processing batch 1518/11884 - Val_Loss: 24.6047\n",
      "Processing batch 1519/11884 - Val_Loss: 24.5095\n",
      "Processing batch 1520/11884 - Val_Loss: 26.9706\n",
      "Processing batch 1521/11884 - Val_Loss: 24.5856\n",
      "Processing batch 1522/11884 - Val_Loss: 25.0424\n",
      "Processing batch 1523/11884 - Val_Loss: 23.9256\n",
      "Processing batch 1524/11884 - Val_Loss: 25.4958\n",
      "Processing batch 1525/11884 - Val_Loss: 25.6086\n",
      "Processing batch 1526/11884 - Val_Loss: 26.1113\n",
      "Processing batch 1527/11884 - Val_Loss: 26.0597\n",
      "Processing batch 1528/11884 - Val_Loss: 26.6014\n",
      "Processing batch 1529/11884 - Val_Loss: 27.2020\n",
      "Processing batch 1530/11884 - Val_Loss: 25.3217\n",
      "Processing batch 1531/11884 - Val_Loss: 24.6200\n",
      "Processing batch 1532/11884 - Val_Loss: 25.3599\n",
      "Processing batch 1533/11884 - Val_Loss: 25.2063\n",
      "Processing batch 1534/11884 - Val_Loss: 26.4767\n",
      "Processing batch 1535/11884 - Val_Loss: 25.0177\n",
      "Processing batch 1536/11884 - Val_Loss: 26.9396\n",
      "Processing batch 1537/11884 - Val_Loss: 24.2390\n",
      "Processing batch 1538/11884 - Val_Loss: 28.1169\n",
      "Processing batch 1539/11884 - Val_Loss: 26.4824\n",
      "Processing batch 1540/11884 - Val_Loss: 27.3044\n",
      "Processing batch 1541/11884 - Val_Loss: 24.2604\n",
      "Processing batch 1542/11884 - Val_Loss: 23.7929\n",
      "Processing batch 1543/11884 - Val_Loss: 24.6646\n",
      "Processing batch 1544/11884 - Val_Loss: 26.6543\n",
      "Processing batch 1545/11884 - Val_Loss: 27.1818\n",
      "Processing batch 1546/11884 - Val_Loss: 24.0284\n",
      "Processing batch 1547/11884 - Val_Loss: 28.7109\n",
      "Processing batch 1548/11884 - Val_Loss: 22.5495\n",
      "Processing batch 1549/11884 - Val_Loss: 26.3058\n",
      "Processing batch 1550/11884 - Val_Loss: 25.5966\n",
      "Processing batch 1551/11884 - Val_Loss: 26.7993\n",
      "Processing batch 1552/11884 - Val_Loss: 24.2554\n",
      "Processing batch 1553/11884 - Val_Loss: 24.3934\n",
      "Processing batch 1554/11884 - Val_Loss: 25.4557\n",
      "Processing batch 1555/11884 - Val_Loss: 26.0311\n",
      "Processing batch 1556/11884 - Val_Loss: 24.7516\n",
      "Processing batch 1557/11884 - Val_Loss: 27.3888\n",
      "Processing batch 1558/11884 - Val_Loss: 25.9330\n",
      "Processing batch 1559/11884 - Val_Loss: 24.1347\n",
      "Processing batch 1560/11884 - Val_Loss: 26.6671\n",
      "Processing batch 1561/11884 - Val_Loss: 22.8715\n",
      "Processing batch 1562/11884 - Val_Loss: 23.4639\n",
      "Processing batch 1563/11884 - Val_Loss: 27.8461\n",
      "Processing batch 1564/11884 - Val_Loss: 24.7538\n",
      "Processing batch 1565/11884 - Val_Loss: 25.2356\n",
      "Processing batch 1566/11884 - Val_Loss: 26.8174\n",
      "Processing batch 1567/11884 - Val_Loss: 24.9016\n",
      "Processing batch 1568/11884 - Val_Loss: 27.1213\n",
      "Processing batch 1569/11884 - Val_Loss: 24.9101\n",
      "Processing batch 1570/11884 - Val_Loss: 27.5489\n",
      "Processing batch 1571/11884 - Val_Loss: 25.5171\n",
      "Processing batch 1572/11884 - Val_Loss: 29.1032\n",
      "Processing batch 1573/11884 - Val_Loss: 27.1188\n",
      "Processing batch 1574/11884 - Val_Loss: 25.6512\n",
      "Processing batch 1575/11884 - Val_Loss: 26.4461\n",
      "Processing batch 1576/11884 - Val_Loss: 27.0531\n",
      "Processing batch 1577/11884 - Val_Loss: 24.8833\n",
      "Processing batch 1578/11884 - Val_Loss: 25.6986\n",
      "Processing batch 1579/11884 - Val_Loss: 24.2153\n",
      "Processing batch 1580/11884 - Val_Loss: 26.1180\n",
      "Processing batch 1581/11884 - Val_Loss: 25.0225\n",
      "Processing batch 1582/11884 - Val_Loss: 26.8868\n",
      "Processing batch 1583/11884 - Val_Loss: 25.3485\n",
      "Processing batch 1584/11884 - Val_Loss: 29.0049\n",
      "Processing batch 1585/11884 - Val_Loss: 26.5719\n",
      "Processing batch 1586/11884 - Val_Loss: 26.3183\n",
      "Processing batch 1587/11884 - Val_Loss: 25.6732\n",
      "Processing batch 1588/11884 - Val_Loss: 26.3025\n",
      "Processing batch 1589/11884 - Val_Loss: 24.7718\n",
      "Processing batch 1590/11884 - Val_Loss: 23.5670\n",
      "Processing batch 1591/11884 - Val_Loss: 25.0126\n",
      "Processing batch 1592/11884 - Val_Loss: 25.1976\n",
      "Processing batch 1593/11884 - Val_Loss: 25.1659\n",
      "Processing batch 1594/11884 - Val_Loss: 25.3958\n",
      "Processing batch 1595/11884 - Val_Loss: 24.9526\n",
      "Processing batch 1596/11884 - Val_Loss: 25.6579\n",
      "Processing batch 1597/11884 - Val_Loss: 26.2223\n",
      "Processing batch 1598/11884 - Val_Loss: 26.2598\n",
      "Processing batch 1599/11884 - Val_Loss: 26.4231\n",
      "Processing batch 1600/11884 - Val_Loss: 26.1934\n",
      "Processing batch 1601/11884 - Val_Loss: 26.7118\n",
      "Processing batch 1602/11884 - Val_Loss: 26.3268\n",
      "Processing batch 1603/11884 - Val_Loss: 25.3298\n",
      "Processing batch 1604/11884 - Val_Loss: 24.1094\n",
      "Processing batch 1605/11884 - Val_Loss: 24.4986\n",
      "Processing batch 1606/11884 - Val_Loss: 24.4233\n",
      "Processing batch 1607/11884 - Val_Loss: 27.2317\n",
      "Processing batch 1608/11884 - Val_Loss: 26.2597\n",
      "Processing batch 1609/11884 - Val_Loss: 28.6177\n",
      "Processing batch 1610/11884 - Val_Loss: 24.3461\n",
      "Processing batch 1611/11884 - Val_Loss: 27.8943\n",
      "Processing batch 1612/11884 - Val_Loss: 25.6462\n",
      "Processing batch 1613/11884 - Val_Loss: 23.8210\n",
      "Processing batch 1614/11884 - Val_Loss: 28.6631\n",
      "Processing batch 1615/11884 - Val_Loss: 26.0874\n",
      "Processing batch 1616/11884 - Val_Loss: 26.4667\n",
      "Processing batch 1617/11884 - Val_Loss: 24.8930\n",
      "Processing batch 1618/11884 - Val_Loss: 25.6064\n",
      "Processing batch 1619/11884 - Val_Loss: 25.8083\n",
      "Processing batch 1620/11884 - Val_Loss: 26.4022\n",
      "Processing batch 1621/11884 - Val_Loss: 29.0806\n",
      "Processing batch 1622/11884 - Val_Loss: 23.1171\n",
      "Processing batch 1623/11884 - Val_Loss: 27.7761\n",
      "Processing batch 1624/11884 - Val_Loss: 24.4738\n",
      "Processing batch 1625/11884 - Val_Loss: 25.7468\n",
      "Processing batch 1626/11884 - Val_Loss: 29.0261\n",
      "Processing batch 1627/11884 - Val_Loss: 27.7124\n",
      "Processing batch 1628/11884 - Val_Loss: 24.7088\n",
      "Processing batch 1629/11884 - Val_Loss: 26.2039\n",
      "Processing batch 1630/11884 - Val_Loss: 25.9334\n",
      "Processing batch 1631/11884 - Val_Loss: 25.9849\n",
      "Processing batch 1632/11884 - Val_Loss: 30.2005\n",
      "Processing batch 1633/11884 - Val_Loss: 23.0568\n",
      "Processing batch 1634/11884 - Val_Loss: 24.9858\n",
      "Processing batch 1635/11884 - Val_Loss: 26.5218\n",
      "Processing batch 1636/11884 - Val_Loss: 26.4683\n",
      "Processing batch 1637/11884 - Val_Loss: 25.3310\n",
      "Processing batch 1638/11884 - Val_Loss: 31.5287\n",
      "Processing batch 1639/11884 - Val_Loss: 23.9600\n",
      "Processing batch 1640/11884 - Val_Loss: 24.9333\n",
      "Processing batch 1641/11884 - Val_Loss: 25.1439\n",
      "Processing batch 1642/11884 - Val_Loss: 24.6663\n",
      "Processing batch 1643/11884 - Val_Loss: 24.5977\n",
      "Processing batch 1644/11884 - Val_Loss: 23.8512\n",
      "Processing batch 1645/11884 - Val_Loss: 26.0497\n",
      "Processing batch 1646/11884 - Val_Loss: 26.9106\n",
      "Processing batch 1647/11884 - Val_Loss: 23.6341\n",
      "Processing batch 1648/11884 - Val_Loss: 22.1146\n",
      "Processing batch 1649/11884 - Val_Loss: 25.4542\n",
      "Processing batch 1650/11884 - Val_Loss: 25.3401\n",
      "Processing batch 1651/11884 - Val_Loss: 27.9672\n",
      "Processing batch 1652/11884 - Val_Loss: 25.4750\n",
      "Processing batch 1653/11884 - Val_Loss: 25.3160\n",
      "Processing batch 1654/11884 - Val_Loss: 28.2001\n",
      "Processing batch 1655/11884 - Val_Loss: 25.2200\n",
      "Processing batch 1656/11884 - Val_Loss: 21.8597\n",
      "Processing batch 1657/11884 - Val_Loss: 26.6835\n",
      "Processing batch 1658/11884 - Val_Loss: 23.6431\n",
      "Processing batch 1659/11884 - Val_Loss: 25.9122\n",
      "Processing batch 1660/11884 - Val_Loss: 25.7102\n",
      "Processing batch 1661/11884 - Val_Loss: 28.7430\n",
      "Processing batch 1662/11884 - Val_Loss: 22.8881\n",
      "Processing batch 1663/11884 - Val_Loss: 22.6352\n",
      "Processing batch 1664/11884 - Val_Loss: 27.5328\n",
      "Processing batch 1665/11884 - Val_Loss: 27.7094\n",
      "Processing batch 1666/11884 - Val_Loss: 30.1688\n",
      "Processing batch 1667/11884 - Val_Loss: 25.3289\n",
      "Processing batch 1668/11884 - Val_Loss: 26.2501\n",
      "Processing batch 1669/11884 - Val_Loss: 26.4926\n",
      "Processing batch 1670/11884 - Val_Loss: 25.8398\n",
      "Processing batch 1671/11884 - Val_Loss: 24.4211\n",
      "Processing batch 1672/11884 - Val_Loss: 23.6864\n",
      "Processing batch 1673/11884 - Val_Loss: 25.0356\n",
      "Processing batch 1674/11884 - Val_Loss: 26.6902\n",
      "Processing batch 1675/11884 - Val_Loss: 23.5992\n",
      "Processing batch 1676/11884 - Val_Loss: 24.4584\n",
      "Processing batch 1677/11884 - Val_Loss: 25.7531\n",
      "Processing batch 1678/11884 - Val_Loss: 25.7180\n",
      "Processing batch 1679/11884 - Val_Loss: 27.2221\n",
      "Processing batch 1680/11884 - Val_Loss: 25.5669\n",
      "Processing batch 1681/11884 - Val_Loss: 24.9133\n",
      "Processing batch 1682/11884 - Val_Loss: 27.5650\n",
      "Processing batch 1683/11884 - Val_Loss: 25.7537\n",
      "Processing batch 1684/11884 - Val_Loss: 26.4680\n",
      "Processing batch 1685/11884 - Val_Loss: 25.1032\n",
      "Processing batch 1686/11884 - Val_Loss: 24.6789\n",
      "Processing batch 1687/11884 - Val_Loss: 26.4473\n",
      "Processing batch 1688/11884 - Val_Loss: 25.1492\n",
      "Processing batch 1689/11884 - Val_Loss: 27.9147\n",
      "Processing batch 1690/11884 - Val_Loss: 27.5310\n",
      "Processing batch 1691/11884 - Val_Loss: 25.2271\n",
      "Processing batch 1692/11884 - Val_Loss: 23.7566\n",
      "Processing batch 1693/11884 - Val_Loss: 24.7677\n",
      "Processing batch 1694/11884 - Val_Loss: 25.1864\n",
      "Processing batch 1695/11884 - Val_Loss: 26.2433\n",
      "Processing batch 1696/11884 - Val_Loss: 25.5448\n",
      "Processing batch 1697/11884 - Val_Loss: 26.9739\n",
      "Processing batch 1698/11884 - Val_Loss: 26.1725\n",
      "Processing batch 1699/11884 - Val_Loss: 26.9757\n",
      "Processing batch 1700/11884 - Val_Loss: 25.5156\n",
      "Processing batch 1701/11884 - Val_Loss: 25.0056\n",
      "Processing batch 1702/11884 - Val_Loss: 25.8621\n",
      "Processing batch 1703/11884 - Val_Loss: 25.6312\n",
      "Processing batch 1704/11884 - Val_Loss: 26.1727\n",
      "Processing batch 1705/11884 - Val_Loss: 23.5517\n",
      "Processing batch 1706/11884 - Val_Loss: 24.9203\n",
      "Processing batch 1707/11884 - Val_Loss: 22.2248\n",
      "Processing batch 1708/11884 - Val_Loss: 28.2864\n",
      "Processing batch 1709/11884 - Val_Loss: 25.8868\n",
      "Processing batch 1710/11884 - Val_Loss: 24.0017\n",
      "Processing batch 1711/11884 - Val_Loss: 27.3605\n",
      "Processing batch 1712/11884 - Val_Loss: 25.0543\n",
      "Processing batch 1713/11884 - Val_Loss: 27.6560\n",
      "Processing batch 1714/11884 - Val_Loss: 24.0653\n",
      "Processing batch 1715/11884 - Val_Loss: 25.4611\n",
      "Processing batch 1716/11884 - Val_Loss: 24.9066\n",
      "Processing batch 1717/11884 - Val_Loss: 24.7039\n",
      "Processing batch 1718/11884 - Val_Loss: 27.8385\n",
      "Processing batch 1719/11884 - Val_Loss: 27.4893\n",
      "Processing batch 1720/11884 - Val_Loss: 25.5435\n",
      "Processing batch 1721/11884 - Val_Loss: 27.3963\n",
      "Processing batch 1722/11884 - Val_Loss: 23.9927\n",
      "Processing batch 1723/11884 - Val_Loss: 22.8721\n",
      "Processing batch 1724/11884 - Val_Loss: 25.6132\n",
      "Processing batch 1725/11884 - Val_Loss: 25.7952\n",
      "Processing batch 1726/11884 - Val_Loss: 28.4085\n",
      "Processing batch 1727/11884 - Val_Loss: 25.1123\n",
      "Processing batch 1728/11884 - Val_Loss: 27.1615\n",
      "Processing batch 1729/11884 - Val_Loss: 25.7274\n",
      "Processing batch 1730/11884 - Val_Loss: 26.5907\n",
      "Processing batch 1731/11884 - Val_Loss: 27.2356\n",
      "Processing batch 1732/11884 - Val_Loss: 25.8980\n",
      "Processing batch 1733/11884 - Val_Loss: 24.8324\n",
      "Processing batch 1734/11884 - Val_Loss: 27.0078\n",
      "Processing batch 1735/11884 - Val_Loss: 28.4954\n",
      "Processing batch 1736/11884 - Val_Loss: 26.1925\n",
      "Processing batch 1737/11884 - Val_Loss: 26.3046\n",
      "Processing batch 1738/11884 - Val_Loss: 28.4072\n",
      "Processing batch 1739/11884 - Val_Loss: 24.6843\n",
      "Processing batch 1740/11884 - Val_Loss: 25.3514\n",
      "Processing batch 1741/11884 - Val_Loss: 29.0846\n",
      "Processing batch 1742/11884 - Val_Loss: 26.8990\n",
      "Processing batch 1743/11884 - Val_Loss: 24.6727\n",
      "Processing batch 1744/11884 - Val_Loss: 26.3977\n",
      "Processing batch 1745/11884 - Val_Loss: 24.3493\n",
      "Processing batch 1746/11884 - Val_Loss: 26.0588\n",
      "Processing batch 1747/11884 - Val_Loss: 23.1020\n",
      "Processing batch 1748/11884 - Val_Loss: 29.1624\n",
      "Processing batch 1749/11884 - Val_Loss: 26.9899\n",
      "Processing batch 1750/11884 - Val_Loss: 24.0559\n",
      "Processing batch 1751/11884 - Val_Loss: 26.7791\n",
      "Processing batch 1752/11884 - Val_Loss: 23.4303\n",
      "Processing batch 1753/11884 - Val_Loss: 24.5925\n",
      "Processing batch 1754/11884 - Val_Loss: 23.9372\n",
      "Processing batch 1755/11884 - Val_Loss: 23.4170\n",
      "Processing batch 1756/11884 - Val_Loss: 24.7934\n",
      "Processing batch 1757/11884 - Val_Loss: 25.6238\n",
      "Processing batch 1758/11884 - Val_Loss: 25.6189\n",
      "Processing batch 1759/11884 - Val_Loss: 26.1401\n",
      "Processing batch 1760/11884 - Val_Loss: 26.3157\n",
      "Processing batch 1761/11884 - Val_Loss: 24.8006\n",
      "Processing batch 1762/11884 - Val_Loss: 24.4711\n",
      "Processing batch 1763/11884 - Val_Loss: 25.2645\n",
      "Processing batch 1764/11884 - Val_Loss: 25.9318\n",
      "Processing batch 1765/11884 - Val_Loss: 24.1653\n",
      "Processing batch 1766/11884 - Val_Loss: 25.4838\n",
      "Processing batch 1767/11884 - Val_Loss: 24.9895\n",
      "Processing batch 1768/11884 - Val_Loss: 29.8945\n",
      "Processing batch 1769/11884 - Val_Loss: 25.6451\n",
      "Processing batch 1770/11884 - Val_Loss: 27.5216\n",
      "Processing batch 1771/11884 - Val_Loss: 26.6190\n",
      "Processing batch 1772/11884 - Val_Loss: 26.8023\n",
      "Processing batch 1773/11884 - Val_Loss: 26.9582\n",
      "Processing batch 1774/11884 - Val_Loss: 27.6090\n",
      "Processing batch 1775/11884 - Val_Loss: 27.3959\n",
      "Processing batch 1776/11884 - Val_Loss: 25.0879\n",
      "Processing batch 1777/11884 - Val_Loss: 25.0596\n",
      "Processing batch 1778/11884 - Val_Loss: 25.6218\n",
      "Processing batch 1779/11884 - Val_Loss: 27.4790\n",
      "Processing batch 1780/11884 - Val_Loss: 28.5604\n",
      "Processing batch 1781/11884 - Val_Loss: 27.8811\n",
      "Processing batch 1782/11884 - Val_Loss: 25.0746\n",
      "Processing batch 1783/11884 - Val_Loss: 23.9102\n",
      "Processing batch 1784/11884 - Val_Loss: 26.6301\n",
      "Processing batch 1785/11884 - Val_Loss: 26.4305\n",
      "Processing batch 1786/11884 - Val_Loss: 24.3964\n",
      "Processing batch 1787/11884 - Val_Loss: 27.9573\n",
      "Processing batch 1788/11884 - Val_Loss: 24.8315\n",
      "Processing batch 1789/11884 - Val_Loss: 26.0004\n",
      "Processing batch 1790/11884 - Val_Loss: 26.6754\n",
      "Processing batch 1791/11884 - Val_Loss: 24.2523\n",
      "Processing batch 1792/11884 - Val_Loss: 26.2362\n",
      "Processing batch 1793/11884 - Val_Loss: 27.9100\n",
      "Processing batch 1794/11884 - Val_Loss: 25.5469\n",
      "Processing batch 1795/11884 - Val_Loss: 24.8323\n",
      "Processing batch 1796/11884 - Val_Loss: 25.2290\n",
      "Processing batch 1797/11884 - Val_Loss: 25.7709\n",
      "Processing batch 1798/11884 - Val_Loss: 23.5177\n",
      "Processing batch 1799/11884 - Val_Loss: 25.1261\n",
      "Processing batch 1800/11884 - Val_Loss: 26.5915\n",
      "Processing batch 1801/11884 - Val_Loss: 23.9874\n",
      "Processing batch 1802/11884 - Val_Loss: 24.0527\n",
      "Processing batch 1803/11884 - Val_Loss: 25.7824\n",
      "Processing batch 1804/11884 - Val_Loss: 23.6105\n",
      "Processing batch 1805/11884 - Val_Loss: 23.7216\n",
      "Processing batch 1806/11884 - Val_Loss: 24.5887\n",
      "Processing batch 1807/11884 - Val_Loss: 24.2411\n",
      "Processing batch 1808/11884 - Val_Loss: 25.3277\n",
      "Processing batch 1809/11884 - Val_Loss: 25.8122\n",
      "Processing batch 1810/11884 - Val_Loss: 26.7648\n",
      "Processing batch 1811/11884 - Val_Loss: 26.9757\n",
      "Processing batch 1812/11884 - Val_Loss: 24.9740\n",
      "Processing batch 1813/11884 - Val_Loss: 24.7050\n",
      "Processing batch 1814/11884 - Val_Loss: 25.6734\n",
      "Processing batch 1815/11884 - Val_Loss: 27.1827\n",
      "Processing batch 1816/11884 - Val_Loss: 24.5780\n",
      "Processing batch 1817/11884 - Val_Loss: 25.1439\n",
      "Processing batch 1818/11884 - Val_Loss: 25.7390\n",
      "Processing batch 1819/11884 - Val_Loss: 23.1644\n",
      "Processing batch 1820/11884 - Val_Loss: 28.9160\n",
      "Processing batch 1821/11884 - Val_Loss: 25.2218\n",
      "Processing batch 1822/11884 - Val_Loss: 30.0652\n",
      "Processing batch 1823/11884 - Val_Loss: 26.5852\n",
      "Processing batch 1824/11884 - Val_Loss: 27.4466\n",
      "Processing batch 1825/11884 - Val_Loss: 25.6113\n",
      "Processing batch 1826/11884 - Val_Loss: 26.5130\n",
      "Processing batch 1827/11884 - Val_Loss: 27.2796\n",
      "Processing batch 1828/11884 - Val_Loss: 26.1241\n",
      "Processing batch 1829/11884 - Val_Loss: 24.8095\n",
      "Processing batch 1830/11884 - Val_Loss: 24.4800\n",
      "Processing batch 1831/11884 - Val_Loss: 24.2068\n",
      "Processing batch 1832/11884 - Val_Loss: 26.9111\n",
      "Processing batch 1833/11884 - Val_Loss: 26.9865\n",
      "Processing batch 1834/11884 - Val_Loss: 26.9316\n",
      "Processing batch 1835/11884 - Val_Loss: 24.3539\n",
      "Processing batch 1836/11884 - Val_Loss: 26.5127\n",
      "Processing batch 1837/11884 - Val_Loss: 26.1867\n",
      "Processing batch 1838/11884 - Val_Loss: 25.0482\n",
      "Processing batch 1839/11884 - Val_Loss: 23.5749\n",
      "Processing batch 1840/11884 - Val_Loss: 24.1743\n",
      "Processing batch 1841/11884 - Val_Loss: 26.7732\n",
      "Processing batch 1842/11884 - Val_Loss: 26.7170\n",
      "Processing batch 1843/11884 - Val_Loss: 29.1584\n",
      "Processing batch 1844/11884 - Val_Loss: 26.5735\n",
      "Processing batch 1845/11884 - Val_Loss: 27.9925\n",
      "Processing batch 1846/11884 - Val_Loss: 25.8895\n",
      "Processing batch 1847/11884 - Val_Loss: 25.1661\n",
      "Processing batch 1848/11884 - Val_Loss: 22.3038\n",
      "Processing batch 1849/11884 - Val_Loss: 25.2743\n",
      "Processing batch 1850/11884 - Val_Loss: 25.9066\n",
      "Processing batch 1851/11884 - Val_Loss: 27.7744\n",
      "Processing batch 1852/11884 - Val_Loss: 23.2963\n",
      "Processing batch 1853/11884 - Val_Loss: 25.0629\n",
      "Processing batch 1854/11884 - Val_Loss: 24.4849\n",
      "Processing batch 1855/11884 - Val_Loss: 25.2826\n",
      "Processing batch 1856/11884 - Val_Loss: 26.1270\n",
      "Processing batch 1857/11884 - Val_Loss: 24.7416\n",
      "Processing batch 1858/11884 - Val_Loss: 25.9221\n",
      "Processing batch 1859/11884 - Val_Loss: 23.7892\n",
      "Processing batch 1860/11884 - Val_Loss: 27.4399\n",
      "Processing batch 1861/11884 - Val_Loss: 25.8872\n",
      "Processing batch 1862/11884 - Val_Loss: 23.5499\n",
      "Processing batch 1863/11884 - Val_Loss: 24.2400\n",
      "Processing batch 1864/11884 - Val_Loss: 28.3185\n",
      "Processing batch 1865/11884 - Val_Loss: 26.2277\n",
      "Processing batch 1866/11884 - Val_Loss: 21.2612\n",
      "Processing batch 1867/11884 - Val_Loss: 27.5584\n",
      "Processing batch 1868/11884 - Val_Loss: 25.8834\n",
      "Processing batch 1869/11884 - Val_Loss: 26.7103\n",
      "Processing batch 1870/11884 - Val_Loss: 26.2574\n",
      "Processing batch 1871/11884 - Val_Loss: 27.0215\n",
      "Processing batch 1872/11884 - Val_Loss: 25.4334\n",
      "Processing batch 1873/11884 - Val_Loss: 26.5288\n",
      "Processing batch 1874/11884 - Val_Loss: 29.5177\n",
      "Processing batch 1875/11884 - Val_Loss: 24.4193\n",
      "Processing batch 1876/11884 - Val_Loss: 27.5588\n",
      "Processing batch 1877/11884 - Val_Loss: 26.9498\n",
      "Processing batch 1878/11884 - Val_Loss: 26.4012\n",
      "Processing batch 1879/11884 - Val_Loss: 25.7427\n",
      "Processing batch 1880/11884 - Val_Loss: 22.2502\n",
      "Processing batch 1881/11884 - Val_Loss: 25.8531\n",
      "Processing batch 1882/11884 - Val_Loss: 25.8552\n",
      "Processing batch 1883/11884 - Val_Loss: 23.2125\n",
      "Processing batch 1884/11884 - Val_Loss: 26.3409\n",
      "Processing batch 1885/11884 - Val_Loss: 24.4405\n",
      "Processing batch 1886/11884 - Val_Loss: 26.2660\n",
      "Processing batch 1887/11884 - Val_Loss: 23.6200\n",
      "Processing batch 1888/11884 - Val_Loss: 25.9054\n",
      "Processing batch 1889/11884 - Val_Loss: 25.5481\n",
      "Processing batch 1890/11884 - Val_Loss: 26.1319\n",
      "Processing batch 1891/11884 - Val_Loss: 27.7560\n",
      "Processing batch 1892/11884 - Val_Loss: 27.4858\n",
      "Processing batch 1893/11884 - Val_Loss: 23.3234\n",
      "Processing batch 1894/11884 - Val_Loss: 26.4234\n",
      "Processing batch 1895/11884 - Val_Loss: 22.2785\n",
      "Processing batch 1896/11884 - Val_Loss: 23.9949\n",
      "Processing batch 1897/11884 - Val_Loss: 26.8271\n",
      "Processing batch 1898/11884 - Val_Loss: 24.6281\n",
      "Processing batch 1899/11884 - Val_Loss: 24.7439\n",
      "Processing batch 1900/11884 - Val_Loss: 26.1627\n",
      "Processing batch 1901/11884 - Val_Loss: 26.2501\n",
      "Processing batch 1902/11884 - Val_Loss: 24.2128\n",
      "Processing batch 1903/11884 - Val_Loss: 24.5076\n",
      "Processing batch 1904/11884 - Val_Loss: 29.2858\n",
      "Processing batch 1905/11884 - Val_Loss: 26.3434\n",
      "Processing batch 1906/11884 - Val_Loss: 25.0587\n",
      "Processing batch 1907/11884 - Val_Loss: 24.8504\n",
      "Processing batch 1908/11884 - Val_Loss: 26.7961\n",
      "Processing batch 1909/11884 - Val_Loss: 27.2481\n",
      "Processing batch 1910/11884 - Val_Loss: 26.4058\n",
      "Processing batch 1911/11884 - Val_Loss: 26.2131\n",
      "Processing batch 1912/11884 - Val_Loss: 25.0104\n",
      "Processing batch 1913/11884 - Val_Loss: 29.0147\n",
      "Processing batch 1914/11884 - Val_Loss: 25.2099\n",
      "Processing batch 1915/11884 - Val_Loss: 23.2929\n",
      "Processing batch 1916/11884 - Val_Loss: 27.7828\n",
      "Processing batch 1917/11884 - Val_Loss: 25.8229\n",
      "Processing batch 1918/11884 - Val_Loss: 28.7462\n",
      "Processing batch 1919/11884 - Val_Loss: 25.3043\n",
      "Processing batch 1920/11884 - Val_Loss: 24.4981\n",
      "Processing batch 1921/11884 - Val_Loss: 28.2723\n",
      "Processing batch 1922/11884 - Val_Loss: 27.8132\n",
      "Processing batch 1923/11884 - Val_Loss: 26.2409\n",
      "Processing batch 1924/11884 - Val_Loss: 28.2602\n",
      "Processing batch 1925/11884 - Val_Loss: 25.4153\n",
      "Processing batch 1926/11884 - Val_Loss: 26.9132\n",
      "Processing batch 1927/11884 - Val_Loss: 25.2049\n",
      "Processing batch 1928/11884 - Val_Loss: 29.0216\n",
      "Processing batch 1929/11884 - Val_Loss: 25.1992\n",
      "Processing batch 1930/11884 - Val_Loss: 28.6128\n",
      "Processing batch 1931/11884 - Val_Loss: 27.9782\n",
      "Processing batch 1932/11884 - Val_Loss: 27.1322\n",
      "Processing batch 1933/11884 - Val_Loss: 27.4311\n",
      "Processing batch 1934/11884 - Val_Loss: 25.5369\n",
      "Processing batch 1935/11884 - Val_Loss: 27.3053\n",
      "Processing batch 1936/11884 - Val_Loss: 22.6687\n",
      "Processing batch 1937/11884 - Val_Loss: 25.2665\n",
      "Processing batch 1938/11884 - Val_Loss: 25.0557\n",
      "Processing batch 1939/11884 - Val_Loss: 26.4374\n",
      "Processing batch 1940/11884 - Val_Loss: 26.4544\n",
      "Processing batch 1941/11884 - Val_Loss: 27.1343\n",
      "Processing batch 1942/11884 - Val_Loss: 26.2754\n",
      "Processing batch 1943/11884 - Val_Loss: 28.0206\n",
      "Processing batch 1944/11884 - Val_Loss: 23.7732\n",
      "Processing batch 1945/11884 - Val_Loss: 23.4310\n",
      "Processing batch 1946/11884 - Val_Loss: 25.5740\n",
      "Processing batch 1947/11884 - Val_Loss: 28.0503\n",
      "Processing batch 1948/11884 - Val_Loss: 24.9349\n",
      "Processing batch 1949/11884 - Val_Loss: 26.3634\n",
      "Processing batch 1950/11884 - Val_Loss: 27.5545\n",
      "Processing batch 1951/11884 - Val_Loss: 26.7298\n",
      "Processing batch 1952/11884 - Val_Loss: 27.0465\n",
      "Processing batch 1953/11884 - Val_Loss: 26.6841\n",
      "Processing batch 1954/11884 - Val_Loss: 25.8188\n",
      "Processing batch 1955/11884 - Val_Loss: 27.6052\n",
      "Processing batch 1956/11884 - Val_Loss: 26.6790\n",
      "Processing batch 1957/11884 - Val_Loss: 26.4347\n",
      "Processing batch 1958/11884 - Val_Loss: 23.4050\n",
      "Processing batch 1959/11884 - Val_Loss: 27.9987\n",
      "Processing batch 1960/11884 - Val_Loss: 25.5151\n",
      "Processing batch 1961/11884 - Val_Loss: 25.8070\n",
      "Processing batch 1962/11884 - Val_Loss: 25.1944\n",
      "Processing batch 1963/11884 - Val_Loss: 26.5364\n",
      "Processing batch 1964/11884 - Val_Loss: 25.2038\n",
      "Processing batch 1965/11884 - Val_Loss: 23.8113\n",
      "Processing batch 1966/11884 - Val_Loss: 25.3775\n",
      "Processing batch 1967/11884 - Val_Loss: 27.3539\n",
      "Processing batch 1968/11884 - Val_Loss: 25.1969\n",
      "Processing batch 1969/11884 - Val_Loss: 27.2435\n",
      "Processing batch 1970/11884 - Val_Loss: 21.8036\n",
      "Processing batch 1971/11884 - Val_Loss: 25.0363\n",
      "Processing batch 1972/11884 - Val_Loss: 20.4075\n",
      "Processing batch 1973/11884 - Val_Loss: 22.9653\n",
      "Processing batch 1974/11884 - Val_Loss: 27.3005\n",
      "Processing batch 1975/11884 - Val_Loss: 26.3438\n",
      "Processing batch 1976/11884 - Val_Loss: 25.1334\n",
      "Processing batch 1977/11884 - Val_Loss: 26.9899\n",
      "Processing batch 1978/11884 - Val_Loss: 25.0468\n",
      "Processing batch 1979/11884 - Val_Loss: 25.0927\n",
      "Processing batch 1980/11884 - Val_Loss: 23.7717\n",
      "Processing batch 1981/11884 - Val_Loss: 24.2062\n",
      "Processing batch 1982/11884 - Val_Loss: 24.2674\n",
      "Processing batch 1983/11884 - Val_Loss: 26.9747\n",
      "Processing batch 1984/11884 - Val_Loss: 26.7682\n",
      "Processing batch 1985/11884 - Val_Loss: 25.8949\n",
      "Processing batch 1986/11884 - Val_Loss: 24.9964\n",
      "Processing batch 1987/11884 - Val_Loss: 25.1476\n",
      "Processing batch 1988/11884 - Val_Loss: 25.1472\n",
      "Processing batch 1989/11884 - Val_Loss: 28.3298\n",
      "Processing batch 1990/11884 - Val_Loss: 26.8104\n",
      "Processing batch 1991/11884 - Val_Loss: 26.7079\n",
      "Processing batch 1992/11884 - Val_Loss: 23.7678\n",
      "Processing batch 1993/11884 - Val_Loss: 26.2541\n",
      "Processing batch 1994/11884 - Val_Loss: 26.9106\n",
      "Processing batch 1995/11884 - Val_Loss: 27.0535\n",
      "Processing batch 1996/11884 - Val_Loss: 24.1445\n",
      "Processing batch 1997/11884 - Val_Loss: 27.1063\n",
      "Processing batch 1998/11884 - Val_Loss: 26.0968\n",
      "Processing batch 1999/11884 - Val_Loss: 25.1323\n",
      "Processing batch 2000/11884 - Val_Loss: 26.3965\n",
      "Processing batch 2001/11884 - Val_Loss: 25.0889\n",
      "Processing batch 2002/11884 - Val_Loss: 24.5652\n",
      "Processing batch 2003/11884 - Val_Loss: 26.9921\n",
      "Processing batch 2004/11884 - Val_Loss: 26.1182\n",
      "Processing batch 2005/11884 - Val_Loss: 25.0942\n",
      "Processing batch 2006/11884 - Val_Loss: 26.3991\n",
      "Processing batch 2007/11884 - Val_Loss: 28.6745\n",
      "Processing batch 2008/11884 - Val_Loss: 27.0356\n",
      "Processing batch 2009/11884 - Val_Loss: 26.8863\n",
      "Processing batch 2010/11884 - Val_Loss: 26.5923\n",
      "Processing batch 2011/11884 - Val_Loss: 21.5922\n",
      "Processing batch 2012/11884 - Val_Loss: 28.3783\n",
      "Processing batch 2013/11884 - Val_Loss: 25.8727\n",
      "Processing batch 2014/11884 - Val_Loss: 27.9595\n",
      "Processing batch 2015/11884 - Val_Loss: 24.1318\n",
      "Processing batch 2016/11884 - Val_Loss: 26.6472\n",
      "Processing batch 2017/11884 - Val_Loss: 26.2331\n",
      "Processing batch 2018/11884 - Val_Loss: 25.9990\n",
      "Processing batch 2019/11884 - Val_Loss: 23.0278\n",
      "Processing batch 2020/11884 - Val_Loss: 26.0911\n",
      "Processing batch 2021/11884 - Val_Loss: 24.7181\n",
      "Processing batch 2022/11884 - Val_Loss: 24.3386\n",
      "Processing batch 2023/11884 - Val_Loss: 27.0151\n",
      "Processing batch 2024/11884 - Val_Loss: 28.2557\n",
      "Processing batch 2025/11884 - Val_Loss: 28.0026\n",
      "Processing batch 2026/11884 - Val_Loss: 25.3559\n",
      "Processing batch 2027/11884 - Val_Loss: 24.0986\n",
      "Processing batch 2028/11884 - Val_Loss: 25.2143\n",
      "Processing batch 2029/11884 - Val_Loss: 25.5905\n",
      "Processing batch 2030/11884 - Val_Loss: 25.9268\n",
      "Processing batch 2031/11884 - Val_Loss: 25.0166\n",
      "Processing batch 2032/11884 - Val_Loss: 27.9880\n",
      "Processing batch 2033/11884 - Val_Loss: 24.1597\n",
      "Processing batch 2034/11884 - Val_Loss: 26.7324\n",
      "Processing batch 2035/11884 - Val_Loss: 26.2932\n",
      "Processing batch 2036/11884 - Val_Loss: 25.2600\n",
      "Processing batch 2037/11884 - Val_Loss: 26.4605\n",
      "Processing batch 2038/11884 - Val_Loss: 23.5836\n",
      "Processing batch 2039/11884 - Val_Loss: 25.6814\n",
      "Processing batch 2040/11884 - Val_Loss: 27.5453\n",
      "Processing batch 2041/11884 - Val_Loss: 26.6959\n",
      "Processing batch 2042/11884 - Val_Loss: 27.3508\n",
      "Processing batch 2043/11884 - Val_Loss: 26.4364\n",
      "Processing batch 2044/11884 - Val_Loss: 27.0167\n",
      "Processing batch 2045/11884 - Val_Loss: 29.3730\n",
      "Processing batch 2046/11884 - Val_Loss: 23.1275\n",
      "Processing batch 2047/11884 - Val_Loss: 26.1904\n",
      "Processing batch 2048/11884 - Val_Loss: 23.0810\n",
      "Processing batch 2049/11884 - Val_Loss: 25.5897\n",
      "Processing batch 2050/11884 - Val_Loss: 27.8184\n",
      "Processing batch 2051/11884 - Val_Loss: 22.4268\n",
      "Processing batch 2052/11884 - Val_Loss: 26.9121\n",
      "Processing batch 2053/11884 - Val_Loss: 23.4972\n",
      "Processing batch 2054/11884 - Val_Loss: 23.8469\n",
      "Processing batch 2055/11884 - Val_Loss: 26.1196\n",
      "Processing batch 2056/11884 - Val_Loss: 28.3168\n",
      "Processing batch 2057/11884 - Val_Loss: 26.0612\n",
      "Processing batch 2058/11884 - Val_Loss: 26.8609\n",
      "Processing batch 2059/11884 - Val_Loss: 25.7976\n",
      "Processing batch 2060/11884 - Val_Loss: 23.3956\n",
      "Processing batch 2061/11884 - Val_Loss: 26.7100\n",
      "Processing batch 2062/11884 - Val_Loss: 24.0965\n",
      "Processing batch 2063/11884 - Val_Loss: 24.8665\n",
      "Processing batch 2064/11884 - Val_Loss: 24.4187\n",
      "Processing batch 2065/11884 - Val_Loss: 24.2428\n",
      "Processing batch 2066/11884 - Val_Loss: 26.8618\n",
      "Processing batch 2067/11884 - Val_Loss: 24.5731\n",
      "Processing batch 2068/11884 - Val_Loss: 24.8143\n",
      "Processing batch 2069/11884 - Val_Loss: 24.1701\n",
      "Processing batch 2070/11884 - Val_Loss: 24.3136\n",
      "Processing batch 2071/11884 - Val_Loss: 26.6005\n",
      "Processing batch 2072/11884 - Val_Loss: 23.8231\n",
      "Processing batch 2073/11884 - Val_Loss: 25.5519\n",
      "Processing batch 2074/11884 - Val_Loss: 25.7048\n",
      "Processing batch 2075/11884 - Val_Loss: 24.2238\n",
      "Processing batch 2076/11884 - Val_Loss: 24.5310\n",
      "Processing batch 2077/11884 - Val_Loss: 27.2299\n",
      "Processing batch 2078/11884 - Val_Loss: 24.1271\n",
      "Processing batch 2079/11884 - Val_Loss: 25.7657\n",
      "Processing batch 2080/11884 - Val_Loss: 26.3122\n",
      "Processing batch 2081/11884 - Val_Loss: 24.1177\n",
      "Processing batch 2082/11884 - Val_Loss: 25.9800\n",
      "Processing batch 2083/11884 - Val_Loss: 23.6437\n",
      "Processing batch 2084/11884 - Val_Loss: 24.8794\n",
      "Processing batch 2085/11884 - Val_Loss: 22.7408\n",
      "Processing batch 2086/11884 - Val_Loss: 24.9183\n",
      "Processing batch 2087/11884 - Val_Loss: 26.7197\n",
      "Processing batch 2088/11884 - Val_Loss: 25.8524\n",
      "Processing batch 2089/11884 - Val_Loss: 27.3125\n",
      "Processing batch 2090/11884 - Val_Loss: 26.2946\n",
      "Processing batch 2091/11884 - Val_Loss: 22.9767\n",
      "Processing batch 2092/11884 - Val_Loss: 25.2812\n",
      "Processing batch 2093/11884 - Val_Loss: 26.9485\n",
      "Processing batch 2094/11884 - Val_Loss: 24.1388\n",
      "Processing batch 2095/11884 - Val_Loss: 24.5966\n",
      "Processing batch 2096/11884 - Val_Loss: 26.6971\n",
      "Processing batch 2097/11884 - Val_Loss: 29.8313\n",
      "Processing batch 2098/11884 - Val_Loss: 25.7849\n",
      "Processing batch 2099/11884 - Val_Loss: 26.9204\n",
      "Processing batch 2100/11884 - Val_Loss: 24.2717\n",
      "Processing batch 2101/11884 - Val_Loss: 29.1819\n",
      "Processing batch 2102/11884 - Val_Loss: 24.8401\n",
      "Processing batch 2103/11884 - Val_Loss: 26.9686\n",
      "Processing batch 2104/11884 - Val_Loss: 26.0050\n",
      "Processing batch 2105/11884 - Val_Loss: 26.2026\n",
      "Processing batch 2106/11884 - Val_Loss: 25.4551\n",
      "Processing batch 2107/11884 - Val_Loss: 26.4789\n",
      "Processing batch 2108/11884 - Val_Loss: 24.9289\n",
      "Processing batch 2109/11884 - Val_Loss: 24.3652\n",
      "Processing batch 2110/11884 - Val_Loss: 27.8746\n",
      "Processing batch 2111/11884 - Val_Loss: 24.7307\n",
      "Processing batch 2112/11884 - Val_Loss: 24.7216\n",
      "Processing batch 2113/11884 - Val_Loss: 25.3488\n",
      "Processing batch 2114/11884 - Val_Loss: 26.2128\n",
      "Processing batch 2115/11884 - Val_Loss: 23.7307\n",
      "Processing batch 2116/11884 - Val_Loss: 25.3977\n",
      "Processing batch 2117/11884 - Val_Loss: 23.0142\n",
      "Processing batch 2118/11884 - Val_Loss: 26.4823\n",
      "Processing batch 2119/11884 - Val_Loss: 21.8699\n",
      "Processing batch 2120/11884 - Val_Loss: 26.7853\n",
      "Processing batch 2121/11884 - Val_Loss: 25.1998\n",
      "Processing batch 2122/11884 - Val_Loss: 25.7173\n",
      "Processing batch 2123/11884 - Val_Loss: 28.0441\n",
      "Processing batch 2124/11884 - Val_Loss: 27.5461\n",
      "Processing batch 2125/11884 - Val_Loss: 24.4560\n",
      "Processing batch 2126/11884 - Val_Loss: 24.3423\n",
      "Processing batch 2127/11884 - Val_Loss: 27.0287\n",
      "Processing batch 2128/11884 - Val_Loss: 26.1524\n",
      "Processing batch 2129/11884 - Val_Loss: 28.9170\n",
      "Processing batch 2130/11884 - Val_Loss: 21.7559\n",
      "Processing batch 2131/11884 - Val_Loss: 26.6840\n",
      "Processing batch 2132/11884 - Val_Loss: 26.5211\n",
      "Processing batch 2133/11884 - Val_Loss: 28.0229\n",
      "Processing batch 2134/11884 - Val_Loss: 24.3317\n",
      "Processing batch 2135/11884 - Val_Loss: 27.0237\n",
      "Processing batch 2136/11884 - Val_Loss: 26.0138\n",
      "Processing batch 2137/11884 - Val_Loss: 27.7926\n",
      "Processing batch 2138/11884 - Val_Loss: 26.1009\n",
      "Processing batch 2139/11884 - Val_Loss: 23.9002\n",
      "Processing batch 2140/11884 - Val_Loss: 25.9649\n",
      "Processing batch 2141/11884 - Val_Loss: 22.5615\n",
      "Processing batch 2142/11884 - Val_Loss: 25.5853\n",
      "Processing batch 2143/11884 - Val_Loss: 28.1110\n",
      "Processing batch 2144/11884 - Val_Loss: 25.1295\n",
      "Processing batch 2145/11884 - Val_Loss: 28.6785\n",
      "Processing batch 2146/11884 - Val_Loss: 25.5097\n",
      "Processing batch 2147/11884 - Val_Loss: 22.5652\n",
      "Processing batch 2148/11884 - Val_Loss: 26.6743\n",
      "Processing batch 2149/11884 - Val_Loss: 24.4084\n",
      "Processing batch 2150/11884 - Val_Loss: 23.9647\n",
      "Processing batch 2151/11884 - Val_Loss: 27.3669\n",
      "Processing batch 2152/11884 - Val_Loss: 27.4653\n",
      "Processing batch 2153/11884 - Val_Loss: 24.3584\n",
      "Processing batch 2154/11884 - Val_Loss: 27.5935\n",
      "Processing batch 2155/11884 - Val_Loss: 22.6112\n",
      "Processing batch 2156/11884 - Val_Loss: 25.1185\n",
      "Processing batch 2157/11884 - Val_Loss: 27.9780\n",
      "Processing batch 2158/11884 - Val_Loss: 23.8919\n",
      "Processing batch 2159/11884 - Val_Loss: 25.4530\n",
      "Processing batch 2160/11884 - Val_Loss: 23.9020\n",
      "Processing batch 2161/11884 - Val_Loss: 28.5286\n",
      "Processing batch 2162/11884 - Val_Loss: 25.5757\n",
      "Processing batch 2163/11884 - Val_Loss: 25.7387\n",
      "Processing batch 2164/11884 - Val_Loss: 27.4218\n",
      "Processing batch 2165/11884 - Val_Loss: 25.5245\n",
      "Processing batch 2166/11884 - Val_Loss: 26.8752\n",
      "Processing batch 2167/11884 - Val_Loss: 27.7058\n",
      "Processing batch 2168/11884 - Val_Loss: 23.0257\n",
      "Processing batch 2169/11884 - Val_Loss: 26.0056\n",
      "Processing batch 2170/11884 - Val_Loss: 25.4684\n",
      "Processing batch 2171/11884 - Val_Loss: 25.4729\n",
      "Processing batch 2172/11884 - Val_Loss: 26.1039\n",
      "Processing batch 2173/11884 - Val_Loss: 26.0891\n",
      "Processing batch 2174/11884 - Val_Loss: 26.5725\n",
      "Processing batch 2175/11884 - Val_Loss: 25.4490\n",
      "Processing batch 2176/11884 - Val_Loss: 28.4402\n",
      "Processing batch 2177/11884 - Val_Loss: 25.9899\n",
      "Processing batch 2178/11884 - Val_Loss: 24.3572\n",
      "Processing batch 2179/11884 - Val_Loss: 24.8377\n",
      "Processing batch 2180/11884 - Val_Loss: 26.3572\n",
      "Processing batch 2181/11884 - Val_Loss: 24.6091\n",
      "Processing batch 2182/11884 - Val_Loss: 25.2781\n",
      "Processing batch 2183/11884 - Val_Loss: 25.4066\n",
      "Processing batch 2184/11884 - Val_Loss: 24.6069\n",
      "Processing batch 2185/11884 - Val_Loss: 26.0985\n",
      "Processing batch 2186/11884 - Val_Loss: 26.9793\n",
      "Processing batch 2187/11884 - Val_Loss: 26.9601\n",
      "Processing batch 2188/11884 - Val_Loss: 25.6305\n",
      "Processing batch 2189/11884 - Val_Loss: 27.5263\n",
      "Processing batch 2190/11884 - Val_Loss: 25.8499\n",
      "Processing batch 2191/11884 - Val_Loss: 23.4915\n",
      "Processing batch 2192/11884 - Val_Loss: 30.2925\n",
      "Processing batch 2193/11884 - Val_Loss: 23.7785\n",
      "Processing batch 2194/11884 - Val_Loss: 27.1911\n",
      "Processing batch 2195/11884 - Val_Loss: 25.3563\n",
      "Processing batch 2196/11884 - Val_Loss: 23.9380\n",
      "Processing batch 2197/11884 - Val_Loss: 27.2936\n",
      "Processing batch 2198/11884 - Val_Loss: 29.8508\n",
      "Processing batch 2199/11884 - Val_Loss: 25.3679\n",
      "Processing batch 2200/11884 - Val_Loss: 22.0596\n",
      "Processing batch 2201/11884 - Val_Loss: 26.5761\n",
      "Processing batch 2202/11884 - Val_Loss: 26.2477\n",
      "Processing batch 2203/11884 - Val_Loss: 25.9881\n",
      "Processing batch 2204/11884 - Val_Loss: 26.7746\n",
      "Processing batch 2205/11884 - Val_Loss: 23.2800\n",
      "Processing batch 2206/11884 - Val_Loss: 25.7312\n",
      "Processing batch 2207/11884 - Val_Loss: 24.9391\n",
      "Processing batch 2208/11884 - Val_Loss: 25.3270\n",
      "Processing batch 2209/11884 - Val_Loss: 26.1753\n",
      "Processing batch 2210/11884 - Val_Loss: 26.7723\n",
      "Processing batch 2211/11884 - Val_Loss: 22.3728\n",
      "Processing batch 2212/11884 - Val_Loss: 23.4969\n",
      "Processing batch 2213/11884 - Val_Loss: 27.5326\n",
      "Processing batch 2214/11884 - Val_Loss: 23.5718\n",
      "Processing batch 2215/11884 - Val_Loss: 26.7926\n",
      "Processing batch 2216/11884 - Val_Loss: 26.4475\n",
      "Processing batch 2217/11884 - Val_Loss: 28.3641\n",
      "Processing batch 2218/11884 - Val_Loss: 28.7138\n",
      "Processing batch 2219/11884 - Val_Loss: 27.3200\n",
      "Processing batch 2220/11884 - Val_Loss: 25.4642\n",
      "Processing batch 2221/11884 - Val_Loss: 24.7431\n",
      "Processing batch 2222/11884 - Val_Loss: 22.5750\n",
      "Processing batch 2223/11884 - Val_Loss: 27.4411\n",
      "Processing batch 2224/11884 - Val_Loss: 24.3759\n",
      "Processing batch 2225/11884 - Val_Loss: 31.2980\n",
      "Processing batch 2226/11884 - Val_Loss: 21.3814\n",
      "Processing batch 2227/11884 - Val_Loss: 25.3240\n",
      "Processing batch 2228/11884 - Val_Loss: 27.6218\n",
      "Processing batch 2229/11884 - Val_Loss: 28.1303\n",
      "Processing batch 2230/11884 - Val_Loss: 24.3921\n",
      "Processing batch 2231/11884 - Val_Loss: 22.6469\n",
      "Processing batch 2232/11884 - Val_Loss: 26.5035\n",
      "Processing batch 2233/11884 - Val_Loss: 25.1938\n",
      "Processing batch 2234/11884 - Val_Loss: 25.1027\n",
      "Processing batch 2235/11884 - Val_Loss: 25.9950\n",
      "Processing batch 2236/11884 - Val_Loss: 25.4100\n",
      "Processing batch 2237/11884 - Val_Loss: 27.7958\n",
      "Processing batch 2238/11884 - Val_Loss: 27.8944\n",
      "Processing batch 2239/11884 - Val_Loss: 26.3299\n",
      "Processing batch 2240/11884 - Val_Loss: 27.6139\n",
      "Processing batch 2241/11884 - Val_Loss: 30.3638\n",
      "Processing batch 2242/11884 - Val_Loss: 25.5660\n",
      "Processing batch 2243/11884 - Val_Loss: 26.0002\n",
      "Processing batch 2244/11884 - Val_Loss: 23.1512\n",
      "Processing batch 2245/11884 - Val_Loss: 24.7634\n",
      "Processing batch 2246/11884 - Val_Loss: 26.5818\n",
      "Processing batch 2247/11884 - Val_Loss: 23.5710\n",
      "Processing batch 2248/11884 - Val_Loss: 25.3297\n",
      "Processing batch 2249/11884 - Val_Loss: 23.9534\n",
      "Processing batch 2250/11884 - Val_Loss: 27.8722\n",
      "Processing batch 2251/11884 - Val_Loss: 27.2965\n",
      "Processing batch 2252/11884 - Val_Loss: 24.9770\n",
      "Processing batch 2253/11884 - Val_Loss: 25.9187\n",
      "Processing batch 2254/11884 - Val_Loss: 26.0772\n",
      "Processing batch 2255/11884 - Val_Loss: 22.6756\n",
      "Processing batch 2256/11884 - Val_Loss: 22.9422\n",
      "Processing batch 2257/11884 - Val_Loss: 26.5238\n",
      "Processing batch 2258/11884 - Val_Loss: 27.0321\n",
      "Processing batch 2259/11884 - Val_Loss: 25.4009\n",
      "Processing batch 2260/11884 - Val_Loss: 26.5477\n",
      "Processing batch 2261/11884 - Val_Loss: 25.5136\n",
      "Processing batch 2262/11884 - Val_Loss: 25.4567\n",
      "Processing batch 2263/11884 - Val_Loss: 26.6179\n",
      "Processing batch 2264/11884 - Val_Loss: 26.8820\n",
      "Processing batch 2265/11884 - Val_Loss: 26.4776\n",
      "Processing batch 2266/11884 - Val_Loss: 23.8033\n",
      "Processing batch 2267/11884 - Val_Loss: 22.3508\n",
      "Processing batch 2268/11884 - Val_Loss: 25.2116\n",
      "Processing batch 2269/11884 - Val_Loss: 24.3582\n",
      "Processing batch 2270/11884 - Val_Loss: 25.7134\n",
      "Processing batch 2271/11884 - Val_Loss: 23.3241\n",
      "Processing batch 2272/11884 - Val_Loss: 28.8220\n",
      "Processing batch 2273/11884 - Val_Loss: 29.4863\n",
      "Processing batch 2274/11884 - Val_Loss: 23.4564\n",
      "Processing batch 2275/11884 - Val_Loss: 24.1204\n",
      "Processing batch 2276/11884 - Val_Loss: 22.9405\n",
      "Processing batch 2277/11884 - Val_Loss: 27.6379\n",
      "Processing batch 2278/11884 - Val_Loss: 22.6454\n",
      "Processing batch 2279/11884 - Val_Loss: 25.4899\n",
      "Processing batch 2280/11884 - Val_Loss: 24.1727\n",
      "Processing batch 2281/11884 - Val_Loss: 24.0053\n",
      "Processing batch 2282/11884 - Val_Loss: 26.2601\n",
      "Processing batch 2283/11884 - Val_Loss: 25.9317\n",
      "Processing batch 2284/11884 - Val_Loss: 26.3551\n",
      "Processing batch 2285/11884 - Val_Loss: 24.1133\n",
      "Processing batch 2286/11884 - Val_Loss: 28.7667\n",
      "Processing batch 2287/11884 - Val_Loss: 25.9793\n",
      "Processing batch 2288/11884 - Val_Loss: 25.6523\n",
      "Processing batch 2289/11884 - Val_Loss: 24.9786\n",
      "Processing batch 2290/11884 - Val_Loss: 25.1939\n",
      "Processing batch 2291/11884 - Val_Loss: 25.3945\n",
      "Processing batch 2292/11884 - Val_Loss: 26.5521\n",
      "Processing batch 2293/11884 - Val_Loss: 26.6900\n",
      "Processing batch 2294/11884 - Val_Loss: 26.3665\n",
      "Processing batch 2295/11884 - Val_Loss: 24.8124\n",
      "Processing batch 2296/11884 - Val_Loss: 26.9422\n",
      "Processing batch 2297/11884 - Val_Loss: 25.5334\n",
      "Processing batch 2298/11884 - Val_Loss: 23.9693\n",
      "Processing batch 2299/11884 - Val_Loss: 26.2014\n",
      "Processing batch 2300/11884 - Val_Loss: 24.0966\n",
      "Processing batch 2301/11884 - Val_Loss: 26.4378\n",
      "Processing batch 2302/11884 - Val_Loss: 25.0572\n",
      "Processing batch 2303/11884 - Val_Loss: 29.0227\n",
      "Processing batch 2304/11884 - Val_Loss: 24.2384\n",
      "Processing batch 2305/11884 - Val_Loss: 28.0428\n",
      "Processing batch 2306/11884 - Val_Loss: 26.0776\n",
      "Processing batch 2307/11884 - Val_Loss: 25.7387\n",
      "Processing batch 2308/11884 - Val_Loss: 27.5971\n",
      "Processing batch 2309/11884 - Val_Loss: 25.7411\n",
      "Processing batch 2310/11884 - Val_Loss: 27.9660\n",
      "Processing batch 2311/11884 - Val_Loss: 23.8121\n",
      "Processing batch 2312/11884 - Val_Loss: 26.0997\n",
      "Processing batch 2313/11884 - Val_Loss: 26.5696\n",
      "Processing batch 2314/11884 - Val_Loss: 27.4452\n",
      "Processing batch 2315/11884 - Val_Loss: 26.3896\n",
      "Processing batch 2316/11884 - Val_Loss: 27.1479\n",
      "Processing batch 2317/11884 - Val_Loss: 27.7206\n",
      "Processing batch 2318/11884 - Val_Loss: 24.0657\n",
      "Processing batch 2319/11884 - Val_Loss: 26.6819\n",
      "Processing batch 2320/11884 - Val_Loss: 22.8427\n",
      "Processing batch 2321/11884 - Val_Loss: 25.1239\n",
      "Processing batch 2322/11884 - Val_Loss: 25.1558\n",
      "Processing batch 2323/11884 - Val_Loss: 25.2164\n",
      "Processing batch 2324/11884 - Val_Loss: 23.1042\n",
      "Processing batch 2325/11884 - Val_Loss: 26.0919\n",
      "Processing batch 2326/11884 - Val_Loss: 24.1294\n",
      "Processing batch 2327/11884 - Val_Loss: 23.0551\n",
      "Processing batch 2328/11884 - Val_Loss: 28.3375\n",
      "Processing batch 2329/11884 - Val_Loss: 26.3027\n",
      "Processing batch 2330/11884 - Val_Loss: 26.2143\n",
      "Processing batch 2331/11884 - Val_Loss: 27.3278\n",
      "Processing batch 2332/11884 - Val_Loss: 25.9445\n",
      "Processing batch 2333/11884 - Val_Loss: 25.5104\n",
      "Processing batch 2334/11884 - Val_Loss: 27.1490\n",
      "Processing batch 2335/11884 - Val_Loss: 26.3348\n",
      "Processing batch 2336/11884 - Val_Loss: 25.7065\n",
      "Processing batch 2337/11884 - Val_Loss: 27.6490\n",
      "Processing batch 2338/11884 - Val_Loss: 23.8682\n",
      "Processing batch 2339/11884 - Val_Loss: 27.9248\n",
      "Processing batch 2340/11884 - Val_Loss: 26.1621\n",
      "Processing batch 2341/11884 - Val_Loss: 24.2694\n",
      "Processing batch 2342/11884 - Val_Loss: 29.7124\n",
      "Processing batch 2343/11884 - Val_Loss: 27.0885\n",
      "Processing batch 2344/11884 - Val_Loss: 25.7857\n",
      "Processing batch 2345/11884 - Val_Loss: 28.0662\n",
      "Processing batch 2346/11884 - Val_Loss: 25.6742\n",
      "Processing batch 2347/11884 - Val_Loss: 24.3044\n",
      "Processing batch 2348/11884 - Val_Loss: 25.6041\n",
      "Processing batch 2349/11884 - Val_Loss: 28.3702\n",
      "Processing batch 2350/11884 - Val_Loss: 25.7567\n",
      "Processing batch 2351/11884 - Val_Loss: 26.7605\n",
      "Processing batch 2352/11884 - Val_Loss: 23.4893\n",
      "Processing batch 2353/11884 - Val_Loss: 27.5901\n",
      "Processing batch 2354/11884 - Val_Loss: 24.2439\n",
      "Processing batch 2355/11884 - Val_Loss: 25.0982\n",
      "Processing batch 2356/11884 - Val_Loss: 25.8209\n",
      "Processing batch 2357/11884 - Val_Loss: 25.5595\n",
      "Processing batch 2358/11884 - Val_Loss: 26.8016\n",
      "Processing batch 2359/11884 - Val_Loss: 26.9215\n",
      "Processing batch 2360/11884 - Val_Loss: 25.3201\n",
      "Processing batch 2361/11884 - Val_Loss: 24.9393\n",
      "Processing batch 2362/11884 - Val_Loss: 26.8088\n",
      "Processing batch 2363/11884 - Val_Loss: 26.2454\n",
      "Processing batch 2364/11884 - Val_Loss: 26.0884\n",
      "Processing batch 2365/11884 - Val_Loss: 26.0701\n",
      "Processing batch 2366/11884 - Val_Loss: 25.3246\n",
      "Processing batch 2367/11884 - Val_Loss: 27.5517\n",
      "Processing batch 2368/11884 - Val_Loss: 25.5829\n",
      "Processing batch 2369/11884 - Val_Loss: 23.5071\n",
      "Processing batch 2370/11884 - Val_Loss: 23.4552\n",
      "Processing batch 2371/11884 - Val_Loss: 22.7648\n",
      "Processing batch 2372/11884 - Val_Loss: 23.8601\n",
      "Processing batch 2373/11884 - Val_Loss: 23.2543\n",
      "Processing batch 2374/11884 - Val_Loss: 25.8909\n",
      "Processing batch 2375/11884 - Val_Loss: 26.2274\n",
      "Processing batch 2376/11884 - Val_Loss: 23.9783\n",
      "Processing batch 2377/11884 - Val_Loss: 24.5142\n",
      "Processing batch 2378/11884 - Val_Loss: 26.8738\n",
      "Processing batch 2379/11884 - Val_Loss: 24.9503\n",
      "Processing batch 2380/11884 - Val_Loss: 25.3655\n",
      "Processing batch 2381/11884 - Val_Loss: 26.8195\n",
      "Processing batch 2382/11884 - Val_Loss: 24.5488\n",
      "Processing batch 2383/11884 - Val_Loss: 24.1177\n",
      "Processing batch 2384/11884 - Val_Loss: 24.8693\n",
      "Processing batch 2385/11884 - Val_Loss: 24.4305\n",
      "Processing batch 2386/11884 - Val_Loss: 22.4218\n",
      "Processing batch 2387/11884 - Val_Loss: 23.1969\n",
      "Processing batch 2388/11884 - Val_Loss: 26.1917\n",
      "Processing batch 2389/11884 - Val_Loss: 25.8098\n",
      "Processing batch 2390/11884 - Val_Loss: 27.4836\n",
      "Processing batch 2391/11884 - Val_Loss: 26.7241\n",
      "Processing batch 2392/11884 - Val_Loss: 23.6696\n",
      "Processing batch 2393/11884 - Val_Loss: 25.0935\n",
      "Processing batch 2394/11884 - Val_Loss: 26.5271\n",
      "Processing batch 2395/11884 - Val_Loss: 24.8429\n",
      "Processing batch 2396/11884 - Val_Loss: 28.1768\n",
      "Processing batch 2397/11884 - Val_Loss: 25.2091\n",
      "Processing batch 2398/11884 - Val_Loss: 26.9074\n",
      "Processing batch 2399/11884 - Val_Loss: 26.1644\n",
      "Processing batch 2400/11884 - Val_Loss: 23.0457\n",
      "Processing batch 2401/11884 - Val_Loss: 27.1479\n",
      "Processing batch 2402/11884 - Val_Loss: 24.5830\n",
      "Processing batch 2403/11884 - Val_Loss: 27.2309\n",
      "Processing batch 2404/11884 - Val_Loss: 25.1261\n",
      "Processing batch 2405/11884 - Val_Loss: 23.1929\n",
      "Processing batch 2406/11884 - Val_Loss: 25.4646\n",
      "Processing batch 2407/11884 - Val_Loss: 24.3466\n",
      "Processing batch 2408/11884 - Val_Loss: 26.1904\n",
      "Processing batch 2409/11884 - Val_Loss: 24.5429\n",
      "Processing batch 2410/11884 - Val_Loss: 26.1391\n",
      "Processing batch 2411/11884 - Val_Loss: 25.1792\n",
      "Processing batch 2412/11884 - Val_Loss: 25.1263\n",
      "Processing batch 2413/11884 - Val_Loss: 24.1843\n",
      "Processing batch 2414/11884 - Val_Loss: 23.7019\n",
      "Processing batch 2415/11884 - Val_Loss: 24.2699\n",
      "Processing batch 2416/11884 - Val_Loss: 29.1521\n",
      "Processing batch 2417/11884 - Val_Loss: 23.8011\n",
      "Processing batch 2418/11884 - Val_Loss: 24.1733\n",
      "Processing batch 2419/11884 - Val_Loss: 25.7153\n",
      "Processing batch 2420/11884 - Val_Loss: 26.7043\n",
      "Processing batch 2421/11884 - Val_Loss: 27.9358\n",
      "Processing batch 2422/11884 - Val_Loss: 25.7059\n",
      "Processing batch 2423/11884 - Val_Loss: 26.7698\n",
      "Processing batch 2424/11884 - Val_Loss: 26.3863\n",
      "Processing batch 2425/11884 - Val_Loss: 25.2642\n",
      "Processing batch 2426/11884 - Val_Loss: 27.1122\n",
      "Processing batch 2427/11884 - Val_Loss: 24.7692\n",
      "Processing batch 2428/11884 - Val_Loss: 24.0881\n",
      "Processing batch 2429/11884 - Val_Loss: 25.3901\n",
      "Processing batch 2430/11884 - Val_Loss: 25.3472\n",
      "Processing batch 2431/11884 - Val_Loss: 23.7839\n",
      "Processing batch 2432/11884 - Val_Loss: 24.4506\n",
      "Processing batch 2433/11884 - Val_Loss: 25.7296\n",
      "Processing batch 2434/11884 - Val_Loss: 26.0377\n",
      "Processing batch 2435/11884 - Val_Loss: 26.5186\n",
      "Processing batch 2436/11884 - Val_Loss: 22.3820\n",
      "Processing batch 2437/11884 - Val_Loss: 29.9720\n",
      "Processing batch 2438/11884 - Val_Loss: 27.2092\n",
      "Processing batch 2439/11884 - Val_Loss: 24.7489\n",
      "Processing batch 2440/11884 - Val_Loss: 25.5371\n",
      "Processing batch 2441/11884 - Val_Loss: 27.3827\n",
      "Processing batch 2442/11884 - Val_Loss: 24.7418\n",
      "Processing batch 2443/11884 - Val_Loss: 25.8105\n",
      "Processing batch 2444/11884 - Val_Loss: 26.0788\n",
      "Processing batch 2445/11884 - Val_Loss: 24.7473\n",
      "Processing batch 2446/11884 - Val_Loss: 27.7622\n",
      "Processing batch 2447/11884 - Val_Loss: 26.3617\n",
      "Processing batch 2448/11884 - Val_Loss: 27.2494\n",
      "Processing batch 2449/11884 - Val_Loss: 24.9370\n",
      "Processing batch 2450/11884 - Val_Loss: 26.1831\n",
      "Processing batch 2451/11884 - Val_Loss: 26.7485\n",
      "Processing batch 2452/11884 - Val_Loss: 25.5211\n",
      "Processing batch 2453/11884 - Val_Loss: 27.0055\n",
      "Processing batch 2454/11884 - Val_Loss: 29.9451\n",
      "Processing batch 2455/11884 - Val_Loss: 22.6670\n",
      "Processing batch 2456/11884 - Val_Loss: 28.4510\n",
      "Processing batch 2457/11884 - Val_Loss: 27.9197\n",
      "Processing batch 2458/11884 - Val_Loss: 28.7890\n",
      "Processing batch 2459/11884 - Val_Loss: 24.3940\n",
      "Processing batch 2460/11884 - Val_Loss: 25.5313\n",
      "Processing batch 2461/11884 - Val_Loss: 22.7894\n",
      "Processing batch 2462/11884 - Val_Loss: 24.0504\n",
      "Processing batch 2463/11884 - Val_Loss: 26.6666\n",
      "Processing batch 2464/11884 - Val_Loss: 24.3666\n",
      "Processing batch 2465/11884 - Val_Loss: 23.3093\n",
      "Processing batch 2466/11884 - Val_Loss: 28.5119\n",
      "Processing batch 2467/11884 - Val_Loss: 24.7584\n",
      "Processing batch 2468/11884 - Val_Loss: 28.2257\n",
      "Processing batch 2469/11884 - Val_Loss: 27.7147\n",
      "Processing batch 2470/11884 - Val_Loss: 23.3093\n",
      "Processing batch 2471/11884 - Val_Loss: 23.3625\n",
      "Processing batch 2472/11884 - Val_Loss: 24.9441\n",
      "Processing batch 2473/11884 - Val_Loss: 26.7179\n",
      "Processing batch 2474/11884 - Val_Loss: 23.5181\n",
      "Processing batch 2475/11884 - Val_Loss: 28.5651\n",
      "Processing batch 2476/11884 - Val_Loss: 22.5830\n",
      "Processing batch 2477/11884 - Val_Loss: 25.4704\n",
      "Processing batch 2478/11884 - Val_Loss: 23.7335\n",
      "Processing batch 2479/11884 - Val_Loss: 27.4837\n",
      "Processing batch 2480/11884 - Val_Loss: 26.6405\n",
      "Processing batch 2481/11884 - Val_Loss: 27.0994\n",
      "Processing batch 2482/11884 - Val_Loss: 22.4051\n",
      "Processing batch 2483/11884 - Val_Loss: 26.2550\n",
      "Processing batch 2484/11884 - Val_Loss: 25.6693\n",
      "Processing batch 2485/11884 - Val_Loss: 23.8618\n",
      "Processing batch 2486/11884 - Val_Loss: 25.3721\n",
      "Processing batch 2487/11884 - Val_Loss: 27.6606\n",
      "Processing batch 2488/11884 - Val_Loss: 26.2009\n",
      "Processing batch 2489/11884 - Val_Loss: 23.7652\n",
      "Processing batch 2490/11884 - Val_Loss: 21.9081\n",
      "Processing batch 2491/11884 - Val_Loss: 24.4214\n",
      "Processing batch 2492/11884 - Val_Loss: 24.4461\n",
      "Processing batch 2493/11884 - Val_Loss: 22.1816\n",
      "Processing batch 2494/11884 - Val_Loss: 25.6412\n",
      "Processing batch 2495/11884 - Val_Loss: 25.9405\n",
      "Processing batch 2496/11884 - Val_Loss: 23.9554\n",
      "Processing batch 2497/11884 - Val_Loss: 25.0314\n",
      "Processing batch 2498/11884 - Val_Loss: 25.0860\n",
      "Processing batch 2499/11884 - Val_Loss: 26.6979\n",
      "Processing batch 2500/11884 - Val_Loss: 29.5540\n",
      "Processing batch 2501/11884 - Val_Loss: 29.4870\n",
      "Processing batch 2502/11884 - Val_Loss: 27.9507\n",
      "Processing batch 2503/11884 - Val_Loss: 26.2732\n",
      "Processing batch 2504/11884 - Val_Loss: 26.3372\n",
      "Processing batch 2505/11884 - Val_Loss: 26.9073\n",
      "Processing batch 2506/11884 - Val_Loss: 28.6908\n",
      "Processing batch 2507/11884 - Val_Loss: 25.1793\n",
      "Processing batch 2508/11884 - Val_Loss: 28.7803\n",
      "Processing batch 2509/11884 - Val_Loss: 28.0193\n",
      "Processing batch 2510/11884 - Val_Loss: 26.4944\n",
      "Processing batch 2511/11884 - Val_Loss: 26.9078\n",
      "Processing batch 2512/11884 - Val_Loss: 22.8166\n",
      "Processing batch 2513/11884 - Val_Loss: 26.2547\n",
      "Processing batch 2514/11884 - Val_Loss: 25.5189\n",
      "Processing batch 2515/11884 - Val_Loss: 26.2772\n",
      "Processing batch 2516/11884 - Val_Loss: 28.1007\n",
      "Processing batch 2517/11884 - Val_Loss: 26.8488\n",
      "Processing batch 2518/11884 - Val_Loss: 26.1849\n",
      "Processing batch 2519/11884 - Val_Loss: 24.2220\n",
      "Processing batch 2520/11884 - Val_Loss: 24.2780\n",
      "Processing batch 2521/11884 - Val_Loss: 25.2912\n",
      "Processing batch 2522/11884 - Val_Loss: 27.2806\n",
      "Processing batch 2523/11884 - Val_Loss: 25.2520\n",
      "Processing batch 2524/11884 - Val_Loss: 27.5081\n",
      "Processing batch 2525/11884 - Val_Loss: 24.5119\n",
      "Processing batch 2526/11884 - Val_Loss: 25.7141\n",
      "Processing batch 2527/11884 - Val_Loss: 23.8646\n",
      "Processing batch 2528/11884 - Val_Loss: 27.6121\n",
      "Processing batch 2529/11884 - Val_Loss: 22.4743\n",
      "Processing batch 2530/11884 - Val_Loss: 25.9393\n",
      "Processing batch 2531/11884 - Val_Loss: 27.0273\n",
      "Processing batch 2532/11884 - Val_Loss: 23.3969\n",
      "Processing batch 2533/11884 - Val_Loss: 25.8581\n",
      "Processing batch 2534/11884 - Val_Loss: 26.2474\n",
      "Processing batch 2535/11884 - Val_Loss: 24.2587\n",
      "Processing batch 2536/11884 - Val_Loss: 24.1627\n",
      "Processing batch 2537/11884 - Val_Loss: 24.5709\n",
      "Processing batch 2538/11884 - Val_Loss: 27.5313\n",
      "Processing batch 2539/11884 - Val_Loss: 25.2279\n",
      "Processing batch 2540/11884 - Val_Loss: 26.5930\n",
      "Processing batch 2541/11884 - Val_Loss: 28.2932\n",
      "Processing batch 2542/11884 - Val_Loss: 26.7997\n",
      "Processing batch 2543/11884 - Val_Loss: 26.3520\n",
      "Processing batch 2544/11884 - Val_Loss: 25.9426\n",
      "Processing batch 2545/11884 - Val_Loss: 26.3595\n",
      "Processing batch 2546/11884 - Val_Loss: 27.0358\n",
      "Processing batch 2547/11884 - Val_Loss: 22.7914\n",
      "Processing batch 2548/11884 - Val_Loss: 25.5627\n",
      "Processing batch 2549/11884 - Val_Loss: 26.4922\n",
      "Processing batch 2550/11884 - Val_Loss: 27.2575\n",
      "Processing batch 2551/11884 - Val_Loss: 26.8041\n",
      "Processing batch 2552/11884 - Val_Loss: 25.9040\n",
      "Processing batch 2553/11884 - Val_Loss: 25.4840\n",
      "Processing batch 2554/11884 - Val_Loss: 24.5503\n",
      "Processing batch 2555/11884 - Val_Loss: 23.8703\n",
      "Processing batch 2556/11884 - Val_Loss: 26.7134\n",
      "Processing batch 2557/11884 - Val_Loss: 25.7641\n",
      "Processing batch 2558/11884 - Val_Loss: 25.2966\n",
      "Processing batch 2559/11884 - Val_Loss: 27.7153\n",
      "Processing batch 2560/11884 - Val_Loss: 24.9721\n",
      "Processing batch 2561/11884 - Val_Loss: 30.1926\n",
      "Processing batch 2562/11884 - Val_Loss: 27.6683\n",
      "Processing batch 2563/11884 - Val_Loss: 24.9227\n",
      "Processing batch 2564/11884 - Val_Loss: 27.4271\n",
      "Processing batch 2565/11884 - Val_Loss: 27.9980\n",
      "Processing batch 2566/11884 - Val_Loss: 25.2606\n",
      "Processing batch 2567/11884 - Val_Loss: 24.3682\n",
      "Processing batch 2568/11884 - Val_Loss: 23.2555\n",
      "Processing batch 2569/11884 - Val_Loss: 28.0611\n",
      "Processing batch 2570/11884 - Val_Loss: 26.4754\n",
      "Processing batch 2571/11884 - Val_Loss: 26.6834\n",
      "Processing batch 2572/11884 - Val_Loss: 25.6775\n",
      "Processing batch 2573/11884 - Val_Loss: 26.1398\n",
      "Processing batch 2574/11884 - Val_Loss: 26.9024\n",
      "Processing batch 2575/11884 - Val_Loss: 28.7265\n",
      "Processing batch 2576/11884 - Val_Loss: 25.4022\n",
      "Processing batch 2577/11884 - Val_Loss: 25.7763\n",
      "Processing batch 2578/11884 - Val_Loss: 26.9265\n",
      "Processing batch 2579/11884 - Val_Loss: 27.7567\n",
      "Processing batch 2580/11884 - Val_Loss: 25.3381\n",
      "Processing batch 2581/11884 - Val_Loss: 24.8376\n",
      "Processing batch 2582/11884 - Val_Loss: 26.4093\n",
      "Processing batch 2583/11884 - Val_Loss: 25.2730\n",
      "Processing batch 2584/11884 - Val_Loss: 24.6211\n",
      "Processing batch 2585/11884 - Val_Loss: 24.4646\n",
      "Processing batch 2586/11884 - Val_Loss: 29.2025\n",
      "Processing batch 2587/11884 - Val_Loss: 22.8558\n",
      "Processing batch 2588/11884 - Val_Loss: 27.1721\n",
      "Processing batch 2589/11884 - Val_Loss: 24.9356\n",
      "Processing batch 2590/11884 - Val_Loss: 25.2009\n",
      "Processing batch 2591/11884 - Val_Loss: 25.8111\n",
      "Processing batch 2592/11884 - Val_Loss: 26.3054\n",
      "Processing batch 2593/11884 - Val_Loss: 26.9280\n",
      "Processing batch 2594/11884 - Val_Loss: 26.7004\n",
      "Processing batch 2595/11884 - Val_Loss: 28.4543\n",
      "Processing batch 2596/11884 - Val_Loss: 27.6143\n",
      "Processing batch 2597/11884 - Val_Loss: 25.9775\n",
      "Processing batch 2598/11884 - Val_Loss: 25.6021\n",
      "Processing batch 2599/11884 - Val_Loss: 26.5118\n",
      "Processing batch 2600/11884 - Val_Loss: 27.6282\n",
      "Processing batch 2601/11884 - Val_Loss: 28.4928\n",
      "Processing batch 2602/11884 - Val_Loss: 26.7528\n",
      "Processing batch 2603/11884 - Val_Loss: 23.6730\n",
      "Processing batch 2604/11884 - Val_Loss: 26.3444\n",
      "Processing batch 2605/11884 - Val_Loss: 27.5409\n",
      "Processing batch 2606/11884 - Val_Loss: 23.2699\n",
      "Processing batch 2607/11884 - Val_Loss: 25.2417\n",
      "Processing batch 2608/11884 - Val_Loss: 27.1055\n",
      "Processing batch 2609/11884 - Val_Loss: 28.2076\n",
      "Processing batch 2610/11884 - Val_Loss: 22.5656\n",
      "Processing batch 2611/11884 - Val_Loss: 24.5794\n",
      "Processing batch 2612/11884 - Val_Loss: 24.5771\n",
      "Processing batch 2613/11884 - Val_Loss: 24.3023\n",
      "Processing batch 2614/11884 - Val_Loss: 24.1019\n",
      "Processing batch 2615/11884 - Val_Loss: 24.2228\n",
      "Processing batch 2616/11884 - Val_Loss: 27.7048\n",
      "Processing batch 2617/11884 - Val_Loss: 24.6345\n",
      "Processing batch 2618/11884 - Val_Loss: 25.7391\n",
      "Processing batch 2619/11884 - Val_Loss: 28.7036\n",
      "Processing batch 2620/11884 - Val_Loss: 25.9388\n",
      "Processing batch 2621/11884 - Val_Loss: 26.3326\n",
      "Processing batch 2622/11884 - Val_Loss: 24.6408\n",
      "Processing batch 2623/11884 - Val_Loss: 25.0831\n",
      "Processing batch 2624/11884 - Val_Loss: 25.0926\n",
      "Processing batch 2625/11884 - Val_Loss: 27.3822\n",
      "Processing batch 2626/11884 - Val_Loss: 28.8584\n",
      "Processing batch 2627/11884 - Val_Loss: 24.9873\n",
      "Processing batch 2628/11884 - Val_Loss: 25.8104\n",
      "Processing batch 2629/11884 - Val_Loss: 28.7334\n",
      "Processing batch 2630/11884 - Val_Loss: 27.0611\n",
      "Processing batch 2631/11884 - Val_Loss: 23.2685\n",
      "Processing batch 2632/11884 - Val_Loss: 22.8983\n",
      "Processing batch 2633/11884 - Val_Loss: 26.3697\n",
      "Processing batch 2634/11884 - Val_Loss: 25.9344\n",
      "Processing batch 2635/11884 - Val_Loss: 28.1131\n",
      "Processing batch 2636/11884 - Val_Loss: 25.8553\n",
      "Processing batch 2637/11884 - Val_Loss: 27.8746\n",
      "Processing batch 2638/11884 - Val_Loss: 24.8895\n",
      "Processing batch 2639/11884 - Val_Loss: 26.4465\n",
      "Processing batch 2640/11884 - Val_Loss: 25.7389\n",
      "Processing batch 2641/11884 - Val_Loss: 24.7142\n",
      "Processing batch 2642/11884 - Val_Loss: 24.6855\n",
      "Processing batch 2643/11884 - Val_Loss: 25.1087\n",
      "Processing batch 2644/11884 - Val_Loss: 23.2121\n",
      "Processing batch 2645/11884 - Val_Loss: 26.5906\n",
      "Processing batch 2646/11884 - Val_Loss: 24.1690\n",
      "Processing batch 2647/11884 - Val_Loss: 26.7800\n",
      "Processing batch 2648/11884 - Val_Loss: 25.2660\n",
      "Processing batch 2649/11884 - Val_Loss: 25.9612\n",
      "Processing batch 2650/11884 - Val_Loss: 28.0720\n",
      "Processing batch 2651/11884 - Val_Loss: 24.2006\n",
      "Processing batch 2652/11884 - Val_Loss: 24.3545\n",
      "Processing batch 2653/11884 - Val_Loss: 25.0403\n",
      "Processing batch 2654/11884 - Val_Loss: 24.2197\n",
      "Processing batch 2655/11884 - Val_Loss: 26.5904\n",
      "Processing batch 2656/11884 - Val_Loss: 24.9215\n",
      "Processing batch 2657/11884 - Val_Loss: 21.6840\n",
      "Processing batch 2658/11884 - Val_Loss: 27.3933\n",
      "Processing batch 2659/11884 - Val_Loss: 24.2488\n",
      "Processing batch 2660/11884 - Val_Loss: 24.4656\n",
      "Processing batch 2661/11884 - Val_Loss: 25.6135\n",
      "Processing batch 2662/11884 - Val_Loss: 26.4995\n",
      "Processing batch 2663/11884 - Val_Loss: 25.5155\n",
      "Processing batch 2664/11884 - Val_Loss: 26.0769\n",
      "Processing batch 2665/11884 - Val_Loss: 25.5716\n",
      "Processing batch 2666/11884 - Val_Loss: 28.4465\n",
      "Processing batch 2667/11884 - Val_Loss: 29.3548\n",
      "Processing batch 2668/11884 - Val_Loss: 26.3306\n",
      "Processing batch 2669/11884 - Val_Loss: 25.1653\n",
      "Processing batch 2670/11884 - Val_Loss: 24.9711\n",
      "Processing batch 2671/11884 - Val_Loss: 25.0818\n",
      "Processing batch 2672/11884 - Val_Loss: 26.1320\n",
      "Processing batch 2673/11884 - Val_Loss: 27.6342\n",
      "Processing batch 2674/11884 - Val_Loss: 27.1555\n",
      "Processing batch 2675/11884 - Val_Loss: 26.0144\n",
      "Processing batch 2676/11884 - Val_Loss: 26.3179\n",
      "Processing batch 2677/11884 - Val_Loss: 24.4835\n",
      "Processing batch 2678/11884 - Val_Loss: 26.2158\n",
      "Processing batch 2679/11884 - Val_Loss: 24.9513\n",
      "Processing batch 2680/11884 - Val_Loss: 25.3174\n",
      "Processing batch 2681/11884 - Val_Loss: 28.7147\n",
      "Processing batch 2682/11884 - Val_Loss: 26.3237\n",
      "Processing batch 2683/11884 - Val_Loss: 27.3782\n",
      "Processing batch 2684/11884 - Val_Loss: 24.7767\n",
      "Processing batch 2685/11884 - Val_Loss: 23.0035\n",
      "Processing batch 2686/11884 - Val_Loss: 26.1164\n",
      "Processing batch 2687/11884 - Val_Loss: 25.7373\n",
      "Processing batch 2688/11884 - Val_Loss: 27.0895\n",
      "Processing batch 2689/11884 - Val_Loss: 26.1972\n",
      "Processing batch 2690/11884 - Val_Loss: 25.1300\n",
      "Processing batch 2691/11884 - Val_Loss: 25.1125\n",
      "Processing batch 2692/11884 - Val_Loss: 22.6227\n",
      "Processing batch 2693/11884 - Val_Loss: 22.5869\n",
      "Processing batch 2694/11884 - Val_Loss: 27.6490\n",
      "Processing batch 2695/11884 - Val_Loss: 25.7274\n",
      "Processing batch 2696/11884 - Val_Loss: 25.4171\n",
      "Processing batch 2697/11884 - Val_Loss: 23.5798\n",
      "Processing batch 2698/11884 - Val_Loss: 24.2331\n",
      "Processing batch 2699/11884 - Val_Loss: 25.9857\n",
      "Processing batch 2700/11884 - Val_Loss: 27.7214\n",
      "Processing batch 2701/11884 - Val_Loss: 29.7036\n",
      "Processing batch 2702/11884 - Val_Loss: 26.8455\n",
      "Processing batch 2703/11884 - Val_Loss: 26.3995\n",
      "Processing batch 2704/11884 - Val_Loss: 24.2098\n",
      "Processing batch 2705/11884 - Val_Loss: 28.3487\n",
      "Processing batch 2706/11884 - Val_Loss: 27.3249\n",
      "Processing batch 2707/11884 - Val_Loss: 25.8033\n",
      "Processing batch 2708/11884 - Val_Loss: 31.1801\n",
      "Processing batch 2709/11884 - Val_Loss: 26.3193\n",
      "Processing batch 2710/11884 - Val_Loss: 27.7729\n",
      "Processing batch 2711/11884 - Val_Loss: 24.8469\n",
      "Processing batch 2712/11884 - Val_Loss: 26.6530\n",
      "Processing batch 2713/11884 - Val_Loss: 26.5788\n",
      "Processing batch 2714/11884 - Val_Loss: 27.9278\n",
      "Processing batch 2715/11884 - Val_Loss: 25.2797\n",
      "Processing batch 2716/11884 - Val_Loss: 26.2562\n",
      "Processing batch 2717/11884 - Val_Loss: 27.5546\n",
      "Processing batch 2718/11884 - Val_Loss: 26.5447\n",
      "Processing batch 2719/11884 - Val_Loss: 25.0514\n",
      "Processing batch 2720/11884 - Val_Loss: 24.2312\n",
      "Processing batch 2721/11884 - Val_Loss: 26.9962\n",
      "Processing batch 2722/11884 - Val_Loss: 24.6088\n",
      "Processing batch 2723/11884 - Val_Loss: 25.0192\n",
      "Processing batch 2724/11884 - Val_Loss: 24.3476\n",
      "Processing batch 2725/11884 - Val_Loss: 23.1949\n",
      "Processing batch 2726/11884 - Val_Loss: 23.6049\n",
      "Processing batch 2727/11884 - Val_Loss: 28.0379\n",
      "Processing batch 2728/11884 - Val_Loss: 22.6871\n",
      "Processing batch 2729/11884 - Val_Loss: 23.5359\n",
      "Processing batch 2730/11884 - Val_Loss: 26.0440\n",
      "Processing batch 2731/11884 - Val_Loss: 25.9643\n",
      "Processing batch 2732/11884 - Val_Loss: 25.8362\n",
      "Processing batch 2733/11884 - Val_Loss: 28.5796\n",
      "Processing batch 2734/11884 - Val_Loss: 23.6204\n",
      "Processing batch 2735/11884 - Val_Loss: 26.4670\n",
      "Processing batch 2736/11884 - Val_Loss: 26.5070\n",
      "Processing batch 2737/11884 - Val_Loss: 24.8444\n",
      "Processing batch 2738/11884 - Val_Loss: 25.3515\n",
      "Processing batch 2739/11884 - Val_Loss: 24.5447\n",
      "Processing batch 2740/11884 - Val_Loss: 26.1165\n",
      "Processing batch 2741/11884 - Val_Loss: 25.9890\n",
      "Processing batch 2742/11884 - Val_Loss: 22.9889\n",
      "Processing batch 2743/11884 - Val_Loss: 27.4593\n",
      "Processing batch 2744/11884 - Val_Loss: 25.3586\n",
      "Processing batch 2745/11884 - Val_Loss: 24.8684\n",
      "Processing batch 2746/11884 - Val_Loss: 26.4415\n",
      "Processing batch 2747/11884 - Val_Loss: 27.7556\n",
      "Processing batch 2748/11884 - Val_Loss: 29.3190\n",
      "Processing batch 2749/11884 - Val_Loss: 26.8233\n",
      "Processing batch 2750/11884 - Val_Loss: 24.3370\n",
      "Processing batch 2751/11884 - Val_Loss: 27.3807\n",
      "Processing batch 2752/11884 - Val_Loss: 27.1344\n",
      "Processing batch 2753/11884 - Val_Loss: 24.0505\n",
      "Processing batch 2754/11884 - Val_Loss: 24.8449\n",
      "Processing batch 2755/11884 - Val_Loss: 26.3160\n",
      "Processing batch 2756/11884 - Val_Loss: 24.4851\n",
      "Processing batch 2757/11884 - Val_Loss: 23.5416\n",
      "Processing batch 2758/11884 - Val_Loss: 26.1583\n",
      "Processing batch 2759/11884 - Val_Loss: 27.8826\n",
      "Processing batch 2760/11884 - Val_Loss: 28.0615\n",
      "Processing batch 2761/11884 - Val_Loss: 26.1139\n",
      "Processing batch 2762/11884 - Val_Loss: 24.2293\n",
      "Processing batch 2763/11884 - Val_Loss: 23.1092\n",
      "Processing batch 2764/11884 - Val_Loss: 28.1415\n",
      "Processing batch 2765/11884 - Val_Loss: 26.0853\n",
      "Processing batch 2766/11884 - Val_Loss: 24.4668\n",
      "Processing batch 2767/11884 - Val_Loss: 26.2805\n",
      "Processing batch 2768/11884 - Val_Loss: 26.8536\n",
      "Processing batch 2769/11884 - Val_Loss: 24.7756\n",
      "Processing batch 2770/11884 - Val_Loss: 25.9236\n",
      "Processing batch 2771/11884 - Val_Loss: 25.5207\n",
      "Processing batch 2772/11884 - Val_Loss: 26.3754\n",
      "Processing batch 2773/11884 - Val_Loss: 25.1415\n",
      "Processing batch 2774/11884 - Val_Loss: 26.8940\n",
      "Processing batch 2775/11884 - Val_Loss: 21.9664\n",
      "Processing batch 2776/11884 - Val_Loss: 24.4568\n",
      "Processing batch 2777/11884 - Val_Loss: 23.8445\n",
      "Processing batch 2778/11884 - Val_Loss: 24.0906\n",
      "Processing batch 2779/11884 - Val_Loss: 27.0920\n",
      "Processing batch 2780/11884 - Val_Loss: 27.0549\n",
      "Processing batch 2781/11884 - Val_Loss: 27.6015\n",
      "Processing batch 2782/11884 - Val_Loss: 25.8031\n",
      "Processing batch 2783/11884 - Val_Loss: 22.6949\n",
      "Processing batch 2784/11884 - Val_Loss: 26.3034\n",
      "Processing batch 2785/11884 - Val_Loss: 22.6513\n",
      "Processing batch 2786/11884 - Val_Loss: 26.3544\n",
      "Processing batch 2787/11884 - Val_Loss: 23.6135\n",
      "Processing batch 2788/11884 - Val_Loss: 25.5728\n",
      "Processing batch 2789/11884 - Val_Loss: 25.8995\n",
      "Processing batch 2790/11884 - Val_Loss: 28.5481\n",
      "Processing batch 2791/11884 - Val_Loss: 22.8649\n",
      "Processing batch 2792/11884 - Val_Loss: 24.9677\n",
      "Processing batch 2793/11884 - Val_Loss: 24.7979\n",
      "Processing batch 2794/11884 - Val_Loss: 25.9606\n",
      "Processing batch 2795/11884 - Val_Loss: 24.2547\n",
      "Processing batch 2796/11884 - Val_Loss: 25.6410\n",
      "Processing batch 2797/11884 - Val_Loss: 24.4383\n",
      "Processing batch 2798/11884 - Val_Loss: 25.3133\n",
      "Processing batch 2799/11884 - Val_Loss: 26.4391\n",
      "Processing batch 2800/11884 - Val_Loss: 27.3164\n",
      "Processing batch 2801/11884 - Val_Loss: 25.0936\n",
      "Processing batch 2802/11884 - Val_Loss: 23.7496\n",
      "Processing batch 2803/11884 - Val_Loss: 23.8254\n",
      "Processing batch 2804/11884 - Val_Loss: 28.9277\n",
      "Processing batch 2805/11884 - Val_Loss: 24.5093\n",
      "Processing batch 2806/11884 - Val_Loss: 25.4116\n",
      "Processing batch 2807/11884 - Val_Loss: 27.4280\n",
      "Processing batch 2808/11884 - Val_Loss: 27.5472\n",
      "Processing batch 2809/11884 - Val_Loss: 26.7395\n",
      "Processing batch 2810/11884 - Val_Loss: 27.0311\n",
      "Processing batch 2811/11884 - Val_Loss: 26.6649\n",
      "Processing batch 2812/11884 - Val_Loss: 25.8337\n",
      "Processing batch 2813/11884 - Val_Loss: 24.9341\n",
      "Processing batch 2814/11884 - Val_Loss: 23.9291\n",
      "Processing batch 2815/11884 - Val_Loss: 28.1234\n",
      "Processing batch 2816/11884 - Val_Loss: 24.4707\n",
      "Processing batch 2817/11884 - Val_Loss: 24.6153\n",
      "Processing batch 2818/11884 - Val_Loss: 26.0010\n",
      "Processing batch 2819/11884 - Val_Loss: 26.0277\n",
      "Processing batch 2820/11884 - Val_Loss: 26.4889\n",
      "Processing batch 2821/11884 - Val_Loss: 26.2340\n",
      "Processing batch 2822/11884 - Val_Loss: 23.9912\n",
      "Processing batch 2823/11884 - Val_Loss: 21.2614\n",
      "Processing batch 2824/11884 - Val_Loss: 24.3475\n",
      "Processing batch 2825/11884 - Val_Loss: 23.5660\n",
      "Processing batch 2826/11884 - Val_Loss: 27.0203\n",
      "Processing batch 2827/11884 - Val_Loss: 24.8738\n",
      "Processing batch 2828/11884 - Val_Loss: 23.3473\n",
      "Processing batch 2829/11884 - Val_Loss: 24.7548\n",
      "Processing batch 2830/11884 - Val_Loss: 25.5301\n",
      "Processing batch 2831/11884 - Val_Loss: 28.2999\n",
      "Processing batch 2832/11884 - Val_Loss: 28.6436\n",
      "Processing batch 2833/11884 - Val_Loss: 23.7744\n",
      "Processing batch 2834/11884 - Val_Loss: 27.3414\n",
      "Processing batch 2835/11884 - Val_Loss: 24.2550\n",
      "Processing batch 2836/11884 - Val_Loss: 24.8516\n",
      "Processing batch 2837/11884 - Val_Loss: 24.0515\n",
      "Processing batch 2838/11884 - Val_Loss: 23.5146\n",
      "Processing batch 2839/11884 - Val_Loss: 24.2856\n",
      "Processing batch 2840/11884 - Val_Loss: 25.6791\n",
      "Processing batch 2841/11884 - Val_Loss: 25.5020\n",
      "Processing batch 2842/11884 - Val_Loss: 26.4925\n",
      "Processing batch 2843/11884 - Val_Loss: 23.6482\n",
      "Processing batch 2844/11884 - Val_Loss: 23.5207\n",
      "Processing batch 2845/11884 - Val_Loss: 26.6643\n",
      "Processing batch 2846/11884 - Val_Loss: 25.8204\n",
      "Processing batch 2847/11884 - Val_Loss: 26.7397\n",
      "Processing batch 2848/11884 - Val_Loss: 25.6458\n",
      "Processing batch 2849/11884 - Val_Loss: 22.0766\n",
      "Processing batch 2850/11884 - Val_Loss: 26.9663\n",
      "Processing batch 2851/11884 - Val_Loss: 23.4743\n",
      "Processing batch 2852/11884 - Val_Loss: 21.9825\n",
      "Processing batch 2853/11884 - Val_Loss: 27.5385\n",
      "Processing batch 2854/11884 - Val_Loss: 27.4585\n",
      "Processing batch 2855/11884 - Val_Loss: 28.1332\n",
      "Processing batch 2856/11884 - Val_Loss: 22.6879\n",
      "Processing batch 2857/11884 - Val_Loss: 28.5280\n",
      "Processing batch 2858/11884 - Val_Loss: 26.7632\n",
      "Processing batch 2859/11884 - Val_Loss: 26.8857\n",
      "Processing batch 2860/11884 - Val_Loss: 27.4783\n",
      "Processing batch 2861/11884 - Val_Loss: 24.9108\n",
      "Processing batch 2862/11884 - Val_Loss: 24.4470\n",
      "Processing batch 2863/11884 - Val_Loss: 27.6329\n",
      "Processing batch 2864/11884 - Val_Loss: 27.0339\n",
      "Processing batch 2865/11884 - Val_Loss: 28.5481\n",
      "Processing batch 2866/11884 - Val_Loss: 26.3887\n",
      "Processing batch 2867/11884 - Val_Loss: 24.8988\n",
      "Processing batch 2868/11884 - Val_Loss: 25.1976\n",
      "Processing batch 2869/11884 - Val_Loss: 26.5430\n",
      "Processing batch 2870/11884 - Val_Loss: 24.4322\n",
      "Processing batch 2871/11884 - Val_Loss: 30.2106\n",
      "Processing batch 2872/11884 - Val_Loss: 26.9079\n",
      "Processing batch 2873/11884 - Val_Loss: 26.2344\n",
      "Processing batch 2874/11884 - Val_Loss: 26.3437\n",
      "Processing batch 2875/11884 - Val_Loss: 23.1764\n",
      "Processing batch 2876/11884 - Val_Loss: 23.2699\n",
      "Processing batch 2877/11884 - Val_Loss: 22.2044\n",
      "Processing batch 2878/11884 - Val_Loss: 26.1580\n",
      "Processing batch 2879/11884 - Val_Loss: 26.0534\n",
      "Processing batch 2880/11884 - Val_Loss: 25.5220\n",
      "Processing batch 2881/11884 - Val_Loss: 26.6561\n",
      "Processing batch 2882/11884 - Val_Loss: 27.5162\n",
      "Processing batch 2883/11884 - Val_Loss: 27.0179\n",
      "Processing batch 2884/11884 - Val_Loss: 24.8534\n",
      "Processing batch 2885/11884 - Val_Loss: 26.5948\n",
      "Processing batch 2886/11884 - Val_Loss: 26.6680\n",
      "Processing batch 2887/11884 - Val_Loss: 27.9514\n",
      "Processing batch 2888/11884 - Val_Loss: 25.5570\n",
      "Processing batch 2889/11884 - Val_Loss: 23.7383\n",
      "Processing batch 2890/11884 - Val_Loss: 28.6807\n",
      "Processing batch 2891/11884 - Val_Loss: 25.6680\n",
      "Processing batch 2892/11884 - Val_Loss: 24.6523\n",
      "Processing batch 2893/11884 - Val_Loss: 28.3392\n",
      "Processing batch 2894/11884 - Val_Loss: 23.8553\n",
      "Processing batch 2895/11884 - Val_Loss: 25.2717\n",
      "Processing batch 2896/11884 - Val_Loss: 25.1500\n",
      "Processing batch 2897/11884 - Val_Loss: 24.7606\n",
      "Processing batch 2898/11884 - Val_Loss: 24.6282\n",
      "Processing batch 2899/11884 - Val_Loss: 26.9334\n",
      "Processing batch 2900/11884 - Val_Loss: 25.1113\n",
      "Processing batch 2901/11884 - Val_Loss: 21.4079\n",
      "Processing batch 2902/11884 - Val_Loss: 25.4355\n",
      "Processing batch 2903/11884 - Val_Loss: 26.9391\n",
      "Processing batch 2904/11884 - Val_Loss: 26.2712\n",
      "Processing batch 2905/11884 - Val_Loss: 24.9081\n",
      "Processing batch 2906/11884 - Val_Loss: 27.6044\n",
      "Processing batch 2907/11884 - Val_Loss: 27.1189\n",
      "Processing batch 2908/11884 - Val_Loss: 27.0232\n",
      "Processing batch 2909/11884 - Val_Loss: 26.3643\n",
      "Processing batch 2910/11884 - Val_Loss: 29.3525\n",
      "Processing batch 2911/11884 - Val_Loss: 26.2952\n",
      "Processing batch 2912/11884 - Val_Loss: 25.6655\n",
      "Processing batch 2913/11884 - Val_Loss: 26.1084\n",
      "Processing batch 2914/11884 - Val_Loss: 24.7266\n",
      "Processing batch 2915/11884 - Val_Loss: 28.8131\n",
      "Processing batch 2916/11884 - Val_Loss: 24.5310\n",
      "Processing batch 2917/11884 - Val_Loss: 24.3643\n",
      "Processing batch 2918/11884 - Val_Loss: 25.3748\n",
      "Processing batch 2919/11884 - Val_Loss: 28.7936\n",
      "Processing batch 2920/11884 - Val_Loss: 26.4919\n",
      "Processing batch 2921/11884 - Val_Loss: 25.6138\n",
      "Processing batch 2922/11884 - Val_Loss: 24.2254\n",
      "Processing batch 2923/11884 - Val_Loss: 26.7567\n",
      "Processing batch 2924/11884 - Val_Loss: 22.0703\n",
      "Processing batch 2925/11884 - Val_Loss: 26.0056\n",
      "Processing batch 2926/11884 - Val_Loss: 23.4975\n",
      "Processing batch 2927/11884 - Val_Loss: 23.2666\n",
      "Processing batch 2928/11884 - Val_Loss: 26.3331\n",
      "Processing batch 2929/11884 - Val_Loss: 27.1797\n",
      "Processing batch 2930/11884 - Val_Loss: 26.6921\n",
      "Processing batch 2931/11884 - Val_Loss: 24.7368\n",
      "Processing batch 2932/11884 - Val_Loss: 27.7350\n",
      "Processing batch 2933/11884 - Val_Loss: 25.2482\n",
      "Processing batch 2934/11884 - Val_Loss: 26.6645\n",
      "Processing batch 2935/11884 - Val_Loss: 27.1585\n",
      "Processing batch 2936/11884 - Val_Loss: 26.1712\n",
      "Processing batch 2937/11884 - Val_Loss: 22.0234\n",
      "Processing batch 2938/11884 - Val_Loss: 24.2679\n",
      "Processing batch 2939/11884 - Val_Loss: 24.7249\n",
      "Processing batch 2940/11884 - Val_Loss: 27.0504\n",
      "Processing batch 2941/11884 - Val_Loss: 26.7686\n",
      "Processing batch 2942/11884 - Val_Loss: 25.9061\n",
      "Processing batch 2943/11884 - Val_Loss: 26.6766\n",
      "Processing batch 2944/11884 - Val_Loss: 25.0299\n",
      "Processing batch 2945/11884 - Val_Loss: 24.6782\n",
      "Processing batch 2946/11884 - Val_Loss: 23.2031\n",
      "Processing batch 2947/11884 - Val_Loss: 26.6263\n",
      "Processing batch 2948/11884 - Val_Loss: 24.6863\n",
      "Processing batch 2949/11884 - Val_Loss: 24.6471\n",
      "Processing batch 2950/11884 - Val_Loss: 25.9180\n",
      "Processing batch 2951/11884 - Val_Loss: 26.1235\n",
      "Processing batch 2952/11884 - Val_Loss: 24.1369\n",
      "Processing batch 2953/11884 - Val_Loss: 22.4897\n",
      "Processing batch 2954/11884 - Val_Loss: 24.1260\n",
      "Processing batch 2955/11884 - Val_Loss: 25.0921\n",
      "Processing batch 2956/11884 - Val_Loss: 25.3595\n",
      "Processing batch 2957/11884 - Val_Loss: 23.8492\n",
      "Processing batch 2958/11884 - Val_Loss: 26.1259\n",
      "Processing batch 2959/11884 - Val_Loss: 22.6649\n",
      "Processing batch 2960/11884 - Val_Loss: 27.3625\n",
      "Processing batch 2961/11884 - Val_Loss: 25.7843\n",
      "Processing batch 2962/11884 - Val_Loss: 23.7393\n",
      "Processing batch 2963/11884 - Val_Loss: 26.5290\n",
      "Processing batch 2964/11884 - Val_Loss: 26.1840\n",
      "Processing batch 2965/11884 - Val_Loss: 25.1025\n",
      "Processing batch 2966/11884 - Val_Loss: 25.6890\n",
      "Processing batch 2967/11884 - Val_Loss: 25.5044\n",
      "Processing batch 2968/11884 - Val_Loss: 25.3609\n",
      "Processing batch 2969/11884 - Val_Loss: 25.6210\n",
      "Processing batch 2970/11884 - Val_Loss: 23.3567\n",
      "Processing batch 2971/11884 - Val_Loss: 25.4673\n",
      "Processing batch 2972/11884 - Val_Loss: 25.1564\n",
      "Processing batch 2973/11884 - Val_Loss: 27.4151\n",
      "Processing batch 2974/11884 - Val_Loss: 25.3713\n",
      "Processing batch 2975/11884 - Val_Loss: 24.9659\n",
      "Processing batch 2976/11884 - Val_Loss: 25.2661\n",
      "Processing batch 2977/11884 - Val_Loss: 22.4743\n",
      "Processing batch 2978/11884 - Val_Loss: 24.5071\n",
      "Processing batch 2979/11884 - Val_Loss: 25.4547\n",
      "Processing batch 2980/11884 - Val_Loss: 23.9780\n",
      "Processing batch 2981/11884 - Val_Loss: 28.9579\n",
      "Processing batch 2982/11884 - Val_Loss: 27.6154\n",
      "Processing batch 2983/11884 - Val_Loss: 31.0381\n",
      "Processing batch 2984/11884 - Val_Loss: 24.5713\n",
      "Processing batch 2985/11884 - Val_Loss: 25.8815\n",
      "Processing batch 2986/11884 - Val_Loss: 26.0820\n",
      "Processing batch 2987/11884 - Val_Loss: 26.1899\n",
      "Processing batch 2988/11884 - Val_Loss: 26.2960\n",
      "Processing batch 2989/11884 - Val_Loss: 24.5178\n",
      "Processing batch 2990/11884 - Val_Loss: 27.4128\n",
      "Processing batch 2991/11884 - Val_Loss: 26.0521\n",
      "Processing batch 2992/11884 - Val_Loss: 27.5106\n",
      "Processing batch 2993/11884 - Val_Loss: 27.6576\n",
      "Processing batch 2994/11884 - Val_Loss: 25.2668\n",
      "Processing batch 2995/11884 - Val_Loss: 26.9906\n",
      "Processing batch 2996/11884 - Val_Loss: 25.5241\n",
      "Processing batch 2997/11884 - Val_Loss: 22.9505\n",
      "Processing batch 2998/11884 - Val_Loss: 24.4475\n",
      "Processing batch 2999/11884 - Val_Loss: 27.0838\n",
      "Processing batch 3000/11884 - Val_Loss: 24.0939\n",
      "Processing batch 3001/11884 - Val_Loss: 26.4997\n",
      "Processing batch 3002/11884 - Val_Loss: 24.2355\n",
      "Processing batch 3003/11884 - Val_Loss: 24.4737\n",
      "Processing batch 3004/11884 - Val_Loss: 24.8331\n",
      "Processing batch 3005/11884 - Val_Loss: 23.8200\n",
      "Processing batch 3006/11884 - Val_Loss: 25.0935\n",
      "Processing batch 3007/11884 - Val_Loss: 26.5620\n",
      "Processing batch 3008/11884 - Val_Loss: 27.8411\n",
      "Processing batch 3009/11884 - Val_Loss: 26.3313\n",
      "Processing batch 3010/11884 - Val_Loss: 23.9743\n",
      "Processing batch 3011/11884 - Val_Loss: 26.5752\n",
      "Processing batch 3012/11884 - Val_Loss: 25.3526\n",
      "Processing batch 3013/11884 - Val_Loss: 24.2635\n",
      "Processing batch 3014/11884 - Val_Loss: 25.3932\n",
      "Processing batch 3015/11884 - Val_Loss: 24.6867\n",
      "Processing batch 3016/11884 - Val_Loss: 26.0900\n",
      "Processing batch 3017/11884 - Val_Loss: 25.2548\n",
      "Processing batch 3018/11884 - Val_Loss: 26.4285\n",
      "Processing batch 3019/11884 - Val_Loss: 22.7678\n",
      "Processing batch 3020/11884 - Val_Loss: 27.9058\n",
      "Processing batch 3021/11884 - Val_Loss: 27.0335\n",
      "Processing batch 3022/11884 - Val_Loss: 26.6886\n",
      "Processing batch 3023/11884 - Val_Loss: 25.2211\n",
      "Processing batch 3024/11884 - Val_Loss: 24.4139\n",
      "Processing batch 3025/11884 - Val_Loss: 24.6221\n",
      "Processing batch 3026/11884 - Val_Loss: 27.4257\n",
      "Processing batch 3027/11884 - Val_Loss: 29.7507\n",
      "Processing batch 3028/11884 - Val_Loss: 24.0456\n",
      "Processing batch 3029/11884 - Val_Loss: 27.8856\n",
      "Processing batch 3030/11884 - Val_Loss: 26.8805\n",
      "Processing batch 3031/11884 - Val_Loss: 26.5035\n",
      "Processing batch 3032/11884 - Val_Loss: 26.8531\n",
      "Processing batch 3033/11884 - Val_Loss: 26.3617\n",
      "Processing batch 3034/11884 - Val_Loss: 28.5234\n",
      "Processing batch 3035/11884 - Val_Loss: 27.2253\n",
      "Processing batch 3036/11884 - Val_Loss: 26.6280\n",
      "Processing batch 3037/11884 - Val_Loss: 23.3895\n",
      "Processing batch 3038/11884 - Val_Loss: 25.4439\n",
      "Processing batch 3039/11884 - Val_Loss: 29.0559\n",
      "Processing batch 3040/11884 - Val_Loss: 27.6193\n",
      "Processing batch 3041/11884 - Val_Loss: 25.8794\n",
      "Processing batch 3042/11884 - Val_Loss: 24.4796\n",
      "Processing batch 3043/11884 - Val_Loss: 25.4070\n",
      "Processing batch 3044/11884 - Val_Loss: 27.6015\n",
      "Processing batch 3045/11884 - Val_Loss: 26.9529\n",
      "Processing batch 3046/11884 - Val_Loss: 24.2696\n",
      "Processing batch 3047/11884 - Val_Loss: 25.3123\n",
      "Processing batch 3048/11884 - Val_Loss: 24.4761\n",
      "Processing batch 3049/11884 - Val_Loss: 26.9797\n",
      "Processing batch 3050/11884 - Val_Loss: 26.4668\n",
      "Processing batch 3051/11884 - Val_Loss: 26.4205\n",
      "Processing batch 3052/11884 - Val_Loss: 25.8398\n",
      "Processing batch 3053/11884 - Val_Loss: 26.0622\n",
      "Processing batch 3054/11884 - Val_Loss: 27.5404\n",
      "Processing batch 3055/11884 - Val_Loss: 24.7022\n",
      "Processing batch 3056/11884 - Val_Loss: 23.1697\n",
      "Processing batch 3057/11884 - Val_Loss: 26.8745\n",
      "Processing batch 3058/11884 - Val_Loss: 29.3887\n",
      "Processing batch 3059/11884 - Val_Loss: 29.7772\n",
      "Processing batch 3060/11884 - Val_Loss: 24.9557\n",
      "Processing batch 3061/11884 - Val_Loss: 24.4449\n",
      "Processing batch 3062/11884 - Val_Loss: 26.0816\n",
      "Processing batch 3063/11884 - Val_Loss: 26.1517\n",
      "Processing batch 3064/11884 - Val_Loss: 26.2033\n",
      "Processing batch 3065/11884 - Val_Loss: 24.2209\n",
      "Processing batch 3066/11884 - Val_Loss: 24.6210\n",
      "Processing batch 3067/11884 - Val_Loss: 26.8453\n",
      "Processing batch 3068/11884 - Val_Loss: 26.2505\n",
      "Processing batch 3069/11884 - Val_Loss: 25.7958\n",
      "Processing batch 3070/11884 - Val_Loss: 24.5393\n",
      "Processing batch 3071/11884 - Val_Loss: 24.6793\n",
      "Processing batch 3072/11884 - Val_Loss: 37.0723\n",
      "Processing batch 3073/11884 - Val_Loss: 26.2505\n",
      "Processing batch 3074/11884 - Val_Loss: 29.1259\n",
      "Processing batch 3075/11884 - Val_Loss: 27.4298\n",
      "Processing batch 3076/11884 - Val_Loss: 24.4404\n",
      "Processing batch 3077/11884 - Val_Loss: 23.7029\n",
      "Processing batch 3078/11884 - Val_Loss: 25.1132\n",
      "Processing batch 3079/11884 - Val_Loss: 28.9535\n",
      "Processing batch 3080/11884 - Val_Loss: 25.3963\n",
      "Processing batch 3081/11884 - Val_Loss: 28.5103\n",
      "Processing batch 3082/11884 - Val_Loss: 22.6350\n",
      "Processing batch 3083/11884 - Val_Loss: 25.3721\n",
      "Processing batch 3084/11884 - Val_Loss: 26.3060\n",
      "Processing batch 3085/11884 - Val_Loss: 23.8504\n",
      "Processing batch 3086/11884 - Val_Loss: 24.9234\n",
      "Processing batch 3087/11884 - Val_Loss: 25.0014\n",
      "Processing batch 3088/11884 - Val_Loss: 25.6119\n",
      "Processing batch 3089/11884 - Val_Loss: 26.2326\n",
      "Processing batch 3090/11884 - Val_Loss: 27.3274\n",
      "Processing batch 3091/11884 - Val_Loss: 28.7754\n",
      "Processing batch 3092/11884 - Val_Loss: 27.3742\n",
      "Processing batch 3093/11884 - Val_Loss: 25.1437\n",
      "Processing batch 3094/11884 - Val_Loss: 24.4847\n",
      "Processing batch 3095/11884 - Val_Loss: 24.2122\n",
      "Processing batch 3096/11884 - Val_Loss: 24.5504\n",
      "Processing batch 3097/11884 - Val_Loss: 22.6898\n",
      "Processing batch 3098/11884 - Val_Loss: 24.7134\n",
      "Processing batch 3099/11884 - Val_Loss: 26.2991\n",
      "Processing batch 3100/11884 - Val_Loss: 23.7979\n",
      "Processing batch 3101/11884 - Val_Loss: 26.3433\n",
      "Processing batch 3102/11884 - Val_Loss: 26.5144\n",
      "Processing batch 3103/11884 - Val_Loss: 24.5488\n",
      "Processing batch 3104/11884 - Val_Loss: 26.0216\n",
      "Processing batch 3105/11884 - Val_Loss: 25.8751\n",
      "Processing batch 3106/11884 - Val_Loss: 24.2713\n",
      "Processing batch 3107/11884 - Val_Loss: 23.0300\n",
      "Processing batch 3108/11884 - Val_Loss: 25.3719\n",
      "Processing batch 3109/11884 - Val_Loss: 27.6891\n",
      "Processing batch 3110/11884 - Val_Loss: 26.5034\n",
      "Processing batch 3111/11884 - Val_Loss: 22.0305\n",
      "Processing batch 3112/11884 - Val_Loss: 25.8269\n",
      "Processing batch 3113/11884 - Val_Loss: 24.2126\n",
      "Processing batch 3114/11884 - Val_Loss: 27.5198\n",
      "Processing batch 3115/11884 - Val_Loss: 25.6280\n",
      "Processing batch 3116/11884 - Val_Loss: 26.5850\n",
      "Processing batch 3117/11884 - Val_Loss: 26.4125\n",
      "Processing batch 3118/11884 - Val_Loss: 26.2645\n",
      "Processing batch 3119/11884 - Val_Loss: 27.3010\n",
      "Processing batch 3120/11884 - Val_Loss: 27.4094\n",
      "Processing batch 3121/11884 - Val_Loss: 26.6435\n",
      "Processing batch 3122/11884 - Val_Loss: 22.4276\n",
      "Processing batch 3123/11884 - Val_Loss: 23.6982\n",
      "Processing batch 3124/11884 - Val_Loss: 25.9794\n",
      "Processing batch 3125/11884 - Val_Loss: 26.1081\n",
      "Processing batch 3126/11884 - Val_Loss: 23.7155\n",
      "Processing batch 3127/11884 - Val_Loss: 24.6268\n",
      "Processing batch 3128/11884 - Val_Loss: 24.5975\n",
      "Processing batch 3129/11884 - Val_Loss: 29.8989\n",
      "Processing batch 3130/11884 - Val_Loss: 25.1016\n",
      "Processing batch 3131/11884 - Val_Loss: 24.3188\n",
      "Processing batch 3132/11884 - Val_Loss: 26.0956\n",
      "Processing batch 3133/11884 - Val_Loss: 26.2613\n",
      "Processing batch 3134/11884 - Val_Loss: 24.9251\n",
      "Processing batch 3135/11884 - Val_Loss: 25.5380\n",
      "Processing batch 3136/11884 - Val_Loss: 25.4100\n",
      "Processing batch 3137/11884 - Val_Loss: 26.6008\n",
      "Processing batch 3138/11884 - Val_Loss: 25.8946\n",
      "Processing batch 3139/11884 - Val_Loss: 24.7950\n",
      "Processing batch 3140/11884 - Val_Loss: 25.3881\n",
      "Processing batch 3141/11884 - Val_Loss: 25.4842\n",
      "Processing batch 3142/11884 - Val_Loss: 24.1637\n",
      "Processing batch 3143/11884 - Val_Loss: 25.6696\n",
      "Processing batch 3144/11884 - Val_Loss: 24.5426\n",
      "Processing batch 3145/11884 - Val_Loss: 24.0342\n",
      "Processing batch 3146/11884 - Val_Loss: 26.4921\n",
      "Processing batch 3147/11884 - Val_Loss: 24.0314\n",
      "Processing batch 3148/11884 - Val_Loss: 25.7840\n",
      "Processing batch 3149/11884 - Val_Loss: 28.3413\n",
      "Processing batch 3150/11884 - Val_Loss: 25.8777\n",
      "Processing batch 3151/11884 - Val_Loss: 27.2733\n",
      "Processing batch 3152/11884 - Val_Loss: 24.0025\n",
      "Processing batch 3153/11884 - Val_Loss: 24.3855\n",
      "Processing batch 3154/11884 - Val_Loss: 25.8354\n",
      "Processing batch 3155/11884 - Val_Loss: 24.6191\n",
      "Processing batch 3156/11884 - Val_Loss: 25.0036\n",
      "Processing batch 3157/11884 - Val_Loss: 26.0977\n",
      "Processing batch 3158/11884 - Val_Loss: 24.4765\n",
      "Processing batch 3159/11884 - Val_Loss: 28.2054\n",
      "Processing batch 3160/11884 - Val_Loss: 21.8066\n",
      "Processing batch 3161/11884 - Val_Loss: 23.9138\n",
      "Processing batch 3162/11884 - Val_Loss: 27.1251\n",
      "Processing batch 3163/11884 - Val_Loss: 26.1088\n",
      "Processing batch 3164/11884 - Val_Loss: 26.8213\n",
      "Processing batch 3165/11884 - Val_Loss: 28.6209\n",
      "Processing batch 3166/11884 - Val_Loss: 25.9266\n",
      "Processing batch 3167/11884 - Val_Loss: 26.1397\n",
      "Processing batch 3168/11884 - Val_Loss: 27.6161\n",
      "Processing batch 3169/11884 - Val_Loss: 26.5588\n",
      "Processing batch 3170/11884 - Val_Loss: 29.2596\n",
      "Processing batch 3171/11884 - Val_Loss: 23.2347\n",
      "Processing batch 3172/11884 - Val_Loss: 26.7580\n",
      "Processing batch 3173/11884 - Val_Loss: 24.2363\n",
      "Processing batch 3174/11884 - Val_Loss: 25.4689\n",
      "Processing batch 3175/11884 - Val_Loss: 23.9007\n",
      "Processing batch 3176/11884 - Val_Loss: 28.2777\n",
      "Processing batch 3177/11884 - Val_Loss: 23.3932\n",
      "Processing batch 3178/11884 - Val_Loss: 26.8429\n",
      "Processing batch 3179/11884 - Val_Loss: 25.1477\n",
      "Processing batch 3180/11884 - Val_Loss: 25.8434\n",
      "Processing batch 3181/11884 - Val_Loss: 26.2235\n",
      "Processing batch 3182/11884 - Val_Loss: 24.2650\n",
      "Processing batch 3183/11884 - Val_Loss: 27.1332\n",
      "Processing batch 3184/11884 - Val_Loss: 28.2362\n",
      "Processing batch 3185/11884 - Val_Loss: 26.5604\n",
      "Processing batch 3186/11884 - Val_Loss: 24.5545\n",
      "Processing batch 3187/11884 - Val_Loss: 25.1022\n",
      "Processing batch 3188/11884 - Val_Loss: 28.0564\n",
      "Processing batch 3189/11884 - Val_Loss: 25.2368\n",
      "Processing batch 3190/11884 - Val_Loss: 24.0179\n",
      "Processing batch 3191/11884 - Val_Loss: 25.8083\n",
      "Processing batch 3192/11884 - Val_Loss: 25.7612\n",
      "Processing batch 3193/11884 - Val_Loss: 24.2894\n",
      "Processing batch 3194/11884 - Val_Loss: 27.0243\n",
      "Processing batch 3195/11884 - Val_Loss: 23.6887\n",
      "Processing batch 3196/11884 - Val_Loss: 22.0529\n",
      "Processing batch 3197/11884 - Val_Loss: 27.1995\n",
      "Processing batch 3198/11884 - Val_Loss: 24.9439\n",
      "Processing batch 3199/11884 - Val_Loss: 26.2419\n",
      "Processing batch 3200/11884 - Val_Loss: 25.5388\n",
      "Processing batch 3201/11884 - Val_Loss: 26.1018\n",
      "Processing batch 3202/11884 - Val_Loss: 24.5447\n",
      "Processing batch 3203/11884 - Val_Loss: 25.4916\n",
      "Processing batch 3204/11884 - Val_Loss: 26.4640\n",
      "Processing batch 3205/11884 - Val_Loss: 28.0724\n",
      "Processing batch 3206/11884 - Val_Loss: 26.7596\n",
      "Processing batch 3207/11884 - Val_Loss: 29.8765\n",
      "Processing batch 3208/11884 - Val_Loss: 27.2902\n",
      "Processing batch 3209/11884 - Val_Loss: 26.4317\n",
      "Processing batch 3210/11884 - Val_Loss: 25.7737\n",
      "Processing batch 3211/11884 - Val_Loss: 23.7879\n",
      "Processing batch 3212/11884 - Val_Loss: 25.1308\n",
      "Processing batch 3213/11884 - Val_Loss: 26.6886\n",
      "Processing batch 3214/11884 - Val_Loss: 24.9788\n",
      "Processing batch 3215/11884 - Val_Loss: 26.6310\n",
      "Processing batch 3216/11884 - Val_Loss: 27.0103\n",
      "Processing batch 3217/11884 - Val_Loss: 25.6144\n",
      "Processing batch 3218/11884 - Val_Loss: 29.1043\n",
      "Processing batch 3219/11884 - Val_Loss: 26.6649\n",
      "Processing batch 3220/11884 - Val_Loss: 24.8063\n",
      "Processing batch 3221/11884 - Val_Loss: 24.8379\n",
      "Processing batch 3222/11884 - Val_Loss: 27.3598\n",
      "Processing batch 3223/11884 - Val_Loss: 25.6903\n",
      "Processing batch 3224/11884 - Val_Loss: 24.6599\n",
      "Processing batch 3225/11884 - Val_Loss: 28.0890\n",
      "Processing batch 3226/11884 - Val_Loss: 24.3686\n",
      "Processing batch 3227/11884 - Val_Loss: 27.1613\n",
      "Processing batch 3228/11884 - Val_Loss: 27.0363\n",
      "Processing batch 3229/11884 - Val_Loss: 27.0799\n",
      "Processing batch 3230/11884 - Val_Loss: 26.6193\n",
      "Processing batch 3231/11884 - Val_Loss: 24.5662\n",
      "Processing batch 3232/11884 - Val_Loss: 22.6232\n",
      "Processing batch 3233/11884 - Val_Loss: 27.4427\n",
      "Processing batch 3234/11884 - Val_Loss: 26.7196\n",
      "Processing batch 3235/11884 - Val_Loss: 29.3882\n",
      "Processing batch 3236/11884 - Val_Loss: 26.8553\n",
      "Processing batch 3237/11884 - Val_Loss: 26.8056\n",
      "Processing batch 3238/11884 - Val_Loss: 27.8746\n",
      "Processing batch 3239/11884 - Val_Loss: 24.5343\n",
      "Processing batch 3240/11884 - Val_Loss: 26.6524\n",
      "Processing batch 3241/11884 - Val_Loss: 22.5808\n",
      "Processing batch 3242/11884 - Val_Loss: 27.5392\n",
      "Processing batch 3243/11884 - Val_Loss: 26.3642\n",
      "Processing batch 3244/11884 - Val_Loss: 23.1187\n",
      "Processing batch 3245/11884 - Val_Loss: 23.2311\n",
      "Processing batch 3246/11884 - Val_Loss: 26.7417\n",
      "Processing batch 3247/11884 - Val_Loss: 28.5509\n",
      "Processing batch 3248/11884 - Val_Loss: 25.3599\n",
      "Processing batch 3249/11884 - Val_Loss: 26.2299\n",
      "Processing batch 3250/11884 - Val_Loss: 24.5730\n",
      "Processing batch 3251/11884 - Val_Loss: 23.5603\n",
      "Processing batch 3252/11884 - Val_Loss: 23.9769\n",
      "Processing batch 3253/11884 - Val_Loss: 29.3819\n",
      "Processing batch 3254/11884 - Val_Loss: 26.7792\n",
      "Processing batch 3255/11884 - Val_Loss: 27.2632\n",
      "Processing batch 3256/11884 - Val_Loss: 23.8913\n",
      "Processing batch 3257/11884 - Val_Loss: 27.8835\n",
      "Processing batch 3258/11884 - Val_Loss: 27.3905\n",
      "Processing batch 3259/11884 - Val_Loss: 23.5405\n",
      "Processing batch 3260/11884 - Val_Loss: 27.3225\n",
      "Processing batch 3261/11884 - Val_Loss: 27.0283\n",
      "Processing batch 3262/11884 - Val_Loss: 27.2670\n",
      "Processing batch 3263/11884 - Val_Loss: 23.8204\n",
      "Processing batch 3264/11884 - Val_Loss: 26.8548\n",
      "Processing batch 3265/11884 - Val_Loss: 26.1083\n",
      "Processing batch 3266/11884 - Val_Loss: 24.9463\n",
      "Processing batch 3267/11884 - Val_Loss: 24.9197\n",
      "Processing batch 3268/11884 - Val_Loss: 26.4441\n",
      "Processing batch 3269/11884 - Val_Loss: 22.9932\n",
      "Processing batch 3270/11884 - Val_Loss: 23.9209\n",
      "Processing batch 3271/11884 - Val_Loss: 29.5055\n",
      "Processing batch 3272/11884 - Val_Loss: 28.3445\n",
      "Processing batch 3273/11884 - Val_Loss: 28.7767\n",
      "Processing batch 3274/11884 - Val_Loss: 24.3725\n",
      "Processing batch 3275/11884 - Val_Loss: 26.6132\n",
      "Processing batch 3276/11884 - Val_Loss: 25.8243\n",
      "Processing batch 3277/11884 - Val_Loss: 26.1782\n",
      "Processing batch 3278/11884 - Val_Loss: 29.9003\n",
      "Processing batch 3279/11884 - Val_Loss: 25.8645\n",
      "Processing batch 3280/11884 - Val_Loss: 26.4722\n",
      "Processing batch 3281/11884 - Val_Loss: 24.4220\n",
      "Processing batch 3282/11884 - Val_Loss: 22.4942\n",
      "Processing batch 3283/11884 - Val_Loss: 26.8356\n",
      "Processing batch 3284/11884 - Val_Loss: 23.4635\n",
      "Processing batch 3285/11884 - Val_Loss: 27.0805\n",
      "Processing batch 3286/11884 - Val_Loss: 24.1405\n",
      "Processing batch 3287/11884 - Val_Loss: 24.4084\n",
      "Processing batch 3288/11884 - Val_Loss: 26.0642\n",
      "Processing batch 3289/11884 - Val_Loss: 24.7607\n",
      "Processing batch 3290/11884 - Val_Loss: 26.5863\n",
      "Processing batch 3291/11884 - Val_Loss: 25.7542\n",
      "Processing batch 3292/11884 - Val_Loss: 25.2372\n",
      "Processing batch 3293/11884 - Val_Loss: 24.6614\n",
      "Processing batch 3294/11884 - Val_Loss: 26.1493\n",
      "Processing batch 3295/11884 - Val_Loss: 25.6576\n",
      "Processing batch 3296/11884 - Val_Loss: 23.7880\n",
      "Processing batch 3297/11884 - Val_Loss: 24.7017\n",
      "Processing batch 3298/11884 - Val_Loss: 25.1428\n",
      "Processing batch 3299/11884 - Val_Loss: 25.5716\n",
      "Processing batch 3300/11884 - Val_Loss: 30.0561\n",
      "Processing batch 3301/11884 - Val_Loss: 24.1960\n",
      "Processing batch 3302/11884 - Val_Loss: 24.0851\n",
      "Processing batch 3303/11884 - Val_Loss: 24.3181\n",
      "Processing batch 3304/11884 - Val_Loss: 25.8846\n",
      "Processing batch 3305/11884 - Val_Loss: 28.1999\n",
      "Processing batch 3306/11884 - Val_Loss: 26.6553\n",
      "Processing batch 3307/11884 - Val_Loss: 25.9446\n",
      "Processing batch 3308/11884 - Val_Loss: 24.0960\n",
      "Processing batch 3309/11884 - Val_Loss: 26.1984\n",
      "Processing batch 3310/11884 - Val_Loss: 29.0858\n",
      "Processing batch 3311/11884 - Val_Loss: 26.3108\n",
      "Processing batch 3312/11884 - Val_Loss: 24.4953\n",
      "Processing batch 3313/11884 - Val_Loss: 22.9346\n",
      "Processing batch 3314/11884 - Val_Loss: 24.3780\n",
      "Processing batch 3315/11884 - Val_Loss: 23.9621\n",
      "Processing batch 3316/11884 - Val_Loss: 23.8065\n",
      "Processing batch 3317/11884 - Val_Loss: 28.3718\n",
      "Processing batch 3318/11884 - Val_Loss: 25.6670\n",
      "Processing batch 3319/11884 - Val_Loss: 25.5014\n",
      "Processing batch 3320/11884 - Val_Loss: 27.5716\n",
      "Processing batch 3321/11884 - Val_Loss: 23.9890\n",
      "Processing batch 3322/11884 - Val_Loss: 26.4871\n",
      "Processing batch 3323/11884 - Val_Loss: 25.6274\n",
      "Processing batch 3324/11884 - Val_Loss: 23.8457\n",
      "Processing batch 3325/11884 - Val_Loss: 26.6784\n",
      "Processing batch 3326/11884 - Val_Loss: 24.9919\n",
      "Processing batch 3327/11884 - Val_Loss: 24.9402\n",
      "Processing batch 3328/11884 - Val_Loss: 25.7240\n",
      "Processing batch 3329/11884 - Val_Loss: 26.2496\n",
      "Processing batch 3330/11884 - Val_Loss: 25.5888\n",
      "Processing batch 3331/11884 - Val_Loss: 27.8769\n",
      "Processing batch 3332/11884 - Val_Loss: 24.5055\n",
      "Processing batch 3333/11884 - Val_Loss: 29.0247\n",
      "Processing batch 3334/11884 - Val_Loss: 25.8670\n",
      "Processing batch 3335/11884 - Val_Loss: 28.6915\n",
      "Processing batch 3336/11884 - Val_Loss: 23.5109\n",
      "Processing batch 3337/11884 - Val_Loss: 24.7035\n",
      "Processing batch 3338/11884 - Val_Loss: 26.9908\n",
      "Processing batch 3339/11884 - Val_Loss: 25.1560\n",
      "Processing batch 3340/11884 - Val_Loss: 26.5062\n",
      "Processing batch 3341/11884 - Val_Loss: 26.1406\n",
      "Processing batch 3342/11884 - Val_Loss: 25.9222\n",
      "Processing batch 3343/11884 - Val_Loss: 28.2803\n",
      "Processing batch 3344/11884 - Val_Loss: 28.1017\n",
      "Processing batch 3345/11884 - Val_Loss: 27.4990\n",
      "Processing batch 3346/11884 - Val_Loss: 22.9616\n",
      "Processing batch 3347/11884 - Val_Loss: 26.6980\n",
      "Processing batch 3348/11884 - Val_Loss: 24.1013\n",
      "Processing batch 3349/11884 - Val_Loss: 23.6504\n",
      "Processing batch 3350/11884 - Val_Loss: 24.8251\n",
      "Processing batch 3351/11884 - Val_Loss: 24.8816\n",
      "Processing batch 3352/11884 - Val_Loss: 25.6958\n",
      "Processing batch 3353/11884 - Val_Loss: 26.1660\n",
      "Processing batch 3354/11884 - Val_Loss: 24.7355\n",
      "Processing batch 3355/11884 - Val_Loss: 23.9741\n",
      "Processing batch 3356/11884 - Val_Loss: 24.3785\n",
      "Processing batch 3357/11884 - Val_Loss: 22.8594\n",
      "Processing batch 3358/11884 - Val_Loss: 25.3897\n",
      "Processing batch 3359/11884 - Val_Loss: 25.3309\n",
      "Processing batch 3360/11884 - Val_Loss: 25.1512\n",
      "Processing batch 3361/11884 - Val_Loss: 26.9418\n",
      "Processing batch 3362/11884 - Val_Loss: 23.5952\n",
      "Processing batch 3363/11884 - Val_Loss: 25.9734\n",
      "Processing batch 3364/11884 - Val_Loss: 26.0260\n",
      "Processing batch 3365/11884 - Val_Loss: 28.7215\n",
      "Processing batch 3366/11884 - Val_Loss: 26.8488\n",
      "Processing batch 3367/11884 - Val_Loss: 25.9375\n",
      "Processing batch 3368/11884 - Val_Loss: 24.0655\n",
      "Processing batch 3369/11884 - Val_Loss: 26.0464\n",
      "Processing batch 3370/11884 - Val_Loss: 26.9205\n",
      "Processing batch 3371/11884 - Val_Loss: 24.9220\n",
      "Processing batch 3372/11884 - Val_Loss: 24.1233\n",
      "Processing batch 3373/11884 - Val_Loss: 25.4278\n",
      "Processing batch 3374/11884 - Val_Loss: 28.9159\n",
      "Processing batch 3375/11884 - Val_Loss: 26.5939\n",
      "Processing batch 3376/11884 - Val_Loss: 24.2276\n",
      "Processing batch 3377/11884 - Val_Loss: 28.5792\n",
      "Processing batch 3378/11884 - Val_Loss: 27.5063\n",
      "Processing batch 3379/11884 - Val_Loss: 28.0738\n",
      "Processing batch 3380/11884 - Val_Loss: 24.6991\n",
      "Processing batch 3381/11884 - Val_Loss: 25.1705\n",
      "Processing batch 3382/11884 - Val_Loss: 27.3870\n",
      "Processing batch 3383/11884 - Val_Loss: 24.2605\n",
      "Processing batch 3384/11884 - Val_Loss: 27.0065\n",
      "Processing batch 3385/11884 - Val_Loss: 25.1456\n",
      "Processing batch 3386/11884 - Val_Loss: 25.2554\n",
      "Processing batch 3387/11884 - Val_Loss: 23.6276\n",
      "Processing batch 3388/11884 - Val_Loss: 26.5237\n",
      "Processing batch 3389/11884 - Val_Loss: 25.7401\n",
      "Processing batch 3390/11884 - Val_Loss: 25.4528\n",
      "Processing batch 3391/11884 - Val_Loss: 23.8779\n",
      "Processing batch 3392/11884 - Val_Loss: 26.4463\n",
      "Processing batch 3393/11884 - Val_Loss: 23.5260\n",
      "Processing batch 3394/11884 - Val_Loss: 25.3455\n",
      "Processing batch 3395/11884 - Val_Loss: 26.1693\n",
      "Processing batch 3396/11884 - Val_Loss: 25.9875\n",
      "Processing batch 3397/11884 - Val_Loss: 26.7903\n",
      "Processing batch 3398/11884 - Val_Loss: 25.7328\n",
      "Processing batch 3399/11884 - Val_Loss: 28.1972\n",
      "Processing batch 3400/11884 - Val_Loss: 26.2712\n",
      "Processing batch 3401/11884 - Val_Loss: 26.0414\n",
      "Processing batch 3402/11884 - Val_Loss: 25.5827\n",
      "Processing batch 3403/11884 - Val_Loss: 26.4152\n",
      "Processing batch 3404/11884 - Val_Loss: 25.4978\n",
      "Processing batch 3405/11884 - Val_Loss: 27.5041\n",
      "Processing batch 3406/11884 - Val_Loss: 24.4837\n",
      "Processing batch 3407/11884 - Val_Loss: 25.9015\n",
      "Processing batch 3408/11884 - Val_Loss: 24.3493\n",
      "Processing batch 3409/11884 - Val_Loss: 25.3350\n",
      "Processing batch 3410/11884 - Val_Loss: 25.9386\n",
      "Processing batch 3411/11884 - Val_Loss: 25.3202\n",
      "Processing batch 3412/11884 - Val_Loss: 24.8143\n",
      "Processing batch 3413/11884 - Val_Loss: 26.9382\n",
      "Processing batch 3414/11884 - Val_Loss: 24.6229\n",
      "Processing batch 3415/11884 - Val_Loss: 26.9202\n",
      "Processing batch 3416/11884 - Val_Loss: 27.4890\n",
      "Processing batch 3417/11884 - Val_Loss: 22.8485\n",
      "Processing batch 3418/11884 - Val_Loss: 23.4585\n",
      "Processing batch 3419/11884 - Val_Loss: 27.8753\n",
      "Processing batch 3420/11884 - Val_Loss: 25.3506\n",
      "Processing batch 3421/11884 - Val_Loss: 28.7123\n",
      "Processing batch 3422/11884 - Val_Loss: 24.6859\n",
      "Processing batch 3423/11884 - Val_Loss: 25.4197\n",
      "Processing batch 3424/11884 - Val_Loss: 27.6738\n",
      "Processing batch 3425/11884 - Val_Loss: 24.9707\n",
      "Processing batch 3426/11884 - Val_Loss: 26.3203\n",
      "Processing batch 3427/11884 - Val_Loss: 25.1746\n",
      "Processing batch 3428/11884 - Val_Loss: 23.2497\n",
      "Processing batch 3429/11884 - Val_Loss: 28.1449\n",
      "Processing batch 3430/11884 - Val_Loss: 22.8988\n",
      "Processing batch 3431/11884 - Val_Loss: 27.1095\n",
      "Processing batch 3432/11884 - Val_Loss: 25.5196\n",
      "Processing batch 3433/11884 - Val_Loss: 24.0938\n",
      "Processing batch 3434/11884 - Val_Loss: 24.8829\n",
      "Processing batch 3435/11884 - Val_Loss: 24.8767\n",
      "Processing batch 3436/11884 - Val_Loss: 22.5460\n",
      "Processing batch 3437/11884 - Val_Loss: 24.7093\n",
      "Processing batch 3438/11884 - Val_Loss: 24.7450\n",
      "Processing batch 3439/11884 - Val_Loss: 23.5034\n",
      "Processing batch 3440/11884 - Val_Loss: 26.1805\n",
      "Processing batch 3441/11884 - Val_Loss: 22.8437\n",
      "Processing batch 3442/11884 - Val_Loss: 29.3011\n",
      "Processing batch 3443/11884 - Val_Loss: 27.9779\n",
      "Processing batch 3444/11884 - Val_Loss: 26.1995\n",
      "Processing batch 3445/11884 - Val_Loss: 25.4561\n",
      "Processing batch 3446/11884 - Val_Loss: 25.0672\n",
      "Processing batch 3447/11884 - Val_Loss: 26.7984\n",
      "Processing batch 3448/11884 - Val_Loss: 24.5189\n",
      "Processing batch 3449/11884 - Val_Loss: 25.4003\n",
      "Processing batch 3450/11884 - Val_Loss: 23.3329\n",
      "Processing batch 3451/11884 - Val_Loss: 27.1117\n",
      "Processing batch 3452/11884 - Val_Loss: 22.6999\n",
      "Processing batch 3453/11884 - Val_Loss: 25.3114\n",
      "Processing batch 3454/11884 - Val_Loss: 28.4909\n",
      "Processing batch 3455/11884 - Val_Loss: 25.1185\n",
      "Processing batch 3456/11884 - Val_Loss: 28.4883\n",
      "Processing batch 3457/11884 - Val_Loss: 27.0266\n",
      "Processing batch 3458/11884 - Val_Loss: 24.1657\n",
      "Processing batch 3459/11884 - Val_Loss: 25.4944\n",
      "Processing batch 3460/11884 - Val_Loss: 26.0247\n",
      "Processing batch 3461/11884 - Val_Loss: 23.3018\n",
      "Processing batch 3462/11884 - Val_Loss: 26.9784\n",
      "Processing batch 3463/11884 - Val_Loss: 28.5138\n",
      "Processing batch 3464/11884 - Val_Loss: 25.4380\n",
      "Processing batch 3465/11884 - Val_Loss: 27.9161\n",
      "Processing batch 3466/11884 - Val_Loss: 27.4586\n",
      "Processing batch 3467/11884 - Val_Loss: 29.3443\n",
      "Processing batch 3468/11884 - Val_Loss: 24.5563\n",
      "Processing batch 3469/11884 - Val_Loss: 27.1098\n",
      "Processing batch 3470/11884 - Val_Loss: 23.6365\n",
      "Processing batch 3471/11884 - Val_Loss: 25.1745\n",
      "Processing batch 3472/11884 - Val_Loss: 26.1337\n",
      "Processing batch 3473/11884 - Val_Loss: 25.9322\n",
      "Processing batch 3474/11884 - Val_Loss: 24.8751\n",
      "Processing batch 3475/11884 - Val_Loss: 24.2295\n",
      "Processing batch 3476/11884 - Val_Loss: 28.6971\n",
      "Processing batch 3477/11884 - Val_Loss: 27.8016\n",
      "Processing batch 3478/11884 - Val_Loss: 26.8048\n",
      "Processing batch 3479/11884 - Val_Loss: 27.5726\n",
      "Processing batch 3480/11884 - Val_Loss: 29.1108\n",
      "Processing batch 3481/11884 - Val_Loss: 26.7843\n",
      "Processing batch 3482/11884 - Val_Loss: 26.1825\n",
      "Processing batch 3483/11884 - Val_Loss: 24.0354\n",
      "Processing batch 3484/11884 - Val_Loss: 26.5954\n",
      "Processing batch 3485/11884 - Val_Loss: 26.0509\n",
      "Processing batch 3486/11884 - Val_Loss: 27.1890\n",
      "Processing batch 3487/11884 - Val_Loss: 26.4236\n",
      "Processing batch 3488/11884 - Val_Loss: 29.0231\n",
      "Processing batch 3489/11884 - Val_Loss: 27.7269\n",
      "Processing batch 3490/11884 - Val_Loss: 24.7938\n",
      "Processing batch 3491/11884 - Val_Loss: 27.4566\n",
      "Processing batch 3492/11884 - Val_Loss: 29.1181\n",
      "Processing batch 3493/11884 - Val_Loss: 28.5170\n",
      "Processing batch 3494/11884 - Val_Loss: 28.4377\n",
      "Processing batch 3495/11884 - Val_Loss: 23.2533\n",
      "Processing batch 3496/11884 - Val_Loss: 25.7623\n",
      "Processing batch 3497/11884 - Val_Loss: 25.5458\n",
      "Processing batch 3498/11884 - Val_Loss: 27.7750\n",
      "Processing batch 3499/11884 - Val_Loss: 26.0066\n",
      "Processing batch 3500/11884 - Val_Loss: 24.9932\n",
      "Processing batch 3501/11884 - Val_Loss: 23.3768\n",
      "Processing batch 3502/11884 - Val_Loss: 27.5832\n",
      "Processing batch 3503/11884 - Val_Loss: 25.6645\n",
      "Processing batch 3504/11884 - Val_Loss: 27.7911\n",
      "Processing batch 3505/11884 - Val_Loss: 25.7536\n",
      "Processing batch 3506/11884 - Val_Loss: 23.4242\n",
      "Processing batch 3507/11884 - Val_Loss: 26.8739\n",
      "Processing batch 3508/11884 - Val_Loss: 25.0811\n",
      "Processing batch 3509/11884 - Val_Loss: 27.5582\n",
      "Processing batch 3510/11884 - Val_Loss: 25.3908\n",
      "Processing batch 3511/11884 - Val_Loss: 24.1647\n",
      "Processing batch 3512/11884 - Val_Loss: 26.5987\n",
      "Processing batch 3513/11884 - Val_Loss: 27.4044\n",
      "Processing batch 3514/11884 - Val_Loss: 29.5775\n",
      "Processing batch 3515/11884 - Val_Loss: 25.0910\n",
      "Processing batch 3516/11884 - Val_Loss: 27.3153\n",
      "Processing batch 3517/11884 - Val_Loss: 25.7643\n",
      "Processing batch 3518/11884 - Val_Loss: 27.1890\n",
      "Processing batch 3519/11884 - Val_Loss: 24.8264\n",
      "Processing batch 3520/11884 - Val_Loss: 24.6568\n",
      "Processing batch 3521/11884 - Val_Loss: 25.2775\n",
      "Processing batch 3522/11884 - Val_Loss: 25.9559\n",
      "Processing batch 3523/11884 - Val_Loss: 28.0814\n",
      "Processing batch 3524/11884 - Val_Loss: 24.7716\n",
      "Processing batch 3525/11884 - Val_Loss: 26.0201\n",
      "Processing batch 3526/11884 - Val_Loss: 27.2732\n",
      "Processing batch 3527/11884 - Val_Loss: 25.8038\n",
      "Processing batch 3528/11884 - Val_Loss: 26.5871\n",
      "Processing batch 3529/11884 - Val_Loss: 26.2555\n",
      "Processing batch 3530/11884 - Val_Loss: 26.4575\n",
      "Processing batch 3531/11884 - Val_Loss: 25.3875\n",
      "Processing batch 3532/11884 - Val_Loss: 26.8981\n",
      "Processing batch 3533/11884 - Val_Loss: 27.5662\n",
      "Processing batch 3534/11884 - Val_Loss: 24.1849\n",
      "Processing batch 3535/11884 - Val_Loss: 22.8603\n",
      "Processing batch 3536/11884 - Val_Loss: 23.0452\n",
      "Processing batch 3537/11884 - Val_Loss: 25.4564\n",
      "Processing batch 3538/11884 - Val_Loss: 23.4286\n",
      "Processing batch 3539/11884 - Val_Loss: 25.5749\n",
      "Processing batch 3540/11884 - Val_Loss: 26.3553\n",
      "Processing batch 3541/11884 - Val_Loss: 24.8730\n",
      "Processing batch 3542/11884 - Val_Loss: 25.9745\n",
      "Processing batch 3543/11884 - Val_Loss: 27.2679\n",
      "Processing batch 3544/11884 - Val_Loss: 24.0866\n",
      "Processing batch 3545/11884 - Val_Loss: 27.7248\n",
      "Processing batch 3546/11884 - Val_Loss: 23.7946\n",
      "Processing batch 3547/11884 - Val_Loss: 27.1035\n",
      "Processing batch 3548/11884 - Val_Loss: 24.6259\n",
      "Processing batch 3549/11884 - Val_Loss: 28.7886\n",
      "Processing batch 3550/11884 - Val_Loss: 26.4842\n",
      "Processing batch 3551/11884 - Val_Loss: 27.0266\n",
      "Processing batch 3552/11884 - Val_Loss: 23.1056\n",
      "Processing batch 3553/11884 - Val_Loss: 26.3416\n",
      "Processing batch 3554/11884 - Val_Loss: 24.7051\n",
      "Processing batch 3555/11884 - Val_Loss: 23.5957\n",
      "Processing batch 3556/11884 - Val_Loss: 22.3391\n",
      "Processing batch 3557/11884 - Val_Loss: 26.2089\n",
      "Processing batch 3558/11884 - Val_Loss: 25.1044\n",
      "Processing batch 3559/11884 - Val_Loss: 29.1610\n",
      "Processing batch 3560/11884 - Val_Loss: 27.2688\n",
      "Processing batch 3561/11884 - Val_Loss: 25.1942\n",
      "Processing batch 3562/11884 - Val_Loss: 26.3822\n",
      "Processing batch 3563/11884 - Val_Loss: 24.6445\n",
      "Processing batch 3564/11884 - Val_Loss: 26.9286\n",
      "Processing batch 3565/11884 - Val_Loss: 26.1824\n",
      "Processing batch 3566/11884 - Val_Loss: 25.5200\n",
      "Processing batch 3567/11884 - Val_Loss: 27.2734\n",
      "Processing batch 3568/11884 - Val_Loss: 29.1373\n",
      "Processing batch 3569/11884 - Val_Loss: 27.1882\n",
      "Processing batch 3570/11884 - Val_Loss: 27.6932\n",
      "Processing batch 3571/11884 - Val_Loss: 25.5679\n",
      "Processing batch 3572/11884 - Val_Loss: 27.4083\n",
      "Processing batch 3573/11884 - Val_Loss: 28.0298\n",
      "Processing batch 3574/11884 - Val_Loss: 23.3114\n",
      "Processing batch 3575/11884 - Val_Loss: 28.3441\n",
      "Processing batch 3576/11884 - Val_Loss: 25.6991\n",
      "Processing batch 3577/11884 - Val_Loss: 24.8399\n",
      "Processing batch 3578/11884 - Val_Loss: 27.4295\n",
      "Processing batch 3579/11884 - Val_Loss: 26.4687\n",
      "Processing batch 3580/11884 - Val_Loss: 25.4640\n",
      "Processing batch 3581/11884 - Val_Loss: 25.5048\n",
      "Processing batch 3582/11884 - Val_Loss: 27.1550\n",
      "Processing batch 3583/11884 - Val_Loss: 26.3380\n",
      "Processing batch 3584/11884 - Val_Loss: 26.8984\n",
      "Processing batch 3585/11884 - Val_Loss: 25.0940\n",
      "Processing batch 3586/11884 - Val_Loss: 24.2020\n",
      "Processing batch 3587/11884 - Val_Loss: 27.9605\n",
      "Processing batch 3588/11884 - Val_Loss: 26.3501\n",
      "Processing batch 3589/11884 - Val_Loss: 21.5914\n",
      "Processing batch 3590/11884 - Val_Loss: 25.9522\n",
      "Processing batch 3591/11884 - Val_Loss: 23.3192\n",
      "Processing batch 3592/11884 - Val_Loss: 26.3024\n",
      "Processing batch 3593/11884 - Val_Loss: 25.7689\n",
      "Processing batch 3594/11884 - Val_Loss: 26.0002\n",
      "Processing batch 3595/11884 - Val_Loss: 23.3649\n",
      "Processing batch 3596/11884 - Val_Loss: 25.0593\n",
      "Processing batch 3597/11884 - Val_Loss: 24.7751\n",
      "Processing batch 3598/11884 - Val_Loss: 26.0087\n",
      "Processing batch 3599/11884 - Val_Loss: 28.1990\n",
      "Processing batch 3600/11884 - Val_Loss: 27.7442\n",
      "Processing batch 3601/11884 - Val_Loss: 22.7894\n",
      "Processing batch 3602/11884 - Val_Loss: 26.0977\n",
      "Processing batch 3603/11884 - Val_Loss: 23.0344\n",
      "Processing batch 3604/11884 - Val_Loss: 26.1480\n",
      "Processing batch 3605/11884 - Val_Loss: 26.0169\n",
      "Processing batch 3606/11884 - Val_Loss: 24.6775\n",
      "Processing batch 3607/11884 - Val_Loss: 27.1360\n",
      "Processing batch 3608/11884 - Val_Loss: 25.1423\n",
      "Processing batch 3609/11884 - Val_Loss: 25.8904\n",
      "Processing batch 3610/11884 - Val_Loss: 24.3368\n",
      "Processing batch 3611/11884 - Val_Loss: 24.9751\n",
      "Processing batch 3612/11884 - Val_Loss: 22.6475\n",
      "Processing batch 3613/11884 - Val_Loss: 25.9856\n",
      "Processing batch 3614/11884 - Val_Loss: 26.5119\n",
      "Processing batch 3615/11884 - Val_Loss: 25.5590\n",
      "Processing batch 3616/11884 - Val_Loss: 26.2575\n",
      "Processing batch 3617/11884 - Val_Loss: 24.8349\n",
      "Processing batch 3618/11884 - Val_Loss: 26.1388\n",
      "Processing batch 3619/11884 - Val_Loss: 23.6666\n",
      "Processing batch 3620/11884 - Val_Loss: 25.8188\n",
      "Processing batch 3621/11884 - Val_Loss: 25.1103\n",
      "Processing batch 3622/11884 - Val_Loss: 26.7394\n",
      "Processing batch 3623/11884 - Val_Loss: 26.5100\n",
      "Processing batch 3624/11884 - Val_Loss: 24.0942\n",
      "Processing batch 3625/11884 - Val_Loss: 26.6295\n",
      "Processing batch 3626/11884 - Val_Loss: 24.9187\n",
      "Processing batch 3627/11884 - Val_Loss: 24.6238\n",
      "Processing batch 3628/11884 - Val_Loss: 23.7534\n",
      "Processing batch 3629/11884 - Val_Loss: 25.8689\n",
      "Processing batch 3630/11884 - Val_Loss: 27.3269\n",
      "Processing batch 3631/11884 - Val_Loss: 25.3659\n",
      "Processing batch 3632/11884 - Val_Loss: 27.5715\n",
      "Processing batch 3633/11884 - Val_Loss: 26.7377\n",
      "Processing batch 3634/11884 - Val_Loss: 26.3115\n",
      "Processing batch 3635/11884 - Val_Loss: 28.4186\n",
      "Processing batch 3636/11884 - Val_Loss: 26.5909\n",
      "Processing batch 3637/11884 - Val_Loss: 26.1667\n",
      "Processing batch 3638/11884 - Val_Loss: 26.6801\n",
      "Processing batch 3639/11884 - Val_Loss: 24.6826\n",
      "Processing batch 3640/11884 - Val_Loss: 29.1173\n",
      "Processing batch 3641/11884 - Val_Loss: 25.7727\n",
      "Processing batch 3642/11884 - Val_Loss: 25.1039\n",
      "Processing batch 3643/11884 - Val_Loss: 25.7987\n",
      "Processing batch 3644/11884 - Val_Loss: 26.8659\n",
      "Processing batch 3645/11884 - Val_Loss: 23.6994\n",
      "Processing batch 3646/11884 - Val_Loss: 26.6324\n",
      "Processing batch 3647/11884 - Val_Loss: 24.0682\n",
      "Processing batch 3648/11884 - Val_Loss: 24.2569\n",
      "Processing batch 3649/11884 - Val_Loss: 25.7968\n",
      "Processing batch 3650/11884 - Val_Loss: 24.5849\n",
      "Processing batch 3651/11884 - Val_Loss: 25.1507\n",
      "Processing batch 3652/11884 - Val_Loss: 29.2616\n",
      "Processing batch 3653/11884 - Val_Loss: 25.7765\n",
      "Processing batch 3654/11884 - Val_Loss: 25.5391\n",
      "Processing batch 3655/11884 - Val_Loss: 25.6598\n",
      "Processing batch 3656/11884 - Val_Loss: 26.0362\n",
      "Processing batch 3657/11884 - Val_Loss: 26.6959\n",
      "Processing batch 3658/11884 - Val_Loss: 23.5529\n",
      "Processing batch 3659/11884 - Val_Loss: 25.0818\n",
      "Processing batch 3660/11884 - Val_Loss: 24.7126\n",
      "Processing batch 3661/11884 - Val_Loss: 27.2795\n",
      "Processing batch 3662/11884 - Val_Loss: 25.3180\n",
      "Processing batch 3663/11884 - Val_Loss: 23.2394\n",
      "Processing batch 3664/11884 - Val_Loss: 24.4103\n",
      "Processing batch 3665/11884 - Val_Loss: 24.4495\n",
      "Processing batch 3666/11884 - Val_Loss: 23.9666\n",
      "Processing batch 3667/11884 - Val_Loss: 28.8074\n",
      "Processing batch 3668/11884 - Val_Loss: 25.7773\n",
      "Processing batch 3669/11884 - Val_Loss: 22.9506\n",
      "Processing batch 3670/11884 - Val_Loss: 26.1130\n",
      "Processing batch 3671/11884 - Val_Loss: 30.0531\n",
      "Processing batch 3672/11884 - Val_Loss: 27.1999\n",
      "Processing batch 3673/11884 - Val_Loss: 25.9823\n",
      "Processing batch 3674/11884 - Val_Loss: 25.4851\n",
      "Processing batch 3675/11884 - Val_Loss: 24.6015\n",
      "Processing batch 3676/11884 - Val_Loss: 24.4865\n",
      "Processing batch 3677/11884 - Val_Loss: 24.6915\n",
      "Processing batch 3678/11884 - Val_Loss: 21.5883\n",
      "Processing batch 3679/11884 - Val_Loss: 25.5225\n",
      "Processing batch 3680/11884 - Val_Loss: 26.8091\n",
      "Processing batch 3681/11884 - Val_Loss: 24.7358\n",
      "Processing batch 3682/11884 - Val_Loss: 27.8243\n",
      "Processing batch 3683/11884 - Val_Loss: 25.6623\n",
      "Processing batch 3684/11884 - Val_Loss: 25.9030\n",
      "Processing batch 3685/11884 - Val_Loss: 23.3305\n",
      "Processing batch 3686/11884 - Val_Loss: 24.9914\n",
      "Processing batch 3687/11884 - Val_Loss: 25.1748\n",
      "Processing batch 3688/11884 - Val_Loss: 24.3109\n",
      "Processing batch 3689/11884 - Val_Loss: 22.6316\n",
      "Processing batch 3690/11884 - Val_Loss: 24.6332\n",
      "Processing batch 3691/11884 - Val_Loss: 23.8848\n",
      "Processing batch 3692/11884 - Val_Loss: 26.6620\n",
      "Processing batch 3693/11884 - Val_Loss: 24.2528\n",
      "Processing batch 3694/11884 - Val_Loss: 27.7333\n",
      "Processing batch 3695/11884 - Val_Loss: 25.8486\n",
      "Processing batch 3696/11884 - Val_Loss: 26.0144\n",
      "Processing batch 3697/11884 - Val_Loss: 25.9818\n",
      "Processing batch 3698/11884 - Val_Loss: 25.9370\n",
      "Processing batch 3699/11884 - Val_Loss: 20.7597\n",
      "Processing batch 3700/11884 - Val_Loss: 24.9169\n",
      "Processing batch 3701/11884 - Val_Loss: 22.6687\n",
      "Processing batch 3702/11884 - Val_Loss: 25.8014\n",
      "Processing batch 3703/11884 - Val_Loss: 23.5926\n",
      "Processing batch 3704/11884 - Val_Loss: 25.1937\n",
      "Processing batch 3705/11884 - Val_Loss: 24.4931\n",
      "Processing batch 3706/11884 - Val_Loss: 25.6977\n",
      "Processing batch 3707/11884 - Val_Loss: 25.2126\n",
      "Processing batch 3708/11884 - Val_Loss: 26.1863\n",
      "Processing batch 3709/11884 - Val_Loss: 22.8743\n",
      "Processing batch 3710/11884 - Val_Loss: 30.0927\n",
      "Processing batch 3711/11884 - Val_Loss: 27.3955\n",
      "Processing batch 3712/11884 - Val_Loss: 24.3168\n",
      "Processing batch 3713/11884 - Val_Loss: 25.2346\n",
      "Processing batch 3714/11884 - Val_Loss: 25.1145\n",
      "Processing batch 3715/11884 - Val_Loss: 23.0269\n",
      "Processing batch 3716/11884 - Val_Loss: 22.3229\n",
      "Processing batch 3717/11884 - Val_Loss: 25.5199\n",
      "Processing batch 3718/11884 - Val_Loss: 24.9697\n",
      "Processing batch 3719/11884 - Val_Loss: 26.8956\n",
      "Processing batch 3720/11884 - Val_Loss: 24.4939\n",
      "Processing batch 3721/11884 - Val_Loss: 25.2410\n",
      "Processing batch 3722/11884 - Val_Loss: 24.0809\n",
      "Processing batch 3723/11884 - Val_Loss: 23.4528\n",
      "Processing batch 3724/11884 - Val_Loss: 25.5375\n",
      "Processing batch 3725/11884 - Val_Loss: 27.6112\n",
      "Processing batch 3726/11884 - Val_Loss: 27.5980\n",
      "Processing batch 3727/11884 - Val_Loss: 27.7883\n",
      "Processing batch 3728/11884 - Val_Loss: 24.4594\n",
      "Processing batch 3729/11884 - Val_Loss: 25.8499\n",
      "Processing batch 3730/11884 - Val_Loss: 27.5297\n",
      "Processing batch 3731/11884 - Val_Loss: 28.5926\n",
      "Processing batch 3732/11884 - Val_Loss: 26.1666\n",
      "Processing batch 3733/11884 - Val_Loss: 26.9475\n",
      "Processing batch 3734/11884 - Val_Loss: 24.2118\n",
      "Processing batch 3735/11884 - Val_Loss: 27.2990\n",
      "Processing batch 3736/11884 - Val_Loss: 25.5668\n",
      "Processing batch 3737/11884 - Val_Loss: 24.0093\n",
      "Processing batch 3738/11884 - Val_Loss: 25.9293\n",
      "Processing batch 3739/11884 - Val_Loss: 23.3381\n",
      "Processing batch 3740/11884 - Val_Loss: 27.0335\n",
      "Processing batch 3741/11884 - Val_Loss: 24.5326\n",
      "Processing batch 3742/11884 - Val_Loss: 26.7508\n",
      "Processing batch 3743/11884 - Val_Loss: 27.2951\n",
      "Processing batch 3744/11884 - Val_Loss: 28.8938\n",
      "Processing batch 3745/11884 - Val_Loss: 24.6413\n",
      "Processing batch 3746/11884 - Val_Loss: 24.4967\n",
      "Processing batch 3747/11884 - Val_Loss: 24.6494\n",
      "Processing batch 3748/11884 - Val_Loss: 24.0693\n",
      "Processing batch 3749/11884 - Val_Loss: 23.3579\n",
      "Processing batch 3750/11884 - Val_Loss: 24.2338\n",
      "Processing batch 3751/11884 - Val_Loss: 27.3677\n",
      "Processing batch 3752/11884 - Val_Loss: 23.7270\n",
      "Processing batch 3753/11884 - Val_Loss: 26.4201\n",
      "Processing batch 3754/11884 - Val_Loss: 25.3924\n",
      "Processing batch 3755/11884 - Val_Loss: 26.9951\n",
      "Processing batch 3756/11884 - Val_Loss: 25.3715\n",
      "Processing batch 3757/11884 - Val_Loss: 23.4264\n",
      "Processing batch 3758/11884 - Val_Loss: 28.3102\n",
      "Processing batch 3759/11884 - Val_Loss: 26.5787\n",
      "Processing batch 3760/11884 - Val_Loss: 25.2490\n",
      "Processing batch 3761/11884 - Val_Loss: 28.1672\n",
      "Processing batch 3762/11884 - Val_Loss: 27.5312\n",
      "Processing batch 3763/11884 - Val_Loss: 24.5571\n",
      "Processing batch 3764/11884 - Val_Loss: 25.9640\n",
      "Processing batch 3765/11884 - Val_Loss: 25.2368\n",
      "Processing batch 3766/11884 - Val_Loss: 25.6958\n",
      "Processing batch 3767/11884 - Val_Loss: 25.9907\n",
      "Processing batch 3768/11884 - Val_Loss: 26.6658\n",
      "Processing batch 3769/11884 - Val_Loss: 24.8584\n",
      "Processing batch 3770/11884 - Val_Loss: 26.5548\n",
      "Processing batch 3771/11884 - Val_Loss: 27.2141\n",
      "Processing batch 3772/11884 - Val_Loss: 24.5309\n",
      "Processing batch 3773/11884 - Val_Loss: 27.6593\n",
      "Processing batch 3774/11884 - Val_Loss: 25.2214\n",
      "Processing batch 3775/11884 - Val_Loss: 25.4751\n",
      "Processing batch 3776/11884 - Val_Loss: 24.7322\n",
      "Processing batch 3777/11884 - Val_Loss: 28.5146\n",
      "Processing batch 3778/11884 - Val_Loss: 25.9098\n",
      "Processing batch 3779/11884 - Val_Loss: 23.5761\n",
      "Processing batch 3780/11884 - Val_Loss: 26.0480\n",
      "Processing batch 3781/11884 - Val_Loss: 26.6184\n",
      "Processing batch 3782/11884 - Val_Loss: 24.2105\n",
      "Processing batch 3783/11884 - Val_Loss: 26.2100\n",
      "Processing batch 3784/11884 - Val_Loss: 26.0367\n",
      "Processing batch 3785/11884 - Val_Loss: 27.3584\n",
      "Processing batch 3786/11884 - Val_Loss: 26.0308\n",
      "Processing batch 3787/11884 - Val_Loss: 24.0876\n",
      "Processing batch 3788/11884 - Val_Loss: 23.8671\n",
      "Processing batch 3789/11884 - Val_Loss: 21.4092\n",
      "Processing batch 3790/11884 - Val_Loss: 24.3298\n",
      "Processing batch 3791/11884 - Val_Loss: 26.2018\n",
      "Processing batch 3792/11884 - Val_Loss: 26.7202\n",
      "Processing batch 3793/11884 - Val_Loss: 24.1313\n",
      "Processing batch 3794/11884 - Val_Loss: 27.9612\n",
      "Processing batch 3795/11884 - Val_Loss: 26.3627\n",
      "Processing batch 3796/11884 - Val_Loss: 26.6447\n",
      "Processing batch 3797/11884 - Val_Loss: 23.8697\n",
      "Processing batch 3798/11884 - Val_Loss: 24.3676\n",
      "Processing batch 3799/11884 - Val_Loss: 25.5269\n",
      "Processing batch 3800/11884 - Val_Loss: 23.8662\n",
      "Processing batch 3801/11884 - Val_Loss: 25.6967\n",
      "Processing batch 3802/11884 - Val_Loss: 25.5299\n",
      "Processing batch 3803/11884 - Val_Loss: 27.9235\n",
      "Processing batch 3804/11884 - Val_Loss: 26.6248\n",
      "Processing batch 3805/11884 - Val_Loss: 26.4378\n",
      "Processing batch 3806/11884 - Val_Loss: 25.3617\n",
      "Processing batch 3807/11884 - Val_Loss: 26.6366\n",
      "Processing batch 3808/11884 - Val_Loss: 28.1006\n",
      "Processing batch 3809/11884 - Val_Loss: 25.9402\n",
      "Processing batch 3810/11884 - Val_Loss: 25.8734\n",
      "Processing batch 3811/11884 - Val_Loss: 25.7757\n",
      "Processing batch 3812/11884 - Val_Loss: 23.9402\n",
      "Processing batch 3813/11884 - Val_Loss: 27.7354\n",
      "Processing batch 3814/11884 - Val_Loss: 26.0729\n",
      "Processing batch 3815/11884 - Val_Loss: 26.8126\n",
      "Processing batch 3816/11884 - Val_Loss: 26.9443\n",
      "Processing batch 3817/11884 - Val_Loss: 26.5190\n",
      "Processing batch 3818/11884 - Val_Loss: 24.0452\n",
      "Processing batch 3819/11884 - Val_Loss: 23.8928\n",
      "Processing batch 3820/11884 - Val_Loss: 25.6028\n",
      "Processing batch 3821/11884 - Val_Loss: 28.7127\n",
      "Processing batch 3822/11884 - Val_Loss: 25.2020\n",
      "Processing batch 3823/11884 - Val_Loss: 25.1667\n",
      "Processing batch 3824/11884 - Val_Loss: 26.7434\n",
      "Processing batch 3825/11884 - Val_Loss: 26.7122\n",
      "Processing batch 3826/11884 - Val_Loss: 28.8813\n",
      "Processing batch 3827/11884 - Val_Loss: 26.8070\n",
      "Processing batch 3828/11884 - Val_Loss: 27.0022\n",
      "Processing batch 3829/11884 - Val_Loss: 24.2358\n",
      "Processing batch 3830/11884 - Val_Loss: 27.9632\n",
      "Processing batch 3831/11884 - Val_Loss: 22.5066\n",
      "Processing batch 3832/11884 - Val_Loss: 27.7007\n",
      "Processing batch 3833/11884 - Val_Loss: 26.0394\n",
      "Processing batch 3834/11884 - Val_Loss: 26.9414\n",
      "Processing batch 3835/11884 - Val_Loss: 26.4046\n",
      "Processing batch 3836/11884 - Val_Loss: 26.0657\n",
      "Processing batch 3837/11884 - Val_Loss: 22.2652\n",
      "Processing batch 3838/11884 - Val_Loss: 21.7153\n",
      "Processing batch 3839/11884 - Val_Loss: 25.4438\n",
      "Processing batch 3840/11884 - Val_Loss: 24.4409\n",
      "Processing batch 3841/11884 - Val_Loss: 26.0239\n",
      "Processing batch 3842/11884 - Val_Loss: 25.4481\n",
      "Processing batch 3843/11884 - Val_Loss: 24.3375\n",
      "Processing batch 3844/11884 - Val_Loss: 25.3897\n",
      "Processing batch 3845/11884 - Val_Loss: 25.9293\n",
      "Processing batch 3846/11884 - Val_Loss: 23.8615\n",
      "Processing batch 3847/11884 - Val_Loss: 27.5002\n",
      "Processing batch 3848/11884 - Val_Loss: 25.4990\n",
      "Processing batch 3849/11884 - Val_Loss: 26.2036\n",
      "Processing batch 3850/11884 - Val_Loss: 27.5190\n",
      "Processing batch 3851/11884 - Val_Loss: 25.5317\n",
      "Processing batch 3852/11884 - Val_Loss: 23.6491\n",
      "Processing batch 3853/11884 - Val_Loss: 26.8804\n",
      "Processing batch 3854/11884 - Val_Loss: 28.6877\n",
      "Processing batch 3855/11884 - Val_Loss: 25.5394\n",
      "Processing batch 3856/11884 - Val_Loss: 25.8707\n",
      "Processing batch 3857/11884 - Val_Loss: 20.2947\n",
      "Processing batch 3858/11884 - Val_Loss: 26.0642\n",
      "Processing batch 3859/11884 - Val_Loss: 23.8583\n",
      "Processing batch 3860/11884 - Val_Loss: 29.1099\n",
      "Processing batch 3861/11884 - Val_Loss: 24.3067\n",
      "Processing batch 3862/11884 - Val_Loss: 25.6880\n",
      "Processing batch 3863/11884 - Val_Loss: 27.1973\n",
      "Processing batch 3864/11884 - Val_Loss: 28.9471\n",
      "Processing batch 3865/11884 - Val_Loss: 27.7409\n",
      "Processing batch 3866/11884 - Val_Loss: 26.1658\n",
      "Processing batch 3867/11884 - Val_Loss: 24.3058\n",
      "Processing batch 3868/11884 - Val_Loss: 27.4829\n",
      "Processing batch 3869/11884 - Val_Loss: 25.2155\n",
      "Processing batch 3870/11884 - Val_Loss: 26.2549\n",
      "Processing batch 3871/11884 - Val_Loss: 27.0186\n",
      "Processing batch 3872/11884 - Val_Loss: 24.5439\n",
      "Processing batch 3873/11884 - Val_Loss: 24.7008\n",
      "Processing batch 3874/11884 - Val_Loss: 25.5071\n",
      "Processing batch 3875/11884 - Val_Loss: 27.3372\n",
      "Processing batch 3876/11884 - Val_Loss: 27.8721\n",
      "Processing batch 3877/11884 - Val_Loss: 24.5449\n",
      "Processing batch 3878/11884 - Val_Loss: 25.7767\n",
      "Processing batch 3879/11884 - Val_Loss: 25.4592\n",
      "Processing batch 3880/11884 - Val_Loss: 22.5645\n",
      "Processing batch 3881/11884 - Val_Loss: 26.7797\n",
      "Processing batch 3882/11884 - Val_Loss: 28.2381\n",
      "Processing batch 3883/11884 - Val_Loss: 23.0295\n",
      "Processing batch 3884/11884 - Val_Loss: 27.3580\n",
      "Processing batch 3885/11884 - Val_Loss: 26.0617\n",
      "Processing batch 3886/11884 - Val_Loss: 27.2682\n",
      "Processing batch 3887/11884 - Val_Loss: 27.0514\n",
      "Processing batch 3888/11884 - Val_Loss: 27.3156\n",
      "Processing batch 3889/11884 - Val_Loss: 26.4674\n",
      "Processing batch 3890/11884 - Val_Loss: 24.2782\n",
      "Processing batch 3891/11884 - Val_Loss: 22.3234\n",
      "Processing batch 3892/11884 - Val_Loss: 25.9826\n",
      "Processing batch 3893/11884 - Val_Loss: 24.5612\n",
      "Processing batch 3894/11884 - Val_Loss: 28.1083\n",
      "Processing batch 3895/11884 - Val_Loss: 24.9739\n",
      "Processing batch 3896/11884 - Val_Loss: 23.9381\n",
      "Processing batch 3897/11884 - Val_Loss: 26.3875\n",
      "Processing batch 3898/11884 - Val_Loss: 22.4975\n",
      "Processing batch 3899/11884 - Val_Loss: 25.8462\n",
      "Processing batch 3900/11884 - Val_Loss: 28.1835\n",
      "Processing batch 3901/11884 - Val_Loss: 26.4448\n",
      "Processing batch 3902/11884 - Val_Loss: 25.5927\n",
      "Processing batch 3903/11884 - Val_Loss: 26.0530\n",
      "Processing batch 3904/11884 - Val_Loss: 26.6721\n",
      "Processing batch 3905/11884 - Val_Loss: 26.0687\n",
      "Processing batch 3906/11884 - Val_Loss: 27.8145\n",
      "Processing batch 3907/11884 - Val_Loss: 28.3885\n",
      "Processing batch 3908/11884 - Val_Loss: 28.5585\n",
      "Processing batch 3909/11884 - Val_Loss: 21.7317\n",
      "Processing batch 3910/11884 - Val_Loss: 26.6243\n",
      "Processing batch 3911/11884 - Val_Loss: 28.8814\n",
      "Processing batch 3912/11884 - Val_Loss: 27.0956\n",
      "Processing batch 3913/11884 - Val_Loss: 27.3021\n",
      "Processing batch 3914/11884 - Val_Loss: 28.9402\n",
      "Processing batch 3915/11884 - Val_Loss: 24.9035\n",
      "Processing batch 3916/11884 - Val_Loss: 27.7747\n",
      "Processing batch 3917/11884 - Val_Loss: 28.7687\n",
      "Processing batch 3918/11884 - Val_Loss: 24.4552\n",
      "Processing batch 3919/11884 - Val_Loss: 27.7101\n",
      "Processing batch 3920/11884 - Val_Loss: 26.9195\n",
      "Processing batch 3921/11884 - Val_Loss: 23.6053\n",
      "Processing batch 3922/11884 - Val_Loss: 25.2078\n",
      "Processing batch 3923/11884 - Val_Loss: 24.2173\n",
      "Processing batch 3924/11884 - Val_Loss: 24.0706\n",
      "Processing batch 3925/11884 - Val_Loss: 26.0884\n",
      "Processing batch 3926/11884 - Val_Loss: 27.2962\n",
      "Processing batch 3927/11884 - Val_Loss: 26.4813\n",
      "Processing batch 3928/11884 - Val_Loss: 24.1442\n",
      "Processing batch 3929/11884 - Val_Loss: 23.2166\n",
      "Processing batch 3930/11884 - Val_Loss: 23.9615\n",
      "Processing batch 3931/11884 - Val_Loss: 21.8683\n",
      "Processing batch 3932/11884 - Val_Loss: 24.5800\n",
      "Processing batch 3933/11884 - Val_Loss: 25.1828\n",
      "Processing batch 3934/11884 - Val_Loss: 23.7324\n",
      "Processing batch 3935/11884 - Val_Loss: 28.0207\n",
      "Processing batch 3936/11884 - Val_Loss: 24.2471\n",
      "Processing batch 3937/11884 - Val_Loss: 23.0244\n",
      "Processing batch 3938/11884 - Val_Loss: 24.8071\n",
      "Processing batch 3939/11884 - Val_Loss: 27.2089\n",
      "Processing batch 3940/11884 - Val_Loss: 24.7106\n",
      "Processing batch 3941/11884 - Val_Loss: 23.6349\n",
      "Processing batch 3942/11884 - Val_Loss: 24.0907\n",
      "Processing batch 3943/11884 - Val_Loss: 25.9876\n",
      "Processing batch 3944/11884 - Val_Loss: 27.9744\n",
      "Processing batch 3945/11884 - Val_Loss: 26.7337\n",
      "Processing batch 3946/11884 - Val_Loss: 26.4771\n",
      "Processing batch 3947/11884 - Val_Loss: 23.6243\n",
      "Processing batch 3948/11884 - Val_Loss: 25.4487\n",
      "Processing batch 3949/11884 - Val_Loss: 23.9392\n",
      "Processing batch 3950/11884 - Val_Loss: 25.3762\n",
      "Processing batch 3951/11884 - Val_Loss: 24.6821\n",
      "Processing batch 3952/11884 - Val_Loss: 25.7632\n",
      "Processing batch 3953/11884 - Val_Loss: 27.6897\n",
      "Processing batch 3954/11884 - Val_Loss: 24.3185\n",
      "Processing batch 3955/11884 - Val_Loss: 24.8247\n",
      "Processing batch 3956/11884 - Val_Loss: 25.9394\n",
      "Processing batch 3957/11884 - Val_Loss: 25.8730\n",
      "Processing batch 3958/11884 - Val_Loss: 24.9936\n",
      "Processing batch 3959/11884 - Val_Loss: 26.3040\n",
      "Processing batch 3960/11884 - Val_Loss: 24.8788\n",
      "Processing batch 3961/11884 - Val_Loss: 24.3387\n",
      "Processing batch 3962/11884 - Val_Loss: 24.0223\n",
      "Processing batch 3963/11884 - Val_Loss: 25.0421\n",
      "Processing batch 3964/11884 - Val_Loss: 24.2227\n",
      "Processing batch 3965/11884 - Val_Loss: 27.8235\n",
      "Processing batch 3966/11884 - Val_Loss: 26.8129\n",
      "Processing batch 3967/11884 - Val_Loss: 22.8375\n",
      "Processing batch 3968/11884 - Val_Loss: 25.4546\n",
      "Processing batch 3969/11884 - Val_Loss: 23.8040\n",
      "Processing batch 3970/11884 - Val_Loss: 26.4517\n",
      "Processing batch 3971/11884 - Val_Loss: 25.9083\n",
      "Processing batch 3972/11884 - Val_Loss: 24.0834\n",
      "Processing batch 3973/11884 - Val_Loss: 25.2874\n",
      "Processing batch 3974/11884 - Val_Loss: 24.1437\n",
      "Processing batch 3975/11884 - Val_Loss: 24.1839\n",
      "Processing batch 3976/11884 - Val_Loss: 25.7319\n",
      "Processing batch 3977/11884 - Val_Loss: 24.9661\n",
      "Processing batch 3978/11884 - Val_Loss: 23.6180\n",
      "Processing batch 3979/11884 - Val_Loss: 28.1544\n",
      "Processing batch 3980/11884 - Val_Loss: 25.0037\n",
      "Processing batch 3981/11884 - Val_Loss: 25.1177\n",
      "Processing batch 3982/11884 - Val_Loss: 25.0986\n",
      "Processing batch 3983/11884 - Val_Loss: 25.0055\n",
      "Processing batch 3984/11884 - Val_Loss: 25.9166\n",
      "Processing batch 3985/11884 - Val_Loss: 26.3755\n",
      "Processing batch 3986/11884 - Val_Loss: 25.9565\n",
      "Processing batch 3987/11884 - Val_Loss: 25.7126\n",
      "Processing batch 3988/11884 - Val_Loss: 27.2585\n",
      "Processing batch 3989/11884 - Val_Loss: 25.6893\n",
      "Processing batch 3990/11884 - Val_Loss: 24.6238\n",
      "Processing batch 3991/11884 - Val_Loss: 24.3437\n",
      "Processing batch 3992/11884 - Val_Loss: 26.3624\n",
      "Processing batch 3993/11884 - Val_Loss: 26.6990\n",
      "Processing batch 3994/11884 - Val_Loss: 22.0530\n",
      "Processing batch 3995/11884 - Val_Loss: 24.2689\n",
      "Processing batch 3996/11884 - Val_Loss: 24.3195\n",
      "Processing batch 3997/11884 - Val_Loss: 24.8200\n",
      "Processing batch 3998/11884 - Val_Loss: 24.3276\n",
      "Processing batch 3999/11884 - Val_Loss: 28.2837\n",
      "Processing batch 4000/11884 - Val_Loss: 26.7629\n",
      "Processing batch 4001/11884 - Val_Loss: 24.8560\n",
      "Processing batch 4002/11884 - Val_Loss: 28.1032\n",
      "Processing batch 4003/11884 - Val_Loss: 24.9327\n",
      "Processing batch 4004/11884 - Val_Loss: 23.2105\n",
      "Processing batch 4005/11884 - Val_Loss: 26.9090\n",
      "Processing batch 4006/11884 - Val_Loss: 28.6853\n",
      "Processing batch 4007/11884 - Val_Loss: 28.2212\n",
      "Processing batch 4008/11884 - Val_Loss: 26.8281\n",
      "Processing batch 4009/11884 - Val_Loss: 27.3257\n",
      "Processing batch 4010/11884 - Val_Loss: 24.4080\n",
      "Processing batch 4011/11884 - Val_Loss: 27.2295\n",
      "Processing batch 4012/11884 - Val_Loss: 24.5467\n",
      "Processing batch 4013/11884 - Val_Loss: 23.4509\n",
      "Processing batch 4014/11884 - Val_Loss: 29.2537\n",
      "Processing batch 4015/11884 - Val_Loss: 28.0079\n",
      "Processing batch 4016/11884 - Val_Loss: 26.2563\n",
      "Processing batch 4017/11884 - Val_Loss: 24.1906\n",
      "Processing batch 4018/11884 - Val_Loss: 24.0883\n",
      "Processing batch 4019/11884 - Val_Loss: 26.9083\n",
      "Processing batch 4020/11884 - Val_Loss: 23.8041\n",
      "Processing batch 4021/11884 - Val_Loss: 26.6674\n",
      "Processing batch 4022/11884 - Val_Loss: 25.5069\n",
      "Processing batch 4023/11884 - Val_Loss: 24.9392\n",
      "Processing batch 4024/11884 - Val_Loss: 23.5189\n",
      "Processing batch 4025/11884 - Val_Loss: 27.3615\n",
      "Processing batch 4026/11884 - Val_Loss: 27.2551\n",
      "Processing batch 4027/11884 - Val_Loss: 27.8567\n",
      "Processing batch 4028/11884 - Val_Loss: 24.7272\n",
      "Processing batch 4029/11884 - Val_Loss: 22.3328\n",
      "Processing batch 4030/11884 - Val_Loss: 27.1049\n",
      "Processing batch 4031/11884 - Val_Loss: 30.5043\n",
      "Processing batch 4032/11884 - Val_Loss: 25.6258\n",
      "Processing batch 4033/11884 - Val_Loss: 33.7951\n",
      "Processing batch 4034/11884 - Val_Loss: 23.7759\n",
      "Processing batch 4035/11884 - Val_Loss: 27.3659\n",
      "Processing batch 4036/11884 - Val_Loss: 25.1230\n",
      "Processing batch 4037/11884 - Val_Loss: 24.6403\n",
      "Processing batch 4038/11884 - Val_Loss: 25.5429\n",
      "Processing batch 4039/11884 - Val_Loss: 23.1383\n",
      "Processing batch 4040/11884 - Val_Loss: 25.9085\n",
      "Processing batch 4041/11884 - Val_Loss: 25.8206\n",
      "Processing batch 4042/11884 - Val_Loss: 27.7920\n",
      "Processing batch 4043/11884 - Val_Loss: 25.2252\n",
      "Processing batch 4044/11884 - Val_Loss: 27.7323\n",
      "Processing batch 4045/11884 - Val_Loss: 24.1862\n",
      "Processing batch 4046/11884 - Val_Loss: 25.6073\n",
      "Processing batch 4047/11884 - Val_Loss: 27.9903\n",
      "Processing batch 4048/11884 - Val_Loss: 24.7626\n",
      "Processing batch 4049/11884 - Val_Loss: 24.3913\n",
      "Processing batch 4050/11884 - Val_Loss: 27.9613\n",
      "Processing batch 4051/11884 - Val_Loss: 26.8244\n",
      "Processing batch 4052/11884 - Val_Loss: 26.9186\n",
      "Processing batch 4053/11884 - Val_Loss: 22.2577\n",
      "Processing batch 4054/11884 - Val_Loss: 24.9288\n",
      "Processing batch 4055/11884 - Val_Loss: 29.5297\n",
      "Processing batch 4056/11884 - Val_Loss: 25.9731\n",
      "Processing batch 4057/11884 - Val_Loss: 25.4889\n",
      "Processing batch 4058/11884 - Val_Loss: 25.3381\n",
      "Processing batch 4059/11884 - Val_Loss: 24.2934\n",
      "Processing batch 4060/11884 - Val_Loss: 26.6514\n",
      "Processing batch 4061/11884 - Val_Loss: 28.2492\n",
      "Processing batch 4062/11884 - Val_Loss: 26.0730\n",
      "Processing batch 4063/11884 - Val_Loss: 26.6671\n",
      "Processing batch 4064/11884 - Val_Loss: 26.1767\n",
      "Processing batch 4065/11884 - Val_Loss: 25.2433\n",
      "Processing batch 4066/11884 - Val_Loss: 22.9000\n",
      "Processing batch 4067/11884 - Val_Loss: 25.4755\n",
      "Processing batch 4068/11884 - Val_Loss: 31.2933\n",
      "Processing batch 4069/11884 - Val_Loss: 26.9627\n",
      "Processing batch 4070/11884 - Val_Loss: 24.1831\n",
      "Processing batch 4071/11884 - Val_Loss: 25.3159\n",
      "Processing batch 4072/11884 - Val_Loss: 23.5362\n",
      "Processing batch 4073/11884 - Val_Loss: 26.4707\n",
      "Processing batch 4074/11884 - Val_Loss: 23.7850\n",
      "Processing batch 4075/11884 - Val_Loss: 26.9700\n",
      "Processing batch 4076/11884 - Val_Loss: 25.9825\n",
      "Processing batch 4077/11884 - Val_Loss: 26.4621\n",
      "Processing batch 4078/11884 - Val_Loss: 25.0105\n",
      "Processing batch 4079/11884 - Val_Loss: 26.9643\n",
      "Processing batch 4080/11884 - Val_Loss: 25.9634\n",
      "Processing batch 4081/11884 - Val_Loss: 23.5927\n",
      "Processing batch 4082/11884 - Val_Loss: 25.0439\n",
      "Processing batch 4083/11884 - Val_Loss: 25.2670\n",
      "Processing batch 4084/11884 - Val_Loss: 26.3083\n",
      "Processing batch 4085/11884 - Val_Loss: 27.3118\n",
      "Processing batch 4086/11884 - Val_Loss: 24.8782\n",
      "Processing batch 4087/11884 - Val_Loss: 28.2644\n",
      "Processing batch 4088/11884 - Val_Loss: 24.7224\n",
      "Processing batch 4089/11884 - Val_Loss: 23.6743\n",
      "Processing batch 4090/11884 - Val_Loss: 25.7006\n",
      "Processing batch 4091/11884 - Val_Loss: 25.7313\n",
      "Processing batch 4092/11884 - Val_Loss: 27.8878\n",
      "Processing batch 4093/11884 - Val_Loss: 22.8247\n",
      "Processing batch 4094/11884 - Val_Loss: 23.4940\n",
      "Processing batch 4095/11884 - Val_Loss: 27.0022\n",
      "Processing batch 4096/11884 - Val_Loss: 25.7607\n",
      "Processing batch 4097/11884 - Val_Loss: 25.8387\n",
      "Processing batch 4098/11884 - Val_Loss: 25.2433\n",
      "Processing batch 4099/11884 - Val_Loss: 27.2516\n",
      "Processing batch 4100/11884 - Val_Loss: 24.4848\n",
      "Processing batch 4101/11884 - Val_Loss: 24.6835\n",
      "Processing batch 4102/11884 - Val_Loss: 28.4607\n",
      "Processing batch 4103/11884 - Val_Loss: 25.7611\n",
      "Processing batch 4104/11884 - Val_Loss: 24.5139\n",
      "Processing batch 4105/11884 - Val_Loss: 28.6808\n",
      "Processing batch 4106/11884 - Val_Loss: 25.6600\n",
      "Processing batch 4107/11884 - Val_Loss: 24.4324\n",
      "Processing batch 4108/11884 - Val_Loss: 24.3118\n",
      "Processing batch 4109/11884 - Val_Loss: 27.3518\n",
      "Processing batch 4110/11884 - Val_Loss: 26.5969\n",
      "Processing batch 4111/11884 - Val_Loss: 29.8038\n",
      "Processing batch 4112/11884 - Val_Loss: 25.0621\n",
      "Processing batch 4113/11884 - Val_Loss: 26.3085\n",
      "Processing batch 4114/11884 - Val_Loss: 27.5872\n",
      "Processing batch 4115/11884 - Val_Loss: 28.7296\n",
      "Processing batch 4116/11884 - Val_Loss: 25.2247\n",
      "Processing batch 4117/11884 - Val_Loss: 24.9021\n",
      "Processing batch 4118/11884 - Val_Loss: 25.4172\n",
      "Processing batch 4119/11884 - Val_Loss: 26.0786\n",
      "Processing batch 4120/11884 - Val_Loss: 26.4847\n",
      "Processing batch 4121/11884 - Val_Loss: 25.1582\n",
      "Processing batch 4122/11884 - Val_Loss: 24.1918\n",
      "Processing batch 4123/11884 - Val_Loss: 26.7912\n",
      "Processing batch 4124/11884 - Val_Loss: 26.8324\n",
      "Processing batch 4125/11884 - Val_Loss: 26.7603\n",
      "Processing batch 4126/11884 - Val_Loss: 22.5588\n",
      "Processing batch 4127/11884 - Val_Loss: 27.6011\n",
      "Processing batch 4128/11884 - Val_Loss: 23.9853\n",
      "Processing batch 4129/11884 - Val_Loss: 24.9294\n",
      "Processing batch 4130/11884 - Val_Loss: 24.8281\n",
      "Processing batch 4131/11884 - Val_Loss: 27.6582\n",
      "Processing batch 4132/11884 - Val_Loss: 26.4252\n",
      "Processing batch 4133/11884 - Val_Loss: 25.1526\n",
      "Processing batch 4134/11884 - Val_Loss: 28.6494\n",
      "Processing batch 4135/11884 - Val_Loss: 23.1790\n",
      "Processing batch 4136/11884 - Val_Loss: 23.9466\n",
      "Processing batch 4137/11884 - Val_Loss: 26.9648\n",
      "Processing batch 4138/11884 - Val_Loss: 24.6602\n",
      "Processing batch 4139/11884 - Val_Loss: 24.4435\n",
      "Processing batch 4140/11884 - Val_Loss: 24.2494\n",
      "Processing batch 4141/11884 - Val_Loss: 26.3747\n",
      "Processing batch 4142/11884 - Val_Loss: 29.7118\n",
      "Processing batch 4143/11884 - Val_Loss: 23.8483\n",
      "Processing batch 4144/11884 - Val_Loss: 24.7345\n",
      "Processing batch 4145/11884 - Val_Loss: 26.6835\n",
      "Processing batch 4146/11884 - Val_Loss: 25.4965\n",
      "Processing batch 4147/11884 - Val_Loss: 26.3534\n",
      "Processing batch 4148/11884 - Val_Loss: 26.6094\n",
      "Processing batch 4149/11884 - Val_Loss: 26.8453\n",
      "Processing batch 4150/11884 - Val_Loss: 27.4646\n",
      "Processing batch 4151/11884 - Val_Loss: 25.2458\n",
      "Processing batch 4152/11884 - Val_Loss: 24.7373\n",
      "Processing batch 4153/11884 - Val_Loss: 25.2072\n",
      "Processing batch 4154/11884 - Val_Loss: 24.6800\n",
      "Processing batch 4155/11884 - Val_Loss: 26.2827\n",
      "Processing batch 4156/11884 - Val_Loss: 27.6862\n",
      "Processing batch 4157/11884 - Val_Loss: 25.8304\n",
      "Processing batch 4158/11884 - Val_Loss: 27.7129\n",
      "Processing batch 4159/11884 - Val_Loss: 23.8217\n",
      "Processing batch 4160/11884 - Val_Loss: 24.7910\n",
      "Processing batch 4161/11884 - Val_Loss: 25.8405\n",
      "Processing batch 4162/11884 - Val_Loss: 25.9180\n",
      "Processing batch 4163/11884 - Val_Loss: 25.4565\n",
      "Processing batch 4164/11884 - Val_Loss: 23.2909\n",
      "Processing batch 4165/11884 - Val_Loss: 26.0708\n",
      "Processing batch 4166/11884 - Val_Loss: 25.3517\n",
      "Processing batch 4167/11884 - Val_Loss: 26.2040\n",
      "Processing batch 4168/11884 - Val_Loss: 25.0678\n",
      "Processing batch 4169/11884 - Val_Loss: 26.1102\n",
      "Processing batch 4170/11884 - Val_Loss: 27.4031\n",
      "Processing batch 4171/11884 - Val_Loss: 26.9952\n",
      "Processing batch 4172/11884 - Val_Loss: 26.2262\n",
      "Processing batch 4173/11884 - Val_Loss: 26.1800\n",
      "Processing batch 4174/11884 - Val_Loss: 24.6887\n",
      "Processing batch 4175/11884 - Val_Loss: 24.9299\n",
      "Processing batch 4176/11884 - Val_Loss: 25.1740\n",
      "Processing batch 4177/11884 - Val_Loss: 23.7005\n",
      "Processing batch 4178/11884 - Val_Loss: 25.0642\n",
      "Processing batch 4179/11884 - Val_Loss: 28.6845\n",
      "Processing batch 4180/11884 - Val_Loss: 28.7961\n",
      "Processing batch 4181/11884 - Val_Loss: 27.3182\n",
      "Processing batch 4182/11884 - Val_Loss: 23.4052\n",
      "Processing batch 4183/11884 - Val_Loss: 25.4014\n",
      "Processing batch 4184/11884 - Val_Loss: 26.0183\n",
      "Processing batch 4185/11884 - Val_Loss: 23.9677\n",
      "Processing batch 4186/11884 - Val_Loss: 26.7754\n",
      "Processing batch 4187/11884 - Val_Loss: 25.1389\n",
      "Processing batch 4188/11884 - Val_Loss: 25.1971\n",
      "Processing batch 4189/11884 - Val_Loss: 26.3666\n",
      "Processing batch 4190/11884 - Val_Loss: 25.5910\n",
      "Processing batch 4191/11884 - Val_Loss: 29.1438\n",
      "Processing batch 4192/11884 - Val_Loss: 23.9894\n",
      "Processing batch 4193/11884 - Val_Loss: 27.5824\n",
      "Processing batch 4194/11884 - Val_Loss: 27.9030\n",
      "Processing batch 4195/11884 - Val_Loss: 24.6491\n",
      "Processing batch 4196/11884 - Val_Loss: 24.0740\n",
      "Processing batch 4197/11884 - Val_Loss: 25.4944\n",
      "Processing batch 4198/11884 - Val_Loss: 26.8294\n",
      "Processing batch 4199/11884 - Val_Loss: 26.3381\n",
      "Processing batch 4200/11884 - Val_Loss: 25.9474\n",
      "Processing batch 4201/11884 - Val_Loss: 26.1204\n",
      "Processing batch 4202/11884 - Val_Loss: 24.7234\n",
      "Processing batch 4203/11884 - Val_Loss: 27.6376\n",
      "Processing batch 4204/11884 - Val_Loss: 26.5347\n",
      "Processing batch 4205/11884 - Val_Loss: 25.5546\n",
      "Processing batch 4206/11884 - Val_Loss: 29.1679\n",
      "Processing batch 4207/11884 - Val_Loss: 23.7115\n",
      "Processing batch 4208/11884 - Val_Loss: 25.2487\n",
      "Processing batch 4209/11884 - Val_Loss: 27.2102\n",
      "Processing batch 4210/11884 - Val_Loss: 25.8573\n",
      "Processing batch 4211/11884 - Val_Loss: 26.7836\n",
      "Processing batch 4212/11884 - Val_Loss: 26.9974\n",
      "Processing batch 4213/11884 - Val_Loss: 26.6544\n",
      "Processing batch 4214/11884 - Val_Loss: 22.8818\n",
      "Processing batch 4215/11884 - Val_Loss: 27.1111\n",
      "Processing batch 4216/11884 - Val_Loss: 27.5202\n",
      "Processing batch 4217/11884 - Val_Loss: 26.6778\n",
      "Processing batch 4218/11884 - Val_Loss: 26.4504\n",
      "Processing batch 4219/11884 - Val_Loss: 21.3260\n",
      "Processing batch 4220/11884 - Val_Loss: 28.6783\n",
      "Processing batch 4221/11884 - Val_Loss: 27.8261\n",
      "Processing batch 4222/11884 - Val_Loss: 27.3099\n",
      "Processing batch 4223/11884 - Val_Loss: 25.5837\n",
      "Processing batch 4224/11884 - Val_Loss: 27.0463\n",
      "Processing batch 4225/11884 - Val_Loss: 26.5721\n",
      "Processing batch 4226/11884 - Val_Loss: 29.7116\n",
      "Processing batch 4227/11884 - Val_Loss: 26.3704\n",
      "Processing batch 4228/11884 - Val_Loss: 25.8596\n",
      "Processing batch 4229/11884 - Val_Loss: 26.9467\n",
      "Processing batch 4230/11884 - Val_Loss: 25.1707\n",
      "Processing batch 4231/11884 - Val_Loss: 26.4367\n",
      "Processing batch 4232/11884 - Val_Loss: 26.0924\n",
      "Processing batch 4233/11884 - Val_Loss: 24.2343\n",
      "Processing batch 4234/11884 - Val_Loss: 24.9027\n",
      "Processing batch 4235/11884 - Val_Loss: 23.5915\n",
      "Processing batch 4236/11884 - Val_Loss: 27.5647\n",
      "Processing batch 4237/11884 - Val_Loss: 26.1649\n",
      "Processing batch 4238/11884 - Val_Loss: 27.3632\n",
      "Processing batch 4239/11884 - Val_Loss: 26.2815\n",
      "Processing batch 4240/11884 - Val_Loss: 25.6442\n",
      "Processing batch 4241/11884 - Val_Loss: 25.5469\n",
      "Processing batch 4242/11884 - Val_Loss: 27.7933\n",
      "Processing batch 4243/11884 - Val_Loss: 26.8269\n",
      "Processing batch 4244/11884 - Val_Loss: 24.9379\n",
      "Processing batch 4245/11884 - Val_Loss: 27.6842\n",
      "Processing batch 4246/11884 - Val_Loss: 25.0345\n",
      "Processing batch 4247/11884 - Val_Loss: 26.2408\n",
      "Processing batch 4248/11884 - Val_Loss: 25.4795\n",
      "Processing batch 4249/11884 - Val_Loss: 25.2242\n",
      "Processing batch 4250/11884 - Val_Loss: 25.4409\n",
      "Processing batch 4251/11884 - Val_Loss: 26.1561\n",
      "Processing batch 4252/11884 - Val_Loss: 25.7148\n",
      "Processing batch 4253/11884 - Val_Loss: 24.9065\n",
      "Processing batch 4254/11884 - Val_Loss: 28.3357\n",
      "Processing batch 4255/11884 - Val_Loss: 26.4091\n",
      "Processing batch 4256/11884 - Val_Loss: 26.5318\n",
      "Processing batch 4257/11884 - Val_Loss: 25.6893\n",
      "Processing batch 4258/11884 - Val_Loss: 26.8793\n",
      "Processing batch 4259/11884 - Val_Loss: 25.7506\n",
      "Processing batch 4260/11884 - Val_Loss: 25.2916\n",
      "Processing batch 4261/11884 - Val_Loss: 25.7312\n",
      "Processing batch 4262/11884 - Val_Loss: 26.0360\n",
      "Processing batch 4263/11884 - Val_Loss: 24.7566\n",
      "Processing batch 4264/11884 - Val_Loss: 26.6968\n",
      "Processing batch 4265/11884 - Val_Loss: 22.8005\n",
      "Processing batch 4266/11884 - Val_Loss: 25.2474\n",
      "Processing batch 4267/11884 - Val_Loss: 27.1199\n",
      "Processing batch 4268/11884 - Val_Loss: 26.0506\n",
      "Processing batch 4269/11884 - Val_Loss: 28.1170\n",
      "Processing batch 4270/11884 - Val_Loss: 24.5448\n",
      "Processing batch 4271/11884 - Val_Loss: 27.0423\n",
      "Processing batch 4272/11884 - Val_Loss: 27.1583\n",
      "Processing batch 4273/11884 - Val_Loss: 26.1348\n",
      "Processing batch 4274/11884 - Val_Loss: 24.8648\n",
      "Processing batch 4275/11884 - Val_Loss: 23.9356\n",
      "Processing batch 4276/11884 - Val_Loss: 23.3919\n",
      "Processing batch 4277/11884 - Val_Loss: 27.1196\n",
      "Processing batch 4278/11884 - Val_Loss: 24.7196\n",
      "Processing batch 4279/11884 - Val_Loss: 26.3949\n",
      "Processing batch 4280/11884 - Val_Loss: 23.9194\n",
      "Processing batch 4281/11884 - Val_Loss: 23.6927\n",
      "Processing batch 4282/11884 - Val_Loss: 24.7198\n",
      "Processing batch 4283/11884 - Val_Loss: 23.9909\n",
      "Processing batch 4284/11884 - Val_Loss: 24.6720\n",
      "Processing batch 4285/11884 - Val_Loss: 27.9227\n",
      "Processing batch 4286/11884 - Val_Loss: 26.4240\n",
      "Processing batch 4287/11884 - Val_Loss: 23.4090\n",
      "Processing batch 4288/11884 - Val_Loss: 26.7944\n",
      "Processing batch 4289/11884 - Val_Loss: 27.2483\n",
      "Processing batch 4290/11884 - Val_Loss: 26.6149\n",
      "Processing batch 4291/11884 - Val_Loss: 24.9458\n",
      "Processing batch 4292/11884 - Val_Loss: 25.1412\n",
      "Processing batch 4293/11884 - Val_Loss: 26.7111\n",
      "Processing batch 4294/11884 - Val_Loss: 24.7781\n",
      "Processing batch 4295/11884 - Val_Loss: 25.2110\n",
      "Processing batch 4296/11884 - Val_Loss: 24.4119\n",
      "Processing batch 4297/11884 - Val_Loss: 24.5222\n",
      "Processing batch 4298/11884 - Val_Loss: 26.4421\n",
      "Processing batch 4299/11884 - Val_Loss: 21.7803\n",
      "Processing batch 4300/11884 - Val_Loss: 25.3630\n",
      "Processing batch 4301/11884 - Val_Loss: 23.7868\n",
      "Processing batch 4302/11884 - Val_Loss: 24.8600\n",
      "Processing batch 4303/11884 - Val_Loss: 23.6713\n",
      "Processing batch 4304/11884 - Val_Loss: 26.5738\n",
      "Processing batch 4305/11884 - Val_Loss: 26.3929\n",
      "Processing batch 4306/11884 - Val_Loss: 27.2876\n",
      "Processing batch 4307/11884 - Val_Loss: 28.2539\n",
      "Processing batch 4308/11884 - Val_Loss: 24.1683\n",
      "Processing batch 4309/11884 - Val_Loss: 26.6724\n",
      "Processing batch 4310/11884 - Val_Loss: 23.4254\n",
      "Processing batch 4311/11884 - Val_Loss: 27.1634\n",
      "Processing batch 4312/11884 - Val_Loss: 27.2250\n",
      "Processing batch 4313/11884 - Val_Loss: 26.1254\n",
      "Processing batch 4314/11884 - Val_Loss: 27.4667\n",
      "Processing batch 4315/11884 - Val_Loss: 26.5983\n",
      "Processing batch 4316/11884 - Val_Loss: 24.0317\n",
      "Processing batch 4317/11884 - Val_Loss: 23.9594\n",
      "Processing batch 4318/11884 - Val_Loss: 23.2213\n",
      "Processing batch 4319/11884 - Val_Loss: 23.6334\n",
      "Processing batch 4320/11884 - Val_Loss: 24.8264\n",
      "Processing batch 4321/11884 - Val_Loss: 27.0946\n",
      "Processing batch 4322/11884 - Val_Loss: 25.9206\n",
      "Processing batch 4323/11884 - Val_Loss: 25.9482\n",
      "Processing batch 4324/11884 - Val_Loss: 28.1356\n",
      "Processing batch 4325/11884 - Val_Loss: 23.2273\n",
      "Processing batch 4326/11884 - Val_Loss: 26.3177\n",
      "Processing batch 4327/11884 - Val_Loss: 24.8225\n",
      "Processing batch 4328/11884 - Val_Loss: 25.8420\n",
      "Processing batch 4329/11884 - Val_Loss: 25.4339\n",
      "Processing batch 4330/11884 - Val_Loss: 24.8849\n",
      "Processing batch 4331/11884 - Val_Loss: 28.0143\n",
      "Processing batch 4332/11884 - Val_Loss: 27.7368\n",
      "Processing batch 4333/11884 - Val_Loss: 23.1194\n",
      "Processing batch 4334/11884 - Val_Loss: 26.5685\n",
      "Processing batch 4335/11884 - Val_Loss: 23.8470\n",
      "Processing batch 4336/11884 - Val_Loss: 26.3390\n",
      "Processing batch 4337/11884 - Val_Loss: 26.4446\n",
      "Processing batch 4338/11884 - Val_Loss: 26.4295\n",
      "Processing batch 4339/11884 - Val_Loss: 23.7249\n",
      "Processing batch 4340/11884 - Val_Loss: 30.2790\n",
      "Processing batch 4341/11884 - Val_Loss: 26.7509\n",
      "Processing batch 4342/11884 - Val_Loss: 24.3824\n",
      "Processing batch 4343/11884 - Val_Loss: 25.3070\n",
      "Processing batch 4344/11884 - Val_Loss: 26.9034\n",
      "Processing batch 4345/11884 - Val_Loss: 27.1658\n",
      "Processing batch 4346/11884 - Val_Loss: 26.9028\n",
      "Processing batch 4347/11884 - Val_Loss: 24.6251\n",
      "Processing batch 4348/11884 - Val_Loss: 24.6340\n",
      "Processing batch 4349/11884 - Val_Loss: 26.9674\n",
      "Processing batch 4350/11884 - Val_Loss: 24.5766\n",
      "Processing batch 4351/11884 - Val_Loss: 24.4921\n",
      "Processing batch 4352/11884 - Val_Loss: 25.6055\n",
      "Processing batch 4353/11884 - Val_Loss: 26.8935\n",
      "Processing batch 4354/11884 - Val_Loss: 26.9327\n",
      "Processing batch 4355/11884 - Val_Loss: 24.9256\n",
      "Processing batch 4356/11884 - Val_Loss: 26.1787\n",
      "Processing batch 4357/11884 - Val_Loss: 24.9140\n",
      "Processing batch 4358/11884 - Val_Loss: 26.5596\n",
      "Processing batch 4359/11884 - Val_Loss: 27.2175\n",
      "Processing batch 4360/11884 - Val_Loss: 29.2206\n",
      "Processing batch 4361/11884 - Val_Loss: 28.0267\n",
      "Processing batch 4362/11884 - Val_Loss: 24.9622\n",
      "Processing batch 4363/11884 - Val_Loss: 24.7110\n",
      "Processing batch 4364/11884 - Val_Loss: 29.9628\n",
      "Processing batch 4365/11884 - Val_Loss: 26.2928\n",
      "Processing batch 4366/11884 - Val_Loss: 25.1917\n",
      "Processing batch 4367/11884 - Val_Loss: 27.3123\n",
      "Processing batch 4368/11884 - Val_Loss: 20.3600\n",
      "Processing batch 4369/11884 - Val_Loss: 23.4695\n",
      "Processing batch 4370/11884 - Val_Loss: 24.4564\n",
      "Processing batch 4371/11884 - Val_Loss: 27.6461\n",
      "Processing batch 4372/11884 - Val_Loss: 26.3525\n",
      "Processing batch 4373/11884 - Val_Loss: 26.2562\n",
      "Processing batch 4374/11884 - Val_Loss: 26.0999\n",
      "Processing batch 4375/11884 - Val_Loss: 24.3068\n",
      "Processing batch 4376/11884 - Val_Loss: 25.2213\n",
      "Processing batch 4377/11884 - Val_Loss: 24.5339\n",
      "Processing batch 4378/11884 - Val_Loss: 25.2422\n",
      "Processing batch 4379/11884 - Val_Loss: 24.9784\n",
      "Processing batch 4380/11884 - Val_Loss: 25.7762\n",
      "Processing batch 4381/11884 - Val_Loss: 24.6966\n",
      "Processing batch 4382/11884 - Val_Loss: 26.2624\n",
      "Processing batch 4383/11884 - Val_Loss: 26.4002\n",
      "Processing batch 4384/11884 - Val_Loss: 24.0768\n",
      "Processing batch 4385/11884 - Val_Loss: 26.5810\n",
      "Processing batch 4386/11884 - Val_Loss: 26.1202\n",
      "Processing batch 4387/11884 - Val_Loss: 25.8775\n",
      "Processing batch 4388/11884 - Val_Loss: 26.6363\n",
      "Processing batch 4389/11884 - Val_Loss: 25.3275\n",
      "Processing batch 4390/11884 - Val_Loss: 25.0599\n",
      "Processing batch 4391/11884 - Val_Loss: 24.8529\n",
      "Processing batch 4392/11884 - Val_Loss: 24.4242\n",
      "Processing batch 4393/11884 - Val_Loss: 24.0514\n",
      "Processing batch 4394/11884 - Val_Loss: 25.0539\n",
      "Processing batch 4395/11884 - Val_Loss: 26.0616\n",
      "Processing batch 4396/11884 - Val_Loss: 28.6224\n",
      "Processing batch 4397/11884 - Val_Loss: 23.2331\n",
      "Processing batch 4398/11884 - Val_Loss: 25.3835\n",
      "Processing batch 4399/11884 - Val_Loss: 26.4321\n",
      "Processing batch 4400/11884 - Val_Loss: 26.1589\n",
      "Processing batch 4401/11884 - Val_Loss: 28.5956\n",
      "Processing batch 4402/11884 - Val_Loss: 25.4807\n",
      "Processing batch 4403/11884 - Val_Loss: 27.6911\n",
      "Processing batch 4404/11884 - Val_Loss: 27.1212\n",
      "Processing batch 4405/11884 - Val_Loss: 24.9138\n",
      "Processing batch 4406/11884 - Val_Loss: 24.5746\n",
      "Processing batch 4407/11884 - Val_Loss: 25.1414\n",
      "Processing batch 4408/11884 - Val_Loss: 24.5500\n",
      "Processing batch 4409/11884 - Val_Loss: 27.1127\n",
      "Processing batch 4410/11884 - Val_Loss: 25.6195\n",
      "Processing batch 4411/11884 - Val_Loss: 24.0524\n",
      "Processing batch 4412/11884 - Val_Loss: 26.4339\n",
      "Processing batch 4413/11884 - Val_Loss: 25.7138\n",
      "Processing batch 4414/11884 - Val_Loss: 25.5940\n",
      "Processing batch 4415/11884 - Val_Loss: 26.9211\n",
      "Processing batch 4416/11884 - Val_Loss: 27.2101\n",
      "Processing batch 4417/11884 - Val_Loss: 25.5227\n",
      "Processing batch 4418/11884 - Val_Loss: 27.5510\n",
      "Processing batch 4419/11884 - Val_Loss: 26.9662\n",
      "Processing batch 4420/11884 - Val_Loss: 25.5628\n",
      "Processing batch 4421/11884 - Val_Loss: 24.9713\n",
      "Processing batch 4422/11884 - Val_Loss: 26.4434\n",
      "Processing batch 4423/11884 - Val_Loss: 25.6314\n",
      "Processing batch 4424/11884 - Val_Loss: 25.8056\n",
      "Processing batch 4425/11884 - Val_Loss: 23.6499\n",
      "Processing batch 4426/11884 - Val_Loss: 26.7764\n",
      "Processing batch 4427/11884 - Val_Loss: 28.1090\n",
      "Processing batch 4428/11884 - Val_Loss: 24.4462\n",
      "Processing batch 4429/11884 - Val_Loss: 25.0541\n",
      "Processing batch 4430/11884 - Val_Loss: 22.5777\n",
      "Processing batch 4431/11884 - Val_Loss: 22.2048\n",
      "Processing batch 4432/11884 - Val_Loss: 24.4347\n",
      "Processing batch 4433/11884 - Val_Loss: 30.0615\n",
      "Processing batch 4434/11884 - Val_Loss: 25.1420\n",
      "Processing batch 4435/11884 - Val_Loss: 26.5400\n",
      "Processing batch 4436/11884 - Val_Loss: 26.6429\n",
      "Processing batch 4437/11884 - Val_Loss: 23.3202\n",
      "Processing batch 4438/11884 - Val_Loss: 26.9537\n",
      "Processing batch 4439/11884 - Val_Loss: 28.7665\n",
      "Processing batch 4440/11884 - Val_Loss: 26.8967\n",
      "Processing batch 4441/11884 - Val_Loss: 25.0767\n",
      "Processing batch 4442/11884 - Val_Loss: 24.8439\n",
      "Processing batch 4443/11884 - Val_Loss: 26.5383\n",
      "Processing batch 4444/11884 - Val_Loss: 25.6573\n",
      "Processing batch 4445/11884 - Val_Loss: 26.1536\n",
      "Processing batch 4446/11884 - Val_Loss: 23.7905\n",
      "Processing batch 4447/11884 - Val_Loss: 25.1350\n",
      "Processing batch 4448/11884 - Val_Loss: 25.9095\n",
      "Processing batch 4449/11884 - Val_Loss: 25.6163\n",
      "Processing batch 4450/11884 - Val_Loss: 27.5004\n",
      "Processing batch 4451/11884 - Val_Loss: 27.0549\n",
      "Processing batch 4452/11884 - Val_Loss: 24.6854\n",
      "Processing batch 4453/11884 - Val_Loss: 27.3021\n",
      "Processing batch 4454/11884 - Val_Loss: 23.8446\n",
      "Processing batch 4455/11884 - Val_Loss: 27.7803\n",
      "Processing batch 4456/11884 - Val_Loss: 23.7346\n",
      "Processing batch 4457/11884 - Val_Loss: 25.9186\n",
      "Processing batch 4458/11884 - Val_Loss: 26.4437\n",
      "Processing batch 4459/11884 - Val_Loss: 24.3913\n",
      "Processing batch 4460/11884 - Val_Loss: 26.4006\n",
      "Processing batch 4461/11884 - Val_Loss: 25.4923\n",
      "Processing batch 4462/11884 - Val_Loss: 28.2369\n",
      "Processing batch 4463/11884 - Val_Loss: 23.3919\n",
      "Processing batch 4464/11884 - Val_Loss: 25.4963\n",
      "Processing batch 4465/11884 - Val_Loss: 24.5799\n",
      "Processing batch 4466/11884 - Val_Loss: 23.2583\n",
      "Processing batch 4467/11884 - Val_Loss: 24.6208\n",
      "Processing batch 4468/11884 - Val_Loss: 25.7793\n",
      "Processing batch 4469/11884 - Val_Loss: 24.2468\n",
      "Processing batch 4470/11884 - Val_Loss: 23.4645\n",
      "Processing batch 4471/11884 - Val_Loss: 28.5306\n",
      "Processing batch 4472/11884 - Val_Loss: 22.8353\n",
      "Processing batch 4473/11884 - Val_Loss: 27.5653\n",
      "Processing batch 4474/11884 - Val_Loss: 25.1223\n",
      "Processing batch 4475/11884 - Val_Loss: 25.5693\n",
      "Processing batch 4476/11884 - Val_Loss: 25.5678\n",
      "Processing batch 4477/11884 - Val_Loss: 26.3780\n",
      "Processing batch 4478/11884 - Val_Loss: 22.6648\n",
      "Processing batch 4479/11884 - Val_Loss: 24.5313\n",
      "Processing batch 4480/11884 - Val_Loss: 28.4047\n",
      "Processing batch 4481/11884 - Val_Loss: 25.7202\n",
      "Processing batch 4482/11884 - Val_Loss: 23.4643\n",
      "Processing batch 4483/11884 - Val_Loss: 25.2447\n",
      "Processing batch 4484/11884 - Val_Loss: 24.8873\n",
      "Processing batch 4485/11884 - Val_Loss: 26.9659\n",
      "Processing batch 4486/11884 - Val_Loss: 26.1816\n",
      "Processing batch 4487/11884 - Val_Loss: 22.8719\n",
      "Processing batch 4488/11884 - Val_Loss: 24.3521\n",
      "Processing batch 4489/11884 - Val_Loss: 26.2491\n",
      "Processing batch 4490/11884 - Val_Loss: 24.6085\n",
      "Processing batch 4491/11884 - Val_Loss: 27.3891\n",
      "Processing batch 4492/11884 - Val_Loss: 27.0991\n",
      "Processing batch 4493/11884 - Val_Loss: 22.5377\n",
      "Processing batch 4494/11884 - Val_Loss: 21.3308\n",
      "Processing batch 4495/11884 - Val_Loss: 26.5398\n",
      "Processing batch 4496/11884 - Val_Loss: 24.1354\n",
      "Processing batch 4497/11884 - Val_Loss: 25.6376\n",
      "Processing batch 4498/11884 - Val_Loss: 27.9726\n",
      "Processing batch 4499/11884 - Val_Loss: 21.9795\n",
      "Processing batch 4500/11884 - Val_Loss: 26.2314\n",
      "Processing batch 4501/11884 - Val_Loss: 28.1129\n",
      "Processing batch 4502/11884 - Val_Loss: 23.5586\n",
      "Processing batch 4503/11884 - Val_Loss: 24.5976\n",
      "Processing batch 4504/11884 - Val_Loss: 25.4763\n",
      "Processing batch 4505/11884 - Val_Loss: 27.1552\n",
      "Processing batch 4506/11884 - Val_Loss: 23.8930\n",
      "Processing batch 4507/11884 - Val_Loss: 27.4355\n",
      "Processing batch 4508/11884 - Val_Loss: 26.5591\n",
      "Processing batch 4509/11884 - Val_Loss: 25.7919\n",
      "Processing batch 4510/11884 - Val_Loss: 28.3551\n",
      "Processing batch 4511/11884 - Val_Loss: 26.2048\n",
      "Processing batch 4512/11884 - Val_Loss: 25.1051\n",
      "Processing batch 4513/11884 - Val_Loss: 26.6979\n",
      "Processing batch 4514/11884 - Val_Loss: 28.1839\n",
      "Processing batch 4515/11884 - Val_Loss: 26.7462\n",
      "Processing batch 4516/11884 - Val_Loss: 24.5450\n",
      "Processing batch 4517/11884 - Val_Loss: 24.8299\n",
      "Processing batch 4518/11884 - Val_Loss: 25.4142\n",
      "Processing batch 4519/11884 - Val_Loss: 26.5159\n",
      "Processing batch 4520/11884 - Val_Loss: 25.9394\n",
      "Processing batch 4521/11884 - Val_Loss: 25.4008\n",
      "Processing batch 4522/11884 - Val_Loss: 26.4808\n",
      "Processing batch 4523/11884 - Val_Loss: 25.3741\n",
      "Processing batch 4524/11884 - Val_Loss: 27.8717\n",
      "Processing batch 4525/11884 - Val_Loss: 26.7801\n",
      "Processing batch 4526/11884 - Val_Loss: 25.6756\n",
      "Processing batch 4527/11884 - Val_Loss: 25.6689\n",
      "Processing batch 4528/11884 - Val_Loss: 26.9347\n",
      "Processing batch 4529/11884 - Val_Loss: 24.7573\n",
      "Processing batch 4530/11884 - Val_Loss: 25.2052\n",
      "Processing batch 4531/11884 - Val_Loss: 24.1766\n",
      "Processing batch 4532/11884 - Val_Loss: 27.9723\n",
      "Processing batch 4533/11884 - Val_Loss: 26.8271\n",
      "Processing batch 4534/11884 - Val_Loss: 24.7985\n",
      "Processing batch 4535/11884 - Val_Loss: 25.1386\n",
      "Processing batch 4536/11884 - Val_Loss: 29.2127\n",
      "Processing batch 4537/11884 - Val_Loss: 22.2704\n",
      "Processing batch 4538/11884 - Val_Loss: 24.6286\n",
      "Processing batch 4539/11884 - Val_Loss: 24.6448\n",
      "Processing batch 4540/11884 - Val_Loss: 25.9004\n",
      "Processing batch 4541/11884 - Val_Loss: 25.4231\n",
      "Processing batch 4542/11884 - Val_Loss: 25.6290\n",
      "Processing batch 4543/11884 - Val_Loss: 23.4802\n",
      "Processing batch 4544/11884 - Val_Loss: 28.0098\n",
      "Processing batch 4545/11884 - Val_Loss: 24.5550\n",
      "Processing batch 4546/11884 - Val_Loss: 23.9705\n",
      "Processing batch 4547/11884 - Val_Loss: 22.8802\n",
      "Processing batch 4548/11884 - Val_Loss: 26.9508\n",
      "Processing batch 4549/11884 - Val_Loss: 26.9600\n",
      "Processing batch 4550/11884 - Val_Loss: 27.0666\n",
      "Processing batch 4551/11884 - Val_Loss: 21.4659\n",
      "Processing batch 4552/11884 - Val_Loss: 26.8044\n",
      "Processing batch 4553/11884 - Val_Loss: 26.3186\n",
      "Processing batch 4554/11884 - Val_Loss: 25.3187\n",
      "Processing batch 4555/11884 - Val_Loss: 22.7214\n",
      "Processing batch 4556/11884 - Val_Loss: 22.7748\n",
      "Processing batch 4557/11884 - Val_Loss: 23.8333\n",
      "Processing batch 4558/11884 - Val_Loss: 27.0165\n",
      "Processing batch 4559/11884 - Val_Loss: 26.2986\n",
      "Processing batch 4560/11884 - Val_Loss: 26.1900\n",
      "Processing batch 4561/11884 - Val_Loss: 29.1632\n",
      "Processing batch 4562/11884 - Val_Loss: 27.5965\n",
      "Processing batch 4563/11884 - Val_Loss: 27.2113\n",
      "Processing batch 4564/11884 - Val_Loss: 24.9537\n",
      "Processing batch 4565/11884 - Val_Loss: 24.6537\n",
      "Processing batch 4566/11884 - Val_Loss: 26.1204\n",
      "Processing batch 4567/11884 - Val_Loss: 25.7539\n",
      "Processing batch 4568/11884 - Val_Loss: 28.5198\n",
      "Processing batch 4569/11884 - Val_Loss: 24.6123\n",
      "Processing batch 4570/11884 - Val_Loss: 24.2263\n",
      "Processing batch 4571/11884 - Val_Loss: 26.7335\n",
      "Processing batch 4572/11884 - Val_Loss: 25.8733\n",
      "Processing batch 4573/11884 - Val_Loss: 25.3945\n",
      "Processing batch 4574/11884 - Val_Loss: 26.4442\n",
      "Processing batch 4575/11884 - Val_Loss: 24.9604\n",
      "Processing batch 4576/11884 - Val_Loss: 23.6985\n",
      "Processing batch 4577/11884 - Val_Loss: 23.1052\n",
      "Processing batch 4578/11884 - Val_Loss: 28.0812\n",
      "Processing batch 4579/11884 - Val_Loss: 26.7282\n",
      "Processing batch 4580/11884 - Val_Loss: 25.2507\n",
      "Processing batch 4581/11884 - Val_Loss: 27.8204\n",
      "Processing batch 4582/11884 - Val_Loss: 27.4131\n",
      "Processing batch 4583/11884 - Val_Loss: 25.8760\n",
      "Processing batch 4584/11884 - Val_Loss: 25.2164\n",
      "Processing batch 4585/11884 - Val_Loss: 22.5905\n",
      "Processing batch 4586/11884 - Val_Loss: 27.8894\n",
      "Processing batch 4587/11884 - Val_Loss: 27.3647\n",
      "Processing batch 4588/11884 - Val_Loss: 29.4872\n",
      "Processing batch 4589/11884 - Val_Loss: 25.2353\n",
      "Processing batch 4590/11884 - Val_Loss: 26.1532\n",
      "Processing batch 4591/11884 - Val_Loss: 22.3812\n",
      "Processing batch 4592/11884 - Val_Loss: 27.0882\n",
      "Processing batch 4593/11884 - Val_Loss: 28.6625\n",
      "Processing batch 4594/11884 - Val_Loss: 26.7654\n",
      "Processing batch 4595/11884 - Val_Loss: 24.5823\n",
      "Processing batch 4596/11884 - Val_Loss: 23.3524\n",
      "Processing batch 4597/11884 - Val_Loss: 25.8614\n",
      "Processing batch 4598/11884 - Val_Loss: 24.1789\n",
      "Processing batch 4599/11884 - Val_Loss: 24.1681\n",
      "Processing batch 4600/11884 - Val_Loss: 25.8171\n",
      "Processing batch 4601/11884 - Val_Loss: 25.1057\n",
      "Processing batch 4602/11884 - Val_Loss: 24.6115\n",
      "Processing batch 4603/11884 - Val_Loss: 26.2503\n",
      "Processing batch 4604/11884 - Val_Loss: 23.7604\n",
      "Processing batch 4605/11884 - Val_Loss: 25.7978\n",
      "Processing batch 4606/11884 - Val_Loss: 25.5639\n",
      "Processing batch 4607/11884 - Val_Loss: 26.5663\n",
      "Processing batch 4608/11884 - Val_Loss: 23.8224\n",
      "Processing batch 4609/11884 - Val_Loss: 27.5417\n",
      "Processing batch 4610/11884 - Val_Loss: 26.1396\n",
      "Processing batch 4611/11884 - Val_Loss: 25.1953\n",
      "Processing batch 4612/11884 - Val_Loss: 27.9968\n",
      "Processing batch 4613/11884 - Val_Loss: 25.8578\n",
      "Processing batch 4614/11884 - Val_Loss: 24.1813\n",
      "Processing batch 4615/11884 - Val_Loss: 25.4309\n",
      "Processing batch 4616/11884 - Val_Loss: 30.0360\n",
      "Processing batch 4617/11884 - Val_Loss: 27.8596\n",
      "Processing batch 4618/11884 - Val_Loss: 24.8489\n",
      "Processing batch 4619/11884 - Val_Loss: 26.6004\n",
      "Processing batch 4620/11884 - Val_Loss: 23.1566\n",
      "Processing batch 4621/11884 - Val_Loss: 29.3743\n",
      "Processing batch 4622/11884 - Val_Loss: 26.0344\n",
      "Processing batch 4623/11884 - Val_Loss: 26.2391\n",
      "Processing batch 4624/11884 - Val_Loss: 22.7735\n",
      "Processing batch 4625/11884 - Val_Loss: 24.9914\n",
      "Processing batch 4626/11884 - Val_Loss: 26.4461\n",
      "Processing batch 4627/11884 - Val_Loss: 25.1315\n",
      "Processing batch 4628/11884 - Val_Loss: 25.5658\n",
      "Processing batch 4629/11884 - Val_Loss: 23.0177\n",
      "Processing batch 4630/11884 - Val_Loss: 24.5703\n",
      "Processing batch 4631/11884 - Val_Loss: 24.6750\n",
      "Processing batch 4632/11884 - Val_Loss: 26.3500\n",
      "Processing batch 4633/11884 - Val_Loss: 25.5416\n",
      "Processing batch 4634/11884 - Val_Loss: 27.0941\n",
      "Processing batch 4635/11884 - Val_Loss: 24.8068\n",
      "Processing batch 4636/11884 - Val_Loss: 24.2705\n",
      "Processing batch 4637/11884 - Val_Loss: 26.1206\n",
      "Processing batch 4638/11884 - Val_Loss: 24.0440\n",
      "Processing batch 4639/11884 - Val_Loss: 26.3744\n",
      "Processing batch 4640/11884 - Val_Loss: 25.2980\n",
      "Processing batch 4641/11884 - Val_Loss: 24.6637\n",
      "Processing batch 4642/11884 - Val_Loss: 25.2943\n",
      "Processing batch 4643/11884 - Val_Loss: 25.9484\n",
      "Processing batch 4644/11884 - Val_Loss: 26.2193\n",
      "Processing batch 4645/11884 - Val_Loss: 24.4203\n",
      "Processing batch 4646/11884 - Val_Loss: 23.4860\n",
      "Processing batch 4647/11884 - Val_Loss: 28.3215\n",
      "Processing batch 4648/11884 - Val_Loss: 26.3573\n",
      "Processing batch 4649/11884 - Val_Loss: 28.1971\n",
      "Processing batch 4650/11884 - Val_Loss: 28.1587\n",
      "Processing batch 4651/11884 - Val_Loss: 27.4047\n",
      "Processing batch 4652/11884 - Val_Loss: 26.2854\n",
      "Processing batch 4653/11884 - Val_Loss: 27.2516\n",
      "Processing batch 4654/11884 - Val_Loss: 29.5386\n",
      "Processing batch 4655/11884 - Val_Loss: 22.6682\n",
      "Processing batch 4656/11884 - Val_Loss: 23.8644\n",
      "Processing batch 4657/11884 - Val_Loss: 26.2358\n",
      "Processing batch 4658/11884 - Val_Loss: 23.9992\n",
      "Processing batch 4659/11884 - Val_Loss: 23.8139\n",
      "Processing batch 4660/11884 - Val_Loss: 27.2354\n",
      "Processing batch 4661/11884 - Val_Loss: 25.1081\n",
      "Processing batch 4662/11884 - Val_Loss: 26.7902\n",
      "Processing batch 4663/11884 - Val_Loss: 26.1595\n",
      "Processing batch 4664/11884 - Val_Loss: 25.7443\n",
      "Processing batch 4665/11884 - Val_Loss: 28.0016\n",
      "Processing batch 4666/11884 - Val_Loss: 27.6303\n",
      "Processing batch 4667/11884 - Val_Loss: 27.7517\n",
      "Processing batch 4668/11884 - Val_Loss: 26.1390\n",
      "Processing batch 4669/11884 - Val_Loss: 23.4609\n",
      "Processing batch 4670/11884 - Val_Loss: 26.1477\n",
      "Processing batch 4671/11884 - Val_Loss: 22.7303\n",
      "Processing batch 4672/11884 - Val_Loss: 27.5088\n",
      "Processing batch 4673/11884 - Val_Loss: 25.2364\n",
      "Processing batch 4674/11884 - Val_Loss: 26.1613\n",
      "Processing batch 4675/11884 - Val_Loss: 26.4565\n",
      "Processing batch 4676/11884 - Val_Loss: 25.0129\n",
      "Processing batch 4677/11884 - Val_Loss: 26.3534\n",
      "Processing batch 4678/11884 - Val_Loss: 23.3575\n",
      "Processing batch 4679/11884 - Val_Loss: 27.3684\n",
      "Processing batch 4680/11884 - Val_Loss: 25.7590\n",
      "Processing batch 4681/11884 - Val_Loss: 25.9687\n",
      "Processing batch 4682/11884 - Val_Loss: 23.2001\n",
      "Processing batch 4683/11884 - Val_Loss: 24.6245\n",
      "Processing batch 4684/11884 - Val_Loss: 25.0599\n",
      "Processing batch 4685/11884 - Val_Loss: 28.1939\n",
      "Processing batch 4686/11884 - Val_Loss: 26.4688\n",
      "Processing batch 4687/11884 - Val_Loss: 25.8586\n",
      "Processing batch 4688/11884 - Val_Loss: 26.4999\n",
      "Processing batch 4689/11884 - Val_Loss: 23.5332\n",
      "Processing batch 4690/11884 - Val_Loss: 26.9368\n",
      "Processing batch 4691/11884 - Val_Loss: 25.3180\n",
      "Processing batch 4692/11884 - Val_Loss: 25.9794\n",
      "Processing batch 4693/11884 - Val_Loss: 27.8343\n",
      "Processing batch 4694/11884 - Val_Loss: 25.8927\n",
      "Processing batch 4695/11884 - Val_Loss: 26.3408\n",
      "Processing batch 4696/11884 - Val_Loss: 25.6437\n",
      "Processing batch 4697/11884 - Val_Loss: 26.7202\n",
      "Processing batch 4698/11884 - Val_Loss: 26.7675\n",
      "Processing batch 4699/11884 - Val_Loss: 24.7056\n",
      "Processing batch 4700/11884 - Val_Loss: 25.6645\n",
      "Processing batch 4701/11884 - Val_Loss: 24.8309\n",
      "Processing batch 4702/11884 - Val_Loss: 26.5202\n",
      "Processing batch 4703/11884 - Val_Loss: 24.4919\n",
      "Processing batch 4704/11884 - Val_Loss: 27.5798\n",
      "Processing batch 4705/11884 - Val_Loss: 24.9034\n",
      "Processing batch 4706/11884 - Val_Loss: 25.4745\n",
      "Processing batch 4707/11884 - Val_Loss: 28.1659\n",
      "Processing batch 4708/11884 - Val_Loss: 25.8890\n",
      "Processing batch 4709/11884 - Val_Loss: 29.7828\n",
      "Processing batch 4710/11884 - Val_Loss: 25.1147\n",
      "Processing batch 4711/11884 - Val_Loss: 26.7170\n",
      "Processing batch 4712/11884 - Val_Loss: 27.6397\n",
      "Processing batch 4713/11884 - Val_Loss: 25.7464\n",
      "Processing batch 4714/11884 - Val_Loss: 26.4654\n",
      "Processing batch 4715/11884 - Val_Loss: 25.2245\n",
      "Processing batch 4716/11884 - Val_Loss: 29.7354\n",
      "Processing batch 4717/11884 - Val_Loss: 25.7293\n",
      "Processing batch 4718/11884 - Val_Loss: 26.7187\n",
      "Processing batch 4719/11884 - Val_Loss: 26.2559\n",
      "Processing batch 4720/11884 - Val_Loss: 24.0405\n",
      "Processing batch 4721/11884 - Val_Loss: 25.9124\n",
      "Processing batch 4722/11884 - Val_Loss: 26.4069\n",
      "Processing batch 4723/11884 - Val_Loss: 27.1274\n",
      "Processing batch 4724/11884 - Val_Loss: 25.2012\n",
      "Processing batch 4725/11884 - Val_Loss: 26.3880\n",
      "Processing batch 4726/11884 - Val_Loss: 26.8591\n",
      "Processing batch 4727/11884 - Val_Loss: 27.1137\n",
      "Processing batch 4728/11884 - Val_Loss: 25.9724\n",
      "Processing batch 4729/11884 - Val_Loss: 26.3730\n",
      "Processing batch 4730/11884 - Val_Loss: 25.1205\n",
      "Processing batch 4731/11884 - Val_Loss: 29.1931\n",
      "Processing batch 4732/11884 - Val_Loss: 23.2479\n",
      "Processing batch 4733/11884 - Val_Loss: 27.4719\n",
      "Processing batch 4734/11884 - Val_Loss: 30.4767\n",
      "Processing batch 4735/11884 - Val_Loss: 25.5586\n",
      "Processing batch 4736/11884 - Val_Loss: 25.6406\n",
      "Processing batch 4737/11884 - Val_Loss: 26.5656\n",
      "Processing batch 4738/11884 - Val_Loss: 24.4622\n",
      "Processing batch 4739/11884 - Val_Loss: 25.3794\n",
      "Processing batch 4740/11884 - Val_Loss: 26.0573\n",
      "Processing batch 4741/11884 - Val_Loss: 23.2325\n",
      "Processing batch 4742/11884 - Val_Loss: 27.1033\n",
      "Processing batch 4743/11884 - Val_Loss: 24.9532\n",
      "Processing batch 4744/11884 - Val_Loss: 27.8629\n",
      "Processing batch 4745/11884 - Val_Loss: 28.8018\n",
      "Processing batch 4746/11884 - Val_Loss: 28.6867\n",
      "Processing batch 4747/11884 - Val_Loss: 26.1689\n",
      "Processing batch 4748/11884 - Val_Loss: 25.9622\n",
      "Processing batch 4749/11884 - Val_Loss: 24.7818\n",
      "Processing batch 4750/11884 - Val_Loss: 24.8008\n",
      "Processing batch 4751/11884 - Val_Loss: 26.0466\n",
      "Processing batch 4752/11884 - Val_Loss: 23.7585\n",
      "Processing batch 4753/11884 - Val_Loss: 25.8005\n",
      "Processing batch 4754/11884 - Val_Loss: 30.1047\n",
      "Processing batch 4755/11884 - Val_Loss: 27.2866\n",
      "Processing batch 4756/11884 - Val_Loss: 29.9563\n",
      "Processing batch 4757/11884 - Val_Loss: 25.3833\n",
      "Processing batch 4758/11884 - Val_Loss: 27.5936\n",
      "Processing batch 4759/11884 - Val_Loss: 24.0929\n",
      "Processing batch 4760/11884 - Val_Loss: 25.8889\n",
      "Processing batch 4761/11884 - Val_Loss: 26.0371\n",
      "Processing batch 4762/11884 - Val_Loss: 26.9728\n",
      "Processing batch 4763/11884 - Val_Loss: 26.8115\n",
      "Processing batch 4764/11884 - Val_Loss: 23.6578\n",
      "Processing batch 4765/11884 - Val_Loss: 24.5597\n",
      "Processing batch 4766/11884 - Val_Loss: 25.1398\n",
      "Processing batch 4767/11884 - Val_Loss: 26.4628\n",
      "Processing batch 4768/11884 - Val_Loss: 25.3943\n",
      "Processing batch 4769/11884 - Val_Loss: 26.4837\n",
      "Processing batch 4770/11884 - Val_Loss: 25.2245\n",
      "Processing batch 4771/11884 - Val_Loss: 23.4736\n",
      "Processing batch 4772/11884 - Val_Loss: 25.2306\n",
      "Processing batch 4773/11884 - Val_Loss: 24.8559\n",
      "Processing batch 4774/11884 - Val_Loss: 27.4901\n",
      "Processing batch 4775/11884 - Val_Loss: 27.4297\n",
      "Processing batch 4776/11884 - Val_Loss: 27.0918\n",
      "Processing batch 4777/11884 - Val_Loss: 24.5410\n",
      "Processing batch 4778/11884 - Val_Loss: 23.6704\n",
      "Processing batch 4779/11884 - Val_Loss: 22.8380\n",
      "Processing batch 4780/11884 - Val_Loss: 27.5310\n",
      "Processing batch 4781/11884 - Val_Loss: 24.6494\n",
      "Processing batch 4782/11884 - Val_Loss: 25.8910\n",
      "Processing batch 4783/11884 - Val_Loss: 26.1571\n",
      "Processing batch 4784/11884 - Val_Loss: 25.9199\n",
      "Processing batch 4785/11884 - Val_Loss: 26.5326\n",
      "Processing batch 4786/11884 - Val_Loss: 24.0921\n",
      "Processing batch 4787/11884 - Val_Loss: 24.4262\n",
      "Processing batch 4788/11884 - Val_Loss: 26.1452\n",
      "Processing batch 4789/11884 - Val_Loss: 26.7692\n",
      "Processing batch 4790/11884 - Val_Loss: 25.6989\n",
      "Processing batch 4791/11884 - Val_Loss: 25.1216\n",
      "Processing batch 4792/11884 - Val_Loss: 24.6970\n",
      "Processing batch 4793/11884 - Val_Loss: 26.7214\n",
      "Processing batch 4794/11884 - Val_Loss: 23.8902\n",
      "Processing batch 4795/11884 - Val_Loss: 27.0788\n",
      "Processing batch 4796/11884 - Val_Loss: 28.8401\n",
      "Processing batch 4797/11884 - Val_Loss: 25.5455\n",
      "Processing batch 4798/11884 - Val_Loss: 28.1800\n",
      "Processing batch 4799/11884 - Val_Loss: 25.4620\n",
      "Processing batch 4800/11884 - Val_Loss: 26.9127\n",
      "Processing batch 4801/11884 - Val_Loss: 26.6857\n",
      "Processing batch 4802/11884 - Val_Loss: 29.3164\n",
      "Processing batch 4803/11884 - Val_Loss: 26.5163\n",
      "Processing batch 4804/11884 - Val_Loss: 24.7991\n",
      "Processing batch 4805/11884 - Val_Loss: 27.1251\n",
      "Processing batch 4806/11884 - Val_Loss: 27.2415\n",
      "Processing batch 4807/11884 - Val_Loss: 26.8891\n",
      "Processing batch 4808/11884 - Val_Loss: 25.0321\n",
      "Processing batch 4809/11884 - Val_Loss: 27.1979\n",
      "Processing batch 4810/11884 - Val_Loss: 26.6534\n",
      "Processing batch 4811/11884 - Val_Loss: 27.7565\n",
      "Processing batch 4812/11884 - Val_Loss: 25.6622\n",
      "Processing batch 4813/11884 - Val_Loss: 26.0205\n",
      "Processing batch 4814/11884 - Val_Loss: 27.8934\n",
      "Processing batch 4815/11884 - Val_Loss: 24.7597\n",
      "Processing batch 4816/11884 - Val_Loss: 25.1239\n",
      "Processing batch 4817/11884 - Val_Loss: 24.2664\n",
      "Processing batch 4818/11884 - Val_Loss: 27.1164\n",
      "Processing batch 4819/11884 - Val_Loss: 27.6846\n",
      "Processing batch 4820/11884 - Val_Loss: 27.5886\n",
      "Processing batch 4821/11884 - Val_Loss: 28.5257\n",
      "Processing batch 4822/11884 - Val_Loss: 26.3900\n",
      "Processing batch 4823/11884 - Val_Loss: 25.9389\n",
      "Processing batch 4824/11884 - Val_Loss: 27.4007\n",
      "Processing batch 4825/11884 - Val_Loss: 25.8570\n",
      "Processing batch 4826/11884 - Val_Loss: 25.2638\n",
      "Processing batch 4827/11884 - Val_Loss: 24.7625\n",
      "Processing batch 4828/11884 - Val_Loss: 25.9624\n",
      "Processing batch 4829/11884 - Val_Loss: 26.4171\n",
      "Processing batch 4830/11884 - Val_Loss: 26.0523\n",
      "Processing batch 4831/11884 - Val_Loss: 23.5538\n",
      "Processing batch 4832/11884 - Val_Loss: 24.5777\n",
      "Processing batch 4833/11884 - Val_Loss: 24.0135\n",
      "Processing batch 4834/11884 - Val_Loss: 25.7761\n",
      "Processing batch 4835/11884 - Val_Loss: 22.8052\n",
      "Processing batch 4836/11884 - Val_Loss: 26.0457\n",
      "Processing batch 4837/11884 - Val_Loss: 23.7045\n",
      "Processing batch 4838/11884 - Val_Loss: 25.9042\n",
      "Processing batch 4839/11884 - Val_Loss: 25.4699\n",
      "Processing batch 4840/11884 - Val_Loss: 24.4399\n",
      "Processing batch 4841/11884 - Val_Loss: 26.9004\n",
      "Processing batch 4842/11884 - Val_Loss: 26.0129\n",
      "Processing batch 4843/11884 - Val_Loss: 25.8026\n",
      "Processing batch 4844/11884 - Val_Loss: 26.4337\n",
      "Processing batch 4845/11884 - Val_Loss: 29.4176\n",
      "Processing batch 4846/11884 - Val_Loss: 27.5613\n",
      "Processing batch 4847/11884 - Val_Loss: 25.0862\n",
      "Processing batch 4848/11884 - Val_Loss: 27.7408\n",
      "Processing batch 4849/11884 - Val_Loss: 26.1861\n",
      "Processing batch 4850/11884 - Val_Loss: 25.8390\n",
      "Processing batch 4851/11884 - Val_Loss: 28.7696\n",
      "Processing batch 4852/11884 - Val_Loss: 25.8708\n",
      "Processing batch 4853/11884 - Val_Loss: 24.8998\n",
      "Processing batch 4854/11884 - Val_Loss: 23.0822\n",
      "Processing batch 4855/11884 - Val_Loss: 26.0384\n",
      "Processing batch 4856/11884 - Val_Loss: 25.6053\n",
      "Processing batch 4857/11884 - Val_Loss: 24.4688\n",
      "Processing batch 4858/11884 - Val_Loss: 27.3419\n",
      "Processing batch 4859/11884 - Val_Loss: 23.9673\n",
      "Processing batch 4860/11884 - Val_Loss: 25.7509\n",
      "Processing batch 4861/11884 - Val_Loss: 30.4940\n",
      "Processing batch 4862/11884 - Val_Loss: 25.4931\n",
      "Processing batch 4863/11884 - Val_Loss: 26.9994\n",
      "Processing batch 4864/11884 - Val_Loss: 26.7960\n",
      "Processing batch 4865/11884 - Val_Loss: 28.0520\n",
      "Processing batch 4866/11884 - Val_Loss: 25.3585\n",
      "Processing batch 4867/11884 - Val_Loss: 25.5200\n",
      "Processing batch 4868/11884 - Val_Loss: 26.5082\n",
      "Processing batch 4869/11884 - Val_Loss: 26.4116\n",
      "Processing batch 4870/11884 - Val_Loss: 24.6046\n",
      "Processing batch 4871/11884 - Val_Loss: 23.3882\n",
      "Processing batch 4872/11884 - Val_Loss: 27.4633\n",
      "Processing batch 4873/11884 - Val_Loss: 26.3268\n",
      "Processing batch 4874/11884 - Val_Loss: 24.4159\n",
      "Processing batch 4875/11884 - Val_Loss: 25.7807\n",
      "Processing batch 4876/11884 - Val_Loss: 27.4367\n",
      "Processing batch 4877/11884 - Val_Loss: 25.6926\n",
      "Processing batch 4878/11884 - Val_Loss: 25.3063\n",
      "Processing batch 4879/11884 - Val_Loss: 25.3707\n",
      "Processing batch 4880/11884 - Val_Loss: 25.1861\n",
      "Processing batch 4881/11884 - Val_Loss: 26.7001\n",
      "Processing batch 4882/11884 - Val_Loss: 22.5577\n",
      "Processing batch 4883/11884 - Val_Loss: 25.3675\n",
      "Processing batch 4884/11884 - Val_Loss: 23.4928\n",
      "Processing batch 4885/11884 - Val_Loss: 25.6210\n",
      "Processing batch 4886/11884 - Val_Loss: 27.5288\n",
      "Processing batch 4887/11884 - Val_Loss: 28.5493\n",
      "Processing batch 4888/11884 - Val_Loss: 28.4439\n",
      "Processing batch 4889/11884 - Val_Loss: 28.3348\n",
      "Processing batch 4890/11884 - Val_Loss: 23.8340\n",
      "Processing batch 4891/11884 - Val_Loss: 25.1822\n",
      "Processing batch 4892/11884 - Val_Loss: 27.2046\n",
      "Processing batch 4893/11884 - Val_Loss: 26.8534\n",
      "Processing batch 4894/11884 - Val_Loss: 27.8381\n",
      "Processing batch 4895/11884 - Val_Loss: 24.7170\n",
      "Processing batch 4896/11884 - Val_Loss: 29.9348\n",
      "Processing batch 4897/11884 - Val_Loss: 28.3248\n",
      "Processing batch 4898/11884 - Val_Loss: 24.1895\n",
      "Processing batch 4899/11884 - Val_Loss: 26.7125\n",
      "Processing batch 4900/11884 - Val_Loss: 23.8517\n",
      "Processing batch 4901/11884 - Val_Loss: 26.6119\n",
      "Processing batch 4902/11884 - Val_Loss: 28.1139\n",
      "Processing batch 4903/11884 - Val_Loss: 25.2776\n",
      "Processing batch 4904/11884 - Val_Loss: 26.8220\n",
      "Processing batch 4905/11884 - Val_Loss: 27.1894\n",
      "Processing batch 4906/11884 - Val_Loss: 27.9421\n",
      "Processing batch 4907/11884 - Val_Loss: 27.2308\n",
      "Processing batch 4908/11884 - Val_Loss: 25.4939\n",
      "Processing batch 4909/11884 - Val_Loss: 25.1305\n",
      "Processing batch 4910/11884 - Val_Loss: 26.9080\n",
      "Processing batch 4911/11884 - Val_Loss: 26.8462\n",
      "Processing batch 4912/11884 - Val_Loss: 24.7438\n",
      "Processing batch 4913/11884 - Val_Loss: 24.6240\n",
      "Processing batch 4914/11884 - Val_Loss: 26.3154\n",
      "Processing batch 4915/11884 - Val_Loss: 25.8059\n",
      "Processing batch 4916/11884 - Val_Loss: 25.4320\n",
      "Processing batch 4917/11884 - Val_Loss: 29.0580\n",
      "Processing batch 4918/11884 - Val_Loss: 24.0101\n",
      "Processing batch 4919/11884 - Val_Loss: 27.1308\n",
      "Processing batch 4920/11884 - Val_Loss: 25.8770\n",
      "Processing batch 4921/11884 - Val_Loss: 23.8384\n",
      "Processing batch 4922/11884 - Val_Loss: 23.3716\n",
      "Processing batch 4923/11884 - Val_Loss: 25.6482\n",
      "Processing batch 4924/11884 - Val_Loss: 24.0423\n",
      "Processing batch 4925/11884 - Val_Loss: 27.7547\n",
      "Processing batch 4926/11884 - Val_Loss: 25.6759\n",
      "Processing batch 4927/11884 - Val_Loss: 25.5093\n",
      "Processing batch 4928/11884 - Val_Loss: 27.5747\n",
      "Processing batch 4929/11884 - Val_Loss: 23.8816\n",
      "Processing batch 4930/11884 - Val_Loss: 29.1719\n",
      "Processing batch 4931/11884 - Val_Loss: 22.6940\n",
      "Processing batch 4932/11884 - Val_Loss: 24.4344\n",
      "Processing batch 4933/11884 - Val_Loss: 24.4092\n",
      "Processing batch 4934/11884 - Val_Loss: 26.1831\n",
      "Processing batch 4935/11884 - Val_Loss: 26.4613\n",
      "Processing batch 4936/11884 - Val_Loss: 25.7533\n",
      "Processing batch 4937/11884 - Val_Loss: 25.7412\n",
      "Processing batch 4938/11884 - Val_Loss: 26.3856\n",
      "Processing batch 4939/11884 - Val_Loss: 23.6179\n",
      "Processing batch 4940/11884 - Val_Loss: 25.7122\n",
      "Processing batch 4941/11884 - Val_Loss: 27.1122\n",
      "Processing batch 4942/11884 - Val_Loss: 25.7596\n",
      "Processing batch 4943/11884 - Val_Loss: 24.5539\n",
      "Processing batch 4944/11884 - Val_Loss: 26.4858\n",
      "Processing batch 4945/11884 - Val_Loss: 25.6592\n",
      "Processing batch 4946/11884 - Val_Loss: 27.7325\n",
      "Processing batch 4947/11884 - Val_Loss: 27.4105\n",
      "Processing batch 4948/11884 - Val_Loss: 29.0321\n",
      "Processing batch 4949/11884 - Val_Loss: 25.7633\n",
      "Processing batch 4950/11884 - Val_Loss: 26.5830\n",
      "Processing batch 4951/11884 - Val_Loss: 24.8959\n",
      "Processing batch 4952/11884 - Val_Loss: 26.2056\n",
      "Processing batch 4953/11884 - Val_Loss: 27.0620\n",
      "Processing batch 4954/11884 - Val_Loss: 27.3480\n",
      "Processing batch 4955/11884 - Val_Loss: 27.1175\n",
      "Processing batch 4956/11884 - Val_Loss: 21.4604\n",
      "Processing batch 4957/11884 - Val_Loss: 29.7230\n",
      "Processing batch 4958/11884 - Val_Loss: 25.4249\n",
      "Processing batch 4959/11884 - Val_Loss: 24.2807\n",
      "Processing batch 4960/11884 - Val_Loss: 26.6455\n",
      "Processing batch 4961/11884 - Val_Loss: 24.8528\n",
      "Processing batch 4962/11884 - Val_Loss: 25.3427\n",
      "Processing batch 4963/11884 - Val_Loss: 23.9358\n",
      "Processing batch 4964/11884 - Val_Loss: 24.3567\n",
      "Processing batch 4965/11884 - Val_Loss: 25.5033\n",
      "Processing batch 4966/11884 - Val_Loss: 24.5044\n",
      "Processing batch 4967/11884 - Val_Loss: 27.2478\n",
      "Processing batch 4968/11884 - Val_Loss: 21.7910\n",
      "Processing batch 4969/11884 - Val_Loss: 26.9412\n",
      "Processing batch 4970/11884 - Val_Loss: 26.6288\n",
      "Processing batch 4971/11884 - Val_Loss: 27.1270\n",
      "Processing batch 4972/11884 - Val_Loss: 24.6030\n",
      "Processing batch 4973/11884 - Val_Loss: 26.1316\n",
      "Processing batch 4974/11884 - Val_Loss: 25.2509\n",
      "Processing batch 4975/11884 - Val_Loss: 26.8506\n",
      "Processing batch 4976/11884 - Val_Loss: 27.7499\n",
      "Processing batch 4977/11884 - Val_Loss: 28.0897\n",
      "Processing batch 4978/11884 - Val_Loss: 22.2906\n",
      "Processing batch 4979/11884 - Val_Loss: 25.7237\n",
      "Processing batch 4980/11884 - Val_Loss: 24.5249\n",
      "Processing batch 4981/11884 - Val_Loss: 25.2524\n",
      "Processing batch 4982/11884 - Val_Loss: 24.6414\n",
      "Processing batch 4983/11884 - Val_Loss: 24.6734\n",
      "Processing batch 4984/11884 - Val_Loss: 24.9188\n",
      "Processing batch 4985/11884 - Val_Loss: 24.3274\n",
      "Processing batch 4986/11884 - Val_Loss: 26.4245\n",
      "Processing batch 4987/11884 - Val_Loss: 27.3020\n",
      "Processing batch 4988/11884 - Val_Loss: 30.0337\n",
      "Processing batch 4989/11884 - Val_Loss: 24.4789\n",
      "Processing batch 4990/11884 - Val_Loss: 29.6734\n",
      "Processing batch 4991/11884 - Val_Loss: 27.8463\n",
      "Processing batch 4992/11884 - Val_Loss: 24.0708\n",
      "Processing batch 4993/11884 - Val_Loss: 23.1808\n",
      "Processing batch 4994/11884 - Val_Loss: 27.8258\n",
      "Processing batch 4995/11884 - Val_Loss: 25.8354\n",
      "Processing batch 4996/11884 - Val_Loss: 24.9671\n",
      "Processing batch 4997/11884 - Val_Loss: 23.6806\n",
      "Processing batch 4998/11884 - Val_Loss: 26.7246\n",
      "Processing batch 4999/11884 - Val_Loss: 28.5552\n",
      "Processing batch 5000/11884 - Val_Loss: 22.3915\n",
      "Processing batch 5001/11884 - Val_Loss: 28.4049\n",
      "Processing batch 5002/11884 - Val_Loss: 24.7282\n",
      "Processing batch 5003/11884 - Val_Loss: 25.3753\n",
      "Processing batch 5004/11884 - Val_Loss: 26.3632\n",
      "Processing batch 5005/11884 - Val_Loss: 28.9224\n",
      "Processing batch 5006/11884 - Val_Loss: 27.2925\n",
      "Processing batch 5007/11884 - Val_Loss: 24.9538\n",
      "Processing batch 5008/11884 - Val_Loss: 26.5273\n",
      "Processing batch 5009/11884 - Val_Loss: 24.1861\n",
      "Processing batch 5010/11884 - Val_Loss: 25.2695\n",
      "Processing batch 5011/11884 - Val_Loss: 25.6482\n",
      "Processing batch 5012/11884 - Val_Loss: 26.2117\n",
      "Processing batch 5013/11884 - Val_Loss: 23.9974\n",
      "Processing batch 5014/11884 - Val_Loss: 29.3198\n",
      "Processing batch 5015/11884 - Val_Loss: 23.7235\n",
      "Processing batch 5016/11884 - Val_Loss: 28.4357\n",
      "Processing batch 5017/11884 - Val_Loss: 25.3506\n",
      "Processing batch 5018/11884 - Val_Loss: 25.8311\n",
      "Processing batch 5019/11884 - Val_Loss: 25.7371\n",
      "Processing batch 5020/11884 - Val_Loss: 26.9476\n",
      "Processing batch 5021/11884 - Val_Loss: 25.8568\n",
      "Processing batch 5022/11884 - Val_Loss: 26.2521\n",
      "Processing batch 5023/11884 - Val_Loss: 27.0819\n",
      "Processing batch 5024/11884 - Val_Loss: 23.6979\n",
      "Processing batch 5025/11884 - Val_Loss: 25.7570\n",
      "Processing batch 5026/11884 - Val_Loss: 27.2600\n",
      "Processing batch 5027/11884 - Val_Loss: 25.0788\n",
      "Processing batch 5028/11884 - Val_Loss: 26.5680\n",
      "Processing batch 5029/11884 - Val_Loss: 25.5297\n",
      "Processing batch 5030/11884 - Val_Loss: 25.5902\n",
      "Processing batch 5031/11884 - Val_Loss: 24.4975\n",
      "Processing batch 5032/11884 - Val_Loss: 27.1540\n",
      "Processing batch 5033/11884 - Val_Loss: 23.2885\n",
      "Processing batch 5034/11884 - Val_Loss: 22.3414\n",
      "Processing batch 5035/11884 - Val_Loss: 25.5319\n",
      "Processing batch 5036/11884 - Val_Loss: 27.6231\n",
      "Processing batch 5037/11884 - Val_Loss: 24.7807\n",
      "Processing batch 5038/11884 - Val_Loss: 26.2933\n",
      "Processing batch 5039/11884 - Val_Loss: 23.9580\n",
      "Processing batch 5040/11884 - Val_Loss: 22.9038\n",
      "Processing batch 5041/11884 - Val_Loss: 24.5108\n",
      "Processing batch 5042/11884 - Val_Loss: 24.3610\n",
      "Processing batch 5043/11884 - Val_Loss: 23.7559\n",
      "Processing batch 5044/11884 - Val_Loss: 27.8849\n",
      "Processing batch 5045/11884 - Val_Loss: 26.9127\n",
      "Processing batch 5046/11884 - Val_Loss: 25.1487\n",
      "Processing batch 5047/11884 - Val_Loss: 25.7623\n",
      "Processing batch 5048/11884 - Val_Loss: 24.2377\n",
      "Processing batch 5049/11884 - Val_Loss: 27.9263\n",
      "Processing batch 5050/11884 - Val_Loss: 24.8937\n",
      "Processing batch 5051/11884 - Val_Loss: 25.3091\n",
      "Processing batch 5052/11884 - Val_Loss: 25.5309\n",
      "Processing batch 5053/11884 - Val_Loss: 26.2600\n",
      "Processing batch 5054/11884 - Val_Loss: 28.2048\n",
      "Processing batch 5055/11884 - Val_Loss: 28.2406\n",
      "Processing batch 5056/11884 - Val_Loss: 25.4554\n",
      "Processing batch 5057/11884 - Val_Loss: 25.8249\n",
      "Processing batch 5058/11884 - Val_Loss: 24.6923\n",
      "Processing batch 5059/11884 - Val_Loss: 27.5210\n",
      "Processing batch 5060/11884 - Val_Loss: 25.4154\n",
      "Processing batch 5061/11884 - Val_Loss: 28.5257\n",
      "Processing batch 5062/11884 - Val_Loss: 22.1297\n",
      "Processing batch 5063/11884 - Val_Loss: 21.5771\n",
      "Processing batch 5064/11884 - Val_Loss: 26.6213\n",
      "Processing batch 5065/11884 - Val_Loss: 23.8476\n",
      "Processing batch 5066/11884 - Val_Loss: 25.1603\n",
      "Processing batch 5067/11884 - Val_Loss: 25.6101\n",
      "Processing batch 5068/11884 - Val_Loss: 27.0833\n",
      "Processing batch 5069/11884 - Val_Loss: 24.9811\n",
      "Processing batch 5070/11884 - Val_Loss: 23.8450\n",
      "Processing batch 5071/11884 - Val_Loss: 28.5872\n",
      "Processing batch 5072/11884 - Val_Loss: 24.0902\n",
      "Processing batch 5073/11884 - Val_Loss: 26.5563\n",
      "Processing batch 5074/11884 - Val_Loss: 27.0383\n",
      "Processing batch 5075/11884 - Val_Loss: 26.4559\n",
      "Processing batch 5076/11884 - Val_Loss: 25.2245\n",
      "Processing batch 5077/11884 - Val_Loss: 24.5500\n",
      "Processing batch 5078/11884 - Val_Loss: 25.7406\n",
      "Processing batch 5079/11884 - Val_Loss: 26.8941\n",
      "Processing batch 5080/11884 - Val_Loss: 25.5384\n",
      "Processing batch 5081/11884 - Val_Loss: 23.7494\n",
      "Processing batch 5082/11884 - Val_Loss: 26.8209\n",
      "Processing batch 5083/11884 - Val_Loss: 24.3412\n",
      "Processing batch 5084/11884 - Val_Loss: 23.9254\n",
      "Processing batch 5085/11884 - Val_Loss: 25.9667\n",
      "Processing batch 5086/11884 - Val_Loss: 29.0907\n",
      "Processing batch 5087/11884 - Val_Loss: 26.4287\n",
      "Processing batch 5088/11884 - Val_Loss: 25.0408\n",
      "Processing batch 5089/11884 - Val_Loss: 27.6291\n",
      "Processing batch 5090/11884 - Val_Loss: 23.0493\n",
      "Processing batch 5091/11884 - Val_Loss: 25.1045\n",
      "Processing batch 5092/11884 - Val_Loss: 25.3272\n",
      "Processing batch 5093/11884 - Val_Loss: 26.1931\n",
      "Processing batch 5094/11884 - Val_Loss: 27.3311\n",
      "Processing batch 5095/11884 - Val_Loss: 25.1004\n",
      "Processing batch 5096/11884 - Val_Loss: 27.8276\n",
      "Processing batch 5097/11884 - Val_Loss: 26.1121\n",
      "Processing batch 5098/11884 - Val_Loss: 25.3136\n",
      "Processing batch 5099/11884 - Val_Loss: 24.3124\n",
      "Processing batch 5100/11884 - Val_Loss: 26.1176\n",
      "Processing batch 5101/11884 - Val_Loss: 24.1460\n",
      "Processing batch 5102/11884 - Val_Loss: 25.9685\n",
      "Processing batch 5103/11884 - Val_Loss: 26.7822\n",
      "Processing batch 5104/11884 - Val_Loss: 26.3463\n",
      "Processing batch 5105/11884 - Val_Loss: 25.8920\n",
      "Processing batch 5106/11884 - Val_Loss: 23.7773\n",
      "Processing batch 5107/11884 - Val_Loss: 22.4168\n",
      "Processing batch 5108/11884 - Val_Loss: 21.8429\n",
      "Processing batch 5109/11884 - Val_Loss: 27.3333\n",
      "Processing batch 5110/11884 - Val_Loss: 26.6521\n",
      "Processing batch 5111/11884 - Val_Loss: 28.1498\n",
      "Processing batch 5112/11884 - Val_Loss: 23.3730\n",
      "Processing batch 5113/11884 - Val_Loss: 29.4350\n",
      "Processing batch 5114/11884 - Val_Loss: 24.8273\n",
      "Processing batch 5115/11884 - Val_Loss: 26.1332\n",
      "Processing batch 5116/11884 - Val_Loss: 26.6637\n",
      "Processing batch 5117/11884 - Val_Loss: 24.7854\n",
      "Processing batch 5118/11884 - Val_Loss: 29.1460\n",
      "Processing batch 5119/11884 - Val_Loss: 27.7431\n",
      "Processing batch 5120/11884 - Val_Loss: 23.3558\n",
      "Processing batch 5121/11884 - Val_Loss: 25.5148\n",
      "Processing batch 5122/11884 - Val_Loss: 26.9802\n",
      "Processing batch 5123/11884 - Val_Loss: 27.2364\n",
      "Processing batch 5124/11884 - Val_Loss: 25.2749\n",
      "Processing batch 5125/11884 - Val_Loss: 25.8001\n",
      "Processing batch 5126/11884 - Val_Loss: 26.2112\n",
      "Processing batch 5127/11884 - Val_Loss: 26.6137\n",
      "Processing batch 5128/11884 - Val_Loss: 28.2292\n",
      "Processing batch 5129/11884 - Val_Loss: 27.5127\n",
      "Processing batch 5130/11884 - Val_Loss: 25.2173\n",
      "Processing batch 5131/11884 - Val_Loss: 24.9610\n",
      "Processing batch 5132/11884 - Val_Loss: 28.4205\n",
      "Processing batch 5133/11884 - Val_Loss: 28.8364\n",
      "Processing batch 5134/11884 - Val_Loss: 26.1291\n",
      "Processing batch 5135/11884 - Val_Loss: 23.3006\n",
      "Processing batch 5136/11884 - Val_Loss: 23.3475\n",
      "Processing batch 5137/11884 - Val_Loss: 29.4580\n",
      "Processing batch 5138/11884 - Val_Loss: 24.1419\n",
      "Processing batch 5139/11884 - Val_Loss: 25.0456\n",
      "Processing batch 5140/11884 - Val_Loss: 29.2300\n",
      "Processing batch 5141/11884 - Val_Loss: 26.5443\n",
      "Processing batch 5142/11884 - Val_Loss: 24.4941\n",
      "Processing batch 5143/11884 - Val_Loss: 24.4572\n",
      "Processing batch 5144/11884 - Val_Loss: 27.4212\n",
      "Processing batch 5145/11884 - Val_Loss: 28.0650\n",
      "Processing batch 5146/11884 - Val_Loss: 24.6812\n",
      "Processing batch 5147/11884 - Val_Loss: 26.3711\n",
      "Processing batch 5148/11884 - Val_Loss: 26.5135\n",
      "Processing batch 5149/11884 - Val_Loss: 28.2939\n",
      "Processing batch 5150/11884 - Val_Loss: 28.4494\n",
      "Processing batch 5151/11884 - Val_Loss: 26.0432\n",
      "Processing batch 5152/11884 - Val_Loss: 24.7688\n",
      "Processing batch 5153/11884 - Val_Loss: 26.8157\n",
      "Processing batch 5154/11884 - Val_Loss: 24.9804\n",
      "Processing batch 5155/11884 - Val_Loss: 25.7594\n",
      "Processing batch 5156/11884 - Val_Loss: 26.6536\n",
      "Processing batch 5157/11884 - Val_Loss: 28.5656\n",
      "Processing batch 5158/11884 - Val_Loss: 27.3498\n",
      "Processing batch 5159/11884 - Val_Loss: 23.4959\n",
      "Processing batch 5160/11884 - Val_Loss: 27.7344\n",
      "Processing batch 5161/11884 - Val_Loss: 26.2941\n",
      "Processing batch 5162/11884 - Val_Loss: 25.8832\n",
      "Processing batch 5163/11884 - Val_Loss: 26.0138\n",
      "Processing batch 5164/11884 - Val_Loss: 25.3546\n",
      "Processing batch 5165/11884 - Val_Loss: 27.5731\n",
      "Processing batch 5166/11884 - Val_Loss: 25.1643\n",
      "Processing batch 5167/11884 - Val_Loss: 25.3720\n",
      "Processing batch 5168/11884 - Val_Loss: 24.7105\n",
      "Processing batch 5169/11884 - Val_Loss: 24.8113\n",
      "Processing batch 5170/11884 - Val_Loss: 22.8441\n",
      "Processing batch 5171/11884 - Val_Loss: 25.4249\n",
      "Processing batch 5172/11884 - Val_Loss: 28.1364\n",
      "Processing batch 5173/11884 - Val_Loss: 26.7630\n",
      "Processing batch 5174/11884 - Val_Loss: 22.8017\n",
      "Processing batch 5175/11884 - Val_Loss: 25.4347\n",
      "Processing batch 5176/11884 - Val_Loss: 23.9233\n",
      "Processing batch 5177/11884 - Val_Loss: 26.1379\n",
      "Processing batch 5178/11884 - Val_Loss: 24.5239\n",
      "Processing batch 5179/11884 - Val_Loss: 27.0063\n",
      "Processing batch 5180/11884 - Val_Loss: 24.4578\n",
      "Processing batch 5181/11884 - Val_Loss: 24.5227\n",
      "Processing batch 5182/11884 - Val_Loss: 26.3773\n",
      "Processing batch 5183/11884 - Val_Loss: 27.0517\n",
      "Processing batch 5184/11884 - Val_Loss: 27.2424\n",
      "Processing batch 5185/11884 - Val_Loss: 25.3876\n",
      "Processing batch 5186/11884 - Val_Loss: 27.7312\n",
      "Processing batch 5187/11884 - Val_Loss: 24.4647\n",
      "Processing batch 5188/11884 - Val_Loss: 22.9552\n",
      "Processing batch 5189/11884 - Val_Loss: 27.8585\n",
      "Processing batch 5190/11884 - Val_Loss: 23.0738\n",
      "Processing batch 5191/11884 - Val_Loss: 25.8899\n",
      "Processing batch 5192/11884 - Val_Loss: 24.0085\n",
      "Processing batch 5193/11884 - Val_Loss: 24.1743\n",
      "Processing batch 5194/11884 - Val_Loss: 26.3536\n",
      "Processing batch 5195/11884 - Val_Loss: 29.6813\n",
      "Processing batch 5196/11884 - Val_Loss: 24.4605\n",
      "Processing batch 5197/11884 - Val_Loss: 24.1298\n",
      "Processing batch 5198/11884 - Val_Loss: 28.7497\n",
      "Processing batch 5199/11884 - Val_Loss: 28.1092\n",
      "Processing batch 5200/11884 - Val_Loss: 25.4218\n",
      "Processing batch 5201/11884 - Val_Loss: 26.6845\n",
      "Processing batch 5202/11884 - Val_Loss: 25.7898\n",
      "Processing batch 5203/11884 - Val_Loss: 27.5217\n",
      "Processing batch 5204/11884 - Val_Loss: 22.9742\n",
      "Processing batch 5205/11884 - Val_Loss: 21.7125\n",
      "Processing batch 5206/11884 - Val_Loss: 25.8091\n",
      "Processing batch 5207/11884 - Val_Loss: 26.3899\n",
      "Processing batch 5208/11884 - Val_Loss: 25.4090\n",
      "Processing batch 5209/11884 - Val_Loss: 25.6723\n",
      "Processing batch 5210/11884 - Val_Loss: 26.2885\n",
      "Processing batch 5211/11884 - Val_Loss: 24.6028\n",
      "Processing batch 5212/11884 - Val_Loss: 24.4455\n",
      "Processing batch 5213/11884 - Val_Loss: 25.6842\n",
      "Processing batch 5214/11884 - Val_Loss: 23.8268\n",
      "Processing batch 5215/11884 - Val_Loss: 26.5561\n",
      "Processing batch 5216/11884 - Val_Loss: 28.8883\n",
      "Processing batch 5217/11884 - Val_Loss: 25.2525\n",
      "Processing batch 5218/11884 - Val_Loss: 27.3040\n",
      "Processing batch 5219/11884 - Val_Loss: 25.8982\n",
      "Processing batch 5220/11884 - Val_Loss: 23.9574\n",
      "Processing batch 5221/11884 - Val_Loss: 26.0013\n",
      "Processing batch 5222/11884 - Val_Loss: 25.6063\n",
      "Processing batch 5223/11884 - Val_Loss: 26.7805\n",
      "Processing batch 5224/11884 - Val_Loss: 28.0924\n",
      "Processing batch 5225/11884 - Val_Loss: 24.5092\n",
      "Processing batch 5226/11884 - Val_Loss: 25.1051\n",
      "Processing batch 5227/11884 - Val_Loss: 24.3007\n",
      "Processing batch 5228/11884 - Val_Loss: 26.3574\n",
      "Processing batch 5229/11884 - Val_Loss: 24.8346\n",
      "Processing batch 5230/11884 - Val_Loss: 24.5233\n",
      "Processing batch 5231/11884 - Val_Loss: 26.2547\n",
      "Processing batch 5232/11884 - Val_Loss: 24.9035\n",
      "Processing batch 5233/11884 - Val_Loss: 25.9893\n",
      "Processing batch 5234/11884 - Val_Loss: 26.7391\n",
      "Processing batch 5235/11884 - Val_Loss: 25.7916\n",
      "Processing batch 5236/11884 - Val_Loss: 25.8605\n",
      "Processing batch 5237/11884 - Val_Loss: 25.7357\n",
      "Processing batch 5238/11884 - Val_Loss: 27.5157\n",
      "Processing batch 5239/11884 - Val_Loss: 23.7260\n",
      "Processing batch 5240/11884 - Val_Loss: 26.0285\n",
      "Processing batch 5241/11884 - Val_Loss: 25.6455\n",
      "Processing batch 5242/11884 - Val_Loss: 31.2745\n",
      "Processing batch 5243/11884 - Val_Loss: 23.7561\n",
      "Processing batch 5244/11884 - Val_Loss: 26.5554\n",
      "Processing batch 5245/11884 - Val_Loss: 25.0371\n",
      "Processing batch 5246/11884 - Val_Loss: 26.4360\n",
      "Processing batch 5247/11884 - Val_Loss: 24.5306\n",
      "Processing batch 5248/11884 - Val_Loss: 29.1342\n",
      "Processing batch 5249/11884 - Val_Loss: 24.4377\n",
      "Processing batch 5250/11884 - Val_Loss: 28.2446\n",
      "Processing batch 5251/11884 - Val_Loss: 26.8756\n",
      "Processing batch 5252/11884 - Val_Loss: 26.3139\n",
      "Processing batch 5253/11884 - Val_Loss: 23.6554\n",
      "Processing batch 5254/11884 - Val_Loss: 26.2165\n",
      "Processing batch 5255/11884 - Val_Loss: 23.3198\n",
      "Processing batch 5256/11884 - Val_Loss: 25.9067\n",
      "Processing batch 5257/11884 - Val_Loss: 24.4080\n",
      "Processing batch 5258/11884 - Val_Loss: 24.3324\n",
      "Processing batch 5259/11884 - Val_Loss: 25.5081\n",
      "Processing batch 5260/11884 - Val_Loss: 22.5197\n",
      "Processing batch 5261/11884 - Val_Loss: 23.3294\n",
      "Processing batch 5262/11884 - Val_Loss: 26.2955\n",
      "Processing batch 5263/11884 - Val_Loss: 25.6972\n",
      "Processing batch 5264/11884 - Val_Loss: 24.3361\n",
      "Processing batch 5265/11884 - Val_Loss: 22.5387\n",
      "Processing batch 5266/11884 - Val_Loss: 26.1173\n",
      "Processing batch 5267/11884 - Val_Loss: 25.4649\n",
      "Processing batch 5268/11884 - Val_Loss: 27.8647\n",
      "Processing batch 5269/11884 - Val_Loss: 24.0826\n",
      "Processing batch 5270/11884 - Val_Loss: 27.0343\n",
      "Processing batch 5271/11884 - Val_Loss: 26.6779\n",
      "Processing batch 5272/11884 - Val_Loss: 25.2519\n",
      "Processing batch 5273/11884 - Val_Loss: 25.8434\n",
      "Processing batch 5274/11884 - Val_Loss: 26.4723\n",
      "Processing batch 5275/11884 - Val_Loss: 23.7569\n",
      "Processing batch 5276/11884 - Val_Loss: 25.2098\n",
      "Processing batch 5277/11884 - Val_Loss: 24.0553\n",
      "Processing batch 5278/11884 - Val_Loss: 24.8842\n",
      "Processing batch 5279/11884 - Val_Loss: 25.6056\n",
      "Processing batch 5280/11884 - Val_Loss: 26.2297\n",
      "Processing batch 5281/11884 - Val_Loss: 26.5528\n",
      "Processing batch 5282/11884 - Val_Loss: 25.4950\n",
      "Processing batch 5283/11884 - Val_Loss: 23.6144\n",
      "Processing batch 5284/11884 - Val_Loss: 24.6979\n",
      "Processing batch 5285/11884 - Val_Loss: 27.4723\n",
      "Processing batch 5286/11884 - Val_Loss: 22.8331\n",
      "Processing batch 5287/11884 - Val_Loss: 26.3681\n",
      "Processing batch 5288/11884 - Val_Loss: 28.4144\n",
      "Processing batch 5289/11884 - Val_Loss: 25.5801\n",
      "Processing batch 5290/11884 - Val_Loss: 23.3437\n",
      "Processing batch 5291/11884 - Val_Loss: 27.9688\n",
      "Processing batch 5292/11884 - Val_Loss: 23.9708\n",
      "Processing batch 5293/11884 - Val_Loss: 24.0677\n",
      "Processing batch 5294/11884 - Val_Loss: 25.2799\n",
      "Processing batch 5295/11884 - Val_Loss: 26.4420\n",
      "Processing batch 5296/11884 - Val_Loss: 26.5876\n",
      "Processing batch 5297/11884 - Val_Loss: 24.0073\n",
      "Processing batch 5298/11884 - Val_Loss: 28.3188\n",
      "Processing batch 5299/11884 - Val_Loss: 29.9455\n",
      "Processing batch 5300/11884 - Val_Loss: 23.5965\n",
      "Processing batch 5301/11884 - Val_Loss: 25.9791\n",
      "Processing batch 5302/11884 - Val_Loss: 27.5012\n",
      "Processing batch 5303/11884 - Val_Loss: 26.3057\n",
      "Processing batch 5304/11884 - Val_Loss: 24.0648\n",
      "Processing batch 5305/11884 - Val_Loss: 25.0579\n",
      "Processing batch 5306/11884 - Val_Loss: 25.2872\n",
      "Processing batch 5307/11884 - Val_Loss: 25.9286\n",
      "Processing batch 5308/11884 - Val_Loss: 24.1418\n",
      "Processing batch 5309/11884 - Val_Loss: 26.1470\n",
      "Processing batch 5310/11884 - Val_Loss: 26.3515\n",
      "Processing batch 5311/11884 - Val_Loss: 25.8945\n",
      "Processing batch 5312/11884 - Val_Loss: 29.7958\n",
      "Processing batch 5313/11884 - Val_Loss: 26.3536\n",
      "Processing batch 5314/11884 - Val_Loss: 27.8879\n",
      "Processing batch 5315/11884 - Val_Loss: 28.2288\n",
      "Processing batch 5316/11884 - Val_Loss: 27.5510\n",
      "Processing batch 5317/11884 - Val_Loss: 27.1456\n",
      "Processing batch 5318/11884 - Val_Loss: 27.5462\n",
      "Processing batch 5319/11884 - Val_Loss: 26.3869\n",
      "Processing batch 5320/11884 - Val_Loss: 26.0374\n",
      "Processing batch 5321/11884 - Val_Loss: 23.1117\n",
      "Processing batch 5322/11884 - Val_Loss: 26.5351\n",
      "Processing batch 5323/11884 - Val_Loss: 24.3139\n",
      "Processing batch 5324/11884 - Val_Loss: 28.4436\n",
      "Processing batch 5325/11884 - Val_Loss: 23.3075\n",
      "Processing batch 5326/11884 - Val_Loss: 24.7175\n",
      "Processing batch 5327/11884 - Val_Loss: 24.5939\n",
      "Processing batch 5328/11884 - Val_Loss: 25.6550\n",
      "Processing batch 5329/11884 - Val_Loss: 25.6018\n",
      "Processing batch 5330/11884 - Val_Loss: 25.2790\n",
      "Processing batch 5331/11884 - Val_Loss: 26.4457\n",
      "Processing batch 5332/11884 - Val_Loss: 24.2531\n",
      "Processing batch 5333/11884 - Val_Loss: 23.5145\n",
      "Processing batch 5334/11884 - Val_Loss: 25.8162\n",
      "Processing batch 5335/11884 - Val_Loss: 23.9998\n",
      "Processing batch 5336/11884 - Val_Loss: 25.5497\n",
      "Processing batch 5337/11884 - Val_Loss: 25.0305\n",
      "Processing batch 5338/11884 - Val_Loss: 27.6109\n",
      "Processing batch 5339/11884 - Val_Loss: 28.2237\n",
      "Processing batch 5340/11884 - Val_Loss: 24.7604\n",
      "Processing batch 5341/11884 - Val_Loss: 26.5958\n",
      "Processing batch 5342/11884 - Val_Loss: 27.2258\n",
      "Processing batch 5343/11884 - Val_Loss: 26.5714\n",
      "Processing batch 5344/11884 - Val_Loss: 28.5774\n",
      "Processing batch 5345/11884 - Val_Loss: 21.9157\n",
      "Processing batch 5346/11884 - Val_Loss: 25.1040\n",
      "Processing batch 5347/11884 - Val_Loss: 25.5202\n",
      "Processing batch 5348/11884 - Val_Loss: 25.6063\n",
      "Processing batch 5349/11884 - Val_Loss: 23.9292\n",
      "Processing batch 5350/11884 - Val_Loss: 26.0747\n",
      "Processing batch 5351/11884 - Val_Loss: 25.5146\n",
      "Processing batch 5352/11884 - Val_Loss: 24.9480\n",
      "Processing batch 5353/11884 - Val_Loss: 26.1389\n",
      "Processing batch 5354/11884 - Val_Loss: 23.2630\n",
      "Processing batch 5355/11884 - Val_Loss: 26.8006\n",
      "Processing batch 5356/11884 - Val_Loss: 26.4173\n",
      "Processing batch 5357/11884 - Val_Loss: 26.5450\n",
      "Processing batch 5358/11884 - Val_Loss: 29.8929\n",
      "Processing batch 5359/11884 - Val_Loss: 25.3098\n",
      "Processing batch 5360/11884 - Val_Loss: 22.7443\n",
      "Processing batch 5361/11884 - Val_Loss: 26.5804\n",
      "Processing batch 5362/11884 - Val_Loss: 26.5346\n",
      "Processing batch 5363/11884 - Val_Loss: 26.8257\n",
      "Processing batch 5364/11884 - Val_Loss: 23.8777\n",
      "Processing batch 5365/11884 - Val_Loss: 24.7176\n",
      "Processing batch 5366/11884 - Val_Loss: 25.5350\n",
      "Processing batch 5367/11884 - Val_Loss: 26.4755\n",
      "Processing batch 5368/11884 - Val_Loss: 25.7950\n",
      "Processing batch 5369/11884 - Val_Loss: 29.3175\n",
      "Processing batch 5370/11884 - Val_Loss: 23.9508\n",
      "Processing batch 5371/11884 - Val_Loss: 25.7062\n",
      "Processing batch 5372/11884 - Val_Loss: 28.3304\n",
      "Processing batch 5373/11884 - Val_Loss: 26.3997\n",
      "Processing batch 5374/11884 - Val_Loss: 23.8598\n",
      "Processing batch 5375/11884 - Val_Loss: 27.4246\n",
      "Processing batch 5376/11884 - Val_Loss: 26.1403\n",
      "Processing batch 5377/11884 - Val_Loss: 26.2253\n",
      "Processing batch 5378/11884 - Val_Loss: 25.1888\n",
      "Processing batch 5379/11884 - Val_Loss: 26.2810\n",
      "Processing batch 5380/11884 - Val_Loss: 24.1485\n",
      "Processing batch 5381/11884 - Val_Loss: 26.8787\n",
      "Processing batch 5382/11884 - Val_Loss: 27.5783\n",
      "Processing batch 5383/11884 - Val_Loss: 25.6941\n",
      "Processing batch 5384/11884 - Val_Loss: 26.8380\n",
      "Processing batch 5385/11884 - Val_Loss: 24.0340\n",
      "Processing batch 5386/11884 - Val_Loss: 22.8238\n",
      "Processing batch 5387/11884 - Val_Loss: 21.8592\n",
      "Processing batch 5388/11884 - Val_Loss: 26.0317\n",
      "Processing batch 5389/11884 - Val_Loss: 27.1418\n",
      "Processing batch 5390/11884 - Val_Loss: 25.5934\n",
      "Processing batch 5391/11884 - Val_Loss: 27.9974\n",
      "Processing batch 5392/11884 - Val_Loss: 25.6901\n",
      "Processing batch 5393/11884 - Val_Loss: 23.4005\n",
      "Processing batch 5394/11884 - Val_Loss: 26.6847\n",
      "Processing batch 5395/11884 - Val_Loss: 26.1516\n",
      "Processing batch 5396/11884 - Val_Loss: 22.8088\n",
      "Processing batch 5397/11884 - Val_Loss: 26.8244\n",
      "Processing batch 5398/11884 - Val_Loss: 22.4340\n",
      "Processing batch 5399/11884 - Val_Loss: 24.5124\n",
      "Processing batch 5400/11884 - Val_Loss: 26.2090\n",
      "Processing batch 5401/11884 - Val_Loss: 26.8930\n",
      "Processing batch 5402/11884 - Val_Loss: 27.2086\n",
      "Processing batch 5403/11884 - Val_Loss: 27.9034\n",
      "Processing batch 5404/11884 - Val_Loss: 25.5557\n",
      "Processing batch 5405/11884 - Val_Loss: 24.9938\n",
      "Processing batch 5406/11884 - Val_Loss: 27.5810\n",
      "Processing batch 5407/11884 - Val_Loss: 28.6643\n",
      "Processing batch 5408/11884 - Val_Loss: 26.1660\n",
      "Processing batch 5409/11884 - Val_Loss: 29.7333\n",
      "Processing batch 5410/11884 - Val_Loss: 26.7906\n",
      "Processing batch 5411/11884 - Val_Loss: 26.0403\n",
      "Processing batch 5412/11884 - Val_Loss: 26.1847\n",
      "Processing batch 5413/11884 - Val_Loss: 25.8591\n",
      "Processing batch 5414/11884 - Val_Loss: 24.1763\n",
      "Processing batch 5415/11884 - Val_Loss: 25.7911\n",
      "Processing batch 5416/11884 - Val_Loss: 26.0664\n",
      "Processing batch 5417/11884 - Val_Loss: 26.0033\n",
      "Processing batch 5418/11884 - Val_Loss: 27.3383\n",
      "Processing batch 5419/11884 - Val_Loss: 27.1271\n",
      "Processing batch 5420/11884 - Val_Loss: 25.5144\n",
      "Processing batch 5421/11884 - Val_Loss: 26.5602\n",
      "Processing batch 5422/11884 - Val_Loss: 25.5638\n",
      "Processing batch 5423/11884 - Val_Loss: 28.2816\n",
      "Processing batch 5424/11884 - Val_Loss: 23.9885\n",
      "Processing batch 5425/11884 - Val_Loss: 25.1190\n",
      "Processing batch 5426/11884 - Val_Loss: 25.8677\n",
      "Processing batch 5427/11884 - Val_Loss: 25.9419\n",
      "Processing batch 5428/11884 - Val_Loss: 25.8765\n",
      "Processing batch 5429/11884 - Val_Loss: 25.1636\n",
      "Processing batch 5430/11884 - Val_Loss: 23.4806\n",
      "Processing batch 5431/11884 - Val_Loss: 26.0430\n",
      "Processing batch 5432/11884 - Val_Loss: 28.9465\n",
      "Processing batch 5433/11884 - Val_Loss: 26.6531\n",
      "Processing batch 5434/11884 - Val_Loss: 24.7550\n",
      "Processing batch 5435/11884 - Val_Loss: 24.9992\n",
      "Processing batch 5436/11884 - Val_Loss: 23.8340\n",
      "Processing batch 5437/11884 - Val_Loss: 29.3634\n",
      "Processing batch 5438/11884 - Val_Loss: 25.4683\n",
      "Processing batch 5439/11884 - Val_Loss: 28.6824\n",
      "Processing batch 5440/11884 - Val_Loss: 23.0977\n",
      "Processing batch 5441/11884 - Val_Loss: 25.1557\n",
      "Processing batch 5442/11884 - Val_Loss: 23.8608\n",
      "Processing batch 5443/11884 - Val_Loss: 27.9181\n",
      "Processing batch 5444/11884 - Val_Loss: 24.5426\n",
      "Processing batch 5445/11884 - Val_Loss: 26.5586\n",
      "Processing batch 5446/11884 - Val_Loss: 24.8464\n",
      "Processing batch 5447/11884 - Val_Loss: 24.3191\n",
      "Processing batch 5448/11884 - Val_Loss: 24.6213\n",
      "Processing batch 5449/11884 - Val_Loss: 24.9423\n",
      "Processing batch 5450/11884 - Val_Loss: 24.5714\n",
      "Processing batch 5451/11884 - Val_Loss: 25.2194\n",
      "Processing batch 5452/11884 - Val_Loss: 25.9936\n",
      "Processing batch 5453/11884 - Val_Loss: 26.0071\n",
      "Processing batch 5454/11884 - Val_Loss: 26.4590\n",
      "Processing batch 5455/11884 - Val_Loss: 24.9647\n",
      "Processing batch 5456/11884 - Val_Loss: 27.1604\n",
      "Processing batch 5457/11884 - Val_Loss: 24.8158\n",
      "Processing batch 5458/11884 - Val_Loss: 25.4888\n",
      "Processing batch 5459/11884 - Val_Loss: 26.0968\n",
      "Processing batch 5460/11884 - Val_Loss: 24.8707\n",
      "Processing batch 5461/11884 - Val_Loss: 24.8161\n",
      "Processing batch 5462/11884 - Val_Loss: 24.8496\n",
      "Processing batch 5463/11884 - Val_Loss: 28.0338\n",
      "Processing batch 5464/11884 - Val_Loss: 24.6917\n",
      "Processing batch 5465/11884 - Val_Loss: 26.7077\n",
      "Processing batch 5466/11884 - Val_Loss: 27.4582\n",
      "Processing batch 5467/11884 - Val_Loss: 25.2072\n",
      "Processing batch 5468/11884 - Val_Loss: 23.5962\n",
      "Processing batch 5469/11884 - Val_Loss: 25.4716\n",
      "Processing batch 5470/11884 - Val_Loss: 26.5167\n",
      "Processing batch 5471/11884 - Val_Loss: 23.9574\n",
      "Processing batch 5472/11884 - Val_Loss: 26.6624\n",
      "Processing batch 5473/11884 - Val_Loss: 26.2948\n",
      "Processing batch 5474/11884 - Val_Loss: 24.1438\n",
      "Processing batch 5475/11884 - Val_Loss: 24.4931\n",
      "Processing batch 5476/11884 - Val_Loss: 27.2041\n",
      "Processing batch 5477/11884 - Val_Loss: 25.9256\n",
      "Processing batch 5478/11884 - Val_Loss: 24.9342\n",
      "Processing batch 5479/11884 - Val_Loss: 24.9152\n",
      "Processing batch 5480/11884 - Val_Loss: 26.6033\n",
      "Processing batch 5481/11884 - Val_Loss: 23.8780\n",
      "Processing batch 5482/11884 - Val_Loss: 26.3140\n",
      "Processing batch 5483/11884 - Val_Loss: 24.1707\n",
      "Processing batch 5484/11884 - Val_Loss: 25.6998\n",
      "Processing batch 5485/11884 - Val_Loss: 26.4340\n",
      "Processing batch 5486/11884 - Val_Loss: 26.0658\n",
      "Processing batch 5487/11884 - Val_Loss: 26.1890\n",
      "Processing batch 5488/11884 - Val_Loss: 20.0501\n",
      "Processing batch 5489/11884 - Val_Loss: 26.6237\n",
      "Processing batch 5490/11884 - Val_Loss: 24.1290\n",
      "Processing batch 5491/11884 - Val_Loss: 29.2089\n",
      "Processing batch 5492/11884 - Val_Loss: 22.3515\n",
      "Processing batch 5493/11884 - Val_Loss: 28.2422\n",
      "Processing batch 5494/11884 - Val_Loss: 25.4771\n",
      "Processing batch 5495/11884 - Val_Loss: 23.3634\n",
      "Processing batch 5496/11884 - Val_Loss: 27.1334\n",
      "Processing batch 5497/11884 - Val_Loss: 29.1815\n",
      "Processing batch 5498/11884 - Val_Loss: 28.4352\n",
      "Processing batch 5499/11884 - Val_Loss: 27.8342\n",
      "Processing batch 5500/11884 - Val_Loss: 26.2862\n",
      "Processing batch 5501/11884 - Val_Loss: 26.6610\n",
      "Processing batch 5502/11884 - Val_Loss: 25.4444\n",
      "Processing batch 5503/11884 - Val_Loss: 26.6444\n",
      "Processing batch 5504/11884 - Val_Loss: 27.8263\n",
      "Processing batch 5505/11884 - Val_Loss: 24.0370\n",
      "Processing batch 5506/11884 - Val_Loss: 26.1236\n",
      "Processing batch 5507/11884 - Val_Loss: 25.9923\n",
      "Processing batch 5508/11884 - Val_Loss: 25.2369\n",
      "Processing batch 5509/11884 - Val_Loss: 24.1699\n",
      "Processing batch 5510/11884 - Val_Loss: 26.9758\n",
      "Processing batch 5511/11884 - Val_Loss: 28.4860\n",
      "Processing batch 5512/11884 - Val_Loss: 26.1926\n",
      "Processing batch 5513/11884 - Val_Loss: 24.2911\n",
      "Processing batch 5514/11884 - Val_Loss: 26.3636\n",
      "Processing batch 5515/11884 - Val_Loss: 25.2959\n",
      "Processing batch 5516/11884 - Val_Loss: 23.7993\n",
      "Processing batch 5517/11884 - Val_Loss: 25.9213\n",
      "Processing batch 5518/11884 - Val_Loss: 24.3515\n",
      "Processing batch 5519/11884 - Val_Loss: 27.0044\n",
      "Processing batch 5520/11884 - Val_Loss: 23.5791\n",
      "Processing batch 5521/11884 - Val_Loss: 24.5041\n",
      "Processing batch 5522/11884 - Val_Loss: 28.4125\n",
      "Processing batch 5523/11884 - Val_Loss: 29.4671\n",
      "Processing batch 5524/11884 - Val_Loss: 23.5395\n",
      "Processing batch 5525/11884 - Val_Loss: 25.5851\n",
      "Processing batch 5526/11884 - Val_Loss: 26.1466\n",
      "Processing batch 5527/11884 - Val_Loss: 23.9907\n",
      "Processing batch 5528/11884 - Val_Loss: 27.7221\n",
      "Processing batch 5529/11884 - Val_Loss: 26.6082\n",
      "Processing batch 5530/11884 - Val_Loss: 27.2594\n",
      "Processing batch 5531/11884 - Val_Loss: 26.2161\n",
      "Processing batch 5532/11884 - Val_Loss: 25.5788\n",
      "Processing batch 5533/11884 - Val_Loss: 24.5862\n",
      "Processing batch 5534/11884 - Val_Loss: 25.1798\n",
      "Processing batch 5535/11884 - Val_Loss: 27.0655\n",
      "Processing batch 5536/11884 - Val_Loss: 26.4553\n",
      "Processing batch 5537/11884 - Val_Loss: 26.3240\n",
      "Processing batch 5538/11884 - Val_Loss: 27.1561\n",
      "Processing batch 5539/11884 - Val_Loss: 25.9249\n",
      "Processing batch 5540/11884 - Val_Loss: 25.1341\n",
      "Processing batch 5541/11884 - Val_Loss: 24.4920\n",
      "Processing batch 5542/11884 - Val_Loss: 24.0992\n",
      "Processing batch 5543/11884 - Val_Loss: 23.5680\n",
      "Processing batch 5544/11884 - Val_Loss: 23.7685\n",
      "Processing batch 5545/11884 - Val_Loss: 27.3341\n",
      "Processing batch 5546/11884 - Val_Loss: 26.1979\n",
      "Processing batch 5547/11884 - Val_Loss: 24.7713\n",
      "Processing batch 5548/11884 - Val_Loss: 24.7656\n",
      "Processing batch 5549/11884 - Val_Loss: 26.6694\n",
      "Processing batch 5550/11884 - Val_Loss: 27.2514\n",
      "Processing batch 5551/11884 - Val_Loss: 27.0224\n",
      "Processing batch 5552/11884 - Val_Loss: 26.6778\n",
      "Processing batch 5553/11884 - Val_Loss: 28.9671\n",
      "Processing batch 5554/11884 - Val_Loss: 26.5207\n",
      "Processing batch 5555/11884 - Val_Loss: 27.0728\n",
      "Processing batch 5556/11884 - Val_Loss: 29.3997\n",
      "Processing batch 5557/11884 - Val_Loss: 25.3016\n",
      "Processing batch 5558/11884 - Val_Loss: 24.0428\n",
      "Processing batch 5559/11884 - Val_Loss: 26.0317\n",
      "Processing batch 5560/11884 - Val_Loss: 29.4262\n",
      "Processing batch 5561/11884 - Val_Loss: 27.8789\n",
      "Processing batch 5562/11884 - Val_Loss: 25.4414\n",
      "Processing batch 5563/11884 - Val_Loss: 24.8741\n",
      "Processing batch 5564/11884 - Val_Loss: 26.5110\n",
      "Processing batch 5565/11884 - Val_Loss: 25.1311\n",
      "Processing batch 5566/11884 - Val_Loss: 23.7539\n",
      "Processing batch 5567/11884 - Val_Loss: 23.8803\n",
      "Processing batch 5568/11884 - Val_Loss: 28.5525\n",
      "Processing batch 5569/11884 - Val_Loss: 26.0833\n",
      "Processing batch 5570/11884 - Val_Loss: 26.3993\n",
      "Processing batch 5571/11884 - Val_Loss: 24.3462\n",
      "Processing batch 5572/11884 - Val_Loss: 26.2312\n",
      "Processing batch 5573/11884 - Val_Loss: 26.5327\n",
      "Processing batch 5574/11884 - Val_Loss: 26.1844\n",
      "Processing batch 5575/11884 - Val_Loss: 27.5067\n",
      "Processing batch 5576/11884 - Val_Loss: 25.1083\n",
      "Processing batch 5577/11884 - Val_Loss: 25.9107\n",
      "Processing batch 5578/11884 - Val_Loss: 21.9430\n",
      "Processing batch 5579/11884 - Val_Loss: 27.2018\n",
      "Processing batch 5580/11884 - Val_Loss: 25.4150\n",
      "Processing batch 5581/11884 - Val_Loss: 24.9615\n",
      "Processing batch 5582/11884 - Val_Loss: 28.1237\n",
      "Processing batch 5583/11884 - Val_Loss: 24.9149\n",
      "Processing batch 5584/11884 - Val_Loss: 28.4210\n",
      "Processing batch 5585/11884 - Val_Loss: 24.0180\n",
      "Processing batch 5586/11884 - Val_Loss: 25.9220\n",
      "Processing batch 5587/11884 - Val_Loss: 26.4764\n",
      "Processing batch 5588/11884 - Val_Loss: 27.4152\n",
      "Processing batch 5589/11884 - Val_Loss: 24.0792\n",
      "Processing batch 5590/11884 - Val_Loss: 26.8006\n",
      "Processing batch 5591/11884 - Val_Loss: 24.3329\n",
      "Processing batch 5592/11884 - Val_Loss: 23.7105\n",
      "Processing batch 5593/11884 - Val_Loss: 27.5422\n",
      "Processing batch 5594/11884 - Val_Loss: 26.2484\n",
      "Processing batch 5595/11884 - Val_Loss: 25.3547\n",
      "Processing batch 5596/11884 - Val_Loss: 26.0285\n",
      "Processing batch 5597/11884 - Val_Loss: 25.8723\n",
      "Processing batch 5598/11884 - Val_Loss: 23.9358\n",
      "Processing batch 5599/11884 - Val_Loss: 25.0953\n",
      "Processing batch 5600/11884 - Val_Loss: 21.9546\n",
      "Processing batch 5601/11884 - Val_Loss: 25.1652\n",
      "Processing batch 5602/11884 - Val_Loss: 24.0650\n",
      "Processing batch 5603/11884 - Val_Loss: 26.3069\n",
      "Processing batch 5604/11884 - Val_Loss: 27.5555\n",
      "Processing batch 5605/11884 - Val_Loss: 25.3855\n",
      "Processing batch 5606/11884 - Val_Loss: 23.3344\n",
      "Processing batch 5607/11884 - Val_Loss: 26.7639\n",
      "Processing batch 5608/11884 - Val_Loss: 24.3903\n",
      "Processing batch 5609/11884 - Val_Loss: 25.2128\n",
      "Processing batch 5610/11884 - Val_Loss: 27.6098\n",
      "Processing batch 5611/11884 - Val_Loss: 27.3290\n",
      "Processing batch 5612/11884 - Val_Loss: 23.5857\n",
      "Processing batch 5613/11884 - Val_Loss: 26.8146\n",
      "Processing batch 5614/11884 - Val_Loss: 26.1562\n",
      "Processing batch 5615/11884 - Val_Loss: 26.3799\n",
      "Processing batch 5616/11884 - Val_Loss: 23.5417\n",
      "Processing batch 5617/11884 - Val_Loss: 27.5889\n",
      "Processing batch 5618/11884 - Val_Loss: 26.6714\n",
      "Processing batch 5619/11884 - Val_Loss: 22.1615\n",
      "Processing batch 5620/11884 - Val_Loss: 22.9814\n",
      "Processing batch 5621/11884 - Val_Loss: 22.1832\n",
      "Processing batch 5622/11884 - Val_Loss: 25.2076\n",
      "Processing batch 5623/11884 - Val_Loss: 25.7696\n",
      "Processing batch 5624/11884 - Val_Loss: 25.9773\n",
      "Processing batch 5625/11884 - Val_Loss: 26.2758\n",
      "Processing batch 5626/11884 - Val_Loss: 25.4439\n",
      "Processing batch 5627/11884 - Val_Loss: 25.2724\n",
      "Processing batch 5628/11884 - Val_Loss: 25.8890\n",
      "Processing batch 5629/11884 - Val_Loss: 24.9869\n",
      "Processing batch 5630/11884 - Val_Loss: 24.6184\n",
      "Processing batch 5631/11884 - Val_Loss: 28.4046\n",
      "Processing batch 5632/11884 - Val_Loss: 28.0206\n",
      "Processing batch 5633/11884 - Val_Loss: 25.6731\n",
      "Processing batch 5634/11884 - Val_Loss: 25.1130\n",
      "Processing batch 5635/11884 - Val_Loss: 26.7204\n",
      "Processing batch 5636/11884 - Val_Loss: 25.7847\n",
      "Processing batch 5637/11884 - Val_Loss: 26.2432\n",
      "Processing batch 5638/11884 - Val_Loss: 23.9715\n",
      "Processing batch 5639/11884 - Val_Loss: 28.8242\n",
      "Processing batch 5640/11884 - Val_Loss: 26.8675\n",
      "Processing batch 5641/11884 - Val_Loss: 23.4367\n",
      "Processing batch 5642/11884 - Val_Loss: 27.8122\n",
      "Processing batch 5643/11884 - Val_Loss: 28.6339\n",
      "Processing batch 5644/11884 - Val_Loss: 25.7235\n",
      "Processing batch 5645/11884 - Val_Loss: 24.3922\n",
      "Processing batch 5646/11884 - Val_Loss: 24.9288\n",
      "Processing batch 5647/11884 - Val_Loss: 28.5712\n",
      "Processing batch 5648/11884 - Val_Loss: 25.7783\n",
      "Processing batch 5649/11884 - Val_Loss: 26.3446\n",
      "Processing batch 5650/11884 - Val_Loss: 26.5594\n",
      "Processing batch 5651/11884 - Val_Loss: 24.2857\n",
      "Processing batch 5652/11884 - Val_Loss: 24.9317\n",
      "Processing batch 5653/11884 - Val_Loss: 23.0868\n",
      "Processing batch 5654/11884 - Val_Loss: 24.9191\n",
      "Processing batch 5655/11884 - Val_Loss: 28.5052\n",
      "Processing batch 5656/11884 - Val_Loss: 27.6486\n",
      "Processing batch 5657/11884 - Val_Loss: 25.0267\n",
      "Processing batch 5658/11884 - Val_Loss: 25.8464\n",
      "Processing batch 5659/11884 - Val_Loss: 26.0205\n",
      "Processing batch 5660/11884 - Val_Loss: 25.7528\n",
      "Processing batch 5661/11884 - Val_Loss: 24.2903\n",
      "Processing batch 5662/11884 - Val_Loss: 27.9236\n",
      "Processing batch 5663/11884 - Val_Loss: 25.0135\n",
      "Processing batch 5664/11884 - Val_Loss: 24.4814\n",
      "Processing batch 5665/11884 - Val_Loss: 24.0272\n",
      "Processing batch 5666/11884 - Val_Loss: 26.6080\n",
      "Processing batch 5667/11884 - Val_Loss: 29.0903\n",
      "Processing batch 5668/11884 - Val_Loss: 24.2318\n",
      "Processing batch 5669/11884 - Val_Loss: 27.8899\n",
      "Processing batch 5670/11884 - Val_Loss: 24.5896\n",
      "Processing batch 5671/11884 - Val_Loss: 28.0463\n",
      "Processing batch 5672/11884 - Val_Loss: 26.2608\n",
      "Processing batch 5673/11884 - Val_Loss: 26.7119\n",
      "Processing batch 5674/11884 - Val_Loss: 24.1075\n",
      "Processing batch 5675/11884 - Val_Loss: 27.3891\n",
      "Processing batch 5676/11884 - Val_Loss: 27.6520\n",
      "Processing batch 5677/11884 - Val_Loss: 25.3110\n",
      "Processing batch 5678/11884 - Val_Loss: 24.6707\n",
      "Processing batch 5679/11884 - Val_Loss: 24.7785\n",
      "Processing batch 5680/11884 - Val_Loss: 27.5311\n",
      "Processing batch 5681/11884 - Val_Loss: 26.1324\n",
      "Processing batch 5682/11884 - Val_Loss: 27.9656\n",
      "Processing batch 5683/11884 - Val_Loss: 27.2678\n",
      "Processing batch 5684/11884 - Val_Loss: 25.8769\n",
      "Processing batch 5685/11884 - Val_Loss: 25.0196\n",
      "Processing batch 5686/11884 - Val_Loss: 24.7425\n",
      "Processing batch 5687/11884 - Val_Loss: 27.8440\n",
      "Processing batch 5688/11884 - Val_Loss: 26.1490\n",
      "Processing batch 5689/11884 - Val_Loss: 27.7005\n",
      "Processing batch 5690/11884 - Val_Loss: 27.3436\n",
      "Processing batch 5691/11884 - Val_Loss: 25.2844\n",
      "Processing batch 5692/11884 - Val_Loss: 27.5017\n",
      "Processing batch 5693/11884 - Val_Loss: 23.7100\n",
      "Processing batch 5694/11884 - Val_Loss: 26.4806\n",
      "Processing batch 5695/11884 - Val_Loss: 28.2002\n",
      "Processing batch 5696/11884 - Val_Loss: 25.5532\n",
      "Processing batch 5697/11884 - Val_Loss: 26.8246\n",
      "Processing batch 5698/11884 - Val_Loss: 26.3648\n",
      "Processing batch 5699/11884 - Val_Loss: 25.0269\n",
      "Processing batch 5700/11884 - Val_Loss: 30.3201\n",
      "Processing batch 5701/11884 - Val_Loss: 26.2733\n",
      "Processing batch 5702/11884 - Val_Loss: 25.1042\n",
      "Processing batch 5703/11884 - Val_Loss: 26.6354\n",
      "Processing batch 5704/11884 - Val_Loss: 26.6606\n",
      "Processing batch 5705/11884 - Val_Loss: 22.6378\n",
      "Processing batch 5706/11884 - Val_Loss: 28.7578\n",
      "Processing batch 5707/11884 - Val_Loss: 29.5721\n",
      "Processing batch 5708/11884 - Val_Loss: 24.3728\n",
      "Processing batch 5709/11884 - Val_Loss: 26.6794\n",
      "Processing batch 5710/11884 - Val_Loss: 25.4841\n",
      "Processing batch 5711/11884 - Val_Loss: 26.2941\n",
      "Processing batch 5712/11884 - Val_Loss: 23.7793\n",
      "Processing batch 5713/11884 - Val_Loss: 26.0885\n",
      "Processing batch 5714/11884 - Val_Loss: 25.6518\n",
      "Processing batch 5715/11884 - Val_Loss: 23.9322\n",
      "Processing batch 5716/11884 - Val_Loss: 26.2241\n",
      "Processing batch 5717/11884 - Val_Loss: 25.3712\n",
      "Processing batch 5718/11884 - Val_Loss: 25.6150\n",
      "Processing batch 5719/11884 - Val_Loss: 24.3311\n",
      "Processing batch 5720/11884 - Val_Loss: 25.9738\n",
      "Processing batch 5721/11884 - Val_Loss: 26.2958\n",
      "Processing batch 5722/11884 - Val_Loss: 24.0802\n",
      "Processing batch 5723/11884 - Val_Loss: 25.1079\n",
      "Processing batch 5724/11884 - Val_Loss: 31.4276\n",
      "Processing batch 5725/11884 - Val_Loss: 24.1895\n",
      "Processing batch 5726/11884 - Val_Loss: 26.2842\n",
      "Processing batch 5727/11884 - Val_Loss: 25.8503\n",
      "Processing batch 5728/11884 - Val_Loss: 27.0866\n",
      "Processing batch 5729/11884 - Val_Loss: 28.3945\n",
      "Processing batch 5730/11884 - Val_Loss: 27.5226\n",
      "Processing batch 5731/11884 - Val_Loss: 24.3768\n",
      "Processing batch 5732/11884 - Val_Loss: 21.0224\n",
      "Processing batch 5733/11884 - Val_Loss: 25.8561\n",
      "Processing batch 5734/11884 - Val_Loss: 23.9284\n",
      "Processing batch 5735/11884 - Val_Loss: 27.2214\n",
      "Processing batch 5736/11884 - Val_Loss: 28.8674\n",
      "Processing batch 5737/11884 - Val_Loss: 25.1430\n",
      "Processing batch 5738/11884 - Val_Loss: 27.8799\n",
      "Processing batch 5739/11884 - Val_Loss: 27.3724\n",
      "Processing batch 5740/11884 - Val_Loss: 27.0036\n",
      "Processing batch 5741/11884 - Val_Loss: 26.1093\n",
      "Processing batch 5742/11884 - Val_Loss: 23.4875\n",
      "Processing batch 5743/11884 - Val_Loss: 21.5968\n",
      "Processing batch 5744/11884 - Val_Loss: 25.3459\n",
      "Processing batch 5745/11884 - Val_Loss: 29.3837\n",
      "Processing batch 5746/11884 - Val_Loss: 25.5476\n",
      "Processing batch 5747/11884 - Val_Loss: 26.3161\n",
      "Processing batch 5748/11884 - Val_Loss: 24.1591\n",
      "Processing batch 5749/11884 - Val_Loss: 25.3352\n",
      "Processing batch 5750/11884 - Val_Loss: 23.9854\n",
      "Processing batch 5751/11884 - Val_Loss: 23.9141\n",
      "Processing batch 5752/11884 - Val_Loss: 25.3936\n",
      "Processing batch 5753/11884 - Val_Loss: 24.8015\n",
      "Processing batch 5754/11884 - Val_Loss: 28.4961\n",
      "Processing batch 5755/11884 - Val_Loss: 24.9759\n",
      "Processing batch 5756/11884 - Val_Loss: 23.4480\n",
      "Processing batch 5757/11884 - Val_Loss: 27.3501\n",
      "Processing batch 5758/11884 - Val_Loss: 25.0782\n",
      "Processing batch 5759/11884 - Val_Loss: 21.9402\n",
      "Processing batch 5760/11884 - Val_Loss: 27.9505\n",
      "Processing batch 5761/11884 - Val_Loss: 27.7861\n",
      "Processing batch 5762/11884 - Val_Loss: 27.5302\n",
      "Processing batch 5763/11884 - Val_Loss: 26.7184\n",
      "Processing batch 5764/11884 - Val_Loss: 25.1156\n",
      "Processing batch 5765/11884 - Val_Loss: 27.6207\n",
      "Processing batch 5766/11884 - Val_Loss: 24.4781\n",
      "Processing batch 5767/11884 - Val_Loss: 24.4665\n",
      "Processing batch 5768/11884 - Val_Loss: 25.8925\n",
      "Processing batch 5769/11884 - Val_Loss: 26.5739\n",
      "Processing batch 5770/11884 - Val_Loss: 25.8870\n",
      "Processing batch 5771/11884 - Val_Loss: 26.2084\n",
      "Processing batch 5772/11884 - Val_Loss: 23.8097\n",
      "Processing batch 5773/11884 - Val_Loss: 25.8785\n",
      "Processing batch 5774/11884 - Val_Loss: 26.2503\n",
      "Processing batch 5775/11884 - Val_Loss: 28.1876\n",
      "Processing batch 5776/11884 - Val_Loss: 29.4449\n",
      "Processing batch 5777/11884 - Val_Loss: 24.6983\n",
      "Processing batch 5778/11884 - Val_Loss: 29.4316\n",
      "Processing batch 5779/11884 - Val_Loss: 24.9007\n",
      "Processing batch 5780/11884 - Val_Loss: 26.1352\n",
      "Processing batch 5781/11884 - Val_Loss: 23.9922\n",
      "Processing batch 5782/11884 - Val_Loss: 24.2582\n",
      "Processing batch 5783/11884 - Val_Loss: 24.3589\n",
      "Processing batch 5784/11884 - Val_Loss: 26.1402\n",
      "Processing batch 5785/11884 - Val_Loss: 25.3298\n",
      "Processing batch 5786/11884 - Val_Loss: 24.7634\n",
      "Processing batch 5787/11884 - Val_Loss: 25.0198\n",
      "Processing batch 5788/11884 - Val_Loss: 28.5189\n",
      "Processing batch 5789/11884 - Val_Loss: 28.4650\n",
      "Processing batch 5790/11884 - Val_Loss: 27.5519\n",
      "Processing batch 5791/11884 - Val_Loss: 25.6842\n",
      "Processing batch 5792/11884 - Val_Loss: 27.9275\n",
      "Processing batch 5793/11884 - Val_Loss: 27.9961\n",
      "Processing batch 5794/11884 - Val_Loss: 24.1694\n",
      "Processing batch 5795/11884 - Val_Loss: 23.0130\n",
      "Processing batch 5796/11884 - Val_Loss: 25.7974\n",
      "Processing batch 5797/11884 - Val_Loss: 25.1450\n",
      "Processing batch 5798/11884 - Val_Loss: 22.8532\n",
      "Processing batch 5799/11884 - Val_Loss: 30.0513\n",
      "Processing batch 5800/11884 - Val_Loss: 24.7485\n",
      "Processing batch 5801/11884 - Val_Loss: 26.0774\n",
      "Processing batch 5802/11884 - Val_Loss: 28.5936\n",
      "Processing batch 5803/11884 - Val_Loss: 25.3123\n",
      "Processing batch 5804/11884 - Val_Loss: 23.8416\n",
      "Processing batch 5805/11884 - Val_Loss: 25.5673\n",
      "Processing batch 5806/11884 - Val_Loss: 25.3911\n",
      "Processing batch 5807/11884 - Val_Loss: 25.8550\n",
      "Processing batch 5808/11884 - Val_Loss: 25.5568\n",
      "Processing batch 5809/11884 - Val_Loss: 26.6338\n",
      "Processing batch 5810/11884 - Val_Loss: 25.4219\n",
      "Processing batch 5811/11884 - Val_Loss: 26.9238\n",
      "Processing batch 5812/11884 - Val_Loss: 25.1048\n",
      "Processing batch 5813/11884 - Val_Loss: 25.1294\n",
      "Processing batch 5814/11884 - Val_Loss: 22.8582\n",
      "Processing batch 5815/11884 - Val_Loss: 22.7111\n",
      "Processing batch 5816/11884 - Val_Loss: 25.7599\n",
      "Processing batch 5817/11884 - Val_Loss: 26.9966\n",
      "Processing batch 5818/11884 - Val_Loss: 25.1470\n",
      "Processing batch 5819/11884 - Val_Loss: 25.9212\n",
      "Processing batch 5820/11884 - Val_Loss: 25.6898\n",
      "Processing batch 5821/11884 - Val_Loss: 27.2095\n",
      "Processing batch 5822/11884 - Val_Loss: 25.1261\n",
      "Processing batch 5823/11884 - Val_Loss: 25.0305\n",
      "Processing batch 5824/11884 - Val_Loss: 24.4682\n",
      "Processing batch 5825/11884 - Val_Loss: 24.4737\n",
      "Processing batch 5826/11884 - Val_Loss: 32.8737\n",
      "Processing batch 5827/11884 - Val_Loss: 25.1452\n",
      "Processing batch 5828/11884 - Val_Loss: 23.0180\n",
      "Processing batch 5829/11884 - Val_Loss: 28.4202\n",
      "Processing batch 5830/11884 - Val_Loss: 24.2889\n",
      "Processing batch 5831/11884 - Val_Loss: 24.3744\n",
      "Processing batch 5832/11884 - Val_Loss: 28.9068\n",
      "Processing batch 5833/11884 - Val_Loss: 29.5012\n",
      "Processing batch 5834/11884 - Val_Loss: 28.0277\n",
      "Processing batch 5835/11884 - Val_Loss: 26.0101\n",
      "Processing batch 5836/11884 - Val_Loss: 26.8586\n",
      "Processing batch 5837/11884 - Val_Loss: 25.8290\n",
      "Processing batch 5838/11884 - Val_Loss: 27.5198\n",
      "Processing batch 5839/11884 - Val_Loss: 22.9151\n",
      "Processing batch 5840/11884 - Val_Loss: 27.4678\n",
      "Processing batch 5841/11884 - Val_Loss: 26.5757\n",
      "Processing batch 5842/11884 - Val_Loss: 28.8218\n",
      "Processing batch 5843/11884 - Val_Loss: 24.5299\n",
      "Processing batch 5844/11884 - Val_Loss: 26.9298\n",
      "Processing batch 5845/11884 - Val_Loss: 27.2746\n",
      "Processing batch 5846/11884 - Val_Loss: 23.9524\n",
      "Processing batch 5847/11884 - Val_Loss: 23.8539\n",
      "Processing batch 5848/11884 - Val_Loss: 27.9771\n",
      "Processing batch 5849/11884 - Val_Loss: 26.0797\n",
      "Processing batch 5850/11884 - Val_Loss: 26.0983\n",
      "Processing batch 5851/11884 - Val_Loss: 22.7388\n",
      "Processing batch 5852/11884 - Val_Loss: 23.9458\n",
      "Processing batch 5853/11884 - Val_Loss: 27.9205\n",
      "Processing batch 5854/11884 - Val_Loss: 28.0737\n",
      "Processing batch 5855/11884 - Val_Loss: 27.0313\n",
      "Processing batch 5856/11884 - Val_Loss: 27.6439\n",
      "Processing batch 5857/11884 - Val_Loss: 27.4620\n",
      "Processing batch 5858/11884 - Val_Loss: 26.9556\n",
      "Processing batch 5859/11884 - Val_Loss: 22.4205\n",
      "Processing batch 5860/11884 - Val_Loss: 27.0147\n",
      "Processing batch 5861/11884 - Val_Loss: 24.5568\n",
      "Processing batch 5862/11884 - Val_Loss: 22.5527\n",
      "Processing batch 5863/11884 - Val_Loss: 28.7872\n",
      "Processing batch 5864/11884 - Val_Loss: 24.7046\n",
      "Epoch 1/5 - Train loss: 30.5065 - Val loss: 25.7928\n",
      "Processing batch 1/11884 - Loss: 29.7336\n",
      "Processing batch 2/11884 - Loss: 30.5167\n",
      "Processing batch 3/11884 - Loss: 29.1232\n",
      "Processing batch 4/11884 - Loss: 31.0433\n",
      "Processing batch 5/11884 - Loss: 30.1173\n",
      "Processing batch 6/11884 - Loss: 30.1989\n",
      "Processing batch 7/11884 - Loss: 30.4096\n",
      "Processing batch 8/11884 - Loss: 30.6003\n",
      "Processing batch 9/11884 - Loss: 28.9219\n",
      "Processing batch 10/11884 - Loss: 29.4553\n",
      "Processing batch 11/11884 - Loss: 29.7670\n",
      "Processing batch 12/11884 - Loss: 29.7333\n",
      "Processing batch 13/11884 - Loss: 30.7222\n",
      "Processing batch 14/11884 - Loss: 29.7377\n",
      "Processing batch 15/11884 - Loss: 29.9411\n",
      "Processing batch 16/11884 - Loss: 30.4627\n",
      "Processing batch 17/11884 - Loss: 30.6918\n",
      "Processing batch 18/11884 - Loss: 29.6561\n",
      "Processing batch 19/11884 - Loss: 28.4984\n",
      "Processing batch 20/11884 - Loss: 30.8069\n",
      "Processing batch 21/11884 - Loss: 31.5579\n",
      "Processing batch 22/11884 - Loss: 30.6789\n",
      "Processing batch 23/11884 - Loss: 31.1810\n",
      "Processing batch 24/11884 - Loss: 28.6132\n",
      "Processing batch 25/11884 - Loss: 30.8882\n",
      "Processing batch 26/11884 - Loss: 30.7506\n",
      "Processing batch 27/11884 - Loss: 30.4922\n",
      "Processing batch 28/11884 - Loss: 30.0404\n",
      "Processing batch 29/11884 - Loss: 30.8529\n",
      "Processing batch 30/11884 - Loss: 30.3193\n",
      "Processing batch 31/11884 - Loss: 30.4253\n",
      "Processing batch 32/11884 - Loss: 29.5505\n",
      "Processing batch 33/11884 - Loss: 30.5601\n",
      "Processing batch 34/11884 - Loss: 29.0864\n",
      "Processing batch 35/11884 - Loss: 29.4403\n",
      "Processing batch 36/11884 - Loss: 30.3512\n",
      "Processing batch 37/11884 - Loss: 29.7023\n",
      "Processing batch 38/11884 - Loss: 29.5565\n",
      "Processing batch 39/11884 - Loss: 30.1743\n",
      "Processing batch 40/11884 - Loss: 29.8627\n",
      "Processing batch 41/11884 - Loss: 31.1803\n",
      "Processing batch 42/11884 - Loss: 30.5867\n",
      "Processing batch 43/11884 - Loss: 28.6377\n",
      "Processing batch 44/11884 - Loss: 31.0637\n",
      "Processing batch 45/11884 - Loss: 29.7084\n",
      "Processing batch 46/11884 - Loss: 30.9318\n",
      "Processing batch 47/11884 - Loss: 30.8766\n",
      "Processing batch 48/11884 - Loss: 30.2957\n",
      "Processing batch 49/11884 - Loss: 29.6991\n",
      "Processing batch 50/11884 - Loss: 30.2246\n",
      "Processing batch 51/11884 - Loss: 30.3319\n",
      "Processing batch 52/11884 - Loss: 30.8507\n",
      "Processing batch 53/11884 - Loss: 30.6199\n",
      "Processing batch 54/11884 - Loss: 30.1529\n",
      "Processing batch 55/11884 - Loss: 30.3622\n",
      "Processing batch 56/11884 - Loss: 29.9580\n",
      "Processing batch 57/11884 - Loss: 30.6217\n",
      "Processing batch 58/11884 - Loss: 30.0255\n",
      "Processing batch 59/11884 - Loss: 31.0290\n",
      "Processing batch 60/11884 - Loss: 29.6551\n",
      "Processing batch 61/11884 - Loss: 30.2507\n",
      "Processing batch 62/11884 - Loss: 29.7091\n",
      "Processing batch 63/11884 - Loss: 30.9512\n",
      "Processing batch 64/11884 - Loss: 29.9409\n",
      "Processing batch 65/11884 - Loss: 30.2961\n",
      "Processing batch 66/11884 - Loss: 29.5180\n",
      "Processing batch 67/11884 - Loss: 30.9783\n",
      "Processing batch 68/11884 - Loss: 30.6100\n",
      "Processing batch 69/11884 - Loss: 29.5711\n",
      "Processing batch 70/11884 - Loss: 27.9720\n",
      "Processing batch 71/11884 - Loss: 31.3919\n",
      "Processing batch 72/11884 - Loss: 29.6739\n",
      "Processing batch 73/11884 - Loss: 30.1281\n",
      "Processing batch 74/11884 - Loss: 30.9288\n",
      "Processing batch 75/11884 - Loss: 30.0991\n",
      "Processing batch 76/11884 - Loss: 30.7644\n",
      "Processing batch 77/11884 - Loss: 30.2063\n",
      "Processing batch 78/11884 - Loss: 31.9131\n",
      "Processing batch 79/11884 - Loss: 29.5257\n",
      "Processing batch 80/11884 - Loss: 31.6939\n",
      "Processing batch 81/11884 - Loss: 29.9288\n",
      "Processing batch 82/11884 - Loss: 30.4641\n",
      "Processing batch 83/11884 - Loss: 30.1557\n",
      "Processing batch 84/11884 - Loss: 30.8949\n",
      "Processing batch 85/11884 - Loss: 30.8178\n",
      "Processing batch 86/11884 - Loss: 29.5412\n",
      "Processing batch 87/11884 - Loss: 28.8057\n",
      "Processing batch 88/11884 - Loss: 30.7722\n",
      "Processing batch 89/11884 - Loss: 30.0478\n",
      "Processing batch 90/11884 - Loss: 30.3544\n",
      "Processing batch 91/11884 - Loss: 30.0239\n",
      "Processing batch 92/11884 - Loss: 28.9205\n",
      "Processing batch 93/11884 - Loss: 28.5793\n",
      "Processing batch 94/11884 - Loss: 28.4158\n",
      "Processing batch 95/11884 - Loss: 29.3787\n",
      "Processing batch 96/11884 - Loss: 30.2417\n",
      "Processing batch 97/11884 - Loss: 29.6752\n",
      "Processing batch 98/11884 - Loss: 30.4060\n",
      "Processing batch 99/11884 - Loss: 30.5070\n",
      "Processing batch 100/11884 - Loss: 30.3213\n",
      "Processing batch 101/11884 - Loss: 29.1526\n",
      "Processing batch 102/11884 - Loss: 29.7162\n",
      "Processing batch 103/11884 - Loss: 30.4736\n",
      "Processing batch 104/11884 - Loss: 31.1482\n",
      "Processing batch 105/11884 - Loss: 31.0479\n",
      "Processing batch 106/11884 - Loss: 29.5613\n",
      "Processing batch 107/11884 - Loss: 29.1162\n",
      "Processing batch 108/11884 - Loss: 29.8677\n",
      "Processing batch 109/11884 - Loss: 31.5119\n",
      "Processing batch 110/11884 - Loss: 31.5813\n",
      "Processing batch 111/11884 - Loss: 31.0258\n",
      "Processing batch 112/11884 - Loss: 30.3103\n",
      "Processing batch 113/11884 - Loss: 30.4756\n",
      "Processing batch 114/11884 - Loss: 30.4232\n",
      "Processing batch 115/11884 - Loss: 30.4304\n",
      "Processing batch 116/11884 - Loss: 31.3236\n",
      "Processing batch 117/11884 - Loss: 31.1567\n",
      "Processing batch 118/11884 - Loss: 30.0707\n",
      "Processing batch 119/11884 - Loss: 30.4653\n",
      "Processing batch 120/11884 - Loss: 30.9183\n",
      "Processing batch 121/11884 - Loss: 30.5962\n",
      "Processing batch 122/11884 - Loss: 29.9379\n",
      "Processing batch 123/11884 - Loss: 30.4315\n",
      "Processing batch 124/11884 - Loss: 31.2396\n",
      "Processing batch 125/11884 - Loss: 30.4381\n",
      "Processing batch 126/11884 - Loss: 28.6747\n",
      "Processing batch 127/11884 - Loss: 30.4799\n",
      "Processing batch 128/11884 - Loss: 29.4851\n",
      "Processing batch 129/11884 - Loss: 31.0797\n",
      "Processing batch 130/11884 - Loss: 30.3791\n",
      "Processing batch 131/11884 - Loss: 30.1133\n",
      "Processing batch 132/11884 - Loss: 31.8352\n",
      "Processing batch 133/11884 - Loss: 30.1680\n",
      "Processing batch 134/11884 - Loss: 31.3278\n",
      "Processing batch 135/11884 - Loss: 30.1168\n",
      "Processing batch 136/11884 - Loss: 30.2038\n",
      "Processing batch 137/11884 - Loss: 31.8993\n",
      "Processing batch 138/11884 - Loss: 29.8736\n",
      "Processing batch 139/11884 - Loss: 30.0653\n",
      "Processing batch 140/11884 - Loss: 29.3151\n",
      "Processing batch 141/11884 - Loss: 30.6953\n",
      "Processing batch 142/11884 - Loss: 30.2655\n",
      "Processing batch 143/11884 - Loss: 30.4466\n",
      "Processing batch 144/11884 - Loss: 29.9170\n",
      "Processing batch 145/11884 - Loss: 30.2833\n",
      "Processing batch 146/11884 - Loss: 30.6131\n",
      "Processing batch 147/11884 - Loss: 29.2489\n",
      "Processing batch 148/11884 - Loss: 31.3410\n",
      "Processing batch 149/11884 - Loss: 30.2417\n",
      "Processing batch 150/11884 - Loss: 29.3450\n",
      "Processing batch 151/11884 - Loss: 29.4289\n",
      "Processing batch 152/11884 - Loss: 30.7649\n",
      "Processing batch 153/11884 - Loss: 31.3256\n",
      "Processing batch 154/11884 - Loss: 29.5467\n",
      "Processing batch 155/11884 - Loss: 30.3530\n",
      "Processing batch 156/11884 - Loss: 31.4308\n",
      "Processing batch 157/11884 - Loss: 30.9034\n",
      "Processing batch 158/11884 - Loss: 29.0594\n",
      "Processing batch 159/11884 - Loss: 30.7780\n",
      "Processing batch 160/11884 - Loss: 29.0922\n",
      "Processing batch 161/11884 - Loss: 29.3953\n",
      "Processing batch 162/11884 - Loss: 30.6098\n",
      "Processing batch 163/11884 - Loss: 30.6129\n",
      "Processing batch 164/11884 - Loss: 31.2056\n",
      "Processing batch 165/11884 - Loss: 31.8830\n",
      "Processing batch 166/11884 - Loss: 30.0827\n",
      "Processing batch 167/11884 - Loss: 31.8135\n",
      "Processing batch 168/11884 - Loss: 30.4901\n",
      "Processing batch 169/11884 - Loss: 31.7172\n",
      "Processing batch 170/11884 - Loss: 29.7213\n",
      "Processing batch 171/11884 - Loss: 31.1108\n",
      "Processing batch 172/11884 - Loss: 31.0336\n",
      "Processing batch 173/11884 - Loss: 30.5211\n",
      "Processing batch 174/11884 - Loss: 28.3715\n",
      "Processing batch 175/11884 - Loss: 31.5974\n",
      "Processing batch 176/11884 - Loss: 30.0473\n",
      "Processing batch 177/11884 - Loss: 30.5506\n",
      "Processing batch 178/11884 - Loss: 30.4831\n",
      "Processing batch 179/11884 - Loss: 29.5807\n",
      "Processing batch 180/11884 - Loss: 30.1702\n",
      "Processing batch 181/11884 - Loss: 29.5307\n",
      "Processing batch 182/11884 - Loss: 30.5571\n",
      "Processing batch 183/11884 - Loss: 29.7240\n",
      "Processing batch 184/11884 - Loss: 29.2521\n",
      "Processing batch 185/11884 - Loss: 30.0849\n",
      "Processing batch 186/11884 - Loss: 30.4299\n",
      "Processing batch 187/11884 - Loss: 31.4526\n",
      "Processing batch 188/11884 - Loss: 30.7868\n",
      "Processing batch 189/11884 - Loss: 31.4896\n",
      "Processing batch 190/11884 - Loss: 30.7229\n",
      "Processing batch 191/11884 - Loss: 29.9920\n",
      "Processing batch 192/11884 - Loss: 31.4982\n",
      "Processing batch 193/11884 - Loss: 30.5298\n",
      "Processing batch 194/11884 - Loss: 30.1411\n",
      "Processing batch 195/11884 - Loss: 29.8605\n",
      "Processing batch 196/11884 - Loss: 30.1829\n",
      "Processing batch 197/11884 - Loss: 29.8337\n",
      "Processing batch 198/11884 - Loss: 29.9900\n",
      "Processing batch 199/11884 - Loss: 29.8203\n",
      "Processing batch 200/11884 - Loss: 29.5204\n",
      "Processing batch 201/11884 - Loss: 30.5145\n",
      "Processing batch 202/11884 - Loss: 29.8571\n",
      "Processing batch 203/11884 - Loss: 30.9966\n",
      "Processing batch 204/11884 - Loss: 29.8031\n",
      "Processing batch 205/11884 - Loss: 30.0961\n",
      "Processing batch 206/11884 - Loss: 29.0089\n",
      "Processing batch 207/11884 - Loss: 30.9145\n",
      "Processing batch 208/11884 - Loss: 29.6285\n",
      "Processing batch 209/11884 - Loss: 30.7048\n",
      "Processing batch 210/11884 - Loss: 30.8578\n",
      "Processing batch 211/11884 - Loss: 30.1145\n",
      "Processing batch 212/11884 - Loss: 29.8905\n",
      "Processing batch 213/11884 - Loss: 29.8353\n",
      "Processing batch 214/11884 - Loss: 29.9300\n",
      "Processing batch 215/11884 - Loss: 29.7466\n",
      "Processing batch 216/11884 - Loss: 30.6624\n",
      "Processing batch 217/11884 - Loss: 30.6895\n",
      "Processing batch 218/11884 - Loss: 31.4906\n",
      "Processing batch 219/11884 - Loss: 29.7633\n",
      "Processing batch 220/11884 - Loss: 31.2505\n",
      "Processing batch 221/11884 - Loss: 29.1708\n",
      "Processing batch 222/11884 - Loss: 32.4287\n",
      "Processing batch 223/11884 - Loss: 30.6433\n",
      "Processing batch 224/11884 - Loss: 30.5805\n",
      "Processing batch 225/11884 - Loss: 30.6542\n",
      "Processing batch 226/11884 - Loss: 29.8965\n",
      "Processing batch 227/11884 - Loss: 30.1261\n",
      "Processing batch 228/11884 - Loss: 29.5359\n",
      "Processing batch 229/11884 - Loss: 29.5481\n",
      "Processing batch 230/11884 - Loss: 31.3081\n",
      "Processing batch 231/11884 - Loss: 31.6537\n",
      "Processing batch 232/11884 - Loss: 30.1069\n",
      "Processing batch 233/11884 - Loss: 30.7212\n",
      "Processing batch 234/11884 - Loss: 30.3968\n",
      "Processing batch 235/11884 - Loss: 30.3516\n",
      "Processing batch 236/11884 - Loss: 30.4940\n",
      "Processing batch 237/11884 - Loss: 30.6004\n",
      "Processing batch 238/11884 - Loss: 29.4295\n",
      "Processing batch 239/11884 - Loss: 30.4291\n",
      "Processing batch 240/11884 - Loss: 30.0427\n",
      "Processing batch 241/11884 - Loss: 28.4671\n",
      "Processing batch 242/11884 - Loss: 29.9308\n",
      "Processing batch 243/11884 - Loss: 30.9384\n",
      "Processing batch 244/11884 - Loss: 29.7432\n",
      "Processing batch 245/11884 - Loss: 29.0304\n",
      "Processing batch 246/11884 - Loss: 29.4673\n",
      "Processing batch 247/11884 - Loss: 30.8706\n",
      "Processing batch 248/11884 - Loss: 29.4560\n",
      "Processing batch 249/11884 - Loss: 32.1414\n",
      "Processing batch 250/11884 - Loss: 31.2320\n",
      "Processing batch 251/11884 - Loss: 29.8484\n",
      "Processing batch 252/11884 - Loss: 29.7545\n",
      "Processing batch 253/11884 - Loss: 30.7138\n",
      "Processing batch 254/11884 - Loss: 31.6107\n",
      "Processing batch 255/11884 - Loss: 31.3007\n",
      "Processing batch 256/11884 - Loss: 29.3965\n",
      "Processing batch 257/11884 - Loss: 30.7924\n",
      "Processing batch 258/11884 - Loss: 30.5835\n",
      "Processing batch 259/11884 - Loss: 29.4946\n",
      "Processing batch 260/11884 - Loss: 29.7795\n",
      "Processing batch 261/11884 - Loss: 30.7806\n",
      "Processing batch 262/11884 - Loss: 30.1439\n",
      "Processing batch 263/11884 - Loss: 30.2931\n",
      "Processing batch 264/11884 - Loss: 29.6004\n",
      "Processing batch 265/11884 - Loss: 30.2103\n",
      "Processing batch 266/11884 - Loss: 30.3334\n",
      "Processing batch 267/11884 - Loss: 29.7710\n",
      "Processing batch 268/11884 - Loss: 29.6886\n",
      "Processing batch 269/11884 - Loss: 30.8831\n",
      "Processing batch 270/11884 - Loss: 30.9279\n",
      "Processing batch 271/11884 - Loss: 30.6322\n",
      "Processing batch 272/11884 - Loss: 30.3626\n",
      "Processing batch 273/11884 - Loss: 31.0958\n",
      "Processing batch 274/11884 - Loss: 30.4344\n",
      "Processing batch 275/11884 - Loss: 29.6297\n",
      "Processing batch 276/11884 - Loss: 29.3788\n",
      "Processing batch 277/11884 - Loss: 29.6174\n",
      "Processing batch 278/11884 - Loss: 30.1681\n",
      "Processing batch 279/11884 - Loss: 31.0644\n",
      "Processing batch 280/11884 - Loss: 29.6526\n",
      "Processing batch 281/11884 - Loss: 30.2736\n",
      "Processing batch 282/11884 - Loss: 29.6106\n",
      "Processing batch 283/11884 - Loss: 30.5564\n",
      "Processing batch 284/11884 - Loss: 30.4209\n",
      "Processing batch 285/11884 - Loss: 31.6618\n",
      "Processing batch 286/11884 - Loss: 31.7078\n",
      "Processing batch 287/11884 - Loss: 31.1946\n",
      "Processing batch 288/11884 - Loss: 31.8884\n",
      "Processing batch 289/11884 - Loss: 30.3563\n",
      "Processing batch 290/11884 - Loss: 29.3247\n",
      "Processing batch 291/11884 - Loss: 29.3710\n",
      "Processing batch 292/11884 - Loss: 30.9841\n",
      "Processing batch 293/11884 - Loss: 30.7841\n",
      "Processing batch 294/11884 - Loss: 30.2959\n",
      "Processing batch 295/11884 - Loss: 29.9840\n",
      "Processing batch 296/11884 - Loss: 28.9911\n",
      "Processing batch 297/11884 - Loss: 31.4156\n",
      "Processing batch 298/11884 - Loss: 29.6568\n",
      "Processing batch 299/11884 - Loss: 30.8279\n",
      "Processing batch 300/11884 - Loss: 30.6061\n",
      "Processing batch 301/11884 - Loss: 30.8236\n",
      "Processing batch 302/11884 - Loss: 30.6673\n",
      "Processing batch 303/11884 - Loss: 29.2422\n",
      "Processing batch 304/11884 - Loss: 30.5340\n",
      "Processing batch 305/11884 - Loss: 31.0876\n",
      "Processing batch 306/11884 - Loss: 30.7758\n",
      "Processing batch 307/11884 - Loss: 30.6106\n",
      "Processing batch 308/11884 - Loss: 30.6495\n",
      "Processing batch 309/11884 - Loss: 30.1500\n",
      "Processing batch 310/11884 - Loss: 30.8129\n",
      "Processing batch 311/11884 - Loss: 29.6839\n",
      "Processing batch 312/11884 - Loss: 30.9434\n",
      "Processing batch 313/11884 - Loss: 30.9634\n",
      "Processing batch 314/11884 - Loss: 30.5178\n",
      "Processing batch 315/11884 - Loss: 30.8816\n",
      "Processing batch 316/11884 - Loss: 30.7487\n",
      "Processing batch 317/11884 - Loss: 30.0173\n",
      "Processing batch 318/11884 - Loss: 30.4013\n",
      "Processing batch 319/11884 - Loss: 30.5735\n",
      "Processing batch 320/11884 - Loss: 30.9561\n",
      "Processing batch 321/11884 - Loss: 30.8523\n",
      "Processing batch 322/11884 - Loss: 30.1747\n",
      "Processing batch 323/11884 - Loss: 29.7978\n",
      "Processing batch 324/11884 - Loss: 30.7687\n",
      "Processing batch 325/11884 - Loss: 30.4097\n",
      "Processing batch 326/11884 - Loss: 30.3940\n",
      "Processing batch 327/11884 - Loss: 31.4651\n",
      "Processing batch 328/11884 - Loss: 30.9466\n",
      "Processing batch 329/11884 - Loss: 30.9999\n",
      "Processing batch 330/11884 - Loss: 30.3222\n",
      "Processing batch 331/11884 - Loss: 30.3322\n",
      "Processing batch 332/11884 - Loss: 30.5778\n",
      "Processing batch 333/11884 - Loss: 29.9214\n",
      "Processing batch 334/11884 - Loss: 29.7885\n",
      "Processing batch 335/11884 - Loss: 31.1242\n",
      "Processing batch 336/11884 - Loss: 29.5251\n",
      "Processing batch 337/11884 - Loss: 30.9512\n",
      "Processing batch 338/11884 - Loss: 30.1555\n",
      "Processing batch 339/11884 - Loss: 31.2373\n",
      "Processing batch 340/11884 - Loss: 30.6455\n",
      "Processing batch 341/11884 - Loss: 31.0252\n",
      "Processing batch 342/11884 - Loss: 30.8127\n",
      "Processing batch 343/11884 - Loss: 29.7569\n",
      "Processing batch 344/11884 - Loss: 30.6936\n",
      "Processing batch 345/11884 - Loss: 31.3718\n",
      "Processing batch 346/11884 - Loss: 29.9810\n",
      "Processing batch 347/11884 - Loss: 30.2170\n",
      "Processing batch 348/11884 - Loss: 31.3019\n",
      "Processing batch 349/11884 - Loss: 29.9910\n",
      "Processing batch 350/11884 - Loss: 31.4458\n",
      "Processing batch 351/11884 - Loss: 30.1667\n",
      "Processing batch 352/11884 - Loss: 29.8292\n",
      "Processing batch 353/11884 - Loss: 30.0046\n",
      "Processing batch 354/11884 - Loss: 31.2584\n",
      "Processing batch 355/11884 - Loss: 28.9577\n",
      "Processing batch 356/11884 - Loss: 29.7906\n",
      "Processing batch 357/11884 - Loss: 31.1680\n",
      "Processing batch 358/11884 - Loss: 29.7627\n",
      "Processing batch 359/11884 - Loss: 28.2025\n",
      "Processing batch 360/11884 - Loss: 31.2003\n",
      "Processing batch 361/11884 - Loss: 31.4287\n",
      "Processing batch 362/11884 - Loss: 29.9819\n",
      "Processing batch 363/11884 - Loss: 30.8163\n",
      "Processing batch 364/11884 - Loss: 31.4286\n",
      "Processing batch 365/11884 - Loss: 30.2241\n",
      "Processing batch 366/11884 - Loss: 31.1341\n",
      "Processing batch 367/11884 - Loss: 29.9473\n",
      "Processing batch 368/11884 - Loss: 31.0473\n",
      "Processing batch 369/11884 - Loss: 29.8462\n",
      "Processing batch 370/11884 - Loss: 29.2676\n",
      "Processing batch 371/11884 - Loss: 31.4305\n",
      "Processing batch 372/11884 - Loss: 29.7338\n",
      "Processing batch 373/11884 - Loss: 30.0319\n",
      "Processing batch 374/11884 - Loss: 29.8728\n",
      "Processing batch 375/11884 - Loss: 30.2374\n",
      "Processing batch 376/11884 - Loss: 30.6343\n",
      "Processing batch 377/11884 - Loss: 30.8359\n",
      "Processing batch 378/11884 - Loss: 30.4296\n",
      "Processing batch 379/11884 - Loss: 30.4310\n",
      "Processing batch 380/11884 - Loss: 30.4598\n",
      "Processing batch 381/11884 - Loss: 31.6552\n",
      "Processing batch 382/11884 - Loss: 31.3598\n",
      "Processing batch 383/11884 - Loss: 30.0442\n",
      "Processing batch 384/11884 - Loss: 30.3101\n",
      "Processing batch 385/11884 - Loss: 30.2795\n",
      "Processing batch 386/11884 - Loss: 30.2362\n",
      "Processing batch 387/11884 - Loss: 29.4627\n",
      "Processing batch 388/11884 - Loss: 31.4409\n",
      "Processing batch 389/11884 - Loss: 30.9408\n",
      "Processing batch 390/11884 - Loss: 28.6193\n",
      "Processing batch 391/11884 - Loss: 29.1274\n",
      "Processing batch 392/11884 - Loss: 30.6609\n",
      "Processing batch 393/11884 - Loss: 29.4827\n",
      "Processing batch 394/11884 - Loss: 28.6315\n",
      "Processing batch 395/11884 - Loss: 31.1120\n",
      "Processing batch 396/11884 - Loss: 30.7040\n",
      "Processing batch 397/11884 - Loss: 31.0843\n",
      "Processing batch 398/11884 - Loss: 30.5361\n",
      "Processing batch 399/11884 - Loss: 30.5871\n",
      "Processing batch 400/11884 - Loss: 30.1922\n",
      "Processing batch 401/11884 - Loss: 30.4931\n",
      "Processing batch 402/11884 - Loss: 29.8313\n",
      "Processing batch 403/11884 - Loss: 30.6490\n",
      "Processing batch 404/11884 - Loss: 29.6776\n",
      "Processing batch 405/11884 - Loss: 28.8881\n",
      "Processing batch 406/11884 - Loss: 29.8266\n",
      "Processing batch 407/11884 - Loss: 30.2399\n",
      "Processing batch 408/11884 - Loss: 30.7547\n",
      "Processing batch 409/11884 - Loss: 30.6973\n",
      "Processing batch 410/11884 - Loss: 29.2532\n",
      "Processing batch 411/11884 - Loss: 30.4197\n",
      "Processing batch 412/11884 - Loss: 31.4551\n",
      "Processing batch 413/11884 - Loss: 30.3772\n",
      "Processing batch 414/11884 - Loss: 30.9841\n",
      "Processing batch 415/11884 - Loss: 31.3844\n",
      "Processing batch 416/11884 - Loss: 31.1952\n",
      "Processing batch 417/11884 - Loss: 30.8247\n",
      "Processing batch 418/11884 - Loss: 30.7348\n",
      "Processing batch 419/11884 - Loss: 28.5126\n",
      "Processing batch 420/11884 - Loss: 31.0168\n",
      "Processing batch 421/11884 - Loss: 29.6282\n",
      "Processing batch 422/11884 - Loss: 30.4945\n",
      "Processing batch 423/11884 - Loss: 31.4977\n",
      "Processing batch 424/11884 - Loss: 30.1928\n",
      "Processing batch 425/11884 - Loss: 30.0541\n",
      "Processing batch 426/11884 - Loss: 31.3009\n",
      "Processing batch 427/11884 - Loss: 30.1549\n",
      "Processing batch 428/11884 - Loss: 29.4538\n",
      "Processing batch 429/11884 - Loss: 29.9795\n",
      "Processing batch 430/11884 - Loss: 30.1090\n",
      "Processing batch 431/11884 - Loss: 30.9753\n",
      "Processing batch 432/11884 - Loss: 30.6724\n",
      "Processing batch 433/11884 - Loss: 29.7298\n",
      "Processing batch 434/11884 - Loss: 29.5210\n",
      "Processing batch 435/11884 - Loss: 31.1078\n",
      "Processing batch 436/11884 - Loss: 29.1055\n",
      "Processing batch 437/11884 - Loss: 31.2028\n",
      "Processing batch 438/11884 - Loss: 30.7468\n",
      "Processing batch 439/11884 - Loss: 30.3128\n",
      "Processing batch 440/11884 - Loss: 30.5686\n",
      "Processing batch 441/11884 - Loss: 31.2117\n",
      "Processing batch 442/11884 - Loss: 29.4145\n",
      "Processing batch 443/11884 - Loss: 29.2247\n",
      "Processing batch 444/11884 - Loss: 30.2431\n",
      "Processing batch 445/11884 - Loss: 28.7343\n",
      "Processing batch 446/11884 - Loss: 30.8975\n",
      "Processing batch 447/11884 - Loss: 29.4920\n",
      "Processing batch 448/11884 - Loss: 31.1519\n",
      "Processing batch 449/11884 - Loss: 29.2750\n",
      "Processing batch 450/11884 - Loss: 29.0928\n",
      "Processing batch 451/11884 - Loss: 30.3564\n",
      "Processing batch 452/11884 - Loss: 30.0368\n",
      "Processing batch 453/11884 - Loss: 29.7416\n",
      "Processing batch 454/11884 - Loss: 29.3221\n",
      "Processing batch 455/11884 - Loss: 30.0292\n",
      "Processing batch 456/11884 - Loss: 30.5432\n",
      "Processing batch 457/11884 - Loss: 30.9872\n",
      "Processing batch 458/11884 - Loss: 30.6624\n",
      "Processing batch 459/11884 - Loss: 30.2306\n",
      "Processing batch 460/11884 - Loss: 29.2276\n",
      "Processing batch 461/11884 - Loss: 30.9462\n",
      "Processing batch 462/11884 - Loss: 28.7735\n",
      "Processing batch 463/11884 - Loss: 30.7976\n",
      "Processing batch 464/11884 - Loss: 29.7594\n",
      "Processing batch 465/11884 - Loss: 30.0132\n",
      "Processing batch 466/11884 - Loss: 31.3944\n",
      "Processing batch 467/11884 - Loss: 30.0159\n",
      "Processing batch 468/11884 - Loss: 30.4829\n",
      "Processing batch 469/11884 - Loss: 29.1772\n",
      "Processing batch 470/11884 - Loss: 29.0447\n",
      "Processing batch 471/11884 - Loss: 31.0138\n",
      "Processing batch 472/11884 - Loss: 29.3373\n",
      "Processing batch 473/11884 - Loss: 29.5983\n",
      "Processing batch 474/11884 - Loss: 31.1518\n",
      "Processing batch 475/11884 - Loss: 29.9602\n",
      "Processing batch 476/11884 - Loss: 30.1900\n",
      "Processing batch 477/11884 - Loss: 29.4803\n",
      "Processing batch 478/11884 - Loss: 29.7178\n",
      "Processing batch 479/11884 - Loss: 28.8204\n",
      "Processing batch 480/11884 - Loss: 30.2460\n",
      "Processing batch 481/11884 - Loss: 30.0892\n",
      "Processing batch 482/11884 - Loss: 29.5040\n",
      "Processing batch 483/11884 - Loss: 30.8251\n",
      "Processing batch 484/11884 - Loss: 30.5540\n",
      "Processing batch 485/11884 - Loss: 30.6346\n",
      "Processing batch 486/11884 - Loss: 31.1670\n",
      "Processing batch 487/11884 - Loss: 29.6126\n",
      "Processing batch 488/11884 - Loss: 30.8290\n",
      "Processing batch 489/11884 - Loss: 29.3803\n",
      "Processing batch 490/11884 - Loss: 29.6405\n",
      "Processing batch 491/11884 - Loss: 29.9329\n",
      "Processing batch 492/11884 - Loss: 31.8419\n",
      "Processing batch 493/11884 - Loss: 31.3473\n",
      "Processing batch 494/11884 - Loss: 29.0764\n",
      "Processing batch 495/11884 - Loss: 30.2704\n",
      "Processing batch 496/11884 - Loss: 30.1314\n",
      "Processing batch 497/11884 - Loss: 30.4275\n",
      "Processing batch 498/11884 - Loss: 30.9221\n",
      "Processing batch 499/11884 - Loss: 30.2584\n",
      "Processing batch 500/11884 - Loss: 29.9050\n",
      "Processing batch 501/11884 - Loss: 32.1891\n",
      "Processing batch 502/11884 - Loss: 30.2637\n",
      "Processing batch 503/11884 - Loss: 31.5764\n",
      "Processing batch 504/11884 - Loss: 30.5647\n",
      "Processing batch 505/11884 - Loss: 30.7697\n",
      "Processing batch 506/11884 - Loss: 32.5143\n",
      "Processing batch 507/11884 - Loss: 30.9496\n",
      "Processing batch 508/11884 - Loss: 30.3040\n",
      "Processing batch 509/11884 - Loss: 30.4259\n",
      "Processing batch 510/11884 - Loss: 30.5042\n",
      "Processing batch 511/11884 - Loss: 29.8463\n",
      "Processing batch 512/11884 - Loss: 30.4365\n",
      "Processing batch 513/11884 - Loss: 31.8319\n",
      "Processing batch 514/11884 - Loss: 28.9180\n",
      "Processing batch 515/11884 - Loss: 31.1054\n",
      "Processing batch 516/11884 - Loss: 30.1501\n",
      "Processing batch 517/11884 - Loss: 31.7638\n",
      "Processing batch 518/11884 - Loss: 29.7418\n",
      "Processing batch 519/11884 - Loss: 30.5618\n",
      "Processing batch 520/11884 - Loss: 30.8647\n",
      "Processing batch 521/11884 - Loss: 30.0019\n",
      "Processing batch 522/11884 - Loss: 30.9182\n",
      "Processing batch 523/11884 - Loss: 30.4094\n",
      "Processing batch 524/11884 - Loss: 30.4866\n",
      "Processing batch 525/11884 - Loss: 30.5482\n",
      "Processing batch 526/11884 - Loss: 29.3535\n",
      "Processing batch 527/11884 - Loss: 29.9425\n",
      "Processing batch 528/11884 - Loss: 30.1159\n",
      "Processing batch 529/11884 - Loss: 30.4784\n",
      "Processing batch 530/11884 - Loss: 30.1813\n",
      "Processing batch 531/11884 - Loss: 29.6881\n",
      "Processing batch 532/11884 - Loss: 30.6164\n",
      "Processing batch 533/11884 - Loss: 29.7465\n",
      "Processing batch 534/11884 - Loss: 30.9167\n",
      "Processing batch 535/11884 - Loss: 29.3851\n",
      "Processing batch 536/11884 - Loss: 28.7501\n",
      "Processing batch 537/11884 - Loss: 30.9038\n",
      "Processing batch 538/11884 - Loss: 30.0529\n",
      "Processing batch 539/11884 - Loss: 29.9862\n",
      "Processing batch 540/11884 - Loss: 30.8937\n",
      "Processing batch 541/11884 - Loss: 29.8279\n",
      "Processing batch 542/11884 - Loss: 30.1688\n",
      "Processing batch 543/11884 - Loss: 30.3792\n",
      "Processing batch 544/11884 - Loss: 31.6516\n",
      "Processing batch 545/11884 - Loss: 31.6861\n",
      "Processing batch 546/11884 - Loss: 29.9423\n",
      "Processing batch 547/11884 - Loss: 30.2588\n",
      "Processing batch 548/11884 - Loss: 30.0490\n",
      "Processing batch 549/11884 - Loss: 30.9769\n",
      "Processing batch 550/11884 - Loss: 29.8882\n",
      "Processing batch 551/11884 - Loss: 30.9378\n",
      "Processing batch 552/11884 - Loss: 30.1506\n",
      "Processing batch 553/11884 - Loss: 29.9758\n",
      "Processing batch 554/11884 - Loss: 29.8732\n",
      "Processing batch 555/11884 - Loss: 30.4306\n",
      "Processing batch 556/11884 - Loss: 30.2371\n",
      "Processing batch 557/11884 - Loss: 30.9085\n",
      "Processing batch 558/11884 - Loss: 30.0382\n",
      "Processing batch 559/11884 - Loss: 31.2941\n",
      "Processing batch 560/11884 - Loss: 30.0760\n",
      "Processing batch 561/11884 - Loss: 29.3511\n",
      "Processing batch 562/11884 - Loss: 30.0086\n",
      "Processing batch 563/11884 - Loss: 29.8127\n",
      "Processing batch 564/11884 - Loss: 29.3182\n",
      "Processing batch 565/11884 - Loss: 29.8688\n",
      "Processing batch 566/11884 - Loss: 30.8691\n",
      "Processing batch 567/11884 - Loss: 30.0771\n",
      "Processing batch 568/11884 - Loss: 29.3163\n",
      "Processing batch 569/11884 - Loss: 30.2553\n",
      "Processing batch 570/11884 - Loss: 29.3619\n",
      "Processing batch 571/11884 - Loss: 29.7293\n",
      "Processing batch 572/11884 - Loss: 29.2506\n",
      "Processing batch 573/11884 - Loss: 30.6100\n",
      "Processing batch 574/11884 - Loss: 29.8137\n",
      "Processing batch 575/11884 - Loss: 29.8887\n",
      "Processing batch 576/11884 - Loss: 29.1762\n",
      "Processing batch 577/11884 - Loss: 30.9117\n",
      "Processing batch 578/11884 - Loss: 30.7393\n",
      "Processing batch 579/11884 - Loss: 29.6068\n",
      "Processing batch 580/11884 - Loss: 29.8781\n",
      "Processing batch 581/11884 - Loss: 31.3731\n",
      "Processing batch 582/11884 - Loss: 29.8631\n",
      "Processing batch 583/11884 - Loss: 29.3049\n",
      "Processing batch 584/11884 - Loss: 31.0534\n",
      "Processing batch 585/11884 - Loss: 30.7770\n",
      "Processing batch 586/11884 - Loss: 31.1383\n",
      "Processing batch 587/11884 - Loss: 30.2581\n",
      "Processing batch 588/11884 - Loss: 30.9131\n",
      "Processing batch 589/11884 - Loss: 30.6748\n",
      "Processing batch 590/11884 - Loss: 31.0658\n",
      "Processing batch 591/11884 - Loss: 30.5599\n",
      "Processing batch 592/11884 - Loss: 29.6151\n",
      "Processing batch 593/11884 - Loss: 31.9738\n",
      "Processing batch 594/11884 - Loss: 30.1998\n",
      "Processing batch 595/11884 - Loss: 28.2660\n",
      "Processing batch 596/11884 - Loss: 30.0305\n",
      "Processing batch 597/11884 - Loss: 30.6428\n",
      "Processing batch 598/11884 - Loss: 30.9063\n",
      "Processing batch 599/11884 - Loss: 31.0145\n",
      "Processing batch 600/11884 - Loss: 30.1395\n",
      "Processing batch 601/11884 - Loss: 28.8450\n",
      "Processing batch 602/11884 - Loss: 30.6047\n",
      "Processing batch 603/11884 - Loss: 31.0922\n",
      "Processing batch 604/11884 - Loss: 30.2442\n",
      "Processing batch 605/11884 - Loss: 31.5435\n",
      "Processing batch 606/11884 - Loss: 30.8368\n",
      "Processing batch 607/11884 - Loss: 31.4528\n",
      "Processing batch 608/11884 - Loss: 30.1087\n",
      "Processing batch 609/11884 - Loss: 30.1907\n",
      "Processing batch 610/11884 - Loss: 31.9957\n",
      "Processing batch 611/11884 - Loss: 30.8034\n",
      "Processing batch 612/11884 - Loss: 31.0274\n",
      "Processing batch 613/11884 - Loss: 29.7748\n",
      "Processing batch 614/11884 - Loss: 29.1692\n",
      "Processing batch 615/11884 - Loss: 29.8316\n",
      "Processing batch 616/11884 - Loss: 29.8617\n",
      "Processing batch 617/11884 - Loss: 30.6569\n",
      "Processing batch 618/11884 - Loss: 29.6477\n",
      "Processing batch 619/11884 - Loss: 29.4831\n",
      "Processing batch 620/11884 - Loss: 29.5476\n",
      "Processing batch 621/11884 - Loss: 31.0874\n",
      "Processing batch 622/11884 - Loss: 30.4146\n",
      "Processing batch 623/11884 - Loss: 29.9574\n",
      "Processing batch 624/11884 - Loss: 29.4933\n",
      "Processing batch 625/11884 - Loss: 29.4926\n",
      "Processing batch 626/11884 - Loss: 30.5011\n",
      "Processing batch 627/11884 - Loss: 31.6728\n",
      "Processing batch 628/11884 - Loss: 29.2068\n",
      "Processing batch 629/11884 - Loss: 31.0374\n",
      "Processing batch 630/11884 - Loss: 29.2767\n",
      "Processing batch 631/11884 - Loss: 30.8156\n",
      "Processing batch 632/11884 - Loss: 31.0685\n",
      "Processing batch 633/11884 - Loss: 29.1878\n",
      "Processing batch 634/11884 - Loss: 30.4267\n",
      "Processing batch 635/11884 - Loss: 31.8363\n",
      "Processing batch 636/11884 - Loss: 29.9965\n",
      "Processing batch 637/11884 - Loss: 29.5588\n",
      "Processing batch 638/11884 - Loss: 30.9400\n",
      "Processing batch 639/11884 - Loss: 30.8677\n",
      "Processing batch 640/11884 - Loss: 30.9082\n",
      "Processing batch 641/11884 - Loss: 30.3711\n",
      "Processing batch 642/11884 - Loss: 30.2900\n",
      "Processing batch 643/11884 - Loss: 30.3019\n",
      "Processing batch 644/11884 - Loss: 29.8919\n",
      "Processing batch 645/11884 - Loss: 30.1532\n",
      "Processing batch 646/11884 - Loss: 29.9127\n",
      "Processing batch 647/11884 - Loss: 30.1778\n",
      "Processing batch 648/11884 - Loss: 29.3515\n",
      "Processing batch 649/11884 - Loss: 30.2367\n",
      "Processing batch 650/11884 - Loss: 30.6288\n",
      "Processing batch 651/11884 - Loss: 29.4872\n",
      "Processing batch 652/11884 - Loss: 31.0085\n",
      "Processing batch 653/11884 - Loss: 30.9814\n",
      "Processing batch 654/11884 - Loss: 29.2219\n",
      "Processing batch 655/11884 - Loss: 30.6039\n",
      "Processing batch 656/11884 - Loss: 29.8159\n",
      "Processing batch 657/11884 - Loss: 28.6748\n",
      "Processing batch 658/11884 - Loss: 30.3385\n",
      "Processing batch 659/11884 - Loss: 30.3768\n",
      "Processing batch 660/11884 - Loss: 28.9362\n",
      "Processing batch 661/11884 - Loss: 29.4246\n",
      "Processing batch 662/11884 - Loss: 31.0581\n",
      "Processing batch 663/11884 - Loss: 30.1456\n",
      "Processing batch 664/11884 - Loss: 30.4600\n",
      "Processing batch 665/11884 - Loss: 30.1995\n",
      "Processing batch 666/11884 - Loss: 29.1307\n",
      "Processing batch 667/11884 - Loss: 30.7467\n",
      "Processing batch 668/11884 - Loss: 30.0152\n",
      "Processing batch 669/11884 - Loss: 30.9767\n",
      "Processing batch 670/11884 - Loss: 29.6430\n",
      "Processing batch 671/11884 - Loss: 30.5327\n",
      "Processing batch 672/11884 - Loss: 30.4628\n",
      "Processing batch 673/11884 - Loss: 29.4190\n",
      "Processing batch 674/11884 - Loss: 30.9527\n",
      "Processing batch 675/11884 - Loss: 30.4848\n",
      "Processing batch 676/11884 - Loss: 30.5613\n",
      "Processing batch 677/11884 - Loss: 30.3790\n",
      "Processing batch 678/11884 - Loss: 29.9421\n",
      "Processing batch 679/11884 - Loss: 31.2067\n",
      "Processing batch 680/11884 - Loss: 30.1514\n",
      "Processing batch 681/11884 - Loss: 31.1740\n",
      "Processing batch 682/11884 - Loss: 29.9801\n",
      "Processing batch 683/11884 - Loss: 31.4812\n",
      "Processing batch 684/11884 - Loss: 30.1575\n",
      "Processing batch 685/11884 - Loss: 30.3623\n",
      "Processing batch 686/11884 - Loss: 30.3079\n",
      "Processing batch 687/11884 - Loss: 30.5990\n",
      "Processing batch 688/11884 - Loss: 30.8575\n",
      "Processing batch 689/11884 - Loss: 30.5971\n",
      "Processing batch 690/11884 - Loss: 30.3982\n",
      "Processing batch 691/11884 - Loss: 31.0324\n",
      "Processing batch 692/11884 - Loss: 29.7397\n",
      "Processing batch 693/11884 - Loss: 29.7481\n",
      "Processing batch 694/11884 - Loss: 32.0008\n",
      "Processing batch 695/11884 - Loss: 30.7700\n",
      "Processing batch 696/11884 - Loss: 31.2007\n",
      "Processing batch 697/11884 - Loss: 30.1925\n",
      "Processing batch 698/11884 - Loss: 30.3226\n",
      "Processing batch 699/11884 - Loss: 30.1740\n",
      "Processing batch 700/11884 - Loss: 30.1301\n",
      "Processing batch 701/11884 - Loss: 28.4305\n",
      "Processing batch 702/11884 - Loss: 30.9055\n",
      "Processing batch 703/11884 - Loss: 30.3686\n",
      "Processing batch 704/11884 - Loss: 29.7105\n",
      "Processing batch 705/11884 - Loss: 29.6358\n",
      "Processing batch 706/11884 - Loss: 29.7575\n",
      "Processing batch 707/11884 - Loss: 29.3283\n",
      "Processing batch 708/11884 - Loss: 29.7539\n",
      "Processing batch 709/11884 - Loss: 29.7976\n",
      "Processing batch 710/11884 - Loss: 30.4782\n",
      "Processing batch 711/11884 - Loss: 29.7703\n",
      "Processing batch 712/11884 - Loss: 29.9690\n",
      "Processing batch 713/11884 - Loss: 30.6317\n",
      "Processing batch 714/11884 - Loss: 29.0412\n",
      "Processing batch 715/11884 - Loss: 30.5261\n",
      "Processing batch 716/11884 - Loss: 29.8235\n",
      "Processing batch 717/11884 - Loss: 30.6764\n",
      "Processing batch 718/11884 - Loss: 31.1005\n",
      "Processing batch 719/11884 - Loss: 30.5750\n",
      "Processing batch 720/11884 - Loss: 31.6930\n",
      "Processing batch 721/11884 - Loss: 30.4608\n",
      "Processing batch 722/11884 - Loss: 29.8759\n",
      "Processing batch 723/11884 - Loss: 30.0180\n",
      "Processing batch 724/11884 - Loss: 30.4564\n",
      "Processing batch 725/11884 - Loss: 30.0323\n",
      "Processing batch 726/11884 - Loss: 29.7966\n",
      "Processing batch 727/11884 - Loss: 30.5317\n",
      "Processing batch 728/11884 - Loss: 29.2081\n",
      "Processing batch 729/11884 - Loss: 30.5210\n",
      "Processing batch 730/11884 - Loss: 29.9103\n",
      "Processing batch 731/11884 - Loss: 31.0746\n",
      "Processing batch 732/11884 - Loss: 30.4200\n",
      "Processing batch 733/11884 - Loss: 30.2157\n",
      "Processing batch 734/11884 - Loss: 31.0739\n",
      "Processing batch 735/11884 - Loss: 29.9673\n",
      "Processing batch 736/11884 - Loss: 29.2805\n",
      "Processing batch 737/11884 - Loss: 29.5473\n",
      "Processing batch 738/11884 - Loss: 30.6052\n",
      "Processing batch 739/11884 - Loss: 29.8922\n",
      "Processing batch 740/11884 - Loss: 31.0506\n",
      "Processing batch 741/11884 - Loss: 30.7999\n",
      "Processing batch 742/11884 - Loss: 31.8364\n",
      "Processing batch 743/11884 - Loss: 29.9520\n",
      "Processing batch 744/11884 - Loss: 30.2246\n",
      "Processing batch 745/11884 - Loss: 30.9051\n",
      "Processing batch 746/11884 - Loss: 30.5854\n",
      "Processing batch 747/11884 - Loss: 30.6298\n",
      "Processing batch 748/11884 - Loss: 30.4784\n",
      "Processing batch 749/11884 - Loss: 29.2235\n",
      "Processing batch 750/11884 - Loss: 30.8689\n",
      "Processing batch 751/11884 - Loss: 30.4094\n",
      "Processing batch 752/11884 - Loss: 29.4355\n",
      "Processing batch 753/11884 - Loss: 29.9028\n",
      "Processing batch 754/11884 - Loss: 29.8139\n",
      "Processing batch 755/11884 - Loss: 29.1028\n",
      "Processing batch 756/11884 - Loss: 30.6870\n",
      "Processing batch 757/11884 - Loss: 30.1379\n",
      "Processing batch 758/11884 - Loss: 30.6916\n",
      "Processing batch 759/11884 - Loss: 28.7660\n",
      "Processing batch 760/11884 - Loss: 29.8107\n",
      "Processing batch 761/11884 - Loss: 31.0504\n",
      "Processing batch 762/11884 - Loss: 29.4631\n",
      "Processing batch 763/11884 - Loss: 31.7010\n",
      "Processing batch 764/11884 - Loss: 29.8830\n",
      "Processing batch 765/11884 - Loss: 28.9978\n",
      "Processing batch 766/11884 - Loss: 30.9324\n",
      "Processing batch 767/11884 - Loss: 30.4963\n",
      "Processing batch 768/11884 - Loss: 31.6344\n",
      "Processing batch 769/11884 - Loss: 30.4486\n",
      "Processing batch 770/11884 - Loss: 29.2917\n",
      "Processing batch 771/11884 - Loss: 29.7598\n",
      "Processing batch 772/11884 - Loss: 30.2466\n",
      "Processing batch 773/11884 - Loss: 30.0874\n",
      "Processing batch 774/11884 - Loss: 29.8842\n",
      "Processing batch 775/11884 - Loss: 30.8394\n",
      "Processing batch 776/11884 - Loss: 30.8219\n",
      "Processing batch 777/11884 - Loss: 29.5218\n",
      "Processing batch 778/11884 - Loss: 30.1667\n",
      "Processing batch 779/11884 - Loss: 30.6179\n",
      "Processing batch 780/11884 - Loss: 29.6031\n",
      "Processing batch 781/11884 - Loss: 29.9480\n",
      "Processing batch 782/11884 - Loss: 29.9382\n",
      "Processing batch 783/11884 - Loss: 29.8577\n",
      "Processing batch 784/11884 - Loss: 31.0785\n",
      "Processing batch 785/11884 - Loss: 30.5512\n",
      "Processing batch 786/11884 - Loss: 30.2197\n",
      "Processing batch 787/11884 - Loss: 30.5394\n",
      "Processing batch 788/11884 - Loss: 30.9590\n",
      "Processing batch 789/11884 - Loss: 29.6481\n",
      "Processing batch 790/11884 - Loss: 30.8131\n",
      "Processing batch 791/11884 - Loss: 29.7937\n",
      "Processing batch 792/11884 - Loss: 29.7570\n",
      "Processing batch 793/11884 - Loss: 29.2032\n",
      "Processing batch 794/11884 - Loss: 29.4515\n",
      "Processing batch 795/11884 - Loss: 30.4739\n",
      "Processing batch 796/11884 - Loss: 30.4064\n",
      "Processing batch 797/11884 - Loss: 29.1315\n",
      "Processing batch 798/11884 - Loss: 31.9302\n",
      "Processing batch 799/11884 - Loss: 31.1212\n",
      "Processing batch 800/11884 - Loss: 30.7859\n",
      "Processing batch 801/11884 - Loss: 29.4375\n",
      "Processing batch 802/11884 - Loss: 29.8880\n",
      "Processing batch 803/11884 - Loss: 29.5554\n",
      "Processing batch 804/11884 - Loss: 30.8716\n",
      "Processing batch 805/11884 - Loss: 31.0154\n",
      "Processing batch 806/11884 - Loss: 30.9629\n",
      "Processing batch 807/11884 - Loss: 29.9712\n",
      "Processing batch 808/11884 - Loss: 30.5429\n",
      "Processing batch 809/11884 - Loss: 30.3751\n",
      "Processing batch 810/11884 - Loss: 30.1861\n",
      "Processing batch 811/11884 - Loss: 30.4674\n",
      "Processing batch 812/11884 - Loss: 30.5315\n",
      "Processing batch 813/11884 - Loss: 28.8392\n",
      "Processing batch 814/11884 - Loss: 30.9530\n",
      "Processing batch 815/11884 - Loss: 31.4274\n",
      "Processing batch 816/11884 - Loss: 31.0053\n",
      "Processing batch 817/11884 - Loss: 29.8848\n",
      "Processing batch 818/11884 - Loss: 30.1622\n",
      "Processing batch 819/11884 - Loss: 30.9300\n",
      "Processing batch 820/11884 - Loss: 31.4622\n",
      "Processing batch 821/11884 - Loss: 31.2994\n",
      "Processing batch 822/11884 - Loss: 29.8253\n",
      "Processing batch 823/11884 - Loss: 31.0233\n",
      "Processing batch 824/11884 - Loss: 29.6104\n",
      "Processing batch 825/11884 - Loss: 30.6822\n",
      "Processing batch 826/11884 - Loss: 31.3962\n",
      "Processing batch 827/11884 - Loss: 30.2332\n",
      "Processing batch 828/11884 - Loss: 30.0683\n",
      "Processing batch 829/11884 - Loss: 29.8810\n",
      "Processing batch 830/11884 - Loss: 30.4711\n",
      "Processing batch 831/11884 - Loss: 29.9861\n",
      "Processing batch 832/11884 - Loss: 31.3076\n",
      "Processing batch 833/11884 - Loss: 28.7991\n",
      "Processing batch 834/11884 - Loss: 29.9171\n",
      "Processing batch 835/11884 - Loss: 29.8956\n",
      "Processing batch 836/11884 - Loss: 29.5904\n",
      "Processing batch 837/11884 - Loss: 31.2732\n",
      "Processing batch 838/11884 - Loss: 29.6446\n",
      "Processing batch 839/11884 - Loss: 29.4290\n",
      "Processing batch 840/11884 - Loss: 30.0779\n",
      "Processing batch 841/11884 - Loss: 31.0881\n",
      "Processing batch 842/11884 - Loss: 32.5542\n",
      "Processing batch 843/11884 - Loss: 30.3721\n",
      "Processing batch 844/11884 - Loss: 30.7070\n",
      "Processing batch 845/11884 - Loss: 30.0427\n",
      "Processing batch 846/11884 - Loss: 29.7055\n",
      "Processing batch 847/11884 - Loss: 30.3444\n",
      "Processing batch 848/11884 - Loss: 30.4136\n",
      "Processing batch 849/11884 - Loss: 30.8639\n",
      "Processing batch 850/11884 - Loss: 29.3333\n",
      "Processing batch 851/11884 - Loss: 31.2015\n",
      "Processing batch 852/11884 - Loss: 29.3069\n",
      "Processing batch 853/11884 - Loss: 30.5751\n",
      "Processing batch 854/11884 - Loss: 30.4344\n",
      "Processing batch 855/11884 - Loss: 29.8094\n",
      "Processing batch 856/11884 - Loss: 31.2225\n",
      "Processing batch 857/11884 - Loss: 30.8849\n",
      "Processing batch 858/11884 - Loss: 30.5704\n",
      "Processing batch 859/11884 - Loss: 30.4515\n",
      "Processing batch 860/11884 - Loss: 30.4789\n",
      "Processing batch 861/11884 - Loss: 29.9869\n",
      "Processing batch 862/11884 - Loss: 30.4014\n",
      "Processing batch 863/11884 - Loss: 30.7838\n",
      "Processing batch 864/11884 - Loss: 31.2692\n",
      "Processing batch 865/11884 - Loss: 29.4977\n",
      "Processing batch 866/11884 - Loss: 31.1721\n",
      "Processing batch 867/11884 - Loss: 30.0942\n",
      "Processing batch 868/11884 - Loss: 31.2684\n",
      "Processing batch 869/11884 - Loss: 30.2636\n",
      "Processing batch 870/11884 - Loss: 30.4999\n",
      "Processing batch 871/11884 - Loss: 30.9762\n",
      "Processing batch 872/11884 - Loss: 30.6987\n",
      "Processing batch 873/11884 - Loss: 29.4830\n",
      "Processing batch 874/11884 - Loss: 30.6549\n",
      "Processing batch 875/11884 - Loss: 29.6053\n",
      "Processing batch 876/11884 - Loss: 30.0588\n",
      "Processing batch 877/11884 - Loss: 30.3760\n",
      "Processing batch 878/11884 - Loss: 30.8532\n",
      "Processing batch 879/11884 - Loss: 30.8625\n",
      "Processing batch 880/11884 - Loss: 28.7010\n",
      "Processing batch 881/11884 - Loss: 28.8440\n",
      "Processing batch 882/11884 - Loss: 30.5522\n",
      "Processing batch 883/11884 - Loss: 29.8031\n",
      "Processing batch 884/11884 - Loss: 29.8742\n",
      "Processing batch 885/11884 - Loss: 31.6620\n",
      "Processing batch 886/11884 - Loss: 30.0447\n",
      "Processing batch 887/11884 - Loss: 30.6613\n",
      "Processing batch 888/11884 - Loss: 29.7506\n",
      "Processing batch 889/11884 - Loss: 30.2329\n",
      "Processing batch 890/11884 - Loss: 29.5789\n",
      "Processing batch 891/11884 - Loss: 32.2092\n",
      "Processing batch 892/11884 - Loss: 29.6690\n",
      "Processing batch 893/11884 - Loss: 30.2950\n",
      "Processing batch 894/11884 - Loss: 29.5845\n",
      "Processing batch 895/11884 - Loss: 30.0285\n",
      "Processing batch 896/11884 - Loss: 30.4466\n",
      "Processing batch 897/11884 - Loss: 31.2160\n",
      "Processing batch 898/11884 - Loss: 30.3335\n",
      "Processing batch 899/11884 - Loss: 30.5813\n",
      "Processing batch 900/11884 - Loss: 29.6204\n",
      "Processing batch 901/11884 - Loss: 31.0616\n",
      "Processing batch 902/11884 - Loss: 29.5524\n",
      "Processing batch 903/11884 - Loss: 31.3840\n",
      "Processing batch 904/11884 - Loss: 29.4764\n",
      "Processing batch 905/11884 - Loss: 28.9960\n",
      "Processing batch 906/11884 - Loss: 30.9428\n",
      "Processing batch 907/11884 - Loss: 29.2075\n",
      "Processing batch 908/11884 - Loss: 29.7552\n",
      "Processing batch 909/11884 - Loss: 30.8566\n",
      "Processing batch 910/11884 - Loss: 29.3433\n",
      "Processing batch 911/11884 - Loss: 28.4433\n",
      "Processing batch 912/11884 - Loss: 29.9919\n",
      "Processing batch 913/11884 - Loss: 31.0238\n",
      "Processing batch 914/11884 - Loss: 30.8173\n",
      "Processing batch 915/11884 - Loss: 31.3510\n",
      "Processing batch 916/11884 - Loss: 29.4960\n",
      "Processing batch 917/11884 - Loss: 29.5963\n",
      "Processing batch 918/11884 - Loss: 31.0917\n",
      "Processing batch 919/11884 - Loss: 30.0324\n",
      "Processing batch 920/11884 - Loss: 29.6863\n",
      "Processing batch 921/11884 - Loss: 30.0451\n",
      "Processing batch 922/11884 - Loss: 29.9374\n",
      "Processing batch 923/11884 - Loss: 30.3967\n",
      "Processing batch 924/11884 - Loss: 30.5814\n",
      "Processing batch 925/11884 - Loss: 30.6469\n",
      "Processing batch 926/11884 - Loss: 30.9583\n",
      "Processing batch 927/11884 - Loss: 29.9216\n",
      "Processing batch 928/11884 - Loss: 29.6979\n",
      "Processing batch 929/11884 - Loss: 29.7434\n",
      "Processing batch 930/11884 - Loss: 30.4591\n",
      "Processing batch 931/11884 - Loss: 30.1274\n",
      "Processing batch 932/11884 - Loss: 30.5668\n",
      "Processing batch 933/11884 - Loss: 30.0531\n",
      "Processing batch 934/11884 - Loss: 30.6561\n",
      "Processing batch 935/11884 - Loss: 30.6690\n",
      "Processing batch 936/11884 - Loss: 29.7749\n",
      "Processing batch 937/11884 - Loss: 29.0679\n",
      "Processing batch 938/11884 - Loss: 30.0412\n",
      "Processing batch 939/11884 - Loss: 30.6483\n",
      "Processing batch 940/11884 - Loss: 31.1477\n",
      "Processing batch 941/11884 - Loss: 28.9657\n",
      "Processing batch 942/11884 - Loss: 30.2489\n",
      "Processing batch 943/11884 - Loss: 28.9339\n",
      "Processing batch 944/11884 - Loss: 28.9429\n",
      "Processing batch 945/11884 - Loss: 30.3531\n",
      "Processing batch 946/11884 - Loss: 30.1726\n",
      "Processing batch 947/11884 - Loss: 31.4504\n",
      "Processing batch 948/11884 - Loss: 31.0984\n",
      "Processing batch 949/11884 - Loss: 31.7518\n",
      "Processing batch 950/11884 - Loss: 29.8120\n",
      "Processing batch 951/11884 - Loss: 30.9893\n",
      "Processing batch 952/11884 - Loss: 31.2110\n",
      "Processing batch 953/11884 - Loss: 29.2471\n",
      "Processing batch 954/11884 - Loss: 30.3746\n",
      "Processing batch 955/11884 - Loss: 28.4582\n",
      "Processing batch 956/11884 - Loss: 29.0545\n",
      "Processing batch 957/11884 - Loss: 31.7260\n",
      "Processing batch 958/11884 - Loss: 29.5750\n",
      "Processing batch 959/11884 - Loss: 30.1349\n",
      "Processing batch 960/11884 - Loss: 29.7319\n",
      "Processing batch 961/11884 - Loss: 29.8767\n",
      "Processing batch 962/11884 - Loss: 30.0715\n",
      "Processing batch 963/11884 - Loss: 30.9956\n",
      "Processing batch 964/11884 - Loss: 31.6226\n",
      "Processing batch 965/11884 - Loss: 30.6134\n",
      "Processing batch 966/11884 - Loss: 30.0321\n",
      "Processing batch 967/11884 - Loss: 29.5515\n",
      "Processing batch 968/11884 - Loss: 30.8610\n",
      "Processing batch 969/11884 - Loss: 29.1758\n",
      "Processing batch 970/11884 - Loss: 29.4619\n",
      "Processing batch 971/11884 - Loss: 29.9982\n",
      "Processing batch 972/11884 - Loss: 30.5553\n",
      "Processing batch 973/11884 - Loss: 29.2911\n",
      "Processing batch 974/11884 - Loss: 30.5315\n",
      "Processing batch 975/11884 - Loss: 29.4480\n",
      "Processing batch 976/11884 - Loss: 30.7387\n",
      "Processing batch 977/11884 - Loss: 30.6449\n",
      "Processing batch 978/11884 - Loss: 31.2632\n",
      "Processing batch 979/11884 - Loss: 29.7039\n",
      "Processing batch 980/11884 - Loss: 29.6520\n",
      "Processing batch 981/11884 - Loss: 30.7350\n",
      "Processing batch 982/11884 - Loss: 30.5407\n",
      "Processing batch 983/11884 - Loss: 29.4290\n",
      "Processing batch 984/11884 - Loss: 28.8757\n",
      "Processing batch 985/11884 - Loss: 29.8502\n",
      "Processing batch 986/11884 - Loss: 30.6197\n",
      "Processing batch 987/11884 - Loss: 29.9411\n",
      "Processing batch 988/11884 - Loss: 28.0743\n",
      "Processing batch 989/11884 - Loss: 30.8856\n",
      "Processing batch 990/11884 - Loss: 31.1710\n",
      "Processing batch 991/11884 - Loss: 29.2949\n",
      "Processing batch 992/11884 - Loss: 31.0289\n",
      "Processing batch 993/11884 - Loss: 30.5492\n",
      "Processing batch 994/11884 - Loss: 31.0358\n",
      "Processing batch 995/11884 - Loss: 29.6054\n",
      "Processing batch 996/11884 - Loss: 31.2545\n",
      "Processing batch 997/11884 - Loss: 29.4601\n",
      "Processing batch 998/11884 - Loss: 29.0229\n",
      "Processing batch 999/11884 - Loss: 30.3564\n",
      "Processing batch 1000/11884 - Loss: 29.8772\n",
      "Processing batch 1001/11884 - Loss: 30.0159\n",
      "Processing batch 1002/11884 - Loss: 29.6084\n",
      "Processing batch 1003/11884 - Loss: 31.7168\n",
      "Processing batch 1004/11884 - Loss: 30.4880\n",
      "Processing batch 1005/11884 - Loss: 29.6670\n",
      "Processing batch 1006/11884 - Loss: 30.3915\n",
      "Processing batch 1007/11884 - Loss: 31.3830\n",
      "Processing batch 1008/11884 - Loss: 31.6519\n",
      "Processing batch 1009/11884 - Loss: 30.1555\n",
      "Processing batch 1010/11884 - Loss: 29.8963\n",
      "Processing batch 1011/11884 - Loss: 29.4027\n",
      "Processing batch 1012/11884 - Loss: 30.4041\n",
      "Processing batch 1013/11884 - Loss: 29.7162\n",
      "Processing batch 1014/11884 - Loss: 30.1512\n",
      "Processing batch 1015/11884 - Loss: 29.6944\n",
      "Processing batch 1016/11884 - Loss: 30.2794\n",
      "Processing batch 1017/11884 - Loss: 31.6677\n",
      "Processing batch 1018/11884 - Loss: 29.0179\n",
      "Processing batch 1019/11884 - Loss: 29.8215\n",
      "Processing batch 1020/11884 - Loss: 29.9585\n",
      "Processing batch 1021/11884 - Loss: 29.6597\n",
      "Processing batch 1022/11884 - Loss: 30.7161\n",
      "Processing batch 1023/11884 - Loss: 30.5348\n",
      "Processing batch 1024/11884 - Loss: 30.4275\n",
      "Processing batch 1025/11884 - Loss: 30.4740\n",
      "Processing batch 1026/11884 - Loss: 30.8333\n",
      "Processing batch 1027/11884 - Loss: 29.6025\n",
      "Processing batch 1028/11884 - Loss: 30.2272\n",
      "Processing batch 1029/11884 - Loss: 29.8005\n",
      "Processing batch 1030/11884 - Loss: 29.5813\n",
      "Processing batch 1031/11884 - Loss: 30.3100\n",
      "Processing batch 1032/11884 - Loss: 30.8381\n",
      "Processing batch 1033/11884 - Loss: 30.8865\n",
      "Processing batch 1034/11884 - Loss: 31.1334\n",
      "Processing batch 1035/11884 - Loss: 30.1454\n",
      "Processing batch 1036/11884 - Loss: 30.3641\n",
      "Processing batch 1037/11884 - Loss: 30.6545\n",
      "Processing batch 1038/11884 - Loss: 30.1963\n",
      "Processing batch 1039/11884 - Loss: 30.8922\n",
      "Processing batch 1040/11884 - Loss: 29.7501\n",
      "Processing batch 1041/11884 - Loss: 30.2720\n",
      "Processing batch 1042/11884 - Loss: 29.2579\n",
      "Processing batch 1043/11884 - Loss: 31.5848\n",
      "Processing batch 1044/11884 - Loss: 30.4421\n",
      "Processing batch 1045/11884 - Loss: 29.2549\n",
      "Processing batch 1046/11884 - Loss: 30.8849\n",
      "Processing batch 1047/11884 - Loss: 29.7966\n",
      "Processing batch 1048/11884 - Loss: 29.3272\n",
      "Processing batch 1049/11884 - Loss: 31.0759\n",
      "Processing batch 1050/11884 - Loss: 30.1651\n",
      "Processing batch 1051/11884 - Loss: 31.2557\n",
      "Processing batch 1052/11884 - Loss: 31.3843\n",
      "Processing batch 1053/11884 - Loss: 30.8544\n",
      "Processing batch 1054/11884 - Loss: 30.6929\n",
      "Processing batch 1055/11884 - Loss: 29.6179\n",
      "Processing batch 1056/11884 - Loss: 30.6755\n",
      "Processing batch 1057/11884 - Loss: 29.5049\n",
      "Processing batch 1058/11884 - Loss: 29.0889\n",
      "Processing batch 1059/11884 - Loss: 29.7314\n",
      "Processing batch 1060/11884 - Loss: 29.9099\n",
      "Processing batch 1061/11884 - Loss: 29.6919\n",
      "Processing batch 1062/11884 - Loss: 29.5220\n",
      "Processing batch 1063/11884 - Loss: 31.7921\n",
      "Processing batch 1064/11884 - Loss: 29.9548\n",
      "Processing batch 1065/11884 - Loss: 30.2199\n",
      "Processing batch 1066/11884 - Loss: 30.5937\n",
      "Processing batch 1067/11884 - Loss: 30.7754\n",
      "Processing batch 1068/11884 - Loss: 29.5478\n",
      "Processing batch 1069/11884 - Loss: 31.1863\n",
      "Processing batch 1070/11884 - Loss: 30.9139\n",
      "Processing batch 1071/11884 - Loss: 30.8523\n",
      "Processing batch 1072/11884 - Loss: 31.1837\n",
      "Processing batch 1073/11884 - Loss: 29.7994\n",
      "Processing batch 1074/11884 - Loss: 30.1773\n",
      "Processing batch 1075/11884 - Loss: 30.8380\n",
      "Processing batch 1076/11884 - Loss: 30.1906\n",
      "Processing batch 1077/11884 - Loss: 29.9433\n",
      "Processing batch 1078/11884 - Loss: 31.2310\n",
      "Processing batch 1079/11884 - Loss: 29.7199\n",
      "Processing batch 1080/11884 - Loss: 30.1841\n",
      "Processing batch 1081/11884 - Loss: 29.6982\n",
      "Processing batch 1082/11884 - Loss: 30.9138\n",
      "Processing batch 1083/11884 - Loss: 29.6904\n",
      "Processing batch 1084/11884 - Loss: 30.6471\n",
      "Processing batch 1085/11884 - Loss: 28.9195\n",
      "Processing batch 1086/11884 - Loss: 30.3050\n",
      "Processing batch 1087/11884 - Loss: 29.7599\n",
      "Processing batch 1088/11884 - Loss: 30.1158\n",
      "Processing batch 1089/11884 - Loss: 31.2610\n",
      "Processing batch 1090/11884 - Loss: 29.6777\n",
      "Processing batch 1091/11884 - Loss: 32.0322\n",
      "Processing batch 1092/11884 - Loss: 29.9663\n",
      "Processing batch 1093/11884 - Loss: 29.3823\n",
      "Processing batch 1094/11884 - Loss: 30.9893\n",
      "Processing batch 1095/11884 - Loss: 30.8742\n",
      "Processing batch 1096/11884 - Loss: 31.2097\n",
      "Processing batch 1097/11884 - Loss: 30.8490\n",
      "Processing batch 1098/11884 - Loss: 30.7071\n",
      "Processing batch 1099/11884 - Loss: 29.7007\n",
      "Processing batch 1100/11884 - Loss: 30.4326\n",
      "Processing batch 1101/11884 - Loss: 30.9189\n",
      "Processing batch 1102/11884 - Loss: 31.9431\n",
      "Processing batch 1103/11884 - Loss: 29.6421\n",
      "Processing batch 1104/11884 - Loss: 30.5827\n",
      "Processing batch 1105/11884 - Loss: 31.2491\n",
      "Processing batch 1106/11884 - Loss: 30.7152\n",
      "Processing batch 1107/11884 - Loss: 30.8767\n",
      "Processing batch 1108/11884 - Loss: 31.0069\n",
      "Processing batch 1109/11884 - Loss: 30.2240\n",
      "Processing batch 1110/11884 - Loss: 30.7443\n",
      "Processing batch 1111/11884 - Loss: 29.6473\n",
      "Processing batch 1112/11884 - Loss: 30.5397\n",
      "Processing batch 1113/11884 - Loss: 30.7544\n",
      "Processing batch 1114/11884 - Loss: 31.4362\n",
      "Processing batch 1115/11884 - Loss: 30.7320\n",
      "Processing batch 1116/11884 - Loss: 31.2493\n",
      "Processing batch 1117/11884 - Loss: 29.5181\n",
      "Processing batch 1118/11884 - Loss: 30.2375\n",
      "Processing batch 1119/11884 - Loss: 29.9824\n",
      "Processing batch 1120/11884 - Loss: 30.3934\n",
      "Processing batch 1121/11884 - Loss: 31.3383\n",
      "Processing batch 1122/11884 - Loss: 30.9128\n",
      "Processing batch 1123/11884 - Loss: 30.6026\n",
      "Processing batch 1124/11884 - Loss: 29.0338\n",
      "Processing batch 1125/11884 - Loss: 30.8917\n",
      "Processing batch 1126/11884 - Loss: 29.8467\n",
      "Processing batch 1127/11884 - Loss: 30.1645\n",
      "Processing batch 1128/11884 - Loss: 29.4654\n",
      "Processing batch 1129/11884 - Loss: 30.0589\n",
      "Processing batch 1130/11884 - Loss: 29.8132\n",
      "Processing batch 1131/11884 - Loss: 31.0683\n",
      "Processing batch 1132/11884 - Loss: 30.5119\n",
      "Processing batch 1133/11884 - Loss: 29.9240\n",
      "Processing batch 1134/11884 - Loss: 29.5741\n",
      "Processing batch 1135/11884 - Loss: 29.6834\n",
      "Processing batch 1136/11884 - Loss: 29.1723\n",
      "Processing batch 1137/11884 - Loss: 31.1983\n",
      "Processing batch 1138/11884 - Loss: 31.8918\n",
      "Processing batch 1139/11884 - Loss: 30.4405\n",
      "Processing batch 1140/11884 - Loss: 30.1792\n",
      "Processing batch 1141/11884 - Loss: 30.4827\n",
      "Processing batch 1142/11884 - Loss: 30.5787\n",
      "Processing batch 1143/11884 - Loss: 30.5300\n",
      "Processing batch 1144/11884 - Loss: 30.0266\n",
      "Processing batch 1145/11884 - Loss: 31.1264\n",
      "Processing batch 1146/11884 - Loss: 29.7238\n",
      "Processing batch 1147/11884 - Loss: 30.5773\n",
      "Processing batch 1148/11884 - Loss: 30.5332\n",
      "Processing batch 1149/11884 - Loss: 29.7031\n",
      "Processing batch 1150/11884 - Loss: 30.2701\n",
      "Processing batch 1151/11884 - Loss: 29.3155\n",
      "Processing batch 1152/11884 - Loss: 31.5318\n",
      "Processing batch 1153/11884 - Loss: 30.7418\n",
      "Processing batch 1154/11884 - Loss: 31.2953\n",
      "Processing batch 1155/11884 - Loss: 30.8440\n",
      "Processing batch 1156/11884 - Loss: 30.9630\n",
      "Processing batch 1157/11884 - Loss: 29.7032\n",
      "Processing batch 1158/11884 - Loss: 30.4440\n",
      "Processing batch 1159/11884 - Loss: 31.3153\n",
      "Processing batch 1160/11884 - Loss: 29.4917\n",
      "Processing batch 1161/11884 - Loss: 30.5161\n",
      "Processing batch 1162/11884 - Loss: 30.9719\n",
      "Processing batch 1163/11884 - Loss: 30.1271\n",
      "Processing batch 1164/11884 - Loss: 30.5634\n",
      "Processing batch 1165/11884 - Loss: 30.6322\n",
      "Processing batch 1166/11884 - Loss: 31.1531\n",
      "Processing batch 1167/11884 - Loss: 30.7175\n",
      "Processing batch 1168/11884 - Loss: 30.5329\n",
      "Processing batch 1169/11884 - Loss: 29.9372\n",
      "Processing batch 1170/11884 - Loss: 28.8189\n",
      "Processing batch 1171/11884 - Loss: 30.6962\n",
      "Processing batch 1172/11884 - Loss: 28.8786\n",
      "Processing batch 1173/11884 - Loss: 30.4375\n",
      "Processing batch 1174/11884 - Loss: 30.7409\n",
      "Processing batch 1175/11884 - Loss: 31.2927\n",
      "Processing batch 1176/11884 - Loss: 30.2163\n",
      "Processing batch 1177/11884 - Loss: 30.4718\n",
      "Processing batch 1178/11884 - Loss: 29.7784\n",
      "Processing batch 1179/11884 - Loss: 30.8340\n",
      "Processing batch 1180/11884 - Loss: 30.2148\n",
      "Processing batch 1181/11884 - Loss: 30.8222\n",
      "Processing batch 1182/11884 - Loss: 29.5360\n",
      "Processing batch 1183/11884 - Loss: 32.0826\n",
      "Processing batch 1184/11884 - Loss: 31.8868\n",
      "Processing batch 1185/11884 - Loss: 29.8981\n",
      "Processing batch 1186/11884 - Loss: 31.2409\n",
      "Processing batch 1187/11884 - Loss: 30.9082\n",
      "Processing batch 1188/11884 - Loss: 30.4062\n",
      "Processing batch 1189/11884 - Loss: 29.6135\n",
      "Processing batch 1190/11884 - Loss: 31.1064\n",
      "Processing batch 1191/11884 - Loss: 31.5502\n",
      "Processing batch 1192/11884 - Loss: 31.3715\n",
      "Processing batch 1193/11884 - Loss: 29.4068\n",
      "Processing batch 1194/11884 - Loss: 29.4405\n",
      "Processing batch 1195/11884 - Loss: 29.6138\n",
      "Processing batch 1196/11884 - Loss: 30.0310\n",
      "Processing batch 1197/11884 - Loss: 29.4008\n",
      "Processing batch 1198/11884 - Loss: 30.5328\n",
      "Processing batch 1199/11884 - Loss: 31.7469\n",
      "Processing batch 1200/11884 - Loss: 29.5984\n",
      "Processing batch 1201/11884 - Loss: 29.0961\n",
      "Processing batch 1202/11884 - Loss: 29.8089\n",
      "Processing batch 1203/11884 - Loss: 30.6346\n",
      "Processing batch 1204/11884 - Loss: 30.1998\n",
      "Processing batch 1205/11884 - Loss: 30.9878\n",
      "Processing batch 1206/11884 - Loss: 30.1643\n",
      "Processing batch 1207/11884 - Loss: 30.4595\n",
      "Processing batch 1208/11884 - Loss: 29.0983\n",
      "Processing batch 1209/11884 - Loss: 29.6876\n",
      "Processing batch 1210/11884 - Loss: 30.2318\n",
      "Processing batch 1211/11884 - Loss: 27.5415\n",
      "Processing batch 1212/11884 - Loss: 29.9437\n",
      "Processing batch 1213/11884 - Loss: 30.4685\n",
      "Processing batch 1214/11884 - Loss: 30.6162\n",
      "Processing batch 1215/11884 - Loss: 30.5960\n",
      "Processing batch 1216/11884 - Loss: 30.0451\n",
      "Processing batch 1217/11884 - Loss: 30.4520\n",
      "Processing batch 1218/11884 - Loss: 30.1893\n",
      "Processing batch 1219/11884 - Loss: 30.3286\n",
      "Processing batch 1220/11884 - Loss: 30.0057\n",
      "Processing batch 1221/11884 - Loss: 30.5221\n",
      "Processing batch 1222/11884 - Loss: 30.6271\n",
      "Processing batch 1223/11884 - Loss: 29.7423\n",
      "Processing batch 1224/11884 - Loss: 30.1094\n",
      "Processing batch 1225/11884 - Loss: 29.9615\n",
      "Processing batch 1226/11884 - Loss: 31.1188\n",
      "Processing batch 1227/11884 - Loss: 30.3114\n",
      "Processing batch 1228/11884 - Loss: 29.9653\n",
      "Processing batch 1229/11884 - Loss: 31.4700\n",
      "Processing batch 1230/11884 - Loss: 30.8778\n",
      "Processing batch 1231/11884 - Loss: 29.7566\n",
      "Processing batch 1232/11884 - Loss: 30.0299\n",
      "Processing batch 1233/11884 - Loss: 31.2015\n",
      "Processing batch 1234/11884 - Loss: 30.4217\n",
      "Processing batch 1235/11884 - Loss: 31.1842\n",
      "Processing batch 1236/11884 - Loss: 29.5005\n",
      "Processing batch 1237/11884 - Loss: 29.7508\n",
      "Processing batch 1238/11884 - Loss: 31.1540\n",
      "Processing batch 1239/11884 - Loss: 31.6097\n",
      "Processing batch 1240/11884 - Loss: 31.2187\n",
      "Processing batch 1241/11884 - Loss: 29.8578\n",
      "Processing batch 1242/11884 - Loss: 30.5116\n",
      "Processing batch 1243/11884 - Loss: 30.4742\n",
      "Processing batch 1244/11884 - Loss: 30.0690\n",
      "Processing batch 1245/11884 - Loss: 29.9507\n",
      "Processing batch 1246/11884 - Loss: 30.2569\n",
      "Processing batch 1247/11884 - Loss: 30.7632\n",
      "Processing batch 1248/11884 - Loss: 29.4442\n",
      "Processing batch 1249/11884 - Loss: 29.8411\n",
      "Processing batch 1250/11884 - Loss: 30.3179\n",
      "Processing batch 1251/11884 - Loss: 31.1009\n",
      "Processing batch 1252/11884 - Loss: 30.6738\n",
      "Processing batch 1253/11884 - Loss: 30.1988\n",
      "Processing batch 1254/11884 - Loss: 31.0530\n",
      "Processing batch 1255/11884 - Loss: 29.9740\n",
      "Processing batch 1256/11884 - Loss: 30.6261\n",
      "Processing batch 1257/11884 - Loss: 31.3426\n",
      "Processing batch 1258/11884 - Loss: 30.9915\n",
      "Processing batch 1259/11884 - Loss: 29.8385\n",
      "Processing batch 1260/11884 - Loss: 30.2278\n",
      "Processing batch 1261/11884 - Loss: 31.1094\n",
      "Processing batch 1262/11884 - Loss: 29.6662\n",
      "Processing batch 1263/11884 - Loss: 30.8973\n",
      "Processing batch 1264/11884 - Loss: 30.5156\n",
      "Processing batch 1265/11884 - Loss: 30.7275\n",
      "Processing batch 1266/11884 - Loss: 30.6758\n",
      "Processing batch 1267/11884 - Loss: 30.3622\n",
      "Processing batch 1268/11884 - Loss: 29.9984\n",
      "Processing batch 1269/11884 - Loss: 29.8465\n",
      "Processing batch 1270/11884 - Loss: 29.9855\n",
      "Processing batch 1271/11884 - Loss: 30.1504\n",
      "Processing batch 1272/11884 - Loss: 30.4267\n",
      "Processing batch 1273/11884 - Loss: 29.7361\n",
      "Processing batch 1274/11884 - Loss: 30.5151\n",
      "Processing batch 1275/11884 - Loss: 29.8770\n",
      "Processing batch 1276/11884 - Loss: 31.3725\n",
      "Processing batch 1277/11884 - Loss: 29.8237\n",
      "Processing batch 1278/11884 - Loss: 30.1295\n",
      "Processing batch 1279/11884 - Loss: 31.2803\n",
      "Processing batch 1280/11884 - Loss: 29.8626\n",
      "Processing batch 1281/11884 - Loss: 29.5318\n",
      "Processing batch 1282/11884 - Loss: 29.8002\n",
      "Processing batch 1283/11884 - Loss: 29.9963\n",
      "Processing batch 1284/11884 - Loss: 31.6802\n",
      "Processing batch 1285/11884 - Loss: 29.9828\n",
      "Processing batch 1286/11884 - Loss: 30.1712\n",
      "Processing batch 1287/11884 - Loss: 30.5473\n",
      "Processing batch 1288/11884 - Loss: 30.3651\n",
      "Processing batch 1289/11884 - Loss: 29.5299\n",
      "Processing batch 1290/11884 - Loss: 29.0480\n",
      "Processing batch 1291/11884 - Loss: 31.4515\n",
      "Processing batch 1292/11884 - Loss: 31.7226\n",
      "Processing batch 1293/11884 - Loss: 29.8801\n",
      "Processing batch 1294/11884 - Loss: 30.0778\n",
      "Processing batch 1295/11884 - Loss: 30.5645\n",
      "Processing batch 1296/11884 - Loss: 31.1470\n",
      "Processing batch 1297/11884 - Loss: 30.9145\n",
      "Processing batch 1298/11884 - Loss: 30.0274\n",
      "Processing batch 1299/11884 - Loss: 29.6782\n",
      "Processing batch 1300/11884 - Loss: 30.1145\n",
      "Processing batch 1301/11884 - Loss: 29.3050\n",
      "Processing batch 1302/11884 - Loss: 30.9831\n",
      "Processing batch 1303/11884 - Loss: 29.6281\n",
      "Processing batch 1304/11884 - Loss: 29.8740\n",
      "Processing batch 1305/11884 - Loss: 30.7583\n",
      "Processing batch 1306/11884 - Loss: 30.9703\n",
      "Processing batch 1307/11884 - Loss: 28.0635\n",
      "Processing batch 1308/11884 - Loss: 29.6647\n",
      "Processing batch 1309/11884 - Loss: 30.4312\n",
      "Processing batch 1310/11884 - Loss: 31.8306\n",
      "Processing batch 1311/11884 - Loss: 30.3845\n",
      "Processing batch 1312/11884 - Loss: 29.9424\n",
      "Processing batch 1313/11884 - Loss: 29.9588\n",
      "Processing batch 1314/11884 - Loss: 30.1835\n",
      "Processing batch 1315/11884 - Loss: 29.0466\n",
      "Processing batch 1316/11884 - Loss: 29.3726\n",
      "Processing batch 1317/11884 - Loss: 30.4326\n",
      "Processing batch 1318/11884 - Loss: 29.2907\n",
      "Processing batch 1319/11884 - Loss: 29.7973\n",
      "Processing batch 1320/11884 - Loss: 29.3046\n",
      "Processing batch 1321/11884 - Loss: 31.5335\n",
      "Processing batch 1322/11884 - Loss: 30.1481\n",
      "Processing batch 1323/11884 - Loss: 31.0627\n",
      "Processing batch 1324/11884 - Loss: 30.0415\n",
      "Processing batch 1325/11884 - Loss: 30.8713\n",
      "Processing batch 1326/11884 - Loss: 31.1985\n",
      "Processing batch 1327/11884 - Loss: 31.6672\n",
      "Processing batch 1328/11884 - Loss: 30.6711\n",
      "Processing batch 1329/11884 - Loss: 30.6762\n",
      "Processing batch 1330/11884 - Loss: 30.2555\n",
      "Processing batch 1331/11884 - Loss: 30.0252\n",
      "Processing batch 1332/11884 - Loss: 30.1750\n",
      "Processing batch 1333/11884 - Loss: 29.8328\n",
      "Processing batch 1334/11884 - Loss: 29.4409\n",
      "Processing batch 1335/11884 - Loss: 31.1902\n",
      "Processing batch 1336/11884 - Loss: 30.3210\n",
      "Processing batch 1337/11884 - Loss: 30.0160\n",
      "Processing batch 1338/11884 - Loss: 30.2881\n",
      "Processing batch 1339/11884 - Loss: 30.4248\n",
      "Processing batch 1340/11884 - Loss: 30.3280\n",
      "Processing batch 1341/11884 - Loss: 32.2427\n",
      "Processing batch 1342/11884 - Loss: 29.4298\n",
      "Processing batch 1343/11884 - Loss: 31.1861\n",
      "Processing batch 1344/11884 - Loss: 30.6731\n",
      "Processing batch 1345/11884 - Loss: 30.8289\n",
      "Processing batch 1346/11884 - Loss: 28.3167\n",
      "Processing batch 1347/11884 - Loss: 29.4833\n",
      "Processing batch 1348/11884 - Loss: 29.1667\n",
      "Processing batch 1349/11884 - Loss: 30.1596\n",
      "Processing batch 1350/11884 - Loss: 30.3449\n",
      "Processing batch 1351/11884 - Loss: 30.8278\n",
      "Processing batch 1352/11884 - Loss: 31.3711\n",
      "Processing batch 1353/11884 - Loss: 30.0883\n",
      "Processing batch 1354/11884 - Loss: 30.5074\n",
      "Processing batch 1355/11884 - Loss: 30.1252\n",
      "Processing batch 1356/11884 - Loss: 30.0018\n",
      "Processing batch 1357/11884 - Loss: 30.8072\n",
      "Processing batch 1358/11884 - Loss: 29.4368\n",
      "Processing batch 1359/11884 - Loss: 30.4803\n",
      "Processing batch 1360/11884 - Loss: 29.9438\n",
      "Processing batch 1361/11884 - Loss: 29.9356\n",
      "Processing batch 1362/11884 - Loss: 31.1240\n",
      "Processing batch 1363/11884 - Loss: 30.5329\n",
      "Processing batch 1364/11884 - Loss: 31.3966\n",
      "Processing batch 1365/11884 - Loss: 29.0254\n",
      "Processing batch 1366/11884 - Loss: 29.6004\n",
      "Processing batch 1367/11884 - Loss: 30.2593\n",
      "Processing batch 1368/11884 - Loss: 31.9101\n",
      "Processing batch 1369/11884 - Loss: 31.8372\n",
      "Processing batch 1370/11884 - Loss: 29.9138\n",
      "Processing batch 1371/11884 - Loss: 29.2208\n",
      "Processing batch 1372/11884 - Loss: 29.6898\n",
      "Processing batch 1373/11884 - Loss: 30.4179\n",
      "Processing batch 1374/11884 - Loss: 31.5888\n",
      "Processing batch 1375/11884 - Loss: 30.2874\n",
      "Processing batch 1376/11884 - Loss: 31.1041\n",
      "Processing batch 1377/11884 - Loss: 30.1369\n",
      "Processing batch 1378/11884 - Loss: 29.2914\n",
      "Processing batch 1379/11884 - Loss: 30.5267\n",
      "Processing batch 1380/11884 - Loss: 30.3209\n",
      "Processing batch 1381/11884 - Loss: 30.1226\n",
      "Processing batch 1382/11884 - Loss: 29.7472\n",
      "Processing batch 1383/11884 - Loss: 29.2453\n",
      "Processing batch 1384/11884 - Loss: 32.7649\n",
      "Processing batch 1385/11884 - Loss: 29.5573\n",
      "Processing batch 1386/11884 - Loss: 30.1050\n",
      "Processing batch 1387/11884 - Loss: 29.6816\n",
      "Processing batch 1388/11884 - Loss: 30.9360\n",
      "Processing batch 1389/11884 - Loss: 28.3081\n",
      "Processing batch 1390/11884 - Loss: 31.6468\n",
      "Processing batch 1391/11884 - Loss: 29.3847\n",
      "Processing batch 1392/11884 - Loss: 29.6843\n",
      "Processing batch 1393/11884 - Loss: 30.4787\n",
      "Processing batch 1394/11884 - Loss: 30.3058\n",
      "Processing batch 1395/11884 - Loss: 28.1554\n",
      "Processing batch 1396/11884 - Loss: 30.1218\n",
      "Processing batch 1397/11884 - Loss: 30.5412\n",
      "Processing batch 1398/11884 - Loss: 29.3302\n",
      "Processing batch 1399/11884 - Loss: 29.8881\n",
      "Processing batch 1400/11884 - Loss: 29.5896\n",
      "Processing batch 1401/11884 - Loss: 31.7453\n",
      "Processing batch 1402/11884 - Loss: 30.5259\n",
      "Processing batch 1403/11884 - Loss: 31.0223\n",
      "Processing batch 1404/11884 - Loss: 28.6376\n",
      "Processing batch 1405/11884 - Loss: 29.5667\n",
      "Processing batch 1406/11884 - Loss: 30.8284\n",
      "Processing batch 1407/11884 - Loss: 30.2694\n",
      "Processing batch 1408/11884 - Loss: 29.7714\n",
      "Processing batch 1409/11884 - Loss: 31.8859\n",
      "Processing batch 1410/11884 - Loss: 29.7078\n",
      "Processing batch 1411/11884 - Loss: 29.5944\n",
      "Processing batch 1412/11884 - Loss: 30.1418\n",
      "Processing batch 1413/11884 - Loss: 30.3124\n",
      "Processing batch 1414/11884 - Loss: 29.7914\n",
      "Processing batch 1415/11884 - Loss: 29.7574\n",
      "Processing batch 1416/11884 - Loss: 29.9407\n",
      "Processing batch 1417/11884 - Loss: 29.7128\n",
      "Processing batch 1418/11884 - Loss: 29.9590\n",
      "Processing batch 1419/11884 - Loss: 30.1548\n",
      "Processing batch 1420/11884 - Loss: 30.0126\n",
      "Processing batch 1421/11884 - Loss: 29.9974\n",
      "Processing batch 1422/11884 - Loss: 30.0240\n",
      "Processing batch 1423/11884 - Loss: 29.9758\n",
      "Processing batch 1424/11884 - Loss: 30.2799\n",
      "Processing batch 1425/11884 - Loss: 29.9836\n",
      "Processing batch 1426/11884 - Loss: 28.7515\n",
      "Processing batch 1427/11884 - Loss: 30.1243\n",
      "Processing batch 1428/11884 - Loss: 29.8895\n",
      "Processing batch 1429/11884 - Loss: 29.8845\n",
      "Processing batch 1430/11884 - Loss: 29.5676\n",
      "Processing batch 1431/11884 - Loss: 29.0785\n",
      "Processing batch 1432/11884 - Loss: 29.3506\n",
      "Processing batch 1433/11884 - Loss: 30.4138\n",
      "Processing batch 1434/11884 - Loss: 28.2758\n",
      "Processing batch 1435/11884 - Loss: 28.7188\n",
      "Processing batch 1436/11884 - Loss: 30.1034\n",
      "Processing batch 1437/11884 - Loss: 30.1866\n",
      "Processing batch 1438/11884 - Loss: 29.9108\n",
      "Processing batch 1439/11884 - Loss: 30.9079\n",
      "Processing batch 1440/11884 - Loss: 30.0561\n",
      "Processing batch 1441/11884 - Loss: 30.0091\n",
      "Processing batch 1442/11884 - Loss: 32.1801\n",
      "Processing batch 1443/11884 - Loss: 29.2471\n",
      "Processing batch 1444/11884 - Loss: 31.0292\n",
      "Processing batch 1445/11884 - Loss: 29.9407\n",
      "Processing batch 1446/11884 - Loss: 29.9035\n",
      "Processing batch 1447/11884 - Loss: 30.6162\n",
      "Processing batch 1448/11884 - Loss: 31.1455\n",
      "Processing batch 1449/11884 - Loss: 30.5912\n",
      "Processing batch 1450/11884 - Loss: 29.4087\n",
      "Processing batch 1451/11884 - Loss: 29.7858\n",
      "Processing batch 1452/11884 - Loss: 29.5047\n",
      "Processing batch 1453/11884 - Loss: 30.5310\n",
      "Processing batch 1454/11884 - Loss: 31.3263\n",
      "Processing batch 1455/11884 - Loss: 28.7519\n",
      "Processing batch 1456/11884 - Loss: 29.5909\n",
      "Processing batch 1457/11884 - Loss: 30.8962\n",
      "Processing batch 1458/11884 - Loss: 29.9034\n",
      "Processing batch 1459/11884 - Loss: 30.6572\n",
      "Processing batch 1460/11884 - Loss: 32.2577\n",
      "Processing batch 1461/11884 - Loss: 30.0552\n",
      "Processing batch 1462/11884 - Loss: 30.1439\n",
      "Processing batch 1463/11884 - Loss: 30.6087\n",
      "Processing batch 1464/11884 - Loss: 31.8523\n",
      "Processing batch 1465/11884 - Loss: 31.0001\n",
      "Processing batch 1466/11884 - Loss: 29.2013\n",
      "Processing batch 1467/11884 - Loss: 29.9939\n",
      "Processing batch 1468/11884 - Loss: 30.6964\n",
      "Processing batch 1469/11884 - Loss: 30.5574\n",
      "Processing batch 1470/11884 - Loss: 31.4441\n",
      "Processing batch 1471/11884 - Loss: 30.3319\n",
      "Processing batch 1472/11884 - Loss: 29.8656\n",
      "Processing batch 1473/11884 - Loss: 29.7601\n",
      "Processing batch 1474/11884 - Loss: 31.4395\n",
      "Processing batch 1475/11884 - Loss: 31.2074\n",
      "Processing batch 1476/11884 - Loss: 31.8856\n",
      "Processing batch 1477/11884 - Loss: 30.3444\n",
      "Processing batch 1478/11884 - Loss: 30.5625\n",
      "Processing batch 1479/11884 - Loss: 31.5052\n",
      "Processing batch 1480/11884 - Loss: 30.0345\n",
      "Processing batch 1481/11884 - Loss: 30.0356\n",
      "Processing batch 1482/11884 - Loss: 29.3066\n",
      "Processing batch 1483/11884 - Loss: 30.3606\n",
      "Processing batch 1484/11884 - Loss: 31.3765\n",
      "Processing batch 1485/11884 - Loss: 29.8979\n",
      "Processing batch 1486/11884 - Loss: 30.1598\n",
      "Processing batch 1487/11884 - Loss: 29.2899\n",
      "Processing batch 1488/11884 - Loss: 30.7666\n",
      "Processing batch 1489/11884 - Loss: 29.9045\n",
      "Processing batch 1490/11884 - Loss: 30.1751\n",
      "Processing batch 1491/11884 - Loss: 31.5164\n",
      "Processing batch 1492/11884 - Loss: 29.5366\n",
      "Processing batch 1493/11884 - Loss: 30.2663\n",
      "Processing batch 1494/11884 - Loss: 30.9384\n",
      "Processing batch 1495/11884 - Loss: 29.6316\n",
      "Processing batch 1496/11884 - Loss: 30.7911\n",
      "Processing batch 1497/11884 - Loss: 29.6954\n",
      "Processing batch 1498/11884 - Loss: 29.2006\n",
      "Processing batch 1499/11884 - Loss: 29.0229\n",
      "Processing batch 1500/11884 - Loss: 31.2793\n",
      "Processing batch 1501/11884 - Loss: 30.9505\n",
      "Processing batch 1502/11884 - Loss: 31.1756\n",
      "Processing batch 1503/11884 - Loss: 30.4213\n",
      "Processing batch 1504/11884 - Loss: 29.8056\n",
      "Processing batch 1505/11884 - Loss: 29.5771\n",
      "Processing batch 1506/11884 - Loss: 30.1800\n",
      "Processing batch 1507/11884 - Loss: 31.0160\n",
      "Processing batch 1508/11884 - Loss: 30.3044\n",
      "Processing batch 1509/11884 - Loss: 30.8305\n",
      "Processing batch 1510/11884 - Loss: 30.0936\n",
      "Processing batch 1511/11884 - Loss: 30.3690\n",
      "Processing batch 1512/11884 - Loss: 30.0972\n",
      "Processing batch 1513/11884 - Loss: 29.8821\n",
      "Processing batch 1514/11884 - Loss: 29.4287\n",
      "Processing batch 1515/11884 - Loss: 29.5350\n",
      "Processing batch 1516/11884 - Loss: 30.4780\n",
      "Processing batch 1517/11884 - Loss: 32.0148\n",
      "Processing batch 1518/11884 - Loss: 31.1140\n",
      "Processing batch 1519/11884 - Loss: 31.4867\n",
      "Processing batch 1520/11884 - Loss: 29.5394\n",
      "Processing batch 1521/11884 - Loss: 30.9931\n",
      "Processing batch 1522/11884 - Loss: 29.6812\n",
      "Processing batch 1523/11884 - Loss: 30.8154\n",
      "Processing batch 1524/11884 - Loss: 30.8228\n",
      "Processing batch 1525/11884 - Loss: 29.7448\n",
      "Processing batch 1526/11884 - Loss: 29.6853\n",
      "Processing batch 1527/11884 - Loss: 30.7627\n",
      "Processing batch 1528/11884 - Loss: 29.6609\n",
      "Processing batch 1529/11884 - Loss: 30.2373\n",
      "Processing batch 1530/11884 - Loss: 29.6436\n",
      "Processing batch 1531/11884 - Loss: 29.8168\n",
      "Processing batch 1532/11884 - Loss: 29.6271\n",
      "Processing batch 1533/11884 - Loss: 30.0286\n",
      "Processing batch 1534/11884 - Loss: 30.6069\n",
      "Processing batch 1535/11884 - Loss: 31.1411\n",
      "Processing batch 1536/11884 - Loss: 31.0383\n",
      "Processing batch 1537/11884 - Loss: 30.3804\n",
      "Processing batch 1538/11884 - Loss: 30.3959\n",
      "Processing batch 1539/11884 - Loss: 29.9237\n",
      "Processing batch 1540/11884 - Loss: 30.0482\n",
      "Processing batch 1541/11884 - Loss: 31.1871\n",
      "Processing batch 1542/11884 - Loss: 30.1490\n",
      "Processing batch 1543/11884 - Loss: 29.9701\n",
      "Processing batch 1544/11884 - Loss: 30.4854\n",
      "Processing batch 1545/11884 - Loss: 30.8820\n",
      "Processing batch 1546/11884 - Loss: 29.3405\n",
      "Processing batch 1547/11884 - Loss: 30.0214\n",
      "Processing batch 1548/11884 - Loss: 29.7537\n",
      "Processing batch 1549/11884 - Loss: 31.6753\n",
      "Processing batch 1550/11884 - Loss: 31.1156\n",
      "Processing batch 1551/11884 - Loss: 28.7745\n",
      "Processing batch 1552/11884 - Loss: 31.0294\n",
      "Processing batch 1553/11884 - Loss: 29.5480\n",
      "Processing batch 1554/11884 - Loss: 30.2088\n",
      "Processing batch 1555/11884 - Loss: 28.3449\n",
      "Processing batch 1556/11884 - Loss: 31.4462\n",
      "Processing batch 1557/11884 - Loss: 29.9276\n",
      "Processing batch 1558/11884 - Loss: 29.6370\n",
      "Processing batch 1559/11884 - Loss: 29.8230\n",
      "Processing batch 1560/11884 - Loss: 31.2288\n",
      "Processing batch 1561/11884 - Loss: 31.9498\n",
      "Processing batch 1562/11884 - Loss: 28.4813\n",
      "Processing batch 1563/11884 - Loss: 30.4174\n",
      "Processing batch 1564/11884 - Loss: 30.4516\n",
      "Processing batch 1565/11884 - Loss: 30.3896\n",
      "Processing batch 1566/11884 - Loss: 29.7687\n",
      "Processing batch 1567/11884 - Loss: 30.0626\n",
      "Processing batch 1568/11884 - Loss: 30.6982\n",
      "Processing batch 1569/11884 - Loss: 30.2112\n",
      "Processing batch 1570/11884 - Loss: 30.8424\n",
      "Processing batch 1571/11884 - Loss: 29.9558\n",
      "Processing batch 1572/11884 - Loss: 30.4289\n",
      "Processing batch 1573/11884 - Loss: 31.3741\n",
      "Processing batch 1574/11884 - Loss: 29.8335\n",
      "Processing batch 1575/11884 - Loss: 29.3198\n",
      "Processing batch 1576/11884 - Loss: 29.7353\n",
      "Processing batch 1577/11884 - Loss: 29.6109\n",
      "Processing batch 1578/11884 - Loss: 31.0667\n",
      "Processing batch 1579/11884 - Loss: 28.8984\n",
      "Processing batch 1580/11884 - Loss: 29.8336\n",
      "Processing batch 1581/11884 - Loss: 29.9004\n",
      "Processing batch 1582/11884 - Loss: 30.2182\n",
      "Processing batch 1583/11884 - Loss: 30.2070\n",
      "Processing batch 1584/11884 - Loss: 31.3450\n",
      "Processing batch 1585/11884 - Loss: 30.4901\n",
      "Processing batch 1586/11884 - Loss: 29.5894\n",
      "Processing batch 1587/11884 - Loss: 30.8731\n",
      "Processing batch 1588/11884 - Loss: 30.4477\n",
      "Processing batch 1589/11884 - Loss: 30.3861\n",
      "Processing batch 1590/11884 - Loss: 28.0491\n",
      "Processing batch 1591/11884 - Loss: 29.6485\n",
      "Processing batch 1592/11884 - Loss: 30.7490\n",
      "Processing batch 1593/11884 - Loss: 29.5829\n",
      "Processing batch 1594/11884 - Loss: 30.8911\n",
      "Processing batch 1595/11884 - Loss: 27.8705\n",
      "Processing batch 1596/11884 - Loss: 29.6404\n",
      "Processing batch 1597/11884 - Loss: 30.7914\n",
      "Processing batch 1598/11884 - Loss: 29.6162\n",
      "Processing batch 1599/11884 - Loss: 30.8973\n",
      "Processing batch 1600/11884 - Loss: 30.7071\n",
      "Processing batch 1601/11884 - Loss: 31.0551\n",
      "Processing batch 1602/11884 - Loss: 30.9433\n",
      "Processing batch 1603/11884 - Loss: 30.3973\n",
      "Processing batch 1604/11884 - Loss: 30.3905\n",
      "Processing batch 1605/11884 - Loss: 30.6793\n",
      "Processing batch 1606/11884 - Loss: 30.4697\n",
      "Processing batch 1607/11884 - Loss: 29.7353\n",
      "Processing batch 1608/11884 - Loss: 30.3870\n",
      "Processing batch 1609/11884 - Loss: 29.5607\n",
      "Processing batch 1610/11884 - Loss: 31.0377\n",
      "Processing batch 1611/11884 - Loss: 30.6712\n",
      "Processing batch 1612/11884 - Loss: 30.1110\n",
      "Processing batch 1613/11884 - Loss: 30.2678\n",
      "Processing batch 1614/11884 - Loss: 30.1429\n",
      "Processing batch 1615/11884 - Loss: 31.2220\n",
      "Processing batch 1616/11884 - Loss: 30.1070\n",
      "Processing batch 1617/11884 - Loss: 31.1463\n",
      "Processing batch 1618/11884 - Loss: 30.4412\n",
      "Processing batch 1619/11884 - Loss: 30.1109\n",
      "Processing batch 1620/11884 - Loss: 31.1824\n",
      "Processing batch 1621/11884 - Loss: 30.2282\n",
      "Processing batch 1622/11884 - Loss: 30.2514\n",
      "Processing batch 1623/11884 - Loss: 29.4522\n",
      "Processing batch 1624/11884 - Loss: 29.8330\n",
      "Processing batch 1625/11884 - Loss: 31.3124\n",
      "Processing batch 1626/11884 - Loss: 31.2535\n",
      "Processing batch 1627/11884 - Loss: 30.4910\n",
      "Processing batch 1628/11884 - Loss: 31.2878\n",
      "Processing batch 1629/11884 - Loss: 29.5270\n",
      "Processing batch 1630/11884 - Loss: 28.6691\n",
      "Processing batch 1631/11884 - Loss: 29.5123\n",
      "Processing batch 1632/11884 - Loss: 30.3221\n",
      "Processing batch 1633/11884 - Loss: 30.9868\n",
      "Processing batch 1634/11884 - Loss: 30.8452\n",
      "Processing batch 1635/11884 - Loss: 30.9037\n",
      "Processing batch 1636/11884 - Loss: 30.3809\n",
      "Processing batch 1637/11884 - Loss: 29.2053\n",
      "Processing batch 1638/11884 - Loss: 28.8288\n",
      "Processing batch 1639/11884 - Loss: 31.1762\n",
      "Processing batch 1640/11884 - Loss: 30.0730\n",
      "Processing batch 1641/11884 - Loss: 29.7793\n",
      "Processing batch 1642/11884 - Loss: 29.0054\n",
      "Processing batch 1643/11884 - Loss: 29.9922\n",
      "Processing batch 1644/11884 - Loss: 30.3153\n",
      "Processing batch 1645/11884 - Loss: 31.6232\n",
      "Processing batch 1646/11884 - Loss: 29.5996\n",
      "Processing batch 1647/11884 - Loss: 30.6717\n",
      "Processing batch 1648/11884 - Loss: 29.1927\n",
      "Processing batch 1649/11884 - Loss: 31.1263\n",
      "Processing batch 1650/11884 - Loss: 29.2353\n",
      "Processing batch 1651/11884 - Loss: 30.3903\n",
      "Processing batch 1652/11884 - Loss: 29.7254\n",
      "Processing batch 1653/11884 - Loss: 30.0735\n",
      "Processing batch 1654/11884 - Loss: 30.1321\n",
      "Processing batch 1655/11884 - Loss: 30.6212\n",
      "Processing batch 1656/11884 - Loss: 30.3400\n",
      "Processing batch 1657/11884 - Loss: 29.9687\n",
      "Processing batch 1658/11884 - Loss: 30.3237\n",
      "Processing batch 1659/11884 - Loss: 29.9808\n",
      "Processing batch 1660/11884 - Loss: 29.4117\n",
      "Processing batch 1661/11884 - Loss: 30.0815\n",
      "Processing batch 1662/11884 - Loss: 29.7315\n",
      "Processing batch 1663/11884 - Loss: 29.1856\n",
      "Processing batch 1664/11884 - Loss: 31.6697\n",
      "Processing batch 1665/11884 - Loss: 30.6536\n",
      "Processing batch 1666/11884 - Loss: 31.1371\n",
      "Processing batch 1667/11884 - Loss: 30.6084\n",
      "Processing batch 1668/11884 - Loss: 29.1460\n",
      "Processing batch 1669/11884 - Loss: 29.7424\n",
      "Processing batch 1670/11884 - Loss: 31.4394\n",
      "Processing batch 1671/11884 - Loss: 31.0182\n",
      "Processing batch 1672/11884 - Loss: 30.5154\n",
      "Processing batch 1673/11884 - Loss: 29.9189\n",
      "Processing batch 1674/11884 - Loss: 30.8899\n",
      "Processing batch 1675/11884 - Loss: 31.2424\n",
      "Processing batch 1676/11884 - Loss: 29.5790\n",
      "Processing batch 1677/11884 - Loss: 31.6273\n",
      "Processing batch 1678/11884 - Loss: 30.5057\n",
      "Processing batch 1679/11884 - Loss: 29.8758\n",
      "Processing batch 1680/11884 - Loss: 30.1777\n",
      "Processing batch 1681/11884 - Loss: 31.2525\n",
      "Processing batch 1682/11884 - Loss: 29.4738\n",
      "Processing batch 1683/11884 - Loss: 29.5666\n",
      "Processing batch 1684/11884 - Loss: 30.5110\n",
      "Processing batch 1685/11884 - Loss: 29.2211\n",
      "Processing batch 1686/11884 - Loss: 29.5624\n",
      "Processing batch 1687/11884 - Loss: 30.2378\n",
      "Processing batch 1688/11884 - Loss: 30.2983\n",
      "Processing batch 1689/11884 - Loss: 29.9689\n",
      "Processing batch 1690/11884 - Loss: 30.6744\n",
      "Processing batch 1691/11884 - Loss: 29.9059\n",
      "Processing batch 1692/11884 - Loss: 29.8043\n",
      "Processing batch 1693/11884 - Loss: 30.2400\n",
      "Processing batch 1694/11884 - Loss: 30.8680\n",
      "Processing batch 1695/11884 - Loss: 31.3302\n",
      "Processing batch 1696/11884 - Loss: 29.1829\n",
      "Processing batch 1697/11884 - Loss: 29.4922\n",
      "Processing batch 1698/11884 - Loss: 31.0361\n",
      "Processing batch 1699/11884 - Loss: 30.8819\n",
      "Processing batch 1700/11884 - Loss: 29.3459\n",
      "Processing batch 1701/11884 - Loss: 30.1024\n",
      "Processing batch 1702/11884 - Loss: 30.8566\n",
      "Processing batch 1703/11884 - Loss: 30.4657\n",
      "Processing batch 1704/11884 - Loss: 30.3562\n",
      "Processing batch 1705/11884 - Loss: 29.7984\n",
      "Processing batch 1706/11884 - Loss: 30.0988\n",
      "Processing batch 1707/11884 - Loss: 30.5892\n",
      "Processing batch 1708/11884 - Loss: 28.7257\n",
      "Processing batch 1709/11884 - Loss: 30.5163\n",
      "Processing batch 1710/11884 - Loss: 30.9408\n",
      "Processing batch 1711/11884 - Loss: 31.3470\n",
      "Processing batch 1712/11884 - Loss: 31.1371\n",
      "Processing batch 1713/11884 - Loss: 30.4928\n",
      "Processing batch 1714/11884 - Loss: 30.9460\n",
      "Processing batch 1715/11884 - Loss: 30.4711\n",
      "Processing batch 1716/11884 - Loss: 30.4313\n",
      "Processing batch 1717/11884 - Loss: 30.3968\n",
      "Processing batch 1718/11884 - Loss: 30.6937\n",
      "Processing batch 1719/11884 - Loss: 30.2256\n",
      "Processing batch 1720/11884 - Loss: 30.5234\n",
      "Processing batch 1721/11884 - Loss: 30.1184\n",
      "Processing batch 1722/11884 - Loss: 29.9607\n",
      "Processing batch 1723/11884 - Loss: 30.1899\n",
      "Processing batch 1724/11884 - Loss: 29.9443\n",
      "Processing batch 1725/11884 - Loss: 29.3355\n",
      "Processing batch 1726/11884 - Loss: 30.4265\n",
      "Processing batch 1727/11884 - Loss: 30.7206\n",
      "Processing batch 1728/11884 - Loss: 29.9746\n",
      "Processing batch 1729/11884 - Loss: 29.9894\n",
      "Processing batch 1730/11884 - Loss: 28.8655\n",
      "Processing batch 1731/11884 - Loss: 29.3539\n",
      "Processing batch 1732/11884 - Loss: 29.6515\n",
      "Processing batch 1733/11884 - Loss: 28.9204\n",
      "Processing batch 1734/11884 - Loss: 29.1096\n",
      "Processing batch 1735/11884 - Loss: 30.4511\n",
      "Processing batch 1736/11884 - Loss: 31.1225\n",
      "Processing batch 1737/11884 - Loss: 29.9551\n",
      "Processing batch 1738/11884 - Loss: 30.9471\n",
      "Processing batch 1739/11884 - Loss: 30.6779\n",
      "Processing batch 1740/11884 - Loss: 30.3066\n",
      "Processing batch 1741/11884 - Loss: 30.1202\n",
      "Processing batch 1742/11884 - Loss: 30.5262\n",
      "Processing batch 1743/11884 - Loss: 29.6562\n",
      "Processing batch 1744/11884 - Loss: 30.6473\n",
      "Processing batch 1745/11884 - Loss: 30.9698\n",
      "Processing batch 1746/11884 - Loss: 29.5891\n",
      "Processing batch 1747/11884 - Loss: 29.9146\n",
      "Processing batch 1748/11884 - Loss: 29.5053\n",
      "Processing batch 1749/11884 - Loss: 29.5488\n",
      "Processing batch 1750/11884 - Loss: 30.1601\n",
      "Processing batch 1751/11884 - Loss: 31.1340\n",
      "Processing batch 1752/11884 - Loss: 29.9004\n",
      "Processing batch 1753/11884 - Loss: 30.2808\n",
      "Processing batch 1754/11884 - Loss: 31.4379\n",
      "Processing batch 1755/11884 - Loss: 30.4883\n",
      "Processing batch 1756/11884 - Loss: 30.3911\n",
      "Processing batch 1757/11884 - Loss: 29.4218\n",
      "Processing batch 1758/11884 - Loss: 30.4261\n",
      "Processing batch 1759/11884 - Loss: 30.1667\n",
      "Processing batch 1760/11884 - Loss: 28.5490\n",
      "Processing batch 1761/11884 - Loss: 29.9912\n",
      "Processing batch 1762/11884 - Loss: 30.8302\n",
      "Processing batch 1763/11884 - Loss: 30.4359\n",
      "Processing batch 1764/11884 - Loss: 30.5614\n",
      "Processing batch 1765/11884 - Loss: 29.0861\n",
      "Processing batch 1766/11884 - Loss: 30.3948\n",
      "Processing batch 1767/11884 - Loss: 30.5433\n",
      "Processing batch 1768/11884 - Loss: 29.1140\n",
      "Processing batch 1769/11884 - Loss: 31.5512\n",
      "Processing batch 1770/11884 - Loss: 30.3052\n",
      "Processing batch 1771/11884 - Loss: 30.8645\n",
      "Processing batch 1772/11884 - Loss: 31.0495\n",
      "Processing batch 1773/11884 - Loss: 29.9147\n",
      "Processing batch 1774/11884 - Loss: 29.7965\n",
      "Processing batch 1775/11884 - Loss: 30.1468\n",
      "Processing batch 1776/11884 - Loss: 29.7290\n",
      "Processing batch 1777/11884 - Loss: 31.6596\n",
      "Processing batch 1778/11884 - Loss: 28.9160\n",
      "Processing batch 1779/11884 - Loss: 30.2342\n",
      "Processing batch 1780/11884 - Loss: 29.7369\n",
      "Processing batch 1781/11884 - Loss: 31.0496\n",
      "Processing batch 1782/11884 - Loss: 30.9032\n",
      "Processing batch 1783/11884 - Loss: 29.5474\n",
      "Processing batch 1784/11884 - Loss: 30.7072\n",
      "Processing batch 1785/11884 - Loss: 29.9448\n",
      "Processing batch 1786/11884 - Loss: 29.7912\n",
      "Processing batch 1787/11884 - Loss: 28.6269\n",
      "Processing batch 1788/11884 - Loss: 29.9050\n",
      "Processing batch 1789/11884 - Loss: 30.5672\n",
      "Processing batch 1790/11884 - Loss: 28.5871\n",
      "Processing batch 1791/11884 - Loss: 29.9573\n",
      "Processing batch 1792/11884 - Loss: 30.4295\n",
      "Processing batch 1793/11884 - Loss: 29.8123\n",
      "Processing batch 1794/11884 - Loss: 30.0319\n",
      "Processing batch 1795/11884 - Loss: 29.1716\n",
      "Processing batch 1796/11884 - Loss: 30.2094\n",
      "Processing batch 1797/11884 - Loss: 29.7844\n",
      "Processing batch 1798/11884 - Loss: 29.2498\n",
      "Processing batch 1799/11884 - Loss: 29.9243\n",
      "Processing batch 1800/11884 - Loss: 29.0705\n",
      "Processing batch 1801/11884 - Loss: 30.3781\n",
      "Processing batch 1802/11884 - Loss: 28.5405\n",
      "Processing batch 1803/11884 - Loss: 32.2031\n",
      "Processing batch 1804/11884 - Loss: 30.7270\n",
      "Processing batch 1805/11884 - Loss: 30.4912\n",
      "Processing batch 1806/11884 - Loss: 32.0160\n",
      "Processing batch 1807/11884 - Loss: 31.1726\n",
      "Processing batch 1808/11884 - Loss: 30.0710\n",
      "Processing batch 1809/11884 - Loss: 30.2328\n",
      "Processing batch 1810/11884 - Loss: 30.7990\n",
      "Processing batch 1811/11884 - Loss: 28.9470\n",
      "Processing batch 1812/11884 - Loss: 30.5101\n",
      "Processing batch 1813/11884 - Loss: 29.6288\n",
      "Processing batch 1814/11884 - Loss: 29.4366\n",
      "Processing batch 1815/11884 - Loss: 29.0420\n",
      "Processing batch 1816/11884 - Loss: 30.4223\n",
      "Processing batch 1817/11884 - Loss: 30.6511\n",
      "Processing batch 1818/11884 - Loss: 30.1941\n",
      "Processing batch 1819/11884 - Loss: 29.8229\n",
      "Processing batch 1820/11884 - Loss: 30.5244\n",
      "Processing batch 1821/11884 - Loss: 28.8851\n",
      "Processing batch 1822/11884 - Loss: 30.7281\n",
      "Processing batch 1823/11884 - Loss: 30.2534\n",
      "Processing batch 1824/11884 - Loss: 30.4380\n",
      "Processing batch 1825/11884 - Loss: 29.5399\n",
      "Processing batch 1826/11884 - Loss: 29.6266\n",
      "Processing batch 1827/11884 - Loss: 30.0122\n",
      "Processing batch 1828/11884 - Loss: 30.0536\n",
      "Processing batch 1829/11884 - Loss: 31.5404\n",
      "Processing batch 1830/11884 - Loss: 30.3904\n",
      "Processing batch 1831/11884 - Loss: 30.9602\n",
      "Processing batch 1832/11884 - Loss: 30.9828\n",
      "Processing batch 1833/11884 - Loss: 29.0345\n",
      "Processing batch 1834/11884 - Loss: 30.0419\n",
      "Processing batch 1835/11884 - Loss: 30.2499\n",
      "Processing batch 1836/11884 - Loss: 30.5150\n",
      "Processing batch 1837/11884 - Loss: 30.2980\n",
      "Processing batch 1838/11884 - Loss: 30.5271\n",
      "Processing batch 1839/11884 - Loss: 29.6230\n",
      "Processing batch 1840/11884 - Loss: 28.9022\n",
      "Processing batch 1841/11884 - Loss: 30.2697\n",
      "Processing batch 1842/11884 - Loss: 29.4822\n",
      "Processing batch 1843/11884 - Loss: 31.1272\n",
      "Processing batch 1844/11884 - Loss: 29.9709\n",
      "Processing batch 1845/11884 - Loss: 30.2108\n",
      "Processing batch 1846/11884 - Loss: 30.6822\n",
      "Processing batch 1847/11884 - Loss: 30.4518\n",
      "Processing batch 1848/11884 - Loss: 30.4257\n",
      "Processing batch 1849/11884 - Loss: 29.3518\n",
      "Processing batch 1850/11884 - Loss: 30.4370\n",
      "Processing batch 1851/11884 - Loss: 29.9662\n",
      "Processing batch 1852/11884 - Loss: 30.2694\n",
      "Processing batch 1853/11884 - Loss: 30.5886\n",
      "Processing batch 1854/11884 - Loss: 29.1768\n",
      "Processing batch 1855/11884 - Loss: 30.1822\n",
      "Processing batch 1856/11884 - Loss: 30.8518\n",
      "Processing batch 1857/11884 - Loss: 29.9507\n",
      "Processing batch 1858/11884 - Loss: 30.4956\n",
      "Processing batch 1859/11884 - Loss: 30.9054\n",
      "Processing batch 1860/11884 - Loss: 30.3699\n",
      "Processing batch 1861/11884 - Loss: 30.2171\n",
      "Processing batch 1862/11884 - Loss: 31.8746\n",
      "Processing batch 1863/11884 - Loss: 29.8435\n",
      "Processing batch 1864/11884 - Loss: 29.7149\n",
      "Processing batch 1865/11884 - Loss: 30.8355\n",
      "Processing batch 1866/11884 - Loss: 30.0576\n",
      "Processing batch 1867/11884 - Loss: 30.8577\n",
      "Processing batch 1868/11884 - Loss: 30.8714\n",
      "Processing batch 1869/11884 - Loss: 30.4494\n",
      "Processing batch 1870/11884 - Loss: 30.2408\n",
      "Processing batch 1871/11884 - Loss: 30.0849\n",
      "Processing batch 1872/11884 - Loss: 31.0106\n",
      "Processing batch 1873/11884 - Loss: 31.3555\n",
      "Processing batch 1874/11884 - Loss: 30.3675\n",
      "Processing batch 1875/11884 - Loss: 29.9239\n",
      "Processing batch 1876/11884 - Loss: 29.9377\n",
      "Processing batch 1877/11884 - Loss: 29.6769\n",
      "Processing batch 1878/11884 - Loss: 30.7801\n",
      "Processing batch 1879/11884 - Loss: 30.2503\n",
      "Processing batch 1880/11884 - Loss: 31.6816\n",
      "Processing batch 1881/11884 - Loss: 30.6214\n",
      "Processing batch 1882/11884 - Loss: 30.5100\n",
      "Processing batch 1883/11884 - Loss: 29.9737\n",
      "Processing batch 1884/11884 - Loss: 30.9467\n",
      "Processing batch 1885/11884 - Loss: 29.6862\n",
      "Processing batch 1886/11884 - Loss: 29.6850\n",
      "Processing batch 1887/11884 - Loss: 30.8635\n",
      "Processing batch 1888/11884 - Loss: 29.5776\n",
      "Processing batch 1889/11884 - Loss: 30.3832\n",
      "Processing batch 1890/11884 - Loss: 32.1045\n",
      "Processing batch 1891/11884 - Loss: 30.1247\n",
      "Processing batch 1892/11884 - Loss: 28.3237\n",
      "Processing batch 1893/11884 - Loss: 29.5580\n",
      "Processing batch 1894/11884 - Loss: 31.4053\n",
      "Processing batch 1895/11884 - Loss: 31.3042\n",
      "Processing batch 1896/11884 - Loss: 31.0072\n",
      "Processing batch 1897/11884 - Loss: 31.2283\n",
      "Processing batch 1898/11884 - Loss: 30.3057\n",
      "Processing batch 1899/11884 - Loss: 30.1035\n",
      "Processing batch 1900/11884 - Loss: 30.1557\n",
      "Processing batch 1901/11884 - Loss: 29.6137\n",
      "Processing batch 1902/11884 - Loss: 31.2556\n",
      "Processing batch 1903/11884 - Loss: 30.2691\n",
      "Processing batch 1904/11884 - Loss: 30.4689\n",
      "Processing batch 1905/11884 - Loss: 31.2060\n",
      "Processing batch 1906/11884 - Loss: 30.8293\n",
      "Processing batch 1907/11884 - Loss: 30.4275\n",
      "Processing batch 1908/11884 - Loss: 30.6768\n",
      "Processing batch 1909/11884 - Loss: 30.1161\n",
      "Processing batch 1910/11884 - Loss: 30.2900\n",
      "Processing batch 1911/11884 - Loss: 30.1587\n",
      "Processing batch 1912/11884 - Loss: 30.1948\n",
      "Processing batch 1913/11884 - Loss: 30.0786\n",
      "Processing batch 1914/11884 - Loss: 31.7217\n",
      "Processing batch 1915/11884 - Loss: 29.4579\n",
      "Processing batch 1916/11884 - Loss: 29.6366\n",
      "Processing batch 1917/11884 - Loss: 29.6755\n",
      "Processing batch 1918/11884 - Loss: 29.6433\n",
      "Processing batch 1919/11884 - Loss: 30.8914\n",
      "Processing batch 1920/11884 - Loss: 30.1297\n",
      "Processing batch 1921/11884 - Loss: 28.9519\n",
      "Processing batch 1922/11884 - Loss: 29.5452\n",
      "Processing batch 1923/11884 - Loss: 31.0604\n",
      "Processing batch 1924/11884 - Loss: 29.4714\n",
      "Processing batch 1925/11884 - Loss: 30.3889\n",
      "Processing batch 1926/11884 - Loss: 29.7633\n",
      "Processing batch 1927/11884 - Loss: 29.8509\n",
      "Processing batch 1928/11884 - Loss: 29.5396\n",
      "Processing batch 1929/11884 - Loss: 30.3138\n",
      "Processing batch 1930/11884 - Loss: 29.6623\n",
      "Processing batch 1931/11884 - Loss: 30.2651\n",
      "Processing batch 1932/11884 - Loss: 29.9308\n",
      "Processing batch 1933/11884 - Loss: 29.2379\n",
      "Processing batch 1934/11884 - Loss: 29.9485\n",
      "Processing batch 1935/11884 - Loss: 30.3000\n",
      "Processing batch 1936/11884 - Loss: 29.9897\n",
      "Processing batch 1937/11884 - Loss: 29.9326\n",
      "Processing batch 1938/11884 - Loss: 29.4436\n",
      "Processing batch 1939/11884 - Loss: 30.4339\n",
      "Processing batch 1940/11884 - Loss: 29.9090\n",
      "Processing batch 1941/11884 - Loss: 30.3272\n",
      "Processing batch 1942/11884 - Loss: 31.1067\n",
      "Processing batch 1943/11884 - Loss: 31.1470\n",
      "Processing batch 1944/11884 - Loss: 31.4346\n",
      "Processing batch 1945/11884 - Loss: 31.0475\n",
      "Processing batch 1946/11884 - Loss: 28.5134\n",
      "Processing batch 1947/11884 - Loss: 29.7094\n",
      "Processing batch 1948/11884 - Loss: 30.3207\n",
      "Processing batch 1949/11884 - Loss: 30.2429\n",
      "Processing batch 1950/11884 - Loss: 31.4816\n",
      "Processing batch 1951/11884 - Loss: 30.3242\n",
      "Processing batch 1952/11884 - Loss: 29.2522\n",
      "Processing batch 1953/11884 - Loss: 30.2887\n",
      "Processing batch 1954/11884 - Loss: 29.6650\n",
      "Processing batch 1955/11884 - Loss: 29.8305\n",
      "Processing batch 1956/11884 - Loss: 30.6308\n",
      "Processing batch 1957/11884 - Loss: 30.9388\n",
      "Processing batch 1958/11884 - Loss: 29.5448\n",
      "Processing batch 1959/11884 - Loss: 29.6191\n",
      "Processing batch 1960/11884 - Loss: 30.5008\n",
      "Processing batch 1961/11884 - Loss: 30.4286\n",
      "Processing batch 1962/11884 - Loss: 30.0958\n",
      "Processing batch 1963/11884 - Loss: 29.6584\n",
      "Processing batch 1964/11884 - Loss: 29.4584\n",
      "Processing batch 1965/11884 - Loss: 29.3283\n",
      "Processing batch 1966/11884 - Loss: 30.0916\n",
      "Processing batch 1967/11884 - Loss: 30.7847\n",
      "Processing batch 1968/11884 - Loss: 29.5499\n",
      "Processing batch 1969/11884 - Loss: 31.6791\n",
      "Processing batch 1970/11884 - Loss: 29.5189\n",
      "Processing batch 1971/11884 - Loss: 30.3724\n",
      "Processing batch 1972/11884 - Loss: 30.3312\n",
      "Processing batch 1973/11884 - Loss: 29.8702\n",
      "Processing batch 1974/11884 - Loss: 31.3816\n",
      "Processing batch 1975/11884 - Loss: 30.9568\n",
      "Processing batch 1976/11884 - Loss: 29.6655\n",
      "Processing batch 1977/11884 - Loss: 29.9057\n",
      "Processing batch 1978/11884 - Loss: 29.4842\n",
      "Processing batch 1979/11884 - Loss: 29.0991\n",
      "Processing batch 1980/11884 - Loss: 30.3337\n",
      "Processing batch 1981/11884 - Loss: 30.1167\n",
      "Processing batch 1982/11884 - Loss: 29.8518\n",
      "Processing batch 1983/11884 - Loss: 30.5895\n",
      "Processing batch 1984/11884 - Loss: 30.5730\n",
      "Processing batch 1985/11884 - Loss: 30.7582\n",
      "Processing batch 1986/11884 - Loss: 30.7252\n",
      "Processing batch 1987/11884 - Loss: 30.6264\n",
      "Processing batch 1988/11884 - Loss: 31.1203\n",
      "Processing batch 1989/11884 - Loss: 30.6723\n",
      "Processing batch 1990/11884 - Loss: 29.7905\n",
      "Processing batch 1991/11884 - Loss: 31.5820\n",
      "Processing batch 1992/11884 - Loss: 30.0250\n",
      "Processing batch 1993/11884 - Loss: 30.4312\n",
      "Processing batch 1994/11884 - Loss: 30.3366\n",
      "Processing batch 1995/11884 - Loss: 30.7182\n",
      "Processing batch 1996/11884 - Loss: 29.9065\n",
      "Processing batch 1997/11884 - Loss: 30.4652\n",
      "Processing batch 1998/11884 - Loss: 30.4700\n",
      "Processing batch 1999/11884 - Loss: 29.3828\n",
      "Processing batch 2000/11884 - Loss: 29.6376\n",
      "Processing batch 2001/11884 - Loss: 30.8943\n",
      "Processing batch 2002/11884 - Loss: 31.3403\n",
      "Processing batch 2003/11884 - Loss: 30.1465\n",
      "Processing batch 2004/11884 - Loss: 30.6083\n",
      "Processing batch 2005/11884 - Loss: 31.0851\n",
      "Processing batch 2006/11884 - Loss: 30.5646\n",
      "Processing batch 2007/11884 - Loss: 29.9942\n",
      "Processing batch 2008/11884 - Loss: 30.6247\n",
      "Processing batch 2009/11884 - Loss: 29.5222\n",
      "Processing batch 2010/11884 - Loss: 30.1502\n",
      "Processing batch 2011/11884 - Loss: 29.4551\n",
      "Processing batch 2012/11884 - Loss: 30.9103\n",
      "Processing batch 2013/11884 - Loss: 29.2215\n",
      "Processing batch 2014/11884 - Loss: 29.7723\n",
      "Processing batch 2015/11884 - Loss: 29.2736\n",
      "Processing batch 2016/11884 - Loss: 30.6616\n",
      "Processing batch 2017/11884 - Loss: 30.1315\n",
      "Processing batch 2018/11884 - Loss: 29.9979\n",
      "Processing batch 2019/11884 - Loss: 30.2863\n",
      "Processing batch 2020/11884 - Loss: 31.7048\n",
      "Processing batch 2021/11884 - Loss: 29.8321\n",
      "Processing batch 2022/11884 - Loss: 29.5853\n",
      "Processing batch 2023/11884 - Loss: 32.0840\n",
      "Processing batch 2024/11884 - Loss: 30.7607\n",
      "Processing batch 2025/11884 - Loss: 30.6948\n",
      "Processing batch 2026/11884 - Loss: 31.5282\n",
      "Processing batch 2027/11884 - Loss: 30.9580\n",
      "Processing batch 2028/11884 - Loss: 28.7359\n",
      "Processing batch 2029/11884 - Loss: 29.9494\n",
      "Processing batch 2030/11884 - Loss: 31.3016\n",
      "Processing batch 2031/11884 - Loss: 29.7449\n",
      "Processing batch 2032/11884 - Loss: 29.4042\n",
      "Processing batch 2033/11884 - Loss: 30.0517\n",
      "Processing batch 2034/11884 - Loss: 29.9745\n",
      "Processing batch 2035/11884 - Loss: 30.9222\n",
      "Processing batch 2036/11884 - Loss: 29.4642\n",
      "Processing batch 2037/11884 - Loss: 30.3144\n",
      "Processing batch 2038/11884 - Loss: 30.2841\n",
      "Processing batch 2039/11884 - Loss: 29.6388\n",
      "Processing batch 2040/11884 - Loss: 29.2638\n",
      "Processing batch 2041/11884 - Loss: 30.2955\n",
      "Processing batch 2042/11884 - Loss: 30.3925\n",
      "Processing batch 2043/11884 - Loss: 30.1147\n",
      "Processing batch 2044/11884 - Loss: 29.7441\n",
      "Processing batch 2045/11884 - Loss: 31.4301\n",
      "Processing batch 2046/11884 - Loss: 29.7808\n",
      "Processing batch 2047/11884 - Loss: 29.3217\n",
      "Processing batch 2048/11884 - Loss: 29.8246\n",
      "Processing batch 2049/11884 - Loss: 30.4520\n",
      "Processing batch 2050/11884 - Loss: 29.9649\n",
      "Processing batch 2051/11884 - Loss: 29.7244\n",
      "Processing batch 2052/11884 - Loss: 28.9847\n",
      "Processing batch 2053/11884 - Loss: 29.8202\n",
      "Processing batch 2054/11884 - Loss: 30.3830\n",
      "Processing batch 2055/11884 - Loss: 29.0081\n",
      "Processing batch 2056/11884 - Loss: 29.8600\n",
      "Processing batch 2057/11884 - Loss: 31.2790\n",
      "Processing batch 2058/11884 - Loss: 31.0008\n",
      "Processing batch 2059/11884 - Loss: 29.3563\n",
      "Processing batch 2060/11884 - Loss: 31.1804\n",
      "Processing batch 2061/11884 - Loss: 30.2608\n",
      "Processing batch 2062/11884 - Loss: 31.8017\n",
      "Processing batch 2063/11884 - Loss: 30.3859\n",
      "Processing batch 2064/11884 - Loss: 31.8596\n",
      "Processing batch 2065/11884 - Loss: 30.8530\n",
      "Processing batch 2066/11884 - Loss: 31.7612\n",
      "Processing batch 2067/11884 - Loss: 30.6366\n",
      "Processing batch 2068/11884 - Loss: 30.7459\n",
      "Processing batch 2069/11884 - Loss: 29.8572\n",
      "Processing batch 2070/11884 - Loss: 29.6054\n",
      "Processing batch 2071/11884 - Loss: 29.9385\n",
      "Processing batch 2072/11884 - Loss: 30.1902\n",
      "Processing batch 2073/11884 - Loss: 30.2825\n",
      "Processing batch 2074/11884 - Loss: 31.2557\n",
      "Processing batch 2075/11884 - Loss: 30.3808\n",
      "Processing batch 2076/11884 - Loss: 30.7831\n",
      "Processing batch 2077/11884 - Loss: 30.7558\n",
      "Processing batch 2078/11884 - Loss: 32.2204\n",
      "Processing batch 2079/11884 - Loss: 29.9171\n",
      "Processing batch 2080/11884 - Loss: 29.5679\n",
      "Processing batch 2081/11884 - Loss: 29.9664\n",
      "Processing batch 2082/11884 - Loss: 31.4883\n",
      "Processing batch 2083/11884 - Loss: 30.2313\n",
      "Processing batch 2084/11884 - Loss: 30.7206\n",
      "Processing batch 2085/11884 - Loss: 29.6049\n",
      "Processing batch 2086/11884 - Loss: 30.7303\n",
      "Processing batch 2087/11884 - Loss: 29.7613\n",
      "Processing batch 2088/11884 - Loss: 31.6144\n",
      "Processing batch 2089/11884 - Loss: 30.6701\n",
      "Processing batch 2090/11884 - Loss: 30.8257\n",
      "Processing batch 2091/11884 - Loss: 29.6032\n",
      "Processing batch 2092/11884 - Loss: 31.6742\n",
      "Processing batch 2093/11884 - Loss: 30.2371\n",
      "Processing batch 2094/11884 - Loss: 31.1668\n",
      "Processing batch 2095/11884 - Loss: 30.1108\n",
      "Processing batch 2096/11884 - Loss: 30.1210\n",
      "Processing batch 2097/11884 - Loss: 28.6298\n",
      "Processing batch 2098/11884 - Loss: 30.4847\n",
      "Processing batch 2099/11884 - Loss: 29.5033\n",
      "Processing batch 2100/11884 - Loss: 30.3998\n",
      "Processing batch 2101/11884 - Loss: 30.6329\n",
      "Processing batch 2102/11884 - Loss: 30.3168\n",
      "Processing batch 2103/11884 - Loss: 29.0136\n",
      "Processing batch 2104/11884 - Loss: 29.9474\n",
      "Processing batch 2105/11884 - Loss: 30.2923\n",
      "Processing batch 2106/11884 - Loss: 29.6661\n",
      "Processing batch 2107/11884 - Loss: 31.3473\n",
      "Processing batch 2108/11884 - Loss: 31.4405\n",
      "Processing batch 2109/11884 - Loss: 32.0217\n",
      "Processing batch 2110/11884 - Loss: 30.3428\n",
      "Processing batch 2111/11884 - Loss: 30.6520\n",
      "Processing batch 2112/11884 - Loss: 29.9860\n",
      "Processing batch 2113/11884 - Loss: 29.5155\n",
      "Processing batch 2114/11884 - Loss: 30.5925\n",
      "Processing batch 2115/11884 - Loss: 30.0620\n",
      "Processing batch 2116/11884 - Loss: 30.3375\n",
      "Processing batch 2117/11884 - Loss: 29.9919\n",
      "Processing batch 2118/11884 - Loss: 29.7756\n",
      "Processing batch 2119/11884 - Loss: 29.9946\n",
      "Processing batch 2120/11884 - Loss: 30.6729\n",
      "Processing batch 2121/11884 - Loss: 29.5570\n",
      "Processing batch 2122/11884 - Loss: 30.2312\n",
      "Processing batch 2123/11884 - Loss: 30.2529\n",
      "Processing batch 2124/11884 - Loss: 30.8656\n",
      "Processing batch 2125/11884 - Loss: 30.8708\n",
      "Processing batch 2126/11884 - Loss: 30.6990\n",
      "Processing batch 2127/11884 - Loss: 30.8704\n",
      "Processing batch 2128/11884 - Loss: 28.9771\n",
      "Processing batch 2129/11884 - Loss: 30.0198\n",
      "Processing batch 2130/11884 - Loss: 29.8653\n",
      "Processing batch 2131/11884 - Loss: 29.9527\n",
      "Processing batch 2132/11884 - Loss: 29.9133\n",
      "Processing batch 2133/11884 - Loss: 31.0464\n",
      "Processing batch 2134/11884 - Loss: 30.4747\n",
      "Processing batch 2135/11884 - Loss: 30.1971\n",
      "Processing batch 2136/11884 - Loss: 31.3421\n",
      "Processing batch 2137/11884 - Loss: 30.0142\n",
      "Processing batch 2138/11884 - Loss: 31.5700\n",
      "Processing batch 2139/11884 - Loss: 29.2715\n",
      "Processing batch 2140/11884 - Loss: 31.3957\n",
      "Processing batch 2141/11884 - Loss: 28.5897\n",
      "Processing batch 2142/11884 - Loss: 30.5212\n",
      "Processing batch 2143/11884 - Loss: 30.4497\n",
      "Processing batch 2144/11884 - Loss: 29.6246\n",
      "Processing batch 2145/11884 - Loss: 29.8937\n",
      "Processing batch 2146/11884 - Loss: 30.1327\n",
      "Processing batch 2147/11884 - Loss: 29.9713\n",
      "Processing batch 2148/11884 - Loss: 30.2970\n",
      "Processing batch 2149/11884 - Loss: 31.1768\n",
      "Processing batch 2150/11884 - Loss: 30.3614\n",
      "Processing batch 2151/11884 - Loss: 28.6506\n",
      "Processing batch 2152/11884 - Loss: 29.5838\n",
      "Processing batch 2153/11884 - Loss: 28.8955\n",
      "Processing batch 2154/11884 - Loss: 30.4065\n",
      "Processing batch 2155/11884 - Loss: 29.7554\n",
      "Processing batch 2156/11884 - Loss: 29.8152\n",
      "Processing batch 2157/11884 - Loss: 29.8168\n",
      "Processing batch 2158/11884 - Loss: 30.1986\n",
      "Processing batch 2159/11884 - Loss: 31.1685\n",
      "Processing batch 2160/11884 - Loss: 29.4064\n",
      "Processing batch 2161/11884 - Loss: 27.7057\n",
      "Processing batch 2162/11884 - Loss: 30.3894\n",
      "Processing batch 2163/11884 - Loss: 30.2542\n",
      "Processing batch 2164/11884 - Loss: 29.1562\n",
      "Processing batch 2165/11884 - Loss: 30.7052\n",
      "Processing batch 2166/11884 - Loss: 29.5384\n",
      "Processing batch 2167/11884 - Loss: 29.6194\n",
      "Processing batch 2168/11884 - Loss: 31.2140\n",
      "Processing batch 2169/11884 - Loss: 30.1932\n",
      "Processing batch 2170/11884 - Loss: 30.1085\n",
      "Processing batch 2171/11884 - Loss: 30.1491\n",
      "Processing batch 2172/11884 - Loss: 30.9827\n",
      "Processing batch 2173/11884 - Loss: 30.4714\n",
      "Processing batch 2174/11884 - Loss: 30.2285\n",
      "Processing batch 2175/11884 - Loss: 31.2511\n",
      "Processing batch 2176/11884 - Loss: 29.5612\n",
      "Processing batch 2177/11884 - Loss: 30.9406\n",
      "Processing batch 2178/11884 - Loss: 30.4677\n",
      "Processing batch 2179/11884 - Loss: 29.3971\n",
      "Processing batch 2180/11884 - Loss: 29.6321\n",
      "Processing batch 2181/11884 - Loss: 30.4337\n",
      "Processing batch 2182/11884 - Loss: 31.4729\n",
      "Processing batch 2183/11884 - Loss: 28.5359\n",
      "Processing batch 2184/11884 - Loss: 31.2176\n",
      "Processing batch 2185/11884 - Loss: 28.7512\n",
      "Processing batch 2186/11884 - Loss: 29.1436\n",
      "Processing batch 2187/11884 - Loss: 30.4301\n",
      "Processing batch 2188/11884 - Loss: 30.9781\n",
      "Processing batch 2189/11884 - Loss: 29.3499\n",
      "Processing batch 2190/11884 - Loss: 30.5309\n",
      "Processing batch 2191/11884 - Loss: 31.7444\n",
      "Processing batch 2192/11884 - Loss: 30.7026\n",
      "Processing batch 2193/11884 - Loss: 30.1046\n",
      "Processing batch 2194/11884 - Loss: 30.7529\n",
      "Processing batch 2195/11884 - Loss: 29.7840\n",
      "Processing batch 2196/11884 - Loss: 31.2520\n",
      "Processing batch 2197/11884 - Loss: 30.6276\n",
      "Processing batch 2198/11884 - Loss: 30.9587\n",
      "Processing batch 2199/11884 - Loss: 30.4541\n",
      "Processing batch 2200/11884 - Loss: 31.8049\n",
      "Processing batch 2201/11884 - Loss: 30.9462\n",
      "Processing batch 2202/11884 - Loss: 30.6481\n",
      "Processing batch 2203/11884 - Loss: 28.4689\n",
      "Processing batch 2204/11884 - Loss: 29.8693\n",
      "Processing batch 2205/11884 - Loss: 31.3849\n",
      "Processing batch 2206/11884 - Loss: 29.4700\n",
      "Processing batch 2207/11884 - Loss: 31.1653\n",
      "Processing batch 2208/11884 - Loss: 31.5328\n",
      "Processing batch 2209/11884 - Loss: 31.2439\n",
      "Processing batch 2210/11884 - Loss: 29.6086\n",
      "Processing batch 2211/11884 - Loss: 30.7151\n",
      "Processing batch 2212/11884 - Loss: 29.3714\n",
      "Processing batch 2213/11884 - Loss: 29.5269\n",
      "Processing batch 2214/11884 - Loss: 30.6319\n",
      "Processing batch 2215/11884 - Loss: 30.2419\n",
      "Processing batch 2216/11884 - Loss: 29.5756\n",
      "Processing batch 2217/11884 - Loss: 31.0406\n",
      "Processing batch 2218/11884 - Loss: 30.3417\n",
      "Processing batch 2219/11884 - Loss: 29.3501\n",
      "Processing batch 2220/11884 - Loss: 30.2880\n",
      "Processing batch 2221/11884 - Loss: 30.6784\n",
      "Processing batch 2222/11884 - Loss: 29.9087\n",
      "Processing batch 2223/11884 - Loss: 29.4399\n",
      "Processing batch 2224/11884 - Loss: 28.7102\n",
      "Processing batch 2225/11884 - Loss: 30.7169\n",
      "Processing batch 2226/11884 - Loss: 30.1285\n",
      "Processing batch 2227/11884 - Loss: 29.7364\n",
      "Processing batch 2228/11884 - Loss: 29.3306\n",
      "Processing batch 2229/11884 - Loss: 31.1315\n",
      "Processing batch 2230/11884 - Loss: 32.4805\n",
      "Processing batch 2231/11884 - Loss: 30.2940\n",
      "Processing batch 2232/11884 - Loss: 29.7434\n",
      "Processing batch 2233/11884 - Loss: 30.1045\n",
      "Processing batch 2234/11884 - Loss: 30.1486\n",
      "Processing batch 2235/11884 - Loss: 30.0442\n",
      "Processing batch 2236/11884 - Loss: 31.0577\n",
      "Processing batch 2237/11884 - Loss: 30.0640\n",
      "Processing batch 2238/11884 - Loss: 30.3710\n",
      "Processing batch 2239/11884 - Loss: 29.5655\n",
      "Processing batch 2240/11884 - Loss: 30.8341\n",
      "Processing batch 2241/11884 - Loss: 29.7453\n",
      "Processing batch 2242/11884 - Loss: 29.7681\n",
      "Processing batch 2243/11884 - Loss: 31.2067\n",
      "Processing batch 2244/11884 - Loss: 30.6348\n",
      "Processing batch 2245/11884 - Loss: 29.9764\n",
      "Processing batch 2246/11884 - Loss: 30.3408\n",
      "Processing batch 2247/11884 - Loss: 31.7815\n",
      "Processing batch 2248/11884 - Loss: 30.3910\n",
      "Processing batch 2249/11884 - Loss: 30.7715\n",
      "Processing batch 2250/11884 - Loss: 29.2493\n",
      "Processing batch 2251/11884 - Loss: 30.4506\n",
      "Processing batch 2252/11884 - Loss: 29.4685\n",
      "Processing batch 2253/11884 - Loss: 29.1809\n",
      "Processing batch 2254/11884 - Loss: 30.7392\n",
      "Processing batch 2255/11884 - Loss: 29.8603\n",
      "Processing batch 2256/11884 - Loss: 31.0011\n",
      "Processing batch 2257/11884 - Loss: 30.3138\n",
      "Processing batch 2258/11884 - Loss: 30.2534\n",
      "Processing batch 2259/11884 - Loss: 30.1875\n",
      "Processing batch 2260/11884 - Loss: 30.2867\n",
      "Processing batch 2261/11884 - Loss: 29.8409\n",
      "Processing batch 2262/11884 - Loss: 30.8439\n",
      "Processing batch 2263/11884 - Loss: 31.3394\n",
      "Processing batch 2264/11884 - Loss: 30.0165\n",
      "Processing batch 2265/11884 - Loss: 30.7951\n",
      "Processing batch 2266/11884 - Loss: 29.4532\n",
      "Processing batch 2267/11884 - Loss: 28.7568\n",
      "Processing batch 2268/11884 - Loss: 30.1944\n",
      "Processing batch 2269/11884 - Loss: 30.2359\n",
      "Processing batch 2270/11884 - Loss: 30.4929\n",
      "Processing batch 2271/11884 - Loss: 28.9275\n",
      "Processing batch 2272/11884 - Loss: 29.9314\n",
      "Processing batch 2273/11884 - Loss: 30.3103\n",
      "Processing batch 2274/11884 - Loss: 28.9595\n",
      "Processing batch 2275/11884 - Loss: 30.0842\n",
      "Processing batch 2276/11884 - Loss: 30.2031\n",
      "Processing batch 2277/11884 - Loss: 31.0799\n",
      "Processing batch 2278/11884 - Loss: 28.5807\n",
      "Processing batch 2279/11884 - Loss: 31.1136\n",
      "Processing batch 2280/11884 - Loss: 29.9650\n",
      "Processing batch 2281/11884 - Loss: 31.4913\n",
      "Processing batch 2282/11884 - Loss: 29.5161\n",
      "Processing batch 2283/11884 - Loss: 32.2225\n",
      "Processing batch 2284/11884 - Loss: 30.6389\n",
      "Processing batch 2285/11884 - Loss: 29.6923\n",
      "Processing batch 2286/11884 - Loss: 29.8757\n",
      "Processing batch 2287/11884 - Loss: 31.0754\n",
      "Processing batch 2288/11884 - Loss: 29.3155\n",
      "Processing batch 2289/11884 - Loss: 30.5087\n",
      "Processing batch 2290/11884 - Loss: 29.7613\n",
      "Processing batch 2291/11884 - Loss: 29.9858\n",
      "Processing batch 2292/11884 - Loss: 29.9977\n",
      "Processing batch 2293/11884 - Loss: 31.0112\n",
      "Processing batch 2294/11884 - Loss: 30.7170\n",
      "Processing batch 2295/11884 - Loss: 30.1973\n",
      "Processing batch 2296/11884 - Loss: 30.0347\n",
      "Processing batch 2297/11884 - Loss: 30.6797\n",
      "Processing batch 2298/11884 - Loss: 29.4915\n",
      "Processing batch 2299/11884 - Loss: 31.4130\n",
      "Processing batch 2300/11884 - Loss: 31.8381\n",
      "Processing batch 2301/11884 - Loss: 30.1003\n",
      "Processing batch 2302/11884 - Loss: 31.3211\n",
      "Processing batch 2303/11884 - Loss: 30.1087\n",
      "Processing batch 2304/11884 - Loss: 31.5422\n",
      "Processing batch 2305/11884 - Loss: 29.7187\n",
      "Processing batch 2306/11884 - Loss: 30.4135\n",
      "Processing batch 2307/11884 - Loss: 29.8957\n",
      "Processing batch 2308/11884 - Loss: 29.7023\n",
      "Processing batch 2309/11884 - Loss: 30.0296\n",
      "Processing batch 2310/11884 - Loss: 30.1440\n",
      "Processing batch 2311/11884 - Loss: 29.0778\n",
      "Processing batch 2312/11884 - Loss: 30.7036\n",
      "Processing batch 2313/11884 - Loss: 30.5728\n",
      "Processing batch 2314/11884 - Loss: 30.7216\n",
      "Processing batch 2315/11884 - Loss: 31.7710\n",
      "Processing batch 2316/11884 - Loss: 29.9080\n",
      "Processing batch 2317/11884 - Loss: 29.9214\n",
      "Processing batch 2318/11884 - Loss: 30.5526\n",
      "Processing batch 2319/11884 - Loss: 29.9515\n",
      "Processing batch 2320/11884 - Loss: 29.7210\n",
      "Processing batch 2321/11884 - Loss: 29.3156\n",
      "Processing batch 2322/11884 - Loss: 31.5918\n",
      "Processing batch 2323/11884 - Loss: 30.7184\n",
      "Processing batch 2324/11884 - Loss: 29.1875\n",
      "Processing batch 2325/11884 - Loss: 30.4259\n",
      "Processing batch 2326/11884 - Loss: 29.2774\n",
      "Processing batch 2327/11884 - Loss: 29.9415\n",
      "Processing batch 2328/11884 - Loss: 29.7253\n",
      "Processing batch 2329/11884 - Loss: 30.7202\n",
      "Processing batch 2330/11884 - Loss: 29.9313\n",
      "Processing batch 2331/11884 - Loss: 28.8910\n",
      "Processing batch 2332/11884 - Loss: 28.6566\n",
      "Processing batch 2333/11884 - Loss: 30.1844\n",
      "Processing batch 2334/11884 - Loss: 30.3300\n",
      "Processing batch 2335/11884 - Loss: 29.7746\n",
      "Processing batch 2336/11884 - Loss: 30.1819\n",
      "Processing batch 2337/11884 - Loss: 30.0995\n",
      "Processing batch 2338/11884 - Loss: 31.0502\n",
      "Processing batch 2339/11884 - Loss: 31.1161\n",
      "Processing batch 2340/11884 - Loss: 29.1549\n",
      "Processing batch 2341/11884 - Loss: 29.5222\n",
      "Processing batch 2342/11884 - Loss: 30.4422\n",
      "Processing batch 2343/11884 - Loss: 29.2029\n",
      "Processing batch 2344/11884 - Loss: 28.7341\n",
      "Processing batch 2345/11884 - Loss: 29.3087\n",
      "Processing batch 2346/11884 - Loss: 30.2130\n",
      "Processing batch 2347/11884 - Loss: 29.2925\n",
      "Processing batch 2348/11884 - Loss: 31.5460\n",
      "Processing batch 2349/11884 - Loss: 32.4763\n",
      "Processing batch 2350/11884 - Loss: 29.4049\n",
      "Processing batch 2351/11884 - Loss: 31.6819\n",
      "Processing batch 2352/11884 - Loss: 29.5676\n",
      "Processing batch 2353/11884 - Loss: 32.4325\n",
      "Processing batch 2354/11884 - Loss: 30.4702\n",
      "Processing batch 2355/11884 - Loss: 31.6283\n",
      "Processing batch 2356/11884 - Loss: 29.1490\n",
      "Processing batch 2357/11884 - Loss: 30.3098\n",
      "Processing batch 2358/11884 - Loss: 31.3793\n",
      "Processing batch 2359/11884 - Loss: 31.0858\n",
      "Processing batch 2360/11884 - Loss: 31.9500\n",
      "Processing batch 2361/11884 - Loss: 31.0878\n",
      "Processing batch 2362/11884 - Loss: 30.3869\n",
      "Processing batch 2363/11884 - Loss: 31.0266\n",
      "Processing batch 2364/11884 - Loss: 29.8477\n",
      "Processing batch 2365/11884 - Loss: 29.8930\n",
      "Processing batch 2366/11884 - Loss: 30.2651\n",
      "Processing batch 2367/11884 - Loss: 30.4344\n",
      "Processing batch 2368/11884 - Loss: 30.1944\n",
      "Processing batch 2369/11884 - Loss: 29.2267\n",
      "Processing batch 2370/11884 - Loss: 31.5957\n",
      "Processing batch 2371/11884 - Loss: 30.3887\n",
      "Processing batch 2372/11884 - Loss: 28.9326\n",
      "Processing batch 2373/11884 - Loss: 29.0643\n",
      "Processing batch 2374/11884 - Loss: 30.3867\n",
      "Processing batch 2375/11884 - Loss: 31.1155\n",
      "Processing batch 2376/11884 - Loss: 29.4156\n",
      "Processing batch 2377/11884 - Loss: 31.8952\n",
      "Processing batch 2378/11884 - Loss: 29.6902\n",
      "Processing batch 2379/11884 - Loss: 29.2904\n",
      "Processing batch 2380/11884 - Loss: 30.4172\n",
      "Processing batch 2381/11884 - Loss: 29.7257\n",
      "Processing batch 2382/11884 - Loss: 30.0816\n",
      "Processing batch 2383/11884 - Loss: 30.5439\n",
      "Processing batch 2384/11884 - Loss: 29.3238\n",
      "Processing batch 2385/11884 - Loss: 30.2909\n",
      "Processing batch 2386/11884 - Loss: 29.4316\n",
      "Processing batch 2387/11884 - Loss: 30.6428\n",
      "Processing batch 2388/11884 - Loss: 29.2341\n",
      "Processing batch 2389/11884 - Loss: 30.5662\n",
      "Processing batch 2390/11884 - Loss: 29.8733\n",
      "Processing batch 2391/11884 - Loss: 30.0131\n",
      "Processing batch 2392/11884 - Loss: 30.0356\n",
      "Processing batch 2393/11884 - Loss: 30.3333\n",
      "Processing batch 2394/11884 - Loss: 30.8958\n",
      "Processing batch 2395/11884 - Loss: 31.3871\n",
      "Processing batch 2396/11884 - Loss: 28.5955\n",
      "Processing batch 2397/11884 - Loss: 29.5373\n",
      "Processing batch 2398/11884 - Loss: 30.0860\n",
      "Processing batch 2399/11884 - Loss: 30.2263\n",
      "Processing batch 2400/11884 - Loss: 30.1571\n",
      "Processing batch 2401/11884 - Loss: 30.6866\n",
      "Processing batch 2402/11884 - Loss: 31.0159\n",
      "Processing batch 2403/11884 - Loss: 31.7183\n",
      "Processing batch 2404/11884 - Loss: 30.4036\n",
      "Processing batch 2405/11884 - Loss: 29.6157\n",
      "Processing batch 2406/11884 - Loss: 29.4237\n",
      "Processing batch 2407/11884 - Loss: 31.0032\n",
      "Processing batch 2408/11884 - Loss: 31.3027\n",
      "Processing batch 2409/11884 - Loss: 30.0108\n",
      "Processing batch 2410/11884 - Loss: 29.0501\n",
      "Processing batch 2411/11884 - Loss: 29.8680\n",
      "Processing batch 2412/11884 - Loss: 29.3783\n",
      "Processing batch 2413/11884 - Loss: 29.5324\n",
      "Processing batch 2414/11884 - Loss: 28.3283\n",
      "Processing batch 2415/11884 - Loss: 29.7051\n",
      "Processing batch 2416/11884 - Loss: 30.7502\n",
      "Processing batch 2417/11884 - Loss: 30.9046\n",
      "Processing batch 2418/11884 - Loss: 29.7655\n",
      "Processing batch 2419/11884 - Loss: 29.8073\n",
      "Processing batch 2420/11884 - Loss: 31.0213\n",
      "Processing batch 2421/11884 - Loss: 29.4519\n",
      "Processing batch 2422/11884 - Loss: 30.4937\n",
      "Processing batch 2423/11884 - Loss: 31.4889\n",
      "Processing batch 2424/11884 - Loss: 30.4602\n",
      "Processing batch 2425/11884 - Loss: 31.1951\n",
      "Processing batch 2426/11884 - Loss: 30.7803\n",
      "Processing batch 2427/11884 - Loss: 30.8400\n",
      "Processing batch 2428/11884 - Loss: 29.4623\n",
      "Processing batch 2429/11884 - Loss: 30.5048\n",
      "Processing batch 2430/11884 - Loss: 29.7264\n",
      "Processing batch 2431/11884 - Loss: 30.3850\n",
      "Processing batch 2432/11884 - Loss: 29.8008\n",
      "Processing batch 2433/11884 - Loss: 30.2167\n",
      "Processing batch 2434/11884 - Loss: 30.6284\n",
      "Processing batch 2435/11884 - Loss: 31.0438\n",
      "Processing batch 2436/11884 - Loss: 29.6869\n",
      "Processing batch 2437/11884 - Loss: 29.4073\n",
      "Processing batch 2438/11884 - Loss: 30.7337\n",
      "Processing batch 2439/11884 - Loss: 30.7502\n",
      "Processing batch 2440/11884 - Loss: 31.2446\n",
      "Processing batch 2441/11884 - Loss: 30.6816\n",
      "Processing batch 2442/11884 - Loss: 30.0548\n",
      "Processing batch 2443/11884 - Loss: 31.0233\n",
      "Processing batch 2444/11884 - Loss: 30.7085\n",
      "Processing batch 2445/11884 - Loss: 29.9205\n",
      "Processing batch 2446/11884 - Loss: 29.8304\n",
      "Processing batch 2447/11884 - Loss: 30.1097\n",
      "Processing batch 2448/11884 - Loss: 30.5315\n",
      "Processing batch 2449/11884 - Loss: 30.0240\n",
      "Processing batch 2450/11884 - Loss: 31.1346\n",
      "Processing batch 2451/11884 - Loss: 29.5465\n",
      "Processing batch 2452/11884 - Loss: 31.4490\n",
      "Processing batch 2453/11884 - Loss: 31.2597\n",
      "Processing batch 2454/11884 - Loss: 29.5520\n",
      "Processing batch 2455/11884 - Loss: 28.6653\n",
      "Processing batch 2456/11884 - Loss: 29.8770\n",
      "Processing batch 2457/11884 - Loss: 30.6151\n",
      "Processing batch 2458/11884 - Loss: 29.7134\n",
      "Processing batch 2459/11884 - Loss: 31.1184\n",
      "Processing batch 2460/11884 - Loss: 30.1554\n",
      "Processing batch 2461/11884 - Loss: 30.5246\n",
      "Processing batch 2462/11884 - Loss: 30.6535\n",
      "Processing batch 2463/11884 - Loss: 31.9555\n",
      "Processing batch 2464/11884 - Loss: 30.5970\n",
      "Processing batch 2465/11884 - Loss: 30.0107\n",
      "Processing batch 2466/11884 - Loss: 30.3891\n",
      "Processing batch 2467/11884 - Loss: 29.5706\n",
      "Processing batch 2468/11884 - Loss: 30.9399\n",
      "Processing batch 2469/11884 - Loss: 31.0437\n",
      "Processing batch 2470/11884 - Loss: 29.4522\n",
      "Processing batch 2471/11884 - Loss: 29.9622\n",
      "Processing batch 2472/11884 - Loss: 30.8130\n",
      "Processing batch 2473/11884 - Loss: 30.0182\n",
      "Processing batch 2474/11884 - Loss: 30.6468\n",
      "Processing batch 2475/11884 - Loss: 30.3142\n",
      "Processing batch 2476/11884 - Loss: 29.9669\n",
      "Processing batch 2477/11884 - Loss: 29.7936\n",
      "Processing batch 2478/11884 - Loss: 29.5892\n",
      "Processing batch 2479/11884 - Loss: 29.4264\n",
      "Processing batch 2480/11884 - Loss: 31.0009\n",
      "Processing batch 2481/11884 - Loss: 29.4309\n",
      "Processing batch 2482/11884 - Loss: 29.1057\n",
      "Processing batch 2483/11884 - Loss: 30.6680\n",
      "Processing batch 2484/11884 - Loss: 30.6542\n",
      "Processing batch 2485/11884 - Loss: 31.1215\n",
      "Processing batch 2486/11884 - Loss: 30.3587\n",
      "Processing batch 2487/11884 - Loss: 29.8253\n",
      "Processing batch 2488/11884 - Loss: 29.6437\n",
      "Processing batch 2489/11884 - Loss: 29.7622\n",
      "Processing batch 2490/11884 - Loss: 30.7530\n",
      "Processing batch 2491/11884 - Loss: 30.4638\n",
      "Processing batch 2492/11884 - Loss: 31.2645\n",
      "Processing batch 2493/11884 - Loss: 28.4605\n",
      "Processing batch 2494/11884 - Loss: 30.0220\n",
      "Processing batch 2495/11884 - Loss: 29.8542\n",
      "Processing batch 2496/11884 - Loss: 28.5126\n",
      "Processing batch 2497/11884 - Loss: 31.5591\n",
      "Processing batch 2498/11884 - Loss: 30.2747\n",
      "Processing batch 2499/11884 - Loss: 29.4221\n",
      "Processing batch 2500/11884 - Loss: 30.2704\n",
      "Processing batch 2501/11884 - Loss: 30.4632\n",
      "Processing batch 2502/11884 - Loss: 30.0707\n",
      "Processing batch 2503/11884 - Loss: 30.0279\n",
      "Processing batch 2504/11884 - Loss: 29.6802\n",
      "Processing batch 2505/11884 - Loss: 31.0518\n",
      "Processing batch 2506/11884 - Loss: 29.2338\n",
      "Processing batch 2507/11884 - Loss: 30.7166\n",
      "Processing batch 2508/11884 - Loss: 29.5074\n",
      "Processing batch 2509/11884 - Loss: 30.6377\n",
      "Processing batch 2510/11884 - Loss: 30.0191\n",
      "Processing batch 2511/11884 - Loss: 30.6433\n",
      "Processing batch 2512/11884 - Loss: 30.9149\n",
      "Processing batch 2513/11884 - Loss: 30.9028\n",
      "Processing batch 2514/11884 - Loss: 30.7197\n",
      "Processing batch 2515/11884 - Loss: 30.7776\n",
      "Processing batch 2516/11884 - Loss: 29.5521\n",
      "Processing batch 2517/11884 - Loss: 29.7881\n",
      "Processing batch 2518/11884 - Loss: 30.3264\n",
      "Processing batch 2519/11884 - Loss: 30.3820\n",
      "Processing batch 2520/11884 - Loss: 29.8900\n",
      "Processing batch 2521/11884 - Loss: 29.9880\n",
      "Processing batch 2522/11884 - Loss: 29.9871\n",
      "Processing batch 2523/11884 - Loss: 30.3272\n",
      "Processing batch 2524/11884 - Loss: 30.3958\n",
      "Processing batch 2525/11884 - Loss: 31.3245\n",
      "Processing batch 2526/11884 - Loss: 29.7804\n",
      "Processing batch 2527/11884 - Loss: 30.5178\n",
      "Processing batch 2528/11884 - Loss: 30.6188\n",
      "Processing batch 2529/11884 - Loss: 30.3512\n",
      "Processing batch 2530/11884 - Loss: 29.6036\n",
      "Processing batch 2531/11884 - Loss: 30.2563\n",
      "Processing batch 2532/11884 - Loss: 31.9678\n",
      "Processing batch 2533/11884 - Loss: 29.6174\n",
      "Processing batch 2534/11884 - Loss: 29.4634\n",
      "Processing batch 2535/11884 - Loss: 30.1913\n",
      "Processing batch 2536/11884 - Loss: 30.2239\n",
      "Processing batch 2537/11884 - Loss: 29.9198\n",
      "Processing batch 2538/11884 - Loss: 29.8221\n",
      "Processing batch 2539/11884 - Loss: 29.4463\n",
      "Processing batch 2540/11884 - Loss: 29.4957\n",
      "Processing batch 2541/11884 - Loss: 30.5383\n",
      "Processing batch 2542/11884 - Loss: 30.7036\n",
      "Processing batch 2543/11884 - Loss: 30.3676\n",
      "Processing batch 2544/11884 - Loss: 28.1585\n",
      "Processing batch 2545/11884 - Loss: 29.5508\n",
      "Processing batch 2546/11884 - Loss: 29.4904\n",
      "Processing batch 2547/11884 - Loss: 30.9059\n",
      "Processing batch 2548/11884 - Loss: 30.5693\n",
      "Processing batch 2549/11884 - Loss: 29.3552\n",
      "Processing batch 2550/11884 - Loss: 29.6289\n",
      "Processing batch 2551/11884 - Loss: 30.7474\n",
      "Processing batch 2552/11884 - Loss: 28.7683\n",
      "Processing batch 2553/11884 - Loss: 29.6978\n",
      "Processing batch 2554/11884 - Loss: 29.2741\n",
      "Processing batch 2555/11884 - Loss: 30.3951\n",
      "Processing batch 2556/11884 - Loss: 30.1682\n",
      "Processing batch 2557/11884 - Loss: 30.0050\n",
      "Processing batch 2558/11884 - Loss: 29.7053\n",
      "Processing batch 2559/11884 - Loss: 28.9442\n",
      "Processing batch 2560/11884 - Loss: 30.4360\n",
      "Processing batch 2561/11884 - Loss: 30.6226\n",
      "Processing batch 2562/11884 - Loss: 29.8406\n",
      "Processing batch 2563/11884 - Loss: 31.0187\n",
      "Processing batch 2564/11884 - Loss: 31.6410\n",
      "Processing batch 2565/11884 - Loss: 30.4093\n",
      "Processing batch 2566/11884 - Loss: 30.6066\n",
      "Processing batch 2567/11884 - Loss: 31.2682\n",
      "Processing batch 2568/11884 - Loss: 31.0383\n",
      "Processing batch 2569/11884 - Loss: 29.7149\n",
      "Processing batch 2570/11884 - Loss: 29.6793\n",
      "Processing batch 2571/11884 - Loss: 30.0730\n",
      "Processing batch 2572/11884 - Loss: 31.2566\n",
      "Processing batch 2573/11884 - Loss: 31.1547\n",
      "Processing batch 2574/11884 - Loss: 30.9186\n",
      "Processing batch 2575/11884 - Loss: 30.0506\n",
      "Processing batch 2576/11884 - Loss: 29.9406\n",
      "Processing batch 2577/11884 - Loss: 31.2096\n",
      "Processing batch 2578/11884 - Loss: 30.6729\n",
      "Processing batch 2579/11884 - Loss: 30.3200\n",
      "Processing batch 2580/11884 - Loss: 30.0603\n",
      "Processing batch 2581/11884 - Loss: 29.6188\n",
      "Processing batch 2582/11884 - Loss: 31.0890\n",
      "Processing batch 2583/11884 - Loss: 30.8061\n",
      "Processing batch 2584/11884 - Loss: 29.6581\n",
      "Processing batch 2585/11884 - Loss: 31.2083\n",
      "Processing batch 2586/11884 - Loss: 30.7514\n",
      "Processing batch 2587/11884 - Loss: 31.0862\n",
      "Processing batch 2588/11884 - Loss: 29.4591\n",
      "Processing batch 2589/11884 - Loss: 30.5381\n",
      "Processing batch 2590/11884 - Loss: 30.5328\n",
      "Processing batch 2591/11884 - Loss: 30.2302\n",
      "Processing batch 2592/11884 - Loss: 30.5634\n",
      "Processing batch 2593/11884 - Loss: 29.6353\n",
      "Processing batch 2594/11884 - Loss: 30.3378\n",
      "Processing batch 2595/11884 - Loss: 30.8069\n",
      "Processing batch 2596/11884 - Loss: 29.2767\n",
      "Processing batch 2597/11884 - Loss: 30.8003\n",
      "Processing batch 2598/11884 - Loss: 30.1728\n",
      "Processing batch 2599/11884 - Loss: 29.5253\n",
      "Processing batch 2600/11884 - Loss: 29.4457\n",
      "Processing batch 2601/11884 - Loss: 29.5205\n",
      "Processing batch 2602/11884 - Loss: 30.6710\n",
      "Processing batch 2603/11884 - Loss: 29.5342\n",
      "Processing batch 2604/11884 - Loss: 30.8188\n",
      "Processing batch 2605/11884 - Loss: 29.9980\n",
      "Processing batch 2606/11884 - Loss: 30.4956\n",
      "Processing batch 2607/11884 - Loss: 29.8231\n",
      "Processing batch 2608/11884 - Loss: 30.4198\n",
      "Processing batch 2609/11884 - Loss: 29.1504\n",
      "Processing batch 2610/11884 - Loss: 29.8024\n",
      "Processing batch 2611/11884 - Loss: 30.8303\n",
      "Processing batch 2612/11884 - Loss: 30.6147\n",
      "Processing batch 2613/11884 - Loss: 30.7710\n",
      "Processing batch 2614/11884 - Loss: 29.2351\n",
      "Processing batch 2615/11884 - Loss: 30.2902\n",
      "Processing batch 2616/11884 - Loss: 30.5582\n",
      "Processing batch 2617/11884 - Loss: 30.1721\n",
      "Processing batch 2618/11884 - Loss: 29.5717\n",
      "Processing batch 2619/11884 - Loss: 29.7525\n",
      "Processing batch 2620/11884 - Loss: 30.0151\n",
      "Processing batch 2621/11884 - Loss: 30.1122\n",
      "Processing batch 2622/11884 - Loss: 30.3357\n",
      "Processing batch 2623/11884 - Loss: 31.7779\n",
      "Processing batch 2624/11884 - Loss: 30.2037\n",
      "Processing batch 2625/11884 - Loss: 29.8563\n",
      "Processing batch 2626/11884 - Loss: 31.9114\n",
      "Processing batch 2627/11884 - Loss: 31.8138\n",
      "Processing batch 2628/11884 - Loss: 31.4668\n",
      "Processing batch 2629/11884 - Loss: 29.6928\n",
      "Processing batch 2630/11884 - Loss: 32.1082\n",
      "Processing batch 2631/11884 - Loss: 30.2910\n",
      "Processing batch 2632/11884 - Loss: 30.7934\n",
      "Processing batch 2633/11884 - Loss: 30.7445\n",
      "Processing batch 2634/11884 - Loss: 30.4468\n",
      "Processing batch 2635/11884 - Loss: 29.0967\n",
      "Processing batch 2636/11884 - Loss: 28.7019\n",
      "Processing batch 2637/11884 - Loss: 30.5623\n",
      "Processing batch 2638/11884 - Loss: 30.1098\n",
      "Processing batch 2639/11884 - Loss: 29.5002\n",
      "Processing batch 2640/11884 - Loss: 29.7879\n",
      "Processing batch 2641/11884 - Loss: 30.5100\n",
      "Processing batch 2642/11884 - Loss: 30.9058\n",
      "Processing batch 2643/11884 - Loss: 30.1181\n",
      "Processing batch 2644/11884 - Loss: 28.5540\n",
      "Processing batch 2645/11884 - Loss: 29.4758\n",
      "Processing batch 2646/11884 - Loss: 29.6184\n",
      "Processing batch 2647/11884 - Loss: 30.5632\n",
      "Processing batch 2648/11884 - Loss: 30.1618\n",
      "Processing batch 2649/11884 - Loss: 30.6946\n",
      "Processing batch 2650/11884 - Loss: 29.6227\n",
      "Processing batch 2651/11884 - Loss: 30.8993\n",
      "Processing batch 2652/11884 - Loss: 30.6777\n",
      "Processing batch 2653/11884 - Loss: 29.5884\n",
      "Processing batch 2654/11884 - Loss: 30.7186\n",
      "Processing batch 2655/11884 - Loss: 29.5764\n",
      "Processing batch 2656/11884 - Loss: 29.9398\n",
      "Processing batch 2657/11884 - Loss: 29.9423\n",
      "Processing batch 2658/11884 - Loss: 30.2353\n",
      "Processing batch 2659/11884 - Loss: 30.0077\n",
      "Processing batch 2660/11884 - Loss: 30.0477\n",
      "Processing batch 2661/11884 - Loss: 30.3869\n",
      "Processing batch 2662/11884 - Loss: 30.3441\n",
      "Processing batch 2663/11884 - Loss: 29.3133\n",
      "Processing batch 2664/11884 - Loss: 32.0020\n",
      "Processing batch 2665/11884 - Loss: 30.3270\n",
      "Processing batch 2666/11884 - Loss: 29.1236\n",
      "Processing batch 2667/11884 - Loss: 29.2739\n",
      "Processing batch 2668/11884 - Loss: 30.0027\n",
      "Processing batch 2669/11884 - Loss: 29.4726\n",
      "Processing batch 2670/11884 - Loss: 30.7123\n",
      "Processing batch 2671/11884 - Loss: 29.5192\n",
      "Processing batch 2672/11884 - Loss: 29.7634\n",
      "Processing batch 2673/11884 - Loss: 30.4401\n",
      "Processing batch 2674/11884 - Loss: 31.2673\n",
      "Processing batch 2675/11884 - Loss: 30.4180\n",
      "Processing batch 2676/11884 - Loss: 31.5271\n",
      "Processing batch 2677/11884 - Loss: 29.8912\n",
      "Processing batch 2678/11884 - Loss: 29.7821\n",
      "Processing batch 2679/11884 - Loss: 30.0199\n",
      "Processing batch 2680/11884 - Loss: 29.6044\n",
      "Processing batch 2681/11884 - Loss: 30.4208\n",
      "Processing batch 2682/11884 - Loss: 29.0131\n",
      "Processing batch 2683/11884 - Loss: 30.2197\n",
      "Processing batch 2684/11884 - Loss: 28.8894\n",
      "Processing batch 2685/11884 - Loss: 29.2406\n",
      "Processing batch 2686/11884 - Loss: 30.1096\n",
      "Processing batch 2687/11884 - Loss: 29.5210\n",
      "Processing batch 2688/11884 - Loss: 31.3019\n",
      "Processing batch 2689/11884 - Loss: 29.9602\n",
      "Processing batch 2690/11884 - Loss: 31.0948\n",
      "Processing batch 2691/11884 - Loss: 30.6745\n",
      "Processing batch 2692/11884 - Loss: 29.9421\n",
      "Processing batch 2693/11884 - Loss: 30.4663\n",
      "Processing batch 2694/11884 - Loss: 30.6319\n",
      "Processing batch 2695/11884 - Loss: 30.4382\n",
      "Processing batch 2696/11884 - Loss: 30.3451\n",
      "Processing batch 2697/11884 - Loss: 31.2828\n",
      "Processing batch 2698/11884 - Loss: 30.7669\n",
      "Processing batch 2699/11884 - Loss: 31.0185\n",
      "Processing batch 2700/11884 - Loss: 28.6394\n",
      "Processing batch 2701/11884 - Loss: 30.1219\n",
      "Processing batch 2702/11884 - Loss: 30.4396\n",
      "Processing batch 2703/11884 - Loss: 31.1587\n",
      "Processing batch 2704/11884 - Loss: 30.0259\n",
      "Processing batch 2705/11884 - Loss: 30.2055\n",
      "Processing batch 2706/11884 - Loss: 29.8969\n",
      "Processing batch 2707/11884 - Loss: 29.7697\n",
      "Processing batch 2708/11884 - Loss: 29.8518\n",
      "Processing batch 2709/11884 - Loss: 30.0620\n",
      "Processing batch 2710/11884 - Loss: 29.7121\n",
      "Processing batch 2711/11884 - Loss: 30.8752\n",
      "Processing batch 2712/11884 - Loss: 31.0046\n",
      "Processing batch 2713/11884 - Loss: 29.6019\n",
      "Processing batch 2714/11884 - Loss: 30.9504\n",
      "Processing batch 2715/11884 - Loss: 28.8493\n",
      "Processing batch 2716/11884 - Loss: 30.5804\n",
      "Processing batch 2717/11884 - Loss: 30.5926\n",
      "Processing batch 2718/11884 - Loss: 30.6004\n",
      "Processing batch 2719/11884 - Loss: 29.1013\n",
      "Processing batch 2720/11884 - Loss: 30.9437\n",
      "Processing batch 2721/11884 - Loss: 31.9368\n",
      "Processing batch 2722/11884 - Loss: 29.8271\n",
      "Processing batch 2723/11884 - Loss: 29.8313\n",
      "Processing batch 2724/11884 - Loss: 29.5146\n",
      "Processing batch 2725/11884 - Loss: 29.1280\n",
      "Processing batch 2726/11884 - Loss: 29.8666\n",
      "Processing batch 2727/11884 - Loss: 30.3610\n",
      "Processing batch 2728/11884 - Loss: 30.3009\n",
      "Processing batch 2729/11884 - Loss: 29.9597\n",
      "Processing batch 2730/11884 - Loss: 29.1860\n",
      "Processing batch 2731/11884 - Loss: 29.1529\n",
      "Processing batch 2732/11884 - Loss: 30.1010\n",
      "Processing batch 2733/11884 - Loss: 29.9301\n",
      "Processing batch 2734/11884 - Loss: 30.0788\n",
      "Processing batch 2735/11884 - Loss: 30.4083\n",
      "Processing batch 2736/11884 - Loss: 29.9315\n",
      "Processing batch 2737/11884 - Loss: 31.1737\n",
      "Processing batch 2738/11884 - Loss: 30.2024\n",
      "Processing batch 2739/11884 - Loss: 28.7715\n",
      "Processing batch 2740/11884 - Loss: 30.5331\n",
      "Processing batch 2741/11884 - Loss: 29.3450\n",
      "Processing batch 2742/11884 - Loss: 29.6032\n",
      "Processing batch 2743/11884 - Loss: 30.9479\n",
      "Processing batch 2744/11884 - Loss: 31.3275\n",
      "Processing batch 2745/11884 - Loss: 27.9736\n",
      "Processing batch 2746/11884 - Loss: 29.3436\n",
      "Processing batch 2747/11884 - Loss: 31.4560\n",
      "Processing batch 2748/11884 - Loss: 28.9502\n",
      "Processing batch 2749/11884 - Loss: 29.7955\n",
      "Processing batch 2750/11884 - Loss: 29.4390\n",
      "Processing batch 2751/11884 - Loss: 29.5778\n",
      "Processing batch 2752/11884 - Loss: 29.6320\n",
      "Processing batch 2753/11884 - Loss: 29.2412\n",
      "Processing batch 2754/11884 - Loss: 31.0199\n",
      "Processing batch 2755/11884 - Loss: 29.7079\n",
      "Processing batch 2756/11884 - Loss: 29.4242\n",
      "Processing batch 2757/11884 - Loss: 29.5346\n",
      "Processing batch 2758/11884 - Loss: 32.0423\n",
      "Processing batch 2759/11884 - Loss: 30.6172\n",
      "Processing batch 2760/11884 - Loss: 30.0230\n",
      "Processing batch 2761/11884 - Loss: 29.6960\n",
      "Processing batch 2762/11884 - Loss: 30.5797\n",
      "Processing batch 2763/11884 - Loss: 30.0199\n",
      "Processing batch 2764/11884 - Loss: 30.3695\n",
      "Processing batch 2765/11884 - Loss: 28.5251\n",
      "Processing batch 2766/11884 - Loss: 31.4281\n",
      "Processing batch 2767/11884 - Loss: 30.4242\n",
      "Processing batch 2768/11884 - Loss: 30.3509\n",
      "Processing batch 2769/11884 - Loss: 30.5814\n",
      "Processing batch 2770/11884 - Loss: 30.6772\n",
      "Processing batch 2771/11884 - Loss: 30.7997\n",
      "Processing batch 2772/11884 - Loss: 30.6537\n",
      "Processing batch 2773/11884 - Loss: 30.8695\n",
      "Processing batch 2774/11884 - Loss: 29.3064\n",
      "Processing batch 2775/11884 - Loss: 30.3141\n",
      "Processing batch 2776/11884 - Loss: 29.4097\n",
      "Processing batch 2777/11884 - Loss: 29.4545\n",
      "Processing batch 2778/11884 - Loss: 31.0654\n",
      "Processing batch 2779/11884 - Loss: 30.5167\n",
      "Processing batch 2780/11884 - Loss: 29.5166\n",
      "Processing batch 2781/11884 - Loss: 28.6045\n",
      "Processing batch 2782/11884 - Loss: 30.6697\n",
      "Processing batch 2783/11884 - Loss: 30.2959\n",
      "Processing batch 2784/11884 - Loss: 31.1357\n",
      "Processing batch 2785/11884 - Loss: 31.5736\n",
      "Processing batch 2786/11884 - Loss: 30.2000\n",
      "Processing batch 2787/11884 - Loss: 30.2026\n",
      "Processing batch 2788/11884 - Loss: 30.6813\n",
      "Processing batch 2789/11884 - Loss: 29.1366\n",
      "Processing batch 2790/11884 - Loss: 29.2328\n",
      "Processing batch 2791/11884 - Loss: 29.4092\n",
      "Processing batch 2792/11884 - Loss: 30.0953\n",
      "Processing batch 2793/11884 - Loss: 30.1237\n",
      "Processing batch 2794/11884 - Loss: 29.2612\n",
      "Processing batch 2795/11884 - Loss: 30.3330\n",
      "Processing batch 2796/11884 - Loss: 30.6530\n",
      "Processing batch 2797/11884 - Loss: 29.2331\n",
      "Processing batch 2798/11884 - Loss: 29.9432\n",
      "Processing batch 2799/11884 - Loss: 28.6486\n",
      "Processing batch 2800/11884 - Loss: 30.1306\n",
      "Processing batch 2801/11884 - Loss: 29.2626\n",
      "Processing batch 2802/11884 - Loss: 31.3372\n",
      "Processing batch 2803/11884 - Loss: 30.3552\n",
      "Processing batch 2804/11884 - Loss: 29.5787\n",
      "Processing batch 2805/11884 - Loss: 30.0967\n",
      "Processing batch 2806/11884 - Loss: 30.3612\n",
      "Processing batch 2807/11884 - Loss: 30.0980\n",
      "Processing batch 2808/11884 - Loss: 29.2964\n",
      "Processing batch 2809/11884 - Loss: 29.9752\n",
      "Processing batch 2810/11884 - Loss: 30.8460\n",
      "Processing batch 2811/11884 - Loss: 29.9037\n",
      "Processing batch 2812/11884 - Loss: 29.1108\n",
      "Processing batch 2813/11884 - Loss: 30.6582\n",
      "Processing batch 2814/11884 - Loss: 31.2198\n",
      "Processing batch 2815/11884 - Loss: 30.2547\n",
      "Processing batch 2816/11884 - Loss: 30.9211\n",
      "Processing batch 2817/11884 - Loss: 28.7449\n",
      "Processing batch 2818/11884 - Loss: 30.5555\n",
      "Processing batch 2819/11884 - Loss: 29.4177\n",
      "Processing batch 2820/11884 - Loss: 29.7976\n",
      "Processing batch 2821/11884 - Loss: 29.8814\n",
      "Processing batch 2822/11884 - Loss: 30.3932\n",
      "Processing batch 2823/11884 - Loss: 30.5101\n",
      "Processing batch 2824/11884 - Loss: 30.1564\n",
      "Processing batch 2825/11884 - Loss: 30.9972\n",
      "Processing batch 2826/11884 - Loss: 29.9048\n",
      "Processing batch 2827/11884 - Loss: 30.8515\n",
      "Processing batch 2828/11884 - Loss: 30.2644\n",
      "Processing batch 2829/11884 - Loss: 30.6210\n",
      "Processing batch 2830/11884 - Loss: 29.1741\n",
      "Processing batch 2831/11884 - Loss: 29.8714\n",
      "Processing batch 2832/11884 - Loss: 29.2833\n",
      "Processing batch 2833/11884 - Loss: 30.9605\n",
      "Processing batch 2834/11884 - Loss: 30.6456\n",
      "Processing batch 2835/11884 - Loss: 30.3694\n",
      "Processing batch 2836/11884 - Loss: 29.7199\n",
      "Processing batch 2837/11884 - Loss: 31.0287\n",
      "Processing batch 2838/11884 - Loss: 28.7665\n",
      "Processing batch 2839/11884 - Loss: 30.1770\n",
      "Processing batch 2840/11884 - Loss: 28.1511\n",
      "Processing batch 2841/11884 - Loss: 30.4532\n",
      "Processing batch 2842/11884 - Loss: 29.5438\n",
      "Processing batch 2843/11884 - Loss: 29.7449\n",
      "Processing batch 2844/11884 - Loss: 30.9864\n",
      "Processing batch 2845/11884 - Loss: 31.9420\n",
      "Processing batch 2846/11884 - Loss: 30.1253\n",
      "Processing batch 2847/11884 - Loss: 30.3050\n",
      "Processing batch 2848/11884 - Loss: 31.2505\n",
      "Processing batch 2849/11884 - Loss: 30.4723\n",
      "Processing batch 2850/11884 - Loss: 30.0978\n",
      "Processing batch 2851/11884 - Loss: 29.4993\n",
      "Processing batch 2852/11884 - Loss: 30.9320\n",
      "Processing batch 2853/11884 - Loss: 30.9916\n",
      "Processing batch 2854/11884 - Loss: 29.0645\n",
      "Processing batch 2855/11884 - Loss: 29.6234\n",
      "Processing batch 2856/11884 - Loss: 29.7250\n",
      "Processing batch 2857/11884 - Loss: 30.5719\n",
      "Processing batch 2858/11884 - Loss: 30.2330\n",
      "Processing batch 2859/11884 - Loss: 29.5403\n",
      "Processing batch 2860/11884 - Loss: 30.3488\n",
      "Processing batch 2861/11884 - Loss: 29.4861\n",
      "Processing batch 2862/11884 - Loss: 29.5543\n",
      "Processing batch 2863/11884 - Loss: 29.7285\n",
      "Processing batch 2864/11884 - Loss: 30.4847\n",
      "Processing batch 2865/11884 - Loss: 30.7451\n",
      "Processing batch 2866/11884 - Loss: 30.7106\n",
      "Processing batch 2867/11884 - Loss: 30.1456\n",
      "Processing batch 2868/11884 - Loss: 30.1882\n",
      "Processing batch 2869/11884 - Loss: 29.5749\n",
      "Processing batch 2870/11884 - Loss: 28.8336\n",
      "Processing batch 2871/11884 - Loss: 30.9383\n",
      "Processing batch 2872/11884 - Loss: 30.1731\n",
      "Processing batch 2873/11884 - Loss: 31.6206\n",
      "Processing batch 2874/11884 - Loss: 29.5744\n",
      "Processing batch 2875/11884 - Loss: 28.1414\n",
      "Processing batch 2876/11884 - Loss: 30.0597\n",
      "Processing batch 2877/11884 - Loss: 29.1652\n",
      "Processing batch 2878/11884 - Loss: 30.7106\n",
      "Processing batch 2879/11884 - Loss: 31.3122\n",
      "Processing batch 2880/11884 - Loss: 29.2014\n",
      "Processing batch 2881/11884 - Loss: 29.9999\n",
      "Processing batch 2882/11884 - Loss: 29.3370\n",
      "Processing batch 2883/11884 - Loss: 30.0555\n",
      "Processing batch 2884/11884 - Loss: 29.1432\n",
      "Processing batch 2885/11884 - Loss: 30.3342\n",
      "Processing batch 2886/11884 - Loss: 30.3142\n",
      "Processing batch 2887/11884 - Loss: 30.5967\n",
      "Processing batch 2888/11884 - Loss: 31.7154\n",
      "Processing batch 2889/11884 - Loss: 30.2258\n",
      "Processing batch 2890/11884 - Loss: 30.4775\n",
      "Processing batch 2891/11884 - Loss: 30.1819\n",
      "Processing batch 2892/11884 - Loss: 29.6138\n",
      "Processing batch 2893/11884 - Loss: 29.8187\n",
      "Processing batch 2894/11884 - Loss: 29.5561\n",
      "Processing batch 2895/11884 - Loss: 30.0140\n",
      "Processing batch 2896/11884 - Loss: 30.3227\n",
      "Processing batch 2897/11884 - Loss: 29.9919\n",
      "Processing batch 2898/11884 - Loss: 29.5215\n",
      "Processing batch 2899/11884 - Loss: 30.1935\n",
      "Processing batch 2900/11884 - Loss: 29.8637\n",
      "Processing batch 2901/11884 - Loss: 30.9630\n",
      "Processing batch 2902/11884 - Loss: 31.5594\n",
      "Processing batch 2903/11884 - Loss: 30.6210\n",
      "Processing batch 2904/11884 - Loss: 30.3368\n",
      "Processing batch 2905/11884 - Loss: 30.6834\n",
      "Processing batch 2906/11884 - Loss: 29.3264\n",
      "Processing batch 2907/11884 - Loss: 29.8435\n",
      "Processing batch 2908/11884 - Loss: 28.8947\n",
      "Processing batch 2909/11884 - Loss: 28.6748\n",
      "Processing batch 2910/11884 - Loss: 30.9790\n",
      "Processing batch 2911/11884 - Loss: 29.5932\n",
      "Processing batch 2912/11884 - Loss: 30.0403\n",
      "Processing batch 2913/11884 - Loss: 29.2175\n",
      "Processing batch 2914/11884 - Loss: 29.7265\n",
      "Processing batch 2915/11884 - Loss: 31.0180\n",
      "Processing batch 2916/11884 - Loss: 29.7201\n",
      "Processing batch 2917/11884 - Loss: 28.1283\n",
      "Processing batch 2918/11884 - Loss: 31.0838\n",
      "Processing batch 2919/11884 - Loss: 29.9769\n",
      "Processing batch 2920/11884 - Loss: 30.9835\n",
      "Processing batch 2921/11884 - Loss: 31.2170\n",
      "Processing batch 2922/11884 - Loss: 31.2752\n",
      "Processing batch 2923/11884 - Loss: 28.9795\n",
      "Processing batch 2924/11884 - Loss: 30.0889\n",
      "Processing batch 2925/11884 - Loss: 31.4655\n",
      "Processing batch 2926/11884 - Loss: 30.6993\n",
      "Processing batch 2927/11884 - Loss: 31.2237\n",
      "Processing batch 2928/11884 - Loss: 29.9136\n",
      "Processing batch 2929/11884 - Loss: 30.3125\n",
      "Processing batch 2930/11884 - Loss: 31.1597\n",
      "Processing batch 2931/11884 - Loss: 30.8077\n",
      "Processing batch 2932/11884 - Loss: 29.8182\n",
      "Processing batch 2933/11884 - Loss: 30.7282\n",
      "Processing batch 2934/11884 - Loss: 29.2782\n",
      "Processing batch 2935/11884 - Loss: 30.8853\n",
      "Processing batch 2936/11884 - Loss: 31.1588\n",
      "Processing batch 2937/11884 - Loss: 31.4839\n",
      "Processing batch 2938/11884 - Loss: 29.6507\n",
      "Processing batch 2939/11884 - Loss: 30.8516\n",
      "Processing batch 2940/11884 - Loss: 29.8607\n",
      "Processing batch 2941/11884 - Loss: 30.3304\n",
      "Processing batch 2942/11884 - Loss: 29.1585\n",
      "Processing batch 2943/11884 - Loss: 30.1088\n",
      "Processing batch 2944/11884 - Loss: 31.3565\n",
      "Processing batch 2945/11884 - Loss: 29.9013\n",
      "Processing batch 2946/11884 - Loss: 31.1120\n",
      "Processing batch 2947/11884 - Loss: 28.8880\n",
      "Processing batch 2948/11884 - Loss: 30.4062\n",
      "Processing batch 2949/11884 - Loss: 29.1195\n",
      "Processing batch 2950/11884 - Loss: 31.1661\n",
      "Processing batch 2951/11884 - Loss: 30.6952\n",
      "Processing batch 2952/11884 - Loss: 30.0708\n",
      "Processing batch 2953/11884 - Loss: 29.7537\n",
      "Processing batch 2954/11884 - Loss: 30.1882\n",
      "Processing batch 2955/11884 - Loss: 30.9543\n",
      "Processing batch 2956/11884 - Loss: 29.2354\n",
      "Processing batch 2957/11884 - Loss: 30.6980\n",
      "Processing batch 2958/11884 - Loss: 29.9618\n",
      "Processing batch 2959/11884 - Loss: 31.4102\n",
      "Processing batch 2960/11884 - Loss: 28.6906\n",
      "Processing batch 2961/11884 - Loss: 30.6771\n",
      "Processing batch 2962/11884 - Loss: 29.3118\n",
      "Processing batch 2963/11884 - Loss: 29.8804\n",
      "Processing batch 2964/11884 - Loss: 30.2650\n",
      "Processing batch 2965/11884 - Loss: 31.8645\n",
      "Processing batch 2966/11884 - Loss: 29.6217\n",
      "Processing batch 2967/11884 - Loss: 31.2016\n",
      "Processing batch 2968/11884 - Loss: 31.5674\n",
      "Processing batch 2969/11884 - Loss: 32.1206\n",
      "Processing batch 2970/11884 - Loss: 29.6978\n",
      "Processing batch 2971/11884 - Loss: 29.6523\n",
      "Processing batch 2972/11884 - Loss: 28.8550\n",
      "Processing batch 2973/11884 - Loss: 30.8088\n",
      "Processing batch 2974/11884 - Loss: 30.1844\n",
      "Processing batch 2975/11884 - Loss: 30.1402\n",
      "Processing batch 2976/11884 - Loss: 31.4547\n",
      "Processing batch 2977/11884 - Loss: 29.8765\n",
      "Processing batch 2978/11884 - Loss: 30.0069\n",
      "Processing batch 2979/11884 - Loss: 30.6369\n",
      "Processing batch 2980/11884 - Loss: 30.1059\n",
      "Processing batch 2981/11884 - Loss: 29.9977\n",
      "Processing batch 2982/11884 - Loss: 31.2191\n",
      "Processing batch 2983/11884 - Loss: 29.8156\n",
      "Processing batch 2984/11884 - Loss: 29.8785\n",
      "Processing batch 2985/11884 - Loss: 29.6304\n",
      "Processing batch 2986/11884 - Loss: 31.4418\n",
      "Processing batch 2987/11884 - Loss: 28.7397\n",
      "Processing batch 2988/11884 - Loss: 28.9632\n",
      "Processing batch 2989/11884 - Loss: 30.5468\n",
      "Processing batch 2990/11884 - Loss: 30.7510\n",
      "Processing batch 2991/11884 - Loss: 30.0557\n",
      "Processing batch 2992/11884 - Loss: 30.2506\n",
      "Processing batch 2993/11884 - Loss: 29.9832\n",
      "Processing batch 2994/11884 - Loss: 28.5551\n",
      "Processing batch 2995/11884 - Loss: 30.2429\n",
      "Processing batch 2996/11884 - Loss: 29.5507\n",
      "Processing batch 2997/11884 - Loss: 30.6386\n",
      "Processing batch 2998/11884 - Loss: 29.8815\n",
      "Processing batch 2999/11884 - Loss: 30.7245\n",
      "Processing batch 3000/11884 - Loss: 30.1639\n",
      "Processing batch 3001/11884 - Loss: 30.0481\n",
      "Processing batch 3002/11884 - Loss: 31.5767\n",
      "Processing batch 3003/11884 - Loss: 29.8583\n",
      "Processing batch 3004/11884 - Loss: 29.9740\n",
      "Processing batch 3005/11884 - Loss: 31.1293\n",
      "Processing batch 3006/11884 - Loss: 31.0857\n",
      "Processing batch 3007/11884 - Loss: 30.5137\n",
      "Processing batch 3008/11884 - Loss: 30.0951\n",
      "Processing batch 3009/11884 - Loss: 30.8923\n",
      "Processing batch 3010/11884 - Loss: 30.8153\n",
      "Processing batch 3011/11884 - Loss: 30.0353\n",
      "Processing batch 3012/11884 - Loss: 29.6064\n",
      "Processing batch 3013/11884 - Loss: 29.7847\n",
      "Processing batch 3014/11884 - Loss: 31.7289\n",
      "Processing batch 3015/11884 - Loss: 29.0025\n",
      "Processing batch 3016/11884 - Loss: 28.9878\n",
      "Processing batch 3017/11884 - Loss: 29.6731\n",
      "Processing batch 3018/11884 - Loss: 30.0888\n",
      "Processing batch 3019/11884 - Loss: 29.3712\n",
      "Processing batch 3020/11884 - Loss: 28.3622\n",
      "Processing batch 3021/11884 - Loss: 30.4657\n",
      "Processing batch 3022/11884 - Loss: 31.0819\n",
      "Processing batch 3023/11884 - Loss: 28.6086\n",
      "Processing batch 3024/11884 - Loss: 30.3495\n",
      "Processing batch 3025/11884 - Loss: 29.9169\n",
      "Processing batch 3026/11884 - Loss: 29.7301\n",
      "Processing batch 3027/11884 - Loss: 30.5381\n",
      "Processing batch 3028/11884 - Loss: 30.3767\n",
      "Processing batch 3029/11884 - Loss: 29.7438\n",
      "Processing batch 3030/11884 - Loss: 30.0146\n",
      "Processing batch 3031/11884 - Loss: 31.5688\n",
      "Processing batch 3032/11884 - Loss: 29.5377\n",
      "Processing batch 3033/11884 - Loss: 31.7421\n",
      "Processing batch 3034/11884 - Loss: 29.2839\n",
      "Processing batch 3035/11884 - Loss: 30.8913\n",
      "Processing batch 3036/11884 - Loss: 29.1927\n",
      "Processing batch 3037/11884 - Loss: 30.2667\n",
      "Processing batch 3038/11884 - Loss: 29.6745\n",
      "Processing batch 3039/11884 - Loss: 29.8053\n",
      "Processing batch 3040/11884 - Loss: 30.0049\n",
      "Processing batch 3041/11884 - Loss: 30.7937\n",
      "Processing batch 3042/11884 - Loss: 31.3882\n",
      "Processing batch 3043/11884 - Loss: 29.2342\n",
      "Processing batch 3044/11884 - Loss: 30.1387\n",
      "Processing batch 3045/11884 - Loss: 29.8674\n",
      "Processing batch 3046/11884 - Loss: 29.0912\n",
      "Processing batch 3047/11884 - Loss: 29.1746\n",
      "Processing batch 3048/11884 - Loss: 30.5315\n",
      "Processing batch 3049/11884 - Loss: 31.5757\n",
      "Processing batch 3050/11884 - Loss: 29.7462\n",
      "Processing batch 3051/11884 - Loss: 30.6171\n",
      "Processing batch 3052/11884 - Loss: 30.8921\n",
      "Processing batch 3053/11884 - Loss: 30.5594\n",
      "Processing batch 3054/11884 - Loss: 29.7246\n",
      "Processing batch 3055/11884 - Loss: 29.5516\n",
      "Processing batch 3056/11884 - Loss: 29.8048\n",
      "Processing batch 3057/11884 - Loss: 29.3602\n",
      "Processing batch 3058/11884 - Loss: 30.1042\n",
      "Processing batch 3059/11884 - Loss: 30.5635\n",
      "Processing batch 3060/11884 - Loss: 30.7043\n",
      "Processing batch 3061/11884 - Loss: 28.7319\n",
      "Processing batch 3062/11884 - Loss: 30.7995\n",
      "Processing batch 3063/11884 - Loss: 29.6142\n",
      "Processing batch 3064/11884 - Loss: 30.8365\n",
      "Processing batch 3065/11884 - Loss: 29.4477\n",
      "Processing batch 3066/11884 - Loss: 30.1999\n",
      "Processing batch 3067/11884 - Loss: 30.3574\n",
      "Processing batch 3068/11884 - Loss: 31.5712\n",
      "Processing batch 3069/11884 - Loss: 30.3772\n",
      "Processing batch 3070/11884 - Loss: 30.2935\n",
      "Processing batch 3071/11884 - Loss: 29.3415\n",
      "Processing batch 3072/11884 - Loss: 30.1009\n",
      "Processing batch 3073/11884 - Loss: 30.5233\n",
      "Processing batch 3074/11884 - Loss: 30.0424\n",
      "Processing batch 3075/11884 - Loss: 30.8704\n",
      "Processing batch 3076/11884 - Loss: 29.0938\n",
      "Processing batch 3077/11884 - Loss: 31.0799\n",
      "Processing batch 3078/11884 - Loss: 30.1919\n",
      "Processing batch 3079/11884 - Loss: 30.1020\n",
      "Processing batch 3080/11884 - Loss: 31.0135\n",
      "Processing batch 3081/11884 - Loss: 30.6589\n",
      "Processing batch 3082/11884 - Loss: 30.6048\n",
      "Processing batch 3083/11884 - Loss: 31.5328\n",
      "Processing batch 3084/11884 - Loss: 29.4195\n",
      "Processing batch 3085/11884 - Loss: 30.6090\n",
      "Processing batch 3086/11884 - Loss: 29.6977\n",
      "Processing batch 3087/11884 - Loss: 30.2033\n",
      "Processing batch 3088/11884 - Loss: 29.6853\n",
      "Processing batch 3089/11884 - Loss: 29.1348\n",
      "Processing batch 3090/11884 - Loss: 30.7904\n",
      "Processing batch 3091/11884 - Loss: 30.5580\n",
      "Processing batch 3092/11884 - Loss: 30.1590\n",
      "Processing batch 3093/11884 - Loss: 29.5099\n",
      "Processing batch 3094/11884 - Loss: 31.3594\n",
      "Processing batch 3095/11884 - Loss: 29.8424\n",
      "Processing batch 3096/11884 - Loss: 30.7451\n",
      "Processing batch 3097/11884 - Loss: 29.5919\n",
      "Processing batch 3098/11884 - Loss: 29.6519\n",
      "Processing batch 3099/11884 - Loss: 30.6406\n",
      "Processing batch 3100/11884 - Loss: 30.0390\n",
      "Processing batch 3101/11884 - Loss: 29.9917\n",
      "Processing batch 3102/11884 - Loss: 31.0290\n",
      "Processing batch 3103/11884 - Loss: 28.7992\n",
      "Processing batch 3104/11884 - Loss: 29.1677\n",
      "Processing batch 3105/11884 - Loss: 29.2909\n",
      "Processing batch 3106/11884 - Loss: 30.2389\n",
      "Processing batch 3107/11884 - Loss: 30.3620\n",
      "Processing batch 3108/11884 - Loss: 29.5638\n",
      "Processing batch 3109/11884 - Loss: 29.9893\n",
      "Processing batch 3110/11884 - Loss: 29.0973\n",
      "Processing batch 3111/11884 - Loss: 29.3286\n",
      "Processing batch 3112/11884 - Loss: 30.4726\n",
      "Processing batch 3113/11884 - Loss: 30.3781\n",
      "Processing batch 3114/11884 - Loss: 29.9829\n",
      "Processing batch 3115/11884 - Loss: 30.7710\n",
      "Processing batch 3116/11884 - Loss: 29.1875\n",
      "Processing batch 3117/11884 - Loss: 29.9688\n",
      "Processing batch 3118/11884 - Loss: 31.7997\n",
      "Processing batch 3119/11884 - Loss: 29.7710\n",
      "Processing batch 3120/11884 - Loss: 31.0832\n",
      "Processing batch 3121/11884 - Loss: 29.5609\n",
      "Processing batch 3122/11884 - Loss: 31.1793\n",
      "Processing batch 3123/11884 - Loss: 30.4106\n",
      "Processing batch 3124/11884 - Loss: 29.4808\n",
      "Processing batch 3125/11884 - Loss: 29.6669\n",
      "Processing batch 3126/11884 - Loss: 29.7243\n",
      "Processing batch 3127/11884 - Loss: 30.0399\n",
      "Processing batch 3128/11884 - Loss: 29.8509\n",
      "Processing batch 3129/11884 - Loss: 29.5394\n",
      "Processing batch 3130/11884 - Loss: 29.7339\n",
      "Processing batch 3131/11884 - Loss: 29.2472\n",
      "Processing batch 3132/11884 - Loss: 30.3507\n",
      "Processing batch 3133/11884 - Loss: 29.6327\n",
      "Processing batch 3134/11884 - Loss: 30.5124\n",
      "Processing batch 3135/11884 - Loss: 30.3629\n",
      "Processing batch 3136/11884 - Loss: 29.4988\n",
      "Processing batch 3137/11884 - Loss: 29.6053\n",
      "Processing batch 3138/11884 - Loss: 29.6946\n",
      "Processing batch 3139/11884 - Loss: 30.4666\n",
      "Processing batch 3140/11884 - Loss: 29.5616\n",
      "Processing batch 3141/11884 - Loss: 30.6639\n",
      "Processing batch 3142/11884 - Loss: 31.5321\n",
      "Processing batch 3143/11884 - Loss: 30.1971\n",
      "Processing batch 3144/11884 - Loss: 30.4170\n",
      "Processing batch 3145/11884 - Loss: 30.9566\n",
      "Processing batch 3146/11884 - Loss: 30.0871\n",
      "Processing batch 3147/11884 - Loss: 31.1405\n",
      "Processing batch 3148/11884 - Loss: 30.7413\n",
      "Processing batch 3149/11884 - Loss: 29.4087\n",
      "Processing batch 3150/11884 - Loss: 30.5579\n",
      "Processing batch 3151/11884 - Loss: 30.1769\n",
      "Processing batch 3152/11884 - Loss: 30.2767\n",
      "Processing batch 3153/11884 - Loss: 30.2668\n",
      "Processing batch 3154/11884 - Loss: 29.7241\n",
      "Processing batch 3155/11884 - Loss: 30.4098\n",
      "Processing batch 3156/11884 - Loss: 30.3791\n",
      "Processing batch 3157/11884 - Loss: 30.0959\n",
      "Processing batch 3158/11884 - Loss: 30.3719\n",
      "Processing batch 3159/11884 - Loss: 29.6669\n",
      "Processing batch 3160/11884 - Loss: 30.2532\n",
      "Processing batch 3161/11884 - Loss: 29.7715\n",
      "Processing batch 3162/11884 - Loss: 29.0115\n",
      "Processing batch 3163/11884 - Loss: 30.4250\n",
      "Processing batch 3164/11884 - Loss: 28.2615\n",
      "Processing batch 3165/11884 - Loss: 30.7726\n",
      "Processing batch 3166/11884 - Loss: 30.2657\n",
      "Processing batch 3167/11884 - Loss: 29.1853\n",
      "Processing batch 3168/11884 - Loss: 30.4855\n",
      "Processing batch 3169/11884 - Loss: 29.6727\n",
      "Processing batch 3170/11884 - Loss: 29.4474\n",
      "Processing batch 3171/11884 - Loss: 28.7564\n",
      "Processing batch 3172/11884 - Loss: 31.0729\n",
      "Processing batch 3173/11884 - Loss: 29.0866\n",
      "Processing batch 3174/11884 - Loss: 30.5615\n",
      "Processing batch 3175/11884 - Loss: 29.6031\n",
      "Processing batch 3176/11884 - Loss: 30.4970\n",
      "Processing batch 3177/11884 - Loss: 29.5224\n",
      "Processing batch 3178/11884 - Loss: 31.9244\n",
      "Processing batch 3179/11884 - Loss: 30.4424\n",
      "Processing batch 3180/11884 - Loss: 30.5673\n",
      "Processing batch 3181/11884 - Loss: 30.5416\n",
      "Processing batch 3182/11884 - Loss: 30.3152\n",
      "Processing batch 3183/11884 - Loss: 29.8467\n",
      "Processing batch 3184/11884 - Loss: 29.8411\n",
      "Processing batch 3185/11884 - Loss: 30.6334\n",
      "Processing batch 3186/11884 - Loss: 30.3104\n",
      "Processing batch 3187/11884 - Loss: 29.9978\n",
      "Processing batch 3188/11884 - Loss: 30.1437\n",
      "Processing batch 3189/11884 - Loss: 30.9953\n",
      "Processing batch 3190/11884 - Loss: 29.6062\n",
      "Processing batch 3191/11884 - Loss: 31.0622\n",
      "Processing batch 3192/11884 - Loss: 29.3500\n",
      "Processing batch 3193/11884 - Loss: 29.8625\n",
      "Processing batch 3194/11884 - Loss: 31.0728\n",
      "Processing batch 3195/11884 - Loss: 30.9953\n",
      "Processing batch 3196/11884 - Loss: 28.9242\n",
      "Processing batch 3197/11884 - Loss: 29.6987\n",
      "Processing batch 3198/11884 - Loss: 30.9190\n",
      "Processing batch 3199/11884 - Loss: 31.1923\n",
      "Processing batch 3200/11884 - Loss: 29.9243\n",
      "Processing batch 3201/11884 - Loss: 30.3202\n",
      "Processing batch 3202/11884 - Loss: 29.9166\n",
      "Processing batch 3203/11884 - Loss: 30.9211\n",
      "Processing batch 3204/11884 - Loss: 30.6840\n",
      "Processing batch 3205/11884 - Loss: 28.8316\n",
      "Processing batch 3206/11884 - Loss: 29.8999\n",
      "Processing batch 3207/11884 - Loss: 30.6202\n",
      "Processing batch 3208/11884 - Loss: 29.6768\n",
      "Processing batch 3209/11884 - Loss: 30.7376\n",
      "Processing batch 3210/11884 - Loss: 29.5484\n",
      "Processing batch 3211/11884 - Loss: 30.3270\n",
      "Processing batch 3212/11884 - Loss: 30.7705\n",
      "Processing batch 3213/11884 - Loss: 29.3653\n",
      "Processing batch 3214/11884 - Loss: 29.6992\n",
      "Processing batch 3215/11884 - Loss: 29.8729\n",
      "Processing batch 3216/11884 - Loss: 29.9711\n",
      "Processing batch 3217/11884 - Loss: 29.8186\n",
      "Processing batch 3218/11884 - Loss: 29.6110\n",
      "Processing batch 3219/11884 - Loss: 29.4208\n",
      "Processing batch 3220/11884 - Loss: 32.1068\n",
      "Processing batch 3221/11884 - Loss: 28.3037\n",
      "Processing batch 3222/11884 - Loss: 28.6663\n",
      "Processing batch 3223/11884 - Loss: 28.1171\n",
      "Processing batch 3224/11884 - Loss: 29.3672\n",
      "Processing batch 3225/11884 - Loss: 30.6136\n",
      "Processing batch 3226/11884 - Loss: 28.3993\n",
      "Processing batch 3227/11884 - Loss: 29.8900\n",
      "Processing batch 3228/11884 - Loss: 30.2125\n",
      "Processing batch 3229/11884 - Loss: 28.5998\n",
      "Processing batch 3230/11884 - Loss: 29.1165\n",
      "Processing batch 3231/11884 - Loss: 30.6329\n",
      "Processing batch 3232/11884 - Loss: 29.7755\n",
      "Processing batch 3233/11884 - Loss: 30.4559\n",
      "Processing batch 3234/11884 - Loss: 30.0626\n",
      "Processing batch 3235/11884 - Loss: 29.6025\n",
      "Processing batch 3236/11884 - Loss: 28.5189\n",
      "Processing batch 3237/11884 - Loss: 29.7224\n",
      "Processing batch 3238/11884 - Loss: 30.8623\n",
      "Processing batch 3239/11884 - Loss: 30.5914\n",
      "Processing batch 3240/11884 - Loss: 30.8115\n",
      "Processing batch 3241/11884 - Loss: 28.9785\n",
      "Processing batch 3242/11884 - Loss: 30.3940\n",
      "Processing batch 3243/11884 - Loss: 30.1969\n",
      "Processing batch 3244/11884 - Loss: 30.9069\n",
      "Processing batch 3245/11884 - Loss: 31.1576\n",
      "Processing batch 3246/11884 - Loss: 28.9186\n",
      "Processing batch 3247/11884 - Loss: 29.4255\n",
      "Processing batch 3248/11884 - Loss: 29.8097\n",
      "Processing batch 3249/11884 - Loss: 29.5264\n",
      "Processing batch 3250/11884 - Loss: 30.0099\n",
      "Processing batch 3251/11884 - Loss: 29.0728\n",
      "Processing batch 3252/11884 - Loss: 32.0133\n",
      "Processing batch 3253/11884 - Loss: 29.6677\n",
      "Processing batch 3254/11884 - Loss: 30.4062\n",
      "Processing batch 3255/11884 - Loss: 30.6510\n",
      "Processing batch 3256/11884 - Loss: 29.7538\n",
      "Processing batch 3257/11884 - Loss: 31.2356\n",
      "Processing batch 3258/11884 - Loss: 29.9971\n",
      "Processing batch 3259/11884 - Loss: 30.2325\n",
      "Processing batch 3260/11884 - Loss: 30.4934\n",
      "Processing batch 3261/11884 - Loss: 30.4479\n",
      "Processing batch 3262/11884 - Loss: 29.6609\n",
      "Processing batch 3263/11884 - Loss: 29.0456\n",
      "Processing batch 3264/11884 - Loss: 30.1239\n",
      "Processing batch 3265/11884 - Loss: 29.5414\n",
      "Processing batch 3266/11884 - Loss: 30.5135\n",
      "Processing batch 3267/11884 - Loss: 29.5885\n",
      "Processing batch 3268/11884 - Loss: 30.2530\n",
      "Processing batch 3269/11884 - Loss: 30.6425\n",
      "Processing batch 3270/11884 - Loss: 29.8084\n",
      "Processing batch 3271/11884 - Loss: 29.8635\n",
      "Processing batch 3272/11884 - Loss: 30.7065\n",
      "Processing batch 3273/11884 - Loss: 30.6376\n",
      "Processing batch 3274/11884 - Loss: 31.0343\n",
      "Processing batch 3275/11884 - Loss: 30.7384\n",
      "Processing batch 3276/11884 - Loss: 29.6911\n",
      "Processing batch 3277/11884 - Loss: 30.3064\n",
      "Processing batch 3278/11884 - Loss: 29.4796\n",
      "Processing batch 3279/11884 - Loss: 30.5535\n",
      "Processing batch 3280/11884 - Loss: 29.3838\n",
      "Processing batch 3281/11884 - Loss: 29.9540\n",
      "Processing batch 3282/11884 - Loss: 30.0588\n",
      "Processing batch 3283/11884 - Loss: 30.1209\n",
      "Processing batch 3284/11884 - Loss: 30.3505\n",
      "Processing batch 3285/11884 - Loss: 30.1947\n",
      "Processing batch 3286/11884 - Loss: 31.2729\n",
      "Processing batch 3287/11884 - Loss: 29.6123\n",
      "Processing batch 3288/11884 - Loss: 28.6961\n",
      "Processing batch 3289/11884 - Loss: 29.0698\n",
      "Processing batch 3290/11884 - Loss: 29.8339\n",
      "Processing batch 3291/11884 - Loss: 31.3838\n",
      "Processing batch 3292/11884 - Loss: 30.7807\n",
      "Processing batch 3293/11884 - Loss: 30.0645\n",
      "Processing batch 3294/11884 - Loss: 29.4445\n",
      "Processing batch 3295/11884 - Loss: 30.0226\n",
      "Processing batch 3296/11884 - Loss: 31.4718\n",
      "Processing batch 3297/11884 - Loss: 31.4056\n",
      "Processing batch 3298/11884 - Loss: 29.6260\n",
      "Processing batch 3299/11884 - Loss: 30.0919\n",
      "Processing batch 3300/11884 - Loss: 29.9162\n",
      "Processing batch 3301/11884 - Loss: 31.1410\n",
      "Processing batch 3302/11884 - Loss: 31.1741\n",
      "Processing batch 3303/11884 - Loss: 30.4155\n",
      "Processing batch 3304/11884 - Loss: 30.2499\n",
      "Processing batch 3305/11884 - Loss: 31.4723\n",
      "Processing batch 3306/11884 - Loss: 29.3320\n",
      "Processing batch 3307/11884 - Loss: 30.3450\n",
      "Processing batch 3308/11884 - Loss: 32.4757\n",
      "Processing batch 3309/11884 - Loss: 30.5910\n",
      "Processing batch 3310/11884 - Loss: 30.3694\n",
      "Processing batch 3311/11884 - Loss: 31.0317\n",
      "Processing batch 3312/11884 - Loss: 29.0310\n",
      "Processing batch 3313/11884 - Loss: 30.7714\n",
      "Processing batch 3314/11884 - Loss: 29.8482\n",
      "Processing batch 3315/11884 - Loss: 32.5079\n",
      "Processing batch 3316/11884 - Loss: 31.0669\n",
      "Processing batch 3317/11884 - Loss: 30.2585\n",
      "Processing batch 3318/11884 - Loss: 30.9893\n",
      "Processing batch 3319/11884 - Loss: 28.7315\n",
      "Processing batch 3320/11884 - Loss: 29.2701\n",
      "Processing batch 3321/11884 - Loss: 29.4372\n",
      "Processing batch 3322/11884 - Loss: 30.6723\n",
      "Processing batch 3323/11884 - Loss: 28.7375\n",
      "Processing batch 3324/11884 - Loss: 30.0770\n",
      "Processing batch 3325/11884 - Loss: 29.6395\n",
      "Processing batch 3326/11884 - Loss: 29.3263\n",
      "Processing batch 3327/11884 - Loss: 31.3694\n",
      "Processing batch 3328/11884 - Loss: 31.4003\n",
      "Processing batch 3329/11884 - Loss: 29.3585\n",
      "Processing batch 3330/11884 - Loss: 30.1219\n",
      "Processing batch 3331/11884 - Loss: 29.6368\n",
      "Processing batch 3332/11884 - Loss: 31.0931\n",
      "Processing batch 3333/11884 - Loss: 29.7637\n",
      "Processing batch 3334/11884 - Loss: 29.1109\n",
      "Processing batch 3335/11884 - Loss: 30.5838\n",
      "Processing batch 3336/11884 - Loss: 30.5841\n",
      "Processing batch 3337/11884 - Loss: 31.0742\n",
      "Processing batch 3338/11884 - Loss: 29.2301\n",
      "Processing batch 3339/11884 - Loss: 29.4224\n",
      "Processing batch 3340/11884 - Loss: 30.7128\n",
      "Processing batch 3341/11884 - Loss: 31.4686\n",
      "Processing batch 3342/11884 - Loss: 29.0631\n",
      "Processing batch 3343/11884 - Loss: 29.7862\n",
      "Processing batch 3344/11884 - Loss: 28.9907\n",
      "Processing batch 3345/11884 - Loss: 30.2475\n",
      "Processing batch 3346/11884 - Loss: 30.3566\n",
      "Processing batch 3347/11884 - Loss: 30.3692\n",
      "Processing batch 3348/11884 - Loss: 29.9619\n",
      "Processing batch 3349/11884 - Loss: 31.4020\n",
      "Processing batch 3350/11884 - Loss: 29.5574\n",
      "Processing batch 3351/11884 - Loss: 30.7583\n",
      "Processing batch 3352/11884 - Loss: 30.7330\n",
      "Processing batch 3353/11884 - Loss: 30.8180\n",
      "Processing batch 3354/11884 - Loss: 28.4581\n",
      "Processing batch 3355/11884 - Loss: 30.4366\n",
      "Processing batch 3356/11884 - Loss: 30.1685\n",
      "Processing batch 3357/11884 - Loss: 31.2500\n",
      "Processing batch 3358/11884 - Loss: 29.5496\n",
      "Processing batch 3359/11884 - Loss: 29.9860\n",
      "Processing batch 3360/11884 - Loss: 31.3951\n",
      "Processing batch 3361/11884 - Loss: 29.5517\n",
      "Processing batch 3362/11884 - Loss: 30.2037\n",
      "Processing batch 3363/11884 - Loss: 30.2609\n",
      "Processing batch 3364/11884 - Loss: 30.0301\n",
      "Processing batch 3365/11884 - Loss: 30.3173\n",
      "Processing batch 3366/11884 - Loss: 29.6902\n",
      "Processing batch 3367/11884 - Loss: 30.1048\n",
      "Processing batch 3368/11884 - Loss: 30.2036\n",
      "Processing batch 3369/11884 - Loss: 30.6407\n",
      "Processing batch 3370/11884 - Loss: 30.7734\n",
      "Processing batch 3371/11884 - Loss: 29.6980\n",
      "Processing batch 3372/11884 - Loss: 30.1127\n",
      "Processing batch 3373/11884 - Loss: 30.1784\n",
      "Processing batch 3374/11884 - Loss: 29.7545\n",
      "Processing batch 3375/11884 - Loss: 28.8906\n",
      "Processing batch 3376/11884 - Loss: 30.5664\n",
      "Processing batch 3377/11884 - Loss: 29.9184\n",
      "Processing batch 3378/11884 - Loss: 31.3329\n",
      "Processing batch 3379/11884 - Loss: 30.2710\n",
      "Processing batch 3380/11884 - Loss: 29.2689\n",
      "Processing batch 3381/11884 - Loss: 28.8868\n",
      "Processing batch 3382/11884 - Loss: 30.0514\n",
      "Processing batch 3383/11884 - Loss: 30.0237\n",
      "Processing batch 3384/11884 - Loss: 29.9792\n",
      "Processing batch 3385/11884 - Loss: 30.0671\n",
      "Processing batch 3386/11884 - Loss: 29.3447\n",
      "Processing batch 3387/11884 - Loss: 29.3389\n",
      "Processing batch 3388/11884 - Loss: 30.3275\n",
      "Processing batch 3389/11884 - Loss: 30.5598\n",
      "Processing batch 3390/11884 - Loss: 29.9230\n",
      "Processing batch 3391/11884 - Loss: 31.4326\n",
      "Processing batch 3392/11884 - Loss: 29.7856\n",
      "Processing batch 3393/11884 - Loss: 30.2775\n",
      "Processing batch 3394/11884 - Loss: 29.6587\n",
      "Processing batch 3395/11884 - Loss: 29.9401\n",
      "Processing batch 3396/11884 - Loss: 28.9908\n",
      "Processing batch 3397/11884 - Loss: 30.7978\n",
      "Processing batch 3398/11884 - Loss: 30.7795\n",
      "Processing batch 3399/11884 - Loss: 30.7777\n",
      "Processing batch 3400/11884 - Loss: 30.4541\n",
      "Processing batch 3401/11884 - Loss: 28.7010\n",
      "Processing batch 3402/11884 - Loss: 29.3619\n",
      "Processing batch 3403/11884 - Loss: 30.2882\n",
      "Processing batch 3404/11884 - Loss: 30.4335\n",
      "Processing batch 3405/11884 - Loss: 30.0556\n",
      "Processing batch 3406/11884 - Loss: 28.7339\n",
      "Processing batch 3407/11884 - Loss: 29.5190\n",
      "Processing batch 3408/11884 - Loss: 30.5405\n",
      "Processing batch 3409/11884 - Loss: 30.0927\n",
      "Processing batch 3410/11884 - Loss: 29.4167\n",
      "Processing batch 3411/11884 - Loss: 30.2222\n",
      "Processing batch 3412/11884 - Loss: 29.9177\n",
      "Processing batch 3413/11884 - Loss: 29.5910\n",
      "Processing batch 3414/11884 - Loss: 30.6391\n",
      "Processing batch 3415/11884 - Loss: 29.2705\n",
      "Processing batch 3416/11884 - Loss: 30.2217\n",
      "Processing batch 3417/11884 - Loss: 30.4259\n",
      "Processing batch 3418/11884 - Loss: 29.5083\n",
      "Processing batch 3419/11884 - Loss: 29.6519\n",
      "Processing batch 3420/11884 - Loss: 29.6423\n",
      "Processing batch 3421/11884 - Loss: 29.3984\n",
      "Processing batch 3422/11884 - Loss: 30.3194\n",
      "Processing batch 3423/11884 - Loss: 28.9182\n",
      "Processing batch 3424/11884 - Loss: 30.8747\n",
      "Processing batch 3425/11884 - Loss: 29.7618\n",
      "Processing batch 3426/11884 - Loss: 29.8547\n",
      "Processing batch 3427/11884 - Loss: 30.2522\n",
      "Processing batch 3428/11884 - Loss: 29.7933\n",
      "Processing batch 3429/11884 - Loss: 29.7672\n",
      "Processing batch 3430/11884 - Loss: 29.0901\n",
      "Processing batch 3431/11884 - Loss: 30.0217\n",
      "Processing batch 3432/11884 - Loss: 29.5112\n",
      "Processing batch 3433/11884 - Loss: 30.8902\n",
      "Processing batch 3434/11884 - Loss: 29.8640\n",
      "Processing batch 3435/11884 - Loss: 30.6883\n",
      "Processing batch 3436/11884 - Loss: 29.8362\n",
      "Processing batch 3437/11884 - Loss: 30.7819\n",
      "Processing batch 3438/11884 - Loss: 29.9503\n",
      "Processing batch 3439/11884 - Loss: 31.5493\n",
      "Processing batch 3440/11884 - Loss: 31.0381\n",
      "Processing batch 3441/11884 - Loss: 30.6085\n",
      "Processing batch 3442/11884 - Loss: 28.8200\n",
      "Processing batch 3443/11884 - Loss: 29.4275\n",
      "Processing batch 3444/11884 - Loss: 30.2549\n",
      "Processing batch 3445/11884 - Loss: 31.2641\n",
      "Processing batch 3446/11884 - Loss: 30.0464\n",
      "Processing batch 3447/11884 - Loss: 29.8708\n",
      "Processing batch 3448/11884 - Loss: 30.1906\n",
      "Processing batch 3449/11884 - Loss: 30.9855\n",
      "Processing batch 3450/11884 - Loss: 30.3242\n",
      "Processing batch 3451/11884 - Loss: 30.1901\n",
      "Processing batch 3452/11884 - Loss: 29.7754\n",
      "Processing batch 3453/11884 - Loss: 30.4258\n",
      "Processing batch 3454/11884 - Loss: 30.1212\n",
      "Processing batch 3455/11884 - Loss: 30.9482\n",
      "Processing batch 3456/11884 - Loss: 29.6254\n",
      "Processing batch 3457/11884 - Loss: 30.1253\n",
      "Processing batch 3458/11884 - Loss: 28.7970\n",
      "Processing batch 3459/11884 - Loss: 31.2030\n",
      "Processing batch 3460/11884 - Loss: 30.4681\n",
      "Processing batch 3461/11884 - Loss: 29.7818\n",
      "Processing batch 3462/11884 - Loss: 30.7022\n",
      "Processing batch 3463/11884 - Loss: 30.7475\n",
      "Processing batch 3464/11884 - Loss: 30.4263\n",
      "Processing batch 3465/11884 - Loss: 29.9697\n",
      "Processing batch 3466/11884 - Loss: 30.7367\n",
      "Processing batch 3467/11884 - Loss: 29.5590\n",
      "Processing batch 3468/11884 - Loss: 29.9180\n",
      "Processing batch 3469/11884 - Loss: 30.2540\n",
      "Processing batch 3470/11884 - Loss: 30.6624\n",
      "Processing batch 3471/11884 - Loss: 29.6781\n",
      "Processing batch 3472/11884 - Loss: 30.3801\n",
      "Processing batch 3473/11884 - Loss: 31.1687\n",
      "Processing batch 3474/11884 - Loss: 29.6622\n",
      "Processing batch 3475/11884 - Loss: 30.9647\n",
      "Processing batch 3476/11884 - Loss: 30.8328\n",
      "Processing batch 3477/11884 - Loss: 29.2500\n",
      "Processing batch 3478/11884 - Loss: 29.7014\n",
      "Processing batch 3479/11884 - Loss: 30.8179\n",
      "Processing batch 3480/11884 - Loss: 30.0434\n",
      "Processing batch 3481/11884 - Loss: 29.6961\n",
      "Processing batch 3482/11884 - Loss: 30.6264\n",
      "Processing batch 3483/11884 - Loss: 29.9973\n",
      "Processing batch 3484/11884 - Loss: 30.9196\n",
      "Processing batch 3485/11884 - Loss: 30.7159\n",
      "Processing batch 3486/11884 - Loss: 31.0950\n",
      "Processing batch 3487/11884 - Loss: 29.3027\n",
      "Processing batch 3488/11884 - Loss: 29.6086\n",
      "Processing batch 3489/11884 - Loss: 29.6845\n",
      "Processing batch 3490/11884 - Loss: 30.7308\n",
      "Processing batch 3491/11884 - Loss: 32.0280\n",
      "Processing batch 3492/11884 - Loss: 31.1180\n",
      "Processing batch 3493/11884 - Loss: 29.6201\n",
      "Processing batch 3494/11884 - Loss: 29.6128\n",
      "Processing batch 3495/11884 - Loss: 30.4213\n",
      "Processing batch 3496/11884 - Loss: 30.4943\n",
      "Processing batch 3497/11884 - Loss: 29.6626\n",
      "Processing batch 3498/11884 - Loss: 29.7282\n",
      "Processing batch 3499/11884 - Loss: 30.1885\n",
      "Processing batch 3500/11884 - Loss: 30.4505\n",
      "Processing batch 3501/11884 - Loss: 30.6654\n",
      "Processing batch 3502/11884 - Loss: 30.6582\n",
      "Processing batch 3503/11884 - Loss: 31.1362\n",
      "Processing batch 3504/11884 - Loss: 28.6636\n",
      "Processing batch 3505/11884 - Loss: 30.2094\n",
      "Processing batch 3506/11884 - Loss: 31.2270\n",
      "Processing batch 3507/11884 - Loss: 30.0064\n",
      "Processing batch 3508/11884 - Loss: 29.2624\n",
      "Processing batch 3509/11884 - Loss: 29.6878\n",
      "Processing batch 3510/11884 - Loss: 31.2463\n",
      "Processing batch 3511/11884 - Loss: 30.0390\n",
      "Processing batch 3512/11884 - Loss: 29.1072\n",
      "Processing batch 3513/11884 - Loss: 29.4549\n",
      "Processing batch 3514/11884 - Loss: 29.8891\n",
      "Processing batch 3515/11884 - Loss: 29.2539\n",
      "Processing batch 3516/11884 - Loss: 30.1043\n",
      "Processing batch 3517/11884 - Loss: 29.7664\n",
      "Processing batch 3518/11884 - Loss: 30.0721\n",
      "Processing batch 3519/11884 - Loss: 30.8349\n",
      "Processing batch 3520/11884 - Loss: 30.1934\n",
      "Processing batch 3521/11884 - Loss: 29.7349\n",
      "Processing batch 3522/11884 - Loss: 31.1444\n",
      "Processing batch 3523/11884 - Loss: 30.2345\n",
      "Processing batch 3524/11884 - Loss: 30.6930\n",
      "Processing batch 3525/11884 - Loss: 29.2752\n",
      "Processing batch 3526/11884 - Loss: 29.3039\n",
      "Processing batch 3527/11884 - Loss: 30.4376\n",
      "Processing batch 3528/11884 - Loss: 31.0123\n",
      "Processing batch 3529/11884 - Loss: 30.2317\n",
      "Processing batch 3530/11884 - Loss: 28.9161\n",
      "Processing batch 3531/11884 - Loss: 30.8142\n",
      "Processing batch 3532/11884 - Loss: 30.5111\n",
      "Processing batch 3533/11884 - Loss: 30.8940\n",
      "Processing batch 3534/11884 - Loss: 29.1751\n",
      "Processing batch 3535/11884 - Loss: 31.1821\n",
      "Processing batch 3536/11884 - Loss: 30.0597\n",
      "Processing batch 3537/11884 - Loss: 31.1571\n",
      "Processing batch 3538/11884 - Loss: 30.1372\n",
      "Processing batch 3539/11884 - Loss: 30.8766\n",
      "Processing batch 3540/11884 - Loss: 29.5632\n",
      "Processing batch 3541/11884 - Loss: 29.7030\n",
      "Processing batch 3542/11884 - Loss: 29.7699\n",
      "Processing batch 3543/11884 - Loss: 30.9676\n",
      "Processing batch 3544/11884 - Loss: 30.2908\n",
      "Processing batch 3545/11884 - Loss: 30.1009\n",
      "Processing batch 3546/11884 - Loss: 29.6273\n",
      "Processing batch 3547/11884 - Loss: 29.9997\n",
      "Processing batch 3548/11884 - Loss: 30.3053\n",
      "Processing batch 3549/11884 - Loss: 30.0152\n",
      "Processing batch 3550/11884 - Loss: 29.5802\n",
      "Processing batch 3551/11884 - Loss: 30.4750\n",
      "Processing batch 3552/11884 - Loss: 31.2900\n",
      "Processing batch 3553/11884 - Loss: 30.9514\n",
      "Processing batch 3554/11884 - Loss: 30.0920\n",
      "Processing batch 3555/11884 - Loss: 29.3539\n",
      "Processing batch 3556/11884 - Loss: 30.0950\n",
      "Processing batch 3557/11884 - Loss: 30.7153\n",
      "Processing batch 3558/11884 - Loss: 29.6002\n",
      "Processing batch 3559/11884 - Loss: 29.3611\n",
      "Processing batch 3560/11884 - Loss: 29.7392\n",
      "Processing batch 3561/11884 - Loss: 29.7961\n",
      "Processing batch 3562/11884 - Loss: 30.3056\n",
      "Processing batch 3563/11884 - Loss: 30.2930\n",
      "Processing batch 3564/11884 - Loss: 29.8423\n",
      "Processing batch 3565/11884 - Loss: 30.7576\n",
      "Processing batch 3566/11884 - Loss: 30.7936\n",
      "Processing batch 3567/11884 - Loss: 30.1635\n",
      "Processing batch 3568/11884 - Loss: 30.0691\n",
      "Processing batch 3569/11884 - Loss: 30.4859\n",
      "Processing batch 3570/11884 - Loss: 30.1454\n",
      "Processing batch 3571/11884 - Loss: 31.0103\n",
      "Processing batch 3572/11884 - Loss: 30.3548\n",
      "Processing batch 3573/11884 - Loss: 30.1010\n",
      "Processing batch 3574/11884 - Loss: 30.6382\n",
      "Processing batch 3575/11884 - Loss: 31.1232\n",
      "Processing batch 3576/11884 - Loss: 29.2958\n",
      "Processing batch 3577/11884 - Loss: 31.4723\n",
      "Processing batch 3578/11884 - Loss: 30.2280\n",
      "Processing batch 3579/11884 - Loss: 29.9417\n",
      "Processing batch 3580/11884 - Loss: 30.8195\n",
      "Processing batch 3581/11884 - Loss: 28.7056\n",
      "Processing batch 3582/11884 - Loss: 30.0688\n",
      "Processing batch 3583/11884 - Loss: 31.1549\n",
      "Processing batch 3584/11884 - Loss: 30.4345\n",
      "Processing batch 3585/11884 - Loss: 28.7726\n",
      "Processing batch 3586/11884 - Loss: 30.0079\n",
      "Processing batch 3587/11884 - Loss: 31.3839\n",
      "Processing batch 3588/11884 - Loss: 28.9602\n",
      "Processing batch 3589/11884 - Loss: 30.7093\n",
      "Processing batch 3590/11884 - Loss: 31.9211\n",
      "Processing batch 3591/11884 - Loss: 31.4797\n",
      "Processing batch 3592/11884 - Loss: 30.5268\n",
      "Processing batch 3593/11884 - Loss: 28.6703\n",
      "Processing batch 3594/11884 - Loss: 30.3499\n",
      "Processing batch 3595/11884 - Loss: 30.0665\n",
      "Processing batch 3596/11884 - Loss: 30.1975\n",
      "Processing batch 3597/11884 - Loss: 31.2441\n",
      "Processing batch 3598/11884 - Loss: 30.8948\n",
      "Processing batch 3599/11884 - Loss: 29.5792\n",
      "Processing batch 3600/11884 - Loss: 30.3356\n",
      "Processing batch 3601/11884 - Loss: 30.8521\n",
      "Processing batch 3602/11884 - Loss: 31.3703\n",
      "Processing batch 3603/11884 - Loss: 29.5623\n",
      "Processing batch 3604/11884 - Loss: 29.7103\n",
      "Processing batch 3605/11884 - Loss: 30.6643\n",
      "Processing batch 3606/11884 - Loss: 30.2205\n",
      "Processing batch 3607/11884 - Loss: 30.0889\n",
      "Processing batch 3608/11884 - Loss: 29.8012\n",
      "Processing batch 3609/11884 - Loss: 30.8969\n",
      "Processing batch 3610/11884 - Loss: 30.5509\n",
      "Processing batch 3611/11884 - Loss: 30.7622\n",
      "Processing batch 3612/11884 - Loss: 30.3414\n",
      "Processing batch 3613/11884 - Loss: 29.8700\n",
      "Processing batch 3614/11884 - Loss: 29.3111\n",
      "Processing batch 3615/11884 - Loss: 29.1519\n",
      "Processing batch 3616/11884 - Loss: 29.5859\n",
      "Processing batch 3617/11884 - Loss: 30.9162\n",
      "Processing batch 3618/11884 - Loss: 30.4960\n",
      "Processing batch 3619/11884 - Loss: 29.9448\n",
      "Processing batch 3620/11884 - Loss: 30.5531\n",
      "Processing batch 3621/11884 - Loss: 30.5463\n",
      "Processing batch 3622/11884 - Loss: 29.8828\n",
      "Processing batch 3623/11884 - Loss: 29.0075\n",
      "Processing batch 3624/11884 - Loss: 30.3921\n",
      "Processing batch 3625/11884 - Loss: 30.6024\n",
      "Processing batch 3626/11884 - Loss: 29.9121\n",
      "Processing batch 3627/11884 - Loss: 29.7691\n",
      "Processing batch 3628/11884 - Loss: 30.5357\n",
      "Processing batch 3629/11884 - Loss: 29.5897\n",
      "Processing batch 3630/11884 - Loss: 28.5548\n",
      "Processing batch 3631/11884 - Loss: 29.4034\n",
      "Processing batch 3632/11884 - Loss: 29.9816\n",
      "Processing batch 3633/11884 - Loss: 31.4367\n",
      "Processing batch 3634/11884 - Loss: 30.5868\n",
      "Processing batch 3635/11884 - Loss: 29.9437\n",
      "Processing batch 3636/11884 - Loss: 30.7883\n",
      "Processing batch 3637/11884 - Loss: 29.1702\n",
      "Processing batch 3638/11884 - Loss: 29.8502\n",
      "Processing batch 3639/11884 - Loss: 30.1460\n",
      "Processing batch 3640/11884 - Loss: 28.8750\n",
      "Processing batch 3641/11884 - Loss: 30.3665\n",
      "Processing batch 3642/11884 - Loss: 30.5941\n",
      "Processing batch 3643/11884 - Loss: 29.6110\n",
      "Processing batch 3644/11884 - Loss: 29.6177\n",
      "Processing batch 3645/11884 - Loss: 28.4946\n",
      "Processing batch 3646/11884 - Loss: 30.5924\n",
      "Processing batch 3647/11884 - Loss: 30.7461\n",
      "Processing batch 3648/11884 - Loss: 29.9232\n",
      "Processing batch 3649/11884 - Loss: 30.4545\n",
      "Processing batch 3650/11884 - Loss: 29.7893\n",
      "Processing batch 3651/11884 - Loss: 31.3148\n",
      "Processing batch 3652/11884 - Loss: 31.3086\n",
      "Processing batch 3653/11884 - Loss: 30.8502\n",
      "Processing batch 3654/11884 - Loss: 30.8630\n",
      "Processing batch 3655/11884 - Loss: 29.3896\n",
      "Processing batch 3656/11884 - Loss: 29.5263\n",
      "Processing batch 3657/11884 - Loss: 28.8178\n",
      "Processing batch 3658/11884 - Loss: 30.3178\n",
      "Processing batch 3659/11884 - Loss: 29.5699\n",
      "Processing batch 3660/11884 - Loss: 31.9178\n",
      "Processing batch 3661/11884 - Loss: 29.2828\n",
      "Processing batch 3662/11884 - Loss: 31.3989\n",
      "Processing batch 3663/11884 - Loss: 30.8662\n",
      "Processing batch 3664/11884 - Loss: 30.3476\n",
      "Processing batch 3665/11884 - Loss: 30.1401\n",
      "Processing batch 3666/11884 - Loss: 30.2162\n",
      "Processing batch 3667/11884 - Loss: 30.3868\n",
      "Processing batch 3668/11884 - Loss: 30.5343\n",
      "Processing batch 3669/11884 - Loss: 30.4155\n",
      "Processing batch 3670/11884 - Loss: 31.1198\n",
      "Processing batch 3671/11884 - Loss: 28.9120\n",
      "Processing batch 3672/11884 - Loss: 30.5007\n",
      "Processing batch 3673/11884 - Loss: 29.9468\n",
      "Processing batch 3674/11884 - Loss: 30.5309\n",
      "Processing batch 3675/11884 - Loss: 29.5101\n",
      "Processing batch 3676/11884 - Loss: 31.6875\n",
      "Processing batch 3677/11884 - Loss: 30.3220\n",
      "Processing batch 3678/11884 - Loss: 31.2847\n",
      "Processing batch 3679/11884 - Loss: 30.5170\n",
      "Processing batch 3680/11884 - Loss: 31.0039\n",
      "Processing batch 3681/11884 - Loss: 30.3814\n",
      "Processing batch 3682/11884 - Loss: 30.2977\n",
      "Processing batch 3683/11884 - Loss: 30.4529\n",
      "Processing batch 3684/11884 - Loss: 29.8386\n",
      "Processing batch 3685/11884 - Loss: 30.8500\n",
      "Processing batch 3686/11884 - Loss: 29.9722\n",
      "Processing batch 3687/11884 - Loss: 29.2698\n",
      "Processing batch 3688/11884 - Loss: 30.5757\n",
      "Processing batch 3689/11884 - Loss: 29.9205\n",
      "Processing batch 3690/11884 - Loss: 31.0836\n",
      "Processing batch 3691/11884 - Loss: 30.3219\n",
      "Processing batch 3692/11884 - Loss: 30.7219\n",
      "Processing batch 3693/11884 - Loss: 28.1316\n",
      "Processing batch 3694/11884 - Loss: 30.7203\n",
      "Processing batch 3695/11884 - Loss: 29.6451\n",
      "Processing batch 3696/11884 - Loss: 30.7721\n",
      "Processing batch 3697/11884 - Loss: 29.6732\n",
      "Processing batch 3698/11884 - Loss: 28.8239\n",
      "Processing batch 3699/11884 - Loss: 30.9598\n",
      "Processing batch 3700/11884 - Loss: 31.3879\n",
      "Processing batch 3701/11884 - Loss: 30.6849\n",
      "Processing batch 3702/11884 - Loss: 31.6632\n",
      "Processing batch 3703/11884 - Loss: 31.0025\n",
      "Processing batch 3704/11884 - Loss: 30.3552\n",
      "Processing batch 3705/11884 - Loss: 30.7388\n",
      "Processing batch 3706/11884 - Loss: 31.3207\n",
      "Processing batch 3707/11884 - Loss: 30.5997\n",
      "Processing batch 3708/11884 - Loss: 30.4534\n",
      "Processing batch 3709/11884 - Loss: 30.8095\n",
      "Processing batch 3710/11884 - Loss: 30.2874\n",
      "Processing batch 3711/11884 - Loss: 30.4560\n",
      "Processing batch 3712/11884 - Loss: 31.3926\n",
      "Processing batch 3713/11884 - Loss: 29.9702\n",
      "Processing batch 3714/11884 - Loss: 30.0990\n",
      "Processing batch 3715/11884 - Loss: 30.8370\n",
      "Processing batch 3716/11884 - Loss: 30.8119\n",
      "Processing batch 3717/11884 - Loss: 29.4756\n",
      "Processing batch 3718/11884 - Loss: 29.5287\n",
      "Processing batch 3719/11884 - Loss: 28.5139\n",
      "Processing batch 3720/11884 - Loss: 30.7805\n",
      "Processing batch 3721/11884 - Loss: 30.0312\n",
      "Processing batch 3722/11884 - Loss: 30.8448\n",
      "Processing batch 3723/11884 - Loss: 30.0280\n",
      "Processing batch 3724/11884 - Loss: 30.5559\n",
      "Processing batch 3725/11884 - Loss: 29.0923\n",
      "Processing batch 3726/11884 - Loss: 29.8970\n",
      "Processing batch 3727/11884 - Loss: 29.1914\n",
      "Processing batch 3728/11884 - Loss: 30.6713\n",
      "Processing batch 3729/11884 - Loss: 31.5513\n",
      "Processing batch 3730/11884 - Loss: 29.2597\n",
      "Processing batch 3731/11884 - Loss: 30.7725\n",
      "Processing batch 3732/11884 - Loss: 29.6808\n",
      "Processing batch 3733/11884 - Loss: 29.5940\n",
      "Processing batch 3734/11884 - Loss: 29.7432\n",
      "Processing batch 3735/11884 - Loss: 29.6439\n",
      "Processing batch 3736/11884 - Loss: 31.3387\n",
      "Processing batch 3737/11884 - Loss: 31.0487\n",
      "Processing batch 3738/11884 - Loss: 30.9714\n",
      "Processing batch 3739/11884 - Loss: 31.0776\n",
      "Processing batch 3740/11884 - Loss: 31.6239\n",
      "Processing batch 3741/11884 - Loss: 29.3042\n",
      "Processing batch 3742/11884 - Loss: 29.5737\n",
      "Processing batch 3743/11884 - Loss: 31.0081\n",
      "Processing batch 3744/11884 - Loss: 30.4697\n",
      "Processing batch 3745/11884 - Loss: 29.0534\n",
      "Processing batch 3746/11884 - Loss: 29.3787\n",
      "Processing batch 3747/11884 - Loss: 31.2473\n",
      "Processing batch 3748/11884 - Loss: 29.9670\n",
      "Processing batch 3749/11884 - Loss: 29.1073\n",
      "Processing batch 3750/11884 - Loss: 29.6245\n",
      "Processing batch 3751/11884 - Loss: 30.9660\n",
      "Processing batch 3752/11884 - Loss: 29.8432\n",
      "Processing batch 3753/11884 - Loss: 29.9716\n",
      "Processing batch 3754/11884 - Loss: 29.7066\n",
      "Processing batch 3755/11884 - Loss: 30.3438\n",
      "Processing batch 3756/11884 - Loss: 29.7696\n",
      "Processing batch 3757/11884 - Loss: 29.3579\n",
      "Processing batch 3758/11884 - Loss: 30.5226\n",
      "Processing batch 3759/11884 - Loss: 30.0740\n",
      "Processing batch 3760/11884 - Loss: 30.7093\n",
      "Processing batch 3761/11884 - Loss: 29.3563\n",
      "Processing batch 3762/11884 - Loss: 29.1951\n",
      "Processing batch 3763/11884 - Loss: 30.4957\n",
      "Processing batch 3764/11884 - Loss: 30.0459\n",
      "Processing batch 3765/11884 - Loss: 29.4274\n",
      "Processing batch 3766/11884 - Loss: 30.5242\n",
      "Processing batch 3767/11884 - Loss: 30.4634\n",
      "Processing batch 3768/11884 - Loss: 28.8541\n",
      "Processing batch 3769/11884 - Loss: 30.7512\n",
      "Processing batch 3770/11884 - Loss: 30.0738\n",
      "Processing batch 3771/11884 - Loss: 28.5546\n",
      "Processing batch 3772/11884 - Loss: 31.0242\n",
      "Processing batch 3773/11884 - Loss: 30.4372\n",
      "Processing batch 3774/11884 - Loss: 29.9999\n",
      "Processing batch 3775/11884 - Loss: 31.0381\n",
      "Processing batch 3776/11884 - Loss: 28.8490\n",
      "Processing batch 3777/11884 - Loss: 30.6294\n",
      "Processing batch 3778/11884 - Loss: 30.5308\n",
      "Processing batch 3779/11884 - Loss: 29.4383\n",
      "Processing batch 3780/11884 - Loss: 29.2510\n",
      "Processing batch 3781/11884 - Loss: 30.4565\n",
      "Processing batch 3782/11884 - Loss: 30.0635\n",
      "Processing batch 3783/11884 - Loss: 29.4710\n",
      "Processing batch 3784/11884 - Loss: 29.7410\n",
      "Processing batch 3785/11884 - Loss: 30.3897\n",
      "Processing batch 3786/11884 - Loss: 29.9137\n",
      "Processing batch 3787/11884 - Loss: 30.1534\n",
      "Processing batch 3788/11884 - Loss: 30.8823\n",
      "Processing batch 3789/11884 - Loss: 28.7889\n",
      "Processing batch 3790/11884 - Loss: 30.0873\n",
      "Processing batch 3791/11884 - Loss: 30.8426\n",
      "Processing batch 3792/11884 - Loss: 30.0997\n",
      "Processing batch 3793/11884 - Loss: 29.6807\n",
      "Processing batch 3794/11884 - Loss: 30.6652\n",
      "Processing batch 3795/11884 - Loss: 31.7523\n",
      "Processing batch 3796/11884 - Loss: 30.5425\n",
      "Processing batch 3797/11884 - Loss: 30.6933\n",
      "Processing batch 3798/11884 - Loss: 29.1995\n",
      "Processing batch 3799/11884 - Loss: 30.5701\n",
      "Processing batch 3800/11884 - Loss: 30.2093\n",
      "Processing batch 3801/11884 - Loss: 30.1085\n",
      "Processing batch 3802/11884 - Loss: 28.8174\n",
      "Processing batch 3803/11884 - Loss: 30.7146\n",
      "Processing batch 3804/11884 - Loss: 30.9249\n",
      "Processing batch 3805/11884 - Loss: 31.1625\n",
      "Processing batch 3806/11884 - Loss: 30.3621\n",
      "Processing batch 3807/11884 - Loss: 30.4184\n",
      "Processing batch 3808/11884 - Loss: 30.9826\n",
      "Processing batch 3809/11884 - Loss: 30.7160\n",
      "Processing batch 3810/11884 - Loss: 29.5967\n",
      "Processing batch 3811/11884 - Loss: 29.4080\n",
      "Processing batch 3812/11884 - Loss: 30.3368\n",
      "Processing batch 3813/11884 - Loss: 30.1688\n",
      "Processing batch 3814/11884 - Loss: 30.0528\n",
      "Processing batch 3815/11884 - Loss: 30.8165\n",
      "Processing batch 3816/11884 - Loss: 29.5966\n",
      "Processing batch 3817/11884 - Loss: 29.8345\n",
      "Processing batch 3818/11884 - Loss: 30.6439\n",
      "Processing batch 3819/11884 - Loss: 30.4397\n",
      "Processing batch 3820/11884 - Loss: 30.1387\n",
      "Processing batch 3821/11884 - Loss: 30.3413\n",
      "Processing batch 3822/11884 - Loss: 30.8181\n",
      "Processing batch 3823/11884 - Loss: 30.1292\n",
      "Processing batch 3824/11884 - Loss: 30.1143\n",
      "Processing batch 3825/11884 - Loss: 30.6356\n",
      "Processing batch 3826/11884 - Loss: 29.6877\n",
      "Processing batch 3827/11884 - Loss: 29.0503\n",
      "Processing batch 3828/11884 - Loss: 29.6542\n",
      "Processing batch 3829/11884 - Loss: 30.9163\n",
      "Processing batch 3830/11884 - Loss: 29.3280\n",
      "Processing batch 3831/11884 - Loss: 31.0510\n",
      "Processing batch 3832/11884 - Loss: 31.6313\n",
      "Processing batch 3833/11884 - Loss: 29.7659\n",
      "Processing batch 3834/11884 - Loss: 30.2191\n",
      "Processing batch 3835/11884 - Loss: 29.4140\n",
      "Processing batch 3836/11884 - Loss: 29.6544\n",
      "Processing batch 3837/11884 - Loss: 30.1499\n",
      "Processing batch 3838/11884 - Loss: 28.8322\n",
      "Processing batch 3839/11884 - Loss: 31.1141\n",
      "Processing batch 3840/11884 - Loss: 30.6113\n",
      "Processing batch 3841/11884 - Loss: 30.7205\n",
      "Processing batch 3842/11884 - Loss: 30.0399\n",
      "Processing batch 3843/11884 - Loss: 30.5634\n",
      "Processing batch 3844/11884 - Loss: 29.5141\n",
      "Processing batch 3845/11884 - Loss: 30.4234\n",
      "Processing batch 3846/11884 - Loss: 29.8966\n",
      "Processing batch 3847/11884 - Loss: 29.8661\n",
      "Processing batch 3848/11884 - Loss: 28.7989\n",
      "Processing batch 3849/11884 - Loss: 31.1999\n",
      "Processing batch 3850/11884 - Loss: 29.9197\n",
      "Processing batch 3851/11884 - Loss: 30.7330\n",
      "Processing batch 3852/11884 - Loss: 29.6491\n",
      "Processing batch 3853/11884 - Loss: 29.4514\n",
      "Processing batch 3854/11884 - Loss: 29.6070\n",
      "Processing batch 3855/11884 - Loss: 30.9131\n",
      "Processing batch 3856/11884 - Loss: 30.6004\n",
      "Processing batch 3857/11884 - Loss: 28.9908\n",
      "Processing batch 3858/11884 - Loss: 30.4172\n",
      "Processing batch 3859/11884 - Loss: 30.7862\n",
      "Processing batch 3860/11884 - Loss: 30.7652\n",
      "Processing batch 3861/11884 - Loss: 30.7046\n",
      "Processing batch 3862/11884 - Loss: 29.8247\n",
      "Processing batch 3863/11884 - Loss: 31.6237\n",
      "Processing batch 3864/11884 - Loss: 29.7766\n",
      "Processing batch 3865/11884 - Loss: 29.3360\n",
      "Processing batch 3866/11884 - Loss: 29.6305\n",
      "Processing batch 3867/11884 - Loss: 29.5723\n",
      "Processing batch 3868/11884 - Loss: 30.0388\n",
      "Processing batch 3869/11884 - Loss: 30.1010\n",
      "Processing batch 3870/11884 - Loss: 29.8948\n",
      "Processing batch 3871/11884 - Loss: 30.8521\n",
      "Processing batch 3872/11884 - Loss: 29.4333\n",
      "Processing batch 3873/11884 - Loss: 29.1619\n",
      "Processing batch 3874/11884 - Loss: 31.0307\n",
      "Processing batch 3875/11884 - Loss: 30.8388\n",
      "Processing batch 3876/11884 - Loss: 30.5946\n",
      "Processing batch 3877/11884 - Loss: 30.5493\n",
      "Processing batch 3878/11884 - Loss: 31.1060\n",
      "Processing batch 3879/11884 - Loss: 30.5838\n",
      "Processing batch 3880/11884 - Loss: 29.1241\n",
      "Processing batch 3881/11884 - Loss: 29.7909\n",
      "Processing batch 3882/11884 - Loss: 29.2959\n",
      "Processing batch 3883/11884 - Loss: 30.3079\n",
      "Processing batch 3884/11884 - Loss: 30.5074\n",
      "Processing batch 3885/11884 - Loss: 32.5076\n",
      "Processing batch 3886/11884 - Loss: 29.9292\n",
      "Processing batch 3887/11884 - Loss: 30.1513\n",
      "Processing batch 3888/11884 - Loss: 28.9225\n",
      "Processing batch 3889/11884 - Loss: 31.1707\n",
      "Processing batch 3890/11884 - Loss: 30.0685\n",
      "Processing batch 3891/11884 - Loss: 30.2420\n",
      "Processing batch 3892/11884 - Loss: 30.8951\n",
      "Processing batch 3893/11884 - Loss: 30.0506\n",
      "Processing batch 3894/11884 - Loss: 30.1587\n",
      "Processing batch 3895/11884 - Loss: 29.6069\n",
      "Processing batch 3896/11884 - Loss: 31.2554\n",
      "Processing batch 3897/11884 - Loss: 29.8139\n",
      "Processing batch 3898/11884 - Loss: 30.4186\n",
      "Processing batch 3899/11884 - Loss: 32.7352\n",
      "Processing batch 3900/11884 - Loss: 30.0675\n",
      "Processing batch 3901/11884 - Loss: 30.5509\n",
      "Processing batch 3902/11884 - Loss: 28.8995\n",
      "Processing batch 3903/11884 - Loss: 29.6033\n",
      "Processing batch 3904/11884 - Loss: 30.8012\n",
      "Processing batch 3905/11884 - Loss: 29.1044\n",
      "Processing batch 3906/11884 - Loss: 30.2870\n",
      "Processing batch 3907/11884 - Loss: 30.2439\n",
      "Processing batch 3908/11884 - Loss: 31.6409\n",
      "Processing batch 3909/11884 - Loss: 30.2057\n",
      "Processing batch 3910/11884 - Loss: 29.4266\n",
      "Processing batch 3911/11884 - Loss: 29.1833\n",
      "Processing batch 3912/11884 - Loss: 29.6391\n",
      "Processing batch 3913/11884 - Loss: 28.8629\n",
      "Processing batch 3914/11884 - Loss: 29.5639\n",
      "Processing batch 3915/11884 - Loss: 30.6509\n",
      "Processing batch 3916/11884 - Loss: 30.8576\n",
      "Processing batch 3917/11884 - Loss: 29.0916\n",
      "Processing batch 3918/11884 - Loss: 29.4626\n",
      "Processing batch 3919/11884 - Loss: 30.7984\n",
      "Processing batch 3920/11884 - Loss: 29.1566\n",
      "Processing batch 3921/11884 - Loss: 29.7403\n",
      "Processing batch 3922/11884 - Loss: 29.4522\n",
      "Processing batch 3923/11884 - Loss: 30.7038\n",
      "Processing batch 3924/11884 - Loss: 29.4499\n",
      "Processing batch 3925/11884 - Loss: 30.9397\n",
      "Processing batch 3926/11884 - Loss: 29.9000\n",
      "Processing batch 3927/11884 - Loss: 30.8147\n",
      "Processing batch 3928/11884 - Loss: 30.3398\n",
      "Processing batch 3929/11884 - Loss: 30.2598\n",
      "Processing batch 3930/11884 - Loss: 29.4551\n",
      "Processing batch 3931/11884 - Loss: 28.6449\n",
      "Processing batch 3932/11884 - Loss: 31.1770\n",
      "Processing batch 3933/11884 - Loss: 29.8057\n",
      "Processing batch 3934/11884 - Loss: 29.6364\n",
      "Processing batch 3935/11884 - Loss: 30.9174\n",
      "Processing batch 3936/11884 - Loss: 28.5688\n",
      "Processing batch 3937/11884 - Loss: 30.9163\n",
      "Processing batch 3938/11884 - Loss: 30.7376\n",
      "Processing batch 3939/11884 - Loss: 28.5418\n",
      "Processing batch 3940/11884 - Loss: 30.4082\n",
      "Processing batch 3941/11884 - Loss: 30.1838\n",
      "Processing batch 3942/11884 - Loss: 29.2022\n",
      "Processing batch 3943/11884 - Loss: 29.7121\n",
      "Processing batch 3944/11884 - Loss: 28.9486\n",
      "Processing batch 3945/11884 - Loss: 32.1046\n",
      "Processing batch 3946/11884 - Loss: 29.2790\n",
      "Processing batch 3947/11884 - Loss: 30.9735\n",
      "Processing batch 3948/11884 - Loss: 30.5275\n",
      "Processing batch 3949/11884 - Loss: 31.2150\n",
      "Processing batch 3950/11884 - Loss: 32.2297\n",
      "Processing batch 3951/11884 - Loss: 29.8998\n",
      "Processing batch 3952/11884 - Loss: 30.5162\n",
      "Processing batch 3953/11884 - Loss: 30.7970\n",
      "Processing batch 3954/11884 - Loss: 29.0307\n",
      "Processing batch 3955/11884 - Loss: 29.7029\n",
      "Processing batch 3956/11884 - Loss: 29.6534\n",
      "Processing batch 3957/11884 - Loss: 30.6199\n",
      "Processing batch 3958/11884 - Loss: 29.7850\n",
      "Processing batch 3959/11884 - Loss: 31.3734\n",
      "Processing batch 3960/11884 - Loss: 30.7632\n",
      "Processing batch 3961/11884 - Loss: 29.8932\n",
      "Processing batch 3962/11884 - Loss: 30.2575\n",
      "Processing batch 3963/11884 - Loss: 30.3601\n",
      "Processing batch 3964/11884 - Loss: 30.6178\n",
      "Processing batch 3965/11884 - Loss: 29.6809\n",
      "Processing batch 3966/11884 - Loss: 31.3405\n",
      "Processing batch 3967/11884 - Loss: 31.3826\n",
      "Processing batch 3968/11884 - Loss: 30.5532\n",
      "Processing batch 3969/11884 - Loss: 30.2666\n",
      "Processing batch 3970/11884 - Loss: 29.7305\n",
      "Processing batch 3971/11884 - Loss: 28.8259\n",
      "Processing batch 3972/11884 - Loss: 30.5359\n",
      "Processing batch 3973/11884 - Loss: 29.0889\n",
      "Processing batch 3974/11884 - Loss: 29.1749\n",
      "Processing batch 3975/11884 - Loss: 30.2960\n",
      "Processing batch 3976/11884 - Loss: 30.8941\n",
      "Processing batch 3977/11884 - Loss: 30.3468\n",
      "Processing batch 3978/11884 - Loss: 31.0070\n",
      "Processing batch 3979/11884 - Loss: 29.9577\n",
      "Processing batch 3980/11884 - Loss: 28.8326\n",
      "Processing batch 3981/11884 - Loss: 29.1721\n",
      "Processing batch 3982/11884 - Loss: 31.6866\n",
      "Processing batch 3983/11884 - Loss: 28.2016\n",
      "Processing batch 3984/11884 - Loss: 31.2024\n",
      "Processing batch 3985/11884 - Loss: 29.6800\n",
      "Processing batch 3986/11884 - Loss: 28.7489\n",
      "Processing batch 3987/11884 - Loss: 30.1532\n",
      "Processing batch 3988/11884 - Loss: 30.7953\n",
      "Processing batch 3989/11884 - Loss: 29.1274\n",
      "Processing batch 3990/11884 - Loss: 30.2151\n",
      "Processing batch 3991/11884 - Loss: 30.8618\n",
      "Processing batch 3992/11884 - Loss: 28.8379\n",
      "Processing batch 3993/11884 - Loss: 29.0777\n",
      "Processing batch 3994/11884 - Loss: 29.0760\n",
      "Processing batch 3995/11884 - Loss: 29.9435\n",
      "Processing batch 3996/11884 - Loss: 31.1359\n",
      "Processing batch 3997/11884 - Loss: 29.9741\n",
      "Processing batch 3998/11884 - Loss: 29.5755\n",
      "Processing batch 3999/11884 - Loss: 29.7772\n",
      "Processing batch 4000/11884 - Loss: 30.9297\n",
      "Processing batch 4001/11884 - Loss: 29.3127\n",
      "Processing batch 4002/11884 - Loss: 29.0442\n",
      "Processing batch 4003/11884 - Loss: 30.7655\n",
      "Processing batch 4004/11884 - Loss: 29.5335\n",
      "Processing batch 4005/11884 - Loss: 30.8895\n",
      "Processing batch 4006/11884 - Loss: 28.8893\n",
      "Processing batch 4007/11884 - Loss: 30.1486\n",
      "Processing batch 4008/11884 - Loss: 29.6424\n",
      "Processing batch 4009/11884 - Loss: 29.7945\n",
      "Processing batch 4010/11884 - Loss: 31.5928\n",
      "Processing batch 4011/11884 - Loss: 30.1407\n",
      "Processing batch 4012/11884 - Loss: 30.7494\n",
      "Processing batch 4013/11884 - Loss: 28.9925\n",
      "Processing batch 4014/11884 - Loss: 29.6116\n",
      "Processing batch 4015/11884 - Loss: 29.3897\n",
      "Processing batch 4016/11884 - Loss: 30.3326\n",
      "Processing batch 4017/11884 - Loss: 30.4910\n",
      "Processing batch 4018/11884 - Loss: 29.5742\n",
      "Processing batch 4019/11884 - Loss: 29.8342\n",
      "Processing batch 4020/11884 - Loss: 29.5436\n",
      "Processing batch 4021/11884 - Loss: 30.0792\n",
      "Processing batch 4022/11884 - Loss: 30.7334\n",
      "Processing batch 4023/11884 - Loss: 29.4337\n",
      "Processing batch 4024/11884 - Loss: 31.6640\n",
      "Processing batch 4025/11884 - Loss: 30.5529\n",
      "Processing batch 4026/11884 - Loss: 29.0034\n",
      "Processing batch 4027/11884 - Loss: 29.4511\n",
      "Processing batch 4028/11884 - Loss: 30.8016\n",
      "Processing batch 4029/11884 - Loss: 29.0782\n",
      "Processing batch 4030/11884 - Loss: 30.0473\n",
      "Processing batch 4031/11884 - Loss: 29.7799\n",
      "Processing batch 4032/11884 - Loss: 29.2304\n",
      "Processing batch 4033/11884 - Loss: 29.6197\n",
      "Processing batch 4034/11884 - Loss: 29.9220\n",
      "Processing batch 4035/11884 - Loss: 29.9907\n",
      "Processing batch 4036/11884 - Loss: 31.2290\n",
      "Processing batch 4037/11884 - Loss: 30.1666\n",
      "Processing batch 4038/11884 - Loss: 29.6131\n",
      "Processing batch 4039/11884 - Loss: 29.9475\n",
      "Processing batch 4040/11884 - Loss: 29.3129\n",
      "Processing batch 4041/11884 - Loss: 30.6189\n",
      "Processing batch 4042/11884 - Loss: 29.8740\n",
      "Processing batch 4043/11884 - Loss: 29.4865\n",
      "Processing batch 4044/11884 - Loss: 30.8688\n",
      "Processing batch 4045/11884 - Loss: 29.8169\n",
      "Processing batch 4046/11884 - Loss: 30.9554\n",
      "Processing batch 4047/11884 - Loss: 29.5772\n",
      "Processing batch 4048/11884 - Loss: 29.9923\n",
      "Processing batch 4049/11884 - Loss: 31.2421\n",
      "Processing batch 4050/11884 - Loss: 29.5465\n",
      "Processing batch 4051/11884 - Loss: 29.5462\n",
      "Processing batch 4052/11884 - Loss: 28.3640\n",
      "Processing batch 4053/11884 - Loss: 27.7309\n",
      "Processing batch 4054/11884 - Loss: 30.1055\n",
      "Processing batch 4055/11884 - Loss: 30.7230\n",
      "Processing batch 4056/11884 - Loss: 30.6411\n",
      "Processing batch 4057/11884 - Loss: 31.6894\n",
      "Processing batch 4058/11884 - Loss: 30.0285\n",
      "Processing batch 4059/11884 - Loss: 30.0916\n",
      "Processing batch 4060/11884 - Loss: 29.7470\n",
      "Processing batch 4061/11884 - Loss: 29.8498\n",
      "Processing batch 4062/11884 - Loss: 30.2189\n",
      "Processing batch 4063/11884 - Loss: 29.7789\n",
      "Processing batch 4064/11884 - Loss: 30.1245\n",
      "Processing batch 4065/11884 - Loss: 29.0817\n",
      "Processing batch 4066/11884 - Loss: 30.2747\n",
      "Processing batch 4067/11884 - Loss: 29.7405\n",
      "Processing batch 4068/11884 - Loss: 30.8792\n",
      "Processing batch 4069/11884 - Loss: 29.2069\n",
      "Processing batch 4070/11884 - Loss: 30.4950\n",
      "Processing batch 4071/11884 - Loss: 29.9115\n",
      "Processing batch 4072/11884 - Loss: 30.9425\n",
      "Processing batch 4073/11884 - Loss: 30.0125\n",
      "Processing batch 4074/11884 - Loss: 29.5962\n",
      "Processing batch 4075/11884 - Loss: 30.2436\n",
      "Processing batch 4076/11884 - Loss: 30.3600\n",
      "Processing batch 4077/11884 - Loss: 30.8349\n",
      "Processing batch 4078/11884 - Loss: 30.6596\n",
      "Processing batch 4079/11884 - Loss: 29.2047\n",
      "Processing batch 4080/11884 - Loss: 29.4765\n",
      "Processing batch 4081/11884 - Loss: 30.3561\n",
      "Processing batch 4082/11884 - Loss: 30.2880\n",
      "Processing batch 4083/11884 - Loss: 30.7452\n",
      "Processing batch 4084/11884 - Loss: 29.2358\n",
      "Processing batch 4085/11884 - Loss: 31.2676\n",
      "Processing batch 4086/11884 - Loss: 31.2575\n",
      "Processing batch 4087/11884 - Loss: 30.7655\n",
      "Processing batch 4088/11884 - Loss: 31.6168\n",
      "Processing batch 4089/11884 - Loss: 30.3740\n",
      "Processing batch 4090/11884 - Loss: 29.4894\n",
      "Processing batch 4091/11884 - Loss: 30.4187\n",
      "Processing batch 4092/11884 - Loss: 29.6928\n",
      "Processing batch 4093/11884 - Loss: 31.9068\n",
      "Processing batch 4094/11884 - Loss: 30.4405\n",
      "Processing batch 4095/11884 - Loss: 29.9946\n",
      "Processing batch 4096/11884 - Loss: 30.7654\n",
      "Processing batch 4097/11884 - Loss: 30.6728\n",
      "Processing batch 4098/11884 - Loss: 29.8688\n",
      "Processing batch 4099/11884 - Loss: 30.7483\n",
      "Processing batch 4100/11884 - Loss: 27.6277\n",
      "Processing batch 4101/11884 - Loss: 29.7361\n",
      "Processing batch 4102/11884 - Loss: 30.7533\n",
      "Processing batch 4103/11884 - Loss: 28.9771\n",
      "Processing batch 4104/11884 - Loss: 29.8056\n",
      "Processing batch 4105/11884 - Loss: 31.1169\n",
      "Processing batch 4106/11884 - Loss: 29.7860\n",
      "Processing batch 4107/11884 - Loss: 29.5205\n",
      "Processing batch 4108/11884 - Loss: 30.1905\n",
      "Processing batch 4109/11884 - Loss: 30.6263\n",
      "Processing batch 4110/11884 - Loss: 29.4158\n",
      "Processing batch 4111/11884 - Loss: 29.1257\n",
      "Processing batch 4112/11884 - Loss: 28.9835\n",
      "Processing batch 4113/11884 - Loss: 30.6261\n",
      "Processing batch 4114/11884 - Loss: 29.2169\n",
      "Processing batch 4115/11884 - Loss: 30.9636\n",
      "Processing batch 4116/11884 - Loss: 30.5769\n",
      "Processing batch 4117/11884 - Loss: 30.5535\n",
      "Processing batch 4118/11884 - Loss: 29.7788\n",
      "Processing batch 4119/11884 - Loss: 30.1017\n",
      "Processing batch 4120/11884 - Loss: 29.7199\n",
      "Processing batch 4121/11884 - Loss: 29.5961\n",
      "Processing batch 4122/11884 - Loss: 30.5221\n",
      "Processing batch 4123/11884 - Loss: 30.6931\n",
      "Processing batch 4124/11884 - Loss: 30.7120\n",
      "Processing batch 4125/11884 - Loss: 31.4303\n",
      "Processing batch 4126/11884 - Loss: 30.6550\n",
      "Processing batch 4127/11884 - Loss: 28.6030\n",
      "Processing batch 4128/11884 - Loss: 31.3553\n",
      "Processing batch 4129/11884 - Loss: 30.2395\n",
      "Processing batch 4130/11884 - Loss: 29.9482\n",
      "Processing batch 4131/11884 - Loss: 30.3542\n",
      "Processing batch 4132/11884 - Loss: 30.5793\n",
      "Processing batch 4133/11884 - Loss: 31.7450\n",
      "Processing batch 4134/11884 - Loss: 30.3173\n",
      "Processing batch 4135/11884 - Loss: 31.5474\n",
      "Processing batch 4136/11884 - Loss: 29.7738\n",
      "Processing batch 4137/11884 - Loss: 30.2905\n",
      "Processing batch 4138/11884 - Loss: 28.3467\n",
      "Processing batch 4139/11884 - Loss: 29.5599\n",
      "Processing batch 4140/11884 - Loss: 29.7082\n",
      "Processing batch 4141/11884 - Loss: 29.5148\n",
      "Processing batch 4142/11884 - Loss: 29.2422\n",
      "Processing batch 4143/11884 - Loss: 29.7410\n",
      "Processing batch 4144/11884 - Loss: 30.2817\n",
      "Processing batch 4145/11884 - Loss: 31.2641\n",
      "Processing batch 4146/11884 - Loss: 30.3662\n",
      "Processing batch 4147/11884 - Loss: 28.6104\n",
      "Processing batch 4148/11884 - Loss: 29.5753\n",
      "Processing batch 4149/11884 - Loss: 29.1492\n",
      "Processing batch 4150/11884 - Loss: 31.4578\n",
      "Processing batch 4151/11884 - Loss: 29.8204\n",
      "Processing batch 4152/11884 - Loss: 29.8129\n",
      "Processing batch 4153/11884 - Loss: 31.0296\n",
      "Processing batch 4154/11884 - Loss: 30.4338\n",
      "Processing batch 4155/11884 - Loss: 29.4805\n",
      "Processing batch 4156/11884 - Loss: 30.8207\n",
      "Processing batch 4157/11884 - Loss: 30.1025\n",
      "Processing batch 4158/11884 - Loss: 29.5345\n",
      "Processing batch 4159/11884 - Loss: 30.6551\n",
      "Processing batch 4160/11884 - Loss: 31.1116\n",
      "Processing batch 4161/11884 - Loss: 29.3614\n",
      "Processing batch 4162/11884 - Loss: 30.9235\n",
      "Processing batch 4163/11884 - Loss: 29.2756\n",
      "Processing batch 4164/11884 - Loss: 31.3107\n",
      "Processing batch 4165/11884 - Loss: 29.4993\n",
      "Processing batch 4166/11884 - Loss: 30.6603\n",
      "Processing batch 4167/11884 - Loss: 29.5127\n",
      "Processing batch 4168/11884 - Loss: 30.6591\n",
      "Processing batch 4169/11884 - Loss: 29.2027\n",
      "Processing batch 4170/11884 - Loss: 29.3023\n",
      "Processing batch 4171/11884 - Loss: 30.4295\n",
      "Processing batch 4172/11884 - Loss: 31.4869\n",
      "Processing batch 4173/11884 - Loss: 30.5180\n",
      "Processing batch 4174/11884 - Loss: 29.7157\n",
      "Processing batch 4175/11884 - Loss: 28.7604\n",
      "Processing batch 4176/11884 - Loss: 29.3426\n",
      "Processing batch 4177/11884 - Loss: 28.7728\n",
      "Processing batch 4178/11884 - Loss: 30.5560\n",
      "Processing batch 4179/11884 - Loss: 30.3104\n",
      "Processing batch 4180/11884 - Loss: 29.7456\n",
      "Processing batch 4181/11884 - Loss: 29.1704\n",
      "Processing batch 4182/11884 - Loss: 30.2431\n",
      "Processing batch 4183/11884 - Loss: 29.6009\n",
      "Processing batch 4184/11884 - Loss: 30.6290\n",
      "Processing batch 4185/11884 - Loss: 29.8036\n",
      "Processing batch 4186/11884 - Loss: 29.8556\n",
      "Processing batch 4187/11884 - Loss: 30.9795\n",
      "Processing batch 4188/11884 - Loss: 30.4088\n",
      "Processing batch 4189/11884 - Loss: 31.5627\n",
      "Processing batch 4190/11884 - Loss: 30.6690\n",
      "Processing batch 4191/11884 - Loss: 29.2108\n",
      "Processing batch 4192/11884 - Loss: 29.6642\n",
      "Processing batch 4193/11884 - Loss: 28.9731\n",
      "Processing batch 4194/11884 - Loss: 29.9287\n",
      "Processing batch 4195/11884 - Loss: 29.5490\n",
      "Processing batch 4196/11884 - Loss: 29.1421\n",
      "Processing batch 4197/11884 - Loss: 29.7934\n",
      "Processing batch 4198/11884 - Loss: 30.6680\n",
      "Processing batch 4199/11884 - Loss: 31.2845\n",
      "Processing batch 4200/11884 - Loss: 29.6215\n",
      "Processing batch 4201/11884 - Loss: 29.9842\n",
      "Processing batch 4202/11884 - Loss: 29.9316\n",
      "Processing batch 4203/11884 - Loss: 29.5818\n",
      "Processing batch 4204/11884 - Loss: 30.1284\n",
      "Processing batch 4205/11884 - Loss: 30.6388\n",
      "Processing batch 4206/11884 - Loss: 31.0436\n",
      "Processing batch 4207/11884 - Loss: 30.5786\n",
      "Processing batch 4208/11884 - Loss: 29.5070\n",
      "Processing batch 4209/11884 - Loss: 30.5173\n",
      "Processing batch 4210/11884 - Loss: 31.1422\n",
      "Processing batch 4211/11884 - Loss: 29.4817\n",
      "Processing batch 4212/11884 - Loss: 30.6935\n",
      "Processing batch 4213/11884 - Loss: 30.1893\n",
      "Processing batch 4214/11884 - Loss: 30.8994\n",
      "Processing batch 4215/11884 - Loss: 30.3623\n",
      "Processing batch 4216/11884 - Loss: 29.5967\n",
      "Processing batch 4217/11884 - Loss: 30.2264\n",
      "Processing batch 4218/11884 - Loss: 28.9062\n",
      "Processing batch 4219/11884 - Loss: 31.0402\n",
      "Processing batch 4220/11884 - Loss: 30.6202\n",
      "Processing batch 4221/11884 - Loss: 29.5748\n",
      "Processing batch 4222/11884 - Loss: 30.1379\n",
      "Processing batch 4223/11884 - Loss: 30.0367\n",
      "Processing batch 4224/11884 - Loss: 29.6738\n",
      "Processing batch 4225/11884 - Loss: 29.9068\n",
      "Processing batch 4226/11884 - Loss: 30.7700\n",
      "Processing batch 4227/11884 - Loss: 29.1864\n",
      "Processing batch 4228/11884 - Loss: 28.9559\n",
      "Processing batch 4229/11884 - Loss: 32.4207\n",
      "Processing batch 4230/11884 - Loss: 29.7722\n",
      "Processing batch 4231/11884 - Loss: 29.1503\n",
      "Processing batch 4232/11884 - Loss: 30.3387\n",
      "Processing batch 4233/11884 - Loss: 30.4366\n",
      "Processing batch 4234/11884 - Loss: 30.0726\n",
      "Processing batch 4235/11884 - Loss: 30.6176\n",
      "Processing batch 4236/11884 - Loss: 30.7807\n",
      "Processing batch 4237/11884 - Loss: 29.9346\n",
      "Processing batch 4238/11884 - Loss: 30.8552\n",
      "Processing batch 4239/11884 - Loss: 30.3129\n",
      "Processing batch 4240/11884 - Loss: 30.2238\n",
      "Processing batch 4241/11884 - Loss: 29.6630\n",
      "Processing batch 4242/11884 - Loss: 31.0024\n",
      "Processing batch 4243/11884 - Loss: 31.2951\n",
      "Processing batch 4244/11884 - Loss: 31.3404\n",
      "Processing batch 4245/11884 - Loss: 30.2948\n",
      "Processing batch 4246/11884 - Loss: 29.8954\n",
      "Processing batch 4247/11884 - Loss: 29.5845\n",
      "Processing batch 4248/11884 - Loss: 30.6549\n",
      "Processing batch 4249/11884 - Loss: 29.4435\n",
      "Processing batch 4250/11884 - Loss: 29.9691\n",
      "Processing batch 4251/11884 - Loss: 30.1181\n",
      "Processing batch 4252/11884 - Loss: 30.8598\n",
      "Processing batch 4253/11884 - Loss: 29.3011\n",
      "Processing batch 4254/11884 - Loss: 30.9699\n",
      "Processing batch 4255/11884 - Loss: 30.9084\n",
      "Processing batch 4256/11884 - Loss: 31.8646\n",
      "Processing batch 4257/11884 - Loss: 29.0262\n",
      "Processing batch 4258/11884 - Loss: 30.7838\n",
      "Processing batch 4259/11884 - Loss: 30.1685\n",
      "Processing batch 4260/11884 - Loss: 28.8787\n",
      "Processing batch 4261/11884 - Loss: 29.3127\n",
      "Processing batch 4262/11884 - Loss: 30.9238\n",
      "Processing batch 4263/11884 - Loss: 29.6248\n",
      "Processing batch 4264/11884 - Loss: 30.7311\n",
      "Processing batch 4265/11884 - Loss: 30.5312\n",
      "Processing batch 4266/11884 - Loss: 30.0430\n",
      "Processing batch 4267/11884 - Loss: 30.4929\n",
      "Processing batch 4268/11884 - Loss: 30.2320\n",
      "Processing batch 4269/11884 - Loss: 31.4645\n",
      "Processing batch 4270/11884 - Loss: 31.0290\n",
      "Processing batch 4271/11884 - Loss: 28.5575\n",
      "Processing batch 4272/11884 - Loss: 30.6546\n",
      "Processing batch 4273/11884 - Loss: 28.8605\n",
      "Processing batch 4274/11884 - Loss: 30.6132\n",
      "Processing batch 4275/11884 - Loss: 29.5893\n",
      "Processing batch 4276/11884 - Loss: 30.1840\n",
      "Processing batch 4277/11884 - Loss: 29.1224\n",
      "Processing batch 4278/11884 - Loss: 30.9207\n",
      "Processing batch 4279/11884 - Loss: 29.8513\n",
      "Processing batch 4280/11884 - Loss: 29.6261\n",
      "Processing batch 4281/11884 - Loss: 30.5084\n",
      "Processing batch 4282/11884 - Loss: 31.3191\n",
      "Processing batch 4283/11884 - Loss: 31.2741\n",
      "Processing batch 4284/11884 - Loss: 30.1592\n",
      "Processing batch 4285/11884 - Loss: 30.3300\n",
      "Processing batch 4286/11884 - Loss: 31.1584\n",
      "Processing batch 4287/11884 - Loss: 30.5020\n",
      "Processing batch 4288/11884 - Loss: 30.1606\n",
      "Processing batch 4289/11884 - Loss: 30.7188\n",
      "Processing batch 4290/11884 - Loss: 31.3940\n",
      "Processing batch 4291/11884 - Loss: 29.6778\n",
      "Processing batch 4292/11884 - Loss: 30.4452\n",
      "Processing batch 4293/11884 - Loss: 29.8764\n",
      "Processing batch 4294/11884 - Loss: 30.3908\n",
      "Processing batch 4295/11884 - Loss: 30.8259\n",
      "Processing batch 4296/11884 - Loss: 30.8619\n",
      "Processing batch 4297/11884 - Loss: 28.9905\n",
      "Processing batch 4298/11884 - Loss: 29.9222\n",
      "Processing batch 4299/11884 - Loss: 29.2694\n",
      "Processing batch 4300/11884 - Loss: 30.5755\n",
      "Processing batch 4301/11884 - Loss: 30.8702\n",
      "Processing batch 4302/11884 - Loss: 30.1989\n",
      "Processing batch 4303/11884 - Loss: 30.4041\n",
      "Processing batch 4304/11884 - Loss: 30.4305\n",
      "Processing batch 4305/11884 - Loss: 29.4954\n",
      "Processing batch 4306/11884 - Loss: 30.0732\n",
      "Processing batch 4307/11884 - Loss: 29.8361\n",
      "Processing batch 4308/11884 - Loss: 30.3414\n",
      "Processing batch 4309/11884 - Loss: 30.4741\n",
      "Processing batch 4310/11884 - Loss: 31.2368\n",
      "Processing batch 4311/11884 - Loss: 30.8961\n",
      "Processing batch 4312/11884 - Loss: 30.9695\n",
      "Processing batch 4313/11884 - Loss: 31.1932\n",
      "Processing batch 4314/11884 - Loss: 30.6437\n",
      "Processing batch 4315/11884 - Loss: 30.2602\n",
      "Processing batch 4316/11884 - Loss: 30.8963\n",
      "Processing batch 4317/11884 - Loss: 30.6353\n",
      "Processing batch 4318/11884 - Loss: 30.6176\n",
      "Processing batch 4319/11884 - Loss: 29.5751\n",
      "Processing batch 4320/11884 - Loss: 31.1067\n",
      "Processing batch 4321/11884 - Loss: 30.9795\n",
      "Processing batch 4322/11884 - Loss: 29.5489\n",
      "Processing batch 4323/11884 - Loss: 29.0542\n",
      "Processing batch 4324/11884 - Loss: 29.2721\n",
      "Processing batch 4325/11884 - Loss: 28.5362\n",
      "Processing batch 4326/11884 - Loss: 29.5429\n",
      "Processing batch 4327/11884 - Loss: 28.6346\n",
      "Processing batch 4328/11884 - Loss: 30.5497\n",
      "Processing batch 4329/11884 - Loss: 30.1678\n",
      "Processing batch 4330/11884 - Loss: 30.2238\n",
      "Processing batch 4331/11884 - Loss: 29.5429\n",
      "Processing batch 4332/11884 - Loss: 31.5574\n",
      "Processing batch 4333/11884 - Loss: 28.8871\n",
      "Processing batch 4334/11884 - Loss: 29.3731\n",
      "Processing batch 4335/11884 - Loss: 29.2337\n",
      "Processing batch 4336/11884 - Loss: 30.1970\n",
      "Processing batch 4337/11884 - Loss: 30.9037\n",
      "Processing batch 4338/11884 - Loss: 30.1036\n",
      "Processing batch 4339/11884 - Loss: 29.6579\n",
      "Processing batch 4340/11884 - Loss: 29.8164\n",
      "Processing batch 4341/11884 - Loss: 29.3865\n",
      "Processing batch 4342/11884 - Loss: 30.4528\n",
      "Processing batch 4343/11884 - Loss: 30.3704\n",
      "Processing batch 4344/11884 - Loss: 31.8540\n",
      "Processing batch 4345/11884 - Loss: 29.9578\n",
      "Processing batch 4346/11884 - Loss: 28.1816\n",
      "Processing batch 4347/11884 - Loss: 28.9208\n",
      "Processing batch 4348/11884 - Loss: 30.5772\n",
      "Processing batch 4349/11884 - Loss: 29.8674\n",
      "Processing batch 4350/11884 - Loss: 30.9779\n",
      "Processing batch 4351/11884 - Loss: 30.4754\n",
      "Processing batch 4352/11884 - Loss: 30.3362\n",
      "Processing batch 4353/11884 - Loss: 29.6768\n",
      "Processing batch 4354/11884 - Loss: 29.6611\n",
      "Processing batch 4355/11884 - Loss: 30.6563\n",
      "Processing batch 4356/11884 - Loss: 30.2852\n",
      "Processing batch 4357/11884 - Loss: 30.2763\n",
      "Processing batch 4358/11884 - Loss: 31.3686\n",
      "Processing batch 4359/11884 - Loss: 30.9965\n",
      "Processing batch 4360/11884 - Loss: 29.4200\n",
      "Processing batch 4361/11884 - Loss: 31.6012\n",
      "Processing batch 4362/11884 - Loss: 30.5811\n",
      "Processing batch 4363/11884 - Loss: 30.9639\n",
      "Processing batch 4364/11884 - Loss: 30.1703\n",
      "Processing batch 4365/11884 - Loss: 29.4929\n",
      "Processing batch 4366/11884 - Loss: 30.7627\n",
      "Processing batch 4367/11884 - Loss: 30.5731\n",
      "Processing batch 4368/11884 - Loss: 30.5342\n",
      "Processing batch 4369/11884 - Loss: 30.0733\n",
      "Processing batch 4370/11884 - Loss: 30.6361\n",
      "Processing batch 4371/11884 - Loss: 29.4474\n",
      "Processing batch 4372/11884 - Loss: 29.1683\n",
      "Processing batch 4373/11884 - Loss: 28.6752\n",
      "Processing batch 4374/11884 - Loss: 29.6255\n",
      "Processing batch 4375/11884 - Loss: 31.2343\n",
      "Processing batch 4376/11884 - Loss: 30.8031\n",
      "Processing batch 4377/11884 - Loss: 29.6911\n",
      "Processing batch 4378/11884 - Loss: 29.8934\n",
      "Processing batch 4379/11884 - Loss: 29.7033\n",
      "Processing batch 4380/11884 - Loss: 29.5611\n",
      "Processing batch 4381/11884 - Loss: 30.6288\n",
      "Processing batch 4382/11884 - Loss: 31.2605\n",
      "Processing batch 4383/11884 - Loss: 29.9765\n",
      "Processing batch 4384/11884 - Loss: 30.1280\n",
      "Processing batch 4385/11884 - Loss: 29.4038\n",
      "Processing batch 4386/11884 - Loss: 29.0963\n",
      "Processing batch 4387/11884 - Loss: 30.0783\n",
      "Processing batch 4388/11884 - Loss: 30.0069\n",
      "Processing batch 4389/11884 - Loss: 30.0339\n",
      "Processing batch 4390/11884 - Loss: 29.0082\n",
      "Processing batch 4391/11884 - Loss: 29.8489\n",
      "Processing batch 4392/11884 - Loss: 29.7277\n",
      "Processing batch 4393/11884 - Loss: 30.6566\n",
      "Processing batch 4394/11884 - Loss: 29.8529\n",
      "Processing batch 4395/11884 - Loss: 29.2012\n",
      "Processing batch 4396/11884 - Loss: 30.3990\n",
      "Processing batch 4397/11884 - Loss: 31.8486\n",
      "Processing batch 4398/11884 - Loss: 28.6154\n",
      "Processing batch 4399/11884 - Loss: 30.9508\n",
      "Processing batch 4400/11884 - Loss: 31.4693\n",
      "Processing batch 4401/11884 - Loss: 29.9507\n",
      "Processing batch 4402/11884 - Loss: 29.5197\n",
      "Processing batch 4403/11884 - Loss: 30.7127\n",
      "Processing batch 4404/11884 - Loss: 30.4566\n",
      "Processing batch 4405/11884 - Loss: 29.7684\n",
      "Processing batch 4406/11884 - Loss: 31.0898\n",
      "Processing batch 4407/11884 - Loss: 31.4245\n",
      "Processing batch 4408/11884 - Loss: 30.0023\n",
      "Processing batch 4409/11884 - Loss: 29.5115\n",
      "Processing batch 4410/11884 - Loss: 29.6595\n",
      "Processing batch 4411/11884 - Loss: 29.4856\n",
      "Processing batch 4412/11884 - Loss: 30.2405\n",
      "Processing batch 4413/11884 - Loss: 29.7484\n",
      "Processing batch 4414/11884 - Loss: 31.0648\n",
      "Processing batch 4415/11884 - Loss: 31.0284\n",
      "Processing batch 4416/11884 - Loss: 30.9670\n",
      "Processing batch 4417/11884 - Loss: 30.3856\n",
      "Processing batch 4418/11884 - Loss: 30.2118\n",
      "Processing batch 4419/11884 - Loss: 30.3940\n",
      "Processing batch 4420/11884 - Loss: 31.1951\n",
      "Processing batch 4421/11884 - Loss: 29.0178\n",
      "Processing batch 4422/11884 - Loss: 30.6466\n",
      "Processing batch 4423/11884 - Loss: 29.8252\n",
      "Processing batch 4424/11884 - Loss: 30.2257\n",
      "Processing batch 4425/11884 - Loss: 30.4968\n",
      "Processing batch 4426/11884 - Loss: 30.7698\n",
      "Processing batch 4427/11884 - Loss: 30.5331\n",
      "Processing batch 4428/11884 - Loss: 29.6016\n",
      "Processing batch 4429/11884 - Loss: 29.7568\n",
      "Processing batch 4430/11884 - Loss: 29.0983\n",
      "Processing batch 4431/11884 - Loss: 30.8330\n",
      "Processing batch 4432/11884 - Loss: 31.0372\n",
      "Processing batch 4433/11884 - Loss: 31.0679\n",
      "Processing batch 4434/11884 - Loss: 29.8079\n",
      "Processing batch 4435/11884 - Loss: 30.1161\n",
      "Processing batch 4436/11884 - Loss: 29.1492\n",
      "Processing batch 4437/11884 - Loss: 29.7444\n",
      "Processing batch 4438/11884 - Loss: 30.5213\n",
      "Processing batch 4439/11884 - Loss: 31.4231\n",
      "Processing batch 4440/11884 - Loss: 29.6540\n",
      "Processing batch 4441/11884 - Loss: 30.5226\n",
      "Processing batch 4442/11884 - Loss: 28.7569\n",
      "Processing batch 4443/11884 - Loss: 31.6883\n",
      "Processing batch 4444/11884 - Loss: 29.9879\n",
      "Processing batch 4445/11884 - Loss: 31.3604\n",
      "Processing batch 4446/11884 - Loss: 29.0482\n",
      "Processing batch 4447/11884 - Loss: 29.7030\n",
      "Processing batch 4448/11884 - Loss: 31.5117\n",
      "Processing batch 4449/11884 - Loss: 29.2265\n",
      "Processing batch 4450/11884 - Loss: 30.0703\n",
      "Processing batch 4451/11884 - Loss: 31.7312\n",
      "Processing batch 4452/11884 - Loss: 30.0928\n",
      "Processing batch 4453/11884 - Loss: 30.2522\n",
      "Processing batch 4454/11884 - Loss: 30.3547\n",
      "Processing batch 4455/11884 - Loss: 30.6887\n",
      "Processing batch 4456/11884 - Loss: 29.7325\n",
      "Processing batch 4457/11884 - Loss: 30.8546\n",
      "Processing batch 4458/11884 - Loss: 29.3345\n",
      "Processing batch 4459/11884 - Loss: 28.9249\n",
      "Processing batch 4460/11884 - Loss: 29.9954\n",
      "Processing batch 4461/11884 - Loss: 29.0068\n",
      "Processing batch 4462/11884 - Loss: 30.3357\n",
      "Processing batch 4463/11884 - Loss: 29.6349\n",
      "Processing batch 4464/11884 - Loss: 30.2731\n",
      "Processing batch 4465/11884 - Loss: 31.2688\n",
      "Processing batch 4466/11884 - Loss: 30.5506\n",
      "Processing batch 4467/11884 - Loss: 30.6435\n",
      "Processing batch 4468/11884 - Loss: 30.9336\n",
      "Processing batch 4469/11884 - Loss: 29.8800\n",
      "Processing batch 4470/11884 - Loss: 30.6173\n",
      "Processing batch 4471/11884 - Loss: 29.9998\n",
      "Processing batch 4472/11884 - Loss: 30.2693\n",
      "Processing batch 4473/11884 - Loss: 30.2689\n",
      "Processing batch 4474/11884 - Loss: 31.0025\n",
      "Processing batch 4475/11884 - Loss: 29.7156\n",
      "Processing batch 4476/11884 - Loss: 30.5758\n",
      "Processing batch 4477/11884 - Loss: 30.5598\n",
      "Processing batch 4478/11884 - Loss: 30.9366\n",
      "Processing batch 4479/11884 - Loss: 30.6296\n",
      "Processing batch 4480/11884 - Loss: 31.0921\n",
      "Processing batch 4481/11884 - Loss: 30.5157\n",
      "Processing batch 4482/11884 - Loss: 31.3685\n",
      "Processing batch 4483/11884 - Loss: 30.4019\n",
      "Processing batch 4484/11884 - Loss: 30.0727\n",
      "Processing batch 4485/11884 - Loss: 29.7284\n",
      "Processing batch 4486/11884 - Loss: 30.5375\n",
      "Processing batch 4487/11884 - Loss: 31.2339\n",
      "Processing batch 4488/11884 - Loss: 30.3810\n",
      "Processing batch 4489/11884 - Loss: 30.0029\n",
      "Processing batch 4490/11884 - Loss: 30.2868\n",
      "Processing batch 4491/11884 - Loss: 30.4794\n",
      "Processing batch 4492/11884 - Loss: 30.5403\n",
      "Processing batch 4493/11884 - Loss: 31.4291\n",
      "Processing batch 4494/11884 - Loss: 29.5509\n",
      "Processing batch 4495/11884 - Loss: 29.4791\n",
      "Processing batch 4496/11884 - Loss: 30.8917\n",
      "Processing batch 4497/11884 - Loss: 30.2913\n",
      "Processing batch 4498/11884 - Loss: 29.7148\n",
      "Processing batch 4499/11884 - Loss: 29.0553\n",
      "Processing batch 4500/11884 - Loss: 30.8091\n",
      "Processing batch 4501/11884 - Loss: 30.7438\n",
      "Processing batch 4502/11884 - Loss: 31.1269\n",
      "Processing batch 4503/11884 - Loss: 29.3484\n",
      "Processing batch 4504/11884 - Loss: 30.0763\n",
      "Processing batch 4505/11884 - Loss: 30.8742\n",
      "Processing batch 4506/11884 - Loss: 30.9176\n",
      "Processing batch 4507/11884 - Loss: 30.1106\n",
      "Processing batch 4508/11884 - Loss: 30.4032\n",
      "Processing batch 4509/11884 - Loss: 29.5787\n",
      "Processing batch 4510/11884 - Loss: 29.8786\n",
      "Processing batch 4511/11884 - Loss: 29.9602\n",
      "Processing batch 4512/11884 - Loss: 29.4955\n",
      "Processing batch 4513/11884 - Loss: 30.3016\n",
      "Processing batch 4514/11884 - Loss: 30.4718\n",
      "Processing batch 4515/11884 - Loss: 30.0372\n",
      "Processing batch 4516/11884 - Loss: 29.7497\n",
      "Processing batch 4517/11884 - Loss: 31.4273\n",
      "Processing batch 4518/11884 - Loss: 30.9449\n",
      "Processing batch 4519/11884 - Loss: 31.6158\n",
      "Processing batch 4520/11884 - Loss: 29.5611\n",
      "Processing batch 4521/11884 - Loss: 29.7021\n",
      "Processing batch 4522/11884 - Loss: 29.5129\n",
      "Processing batch 4523/11884 - Loss: 30.9327\n",
      "Processing batch 4524/11884 - Loss: 29.7379\n",
      "Processing batch 4525/11884 - Loss: 30.1181\n",
      "Processing batch 4526/11884 - Loss: 29.1419\n",
      "Processing batch 4527/11884 - Loss: 29.5664\n",
      "Processing batch 4528/11884 - Loss: 28.7638\n",
      "Processing batch 4529/11884 - Loss: 32.6028\n",
      "Processing batch 4530/11884 - Loss: 29.0784\n",
      "Processing batch 4531/11884 - Loss: 30.1560\n",
      "Processing batch 4532/11884 - Loss: 30.4174\n",
      "Processing batch 4533/11884 - Loss: 30.5281\n",
      "Processing batch 4534/11884 - Loss: 29.9389\n",
      "Processing batch 4535/11884 - Loss: 29.4201\n",
      "Processing batch 4536/11884 - Loss: 29.9654\n",
      "Processing batch 4537/11884 - Loss: 30.2981\n",
      "Processing batch 4538/11884 - Loss: 29.7885\n",
      "Processing batch 4539/11884 - Loss: 28.7491\n",
      "Processing batch 4540/11884 - Loss: 30.2484\n",
      "Processing batch 4541/11884 - Loss: 30.8297\n",
      "Processing batch 4542/11884 - Loss: 30.0323\n",
      "Processing batch 4543/11884 - Loss: 31.6095\n",
      "Processing batch 4544/11884 - Loss: 30.3275\n",
      "Processing batch 4545/11884 - Loss: 31.0963\n",
      "Processing batch 4546/11884 - Loss: 29.7744\n",
      "Processing batch 4547/11884 - Loss: 29.9242\n",
      "Processing batch 4548/11884 - Loss: 30.2249\n",
      "Processing batch 4549/11884 - Loss: 30.7484\n",
      "Processing batch 4550/11884 - Loss: 27.8930\n",
      "Processing batch 4551/11884 - Loss: 29.9890\n",
      "Processing batch 4552/11884 - Loss: 28.7970\n",
      "Processing batch 4553/11884 - Loss: 29.8181\n",
      "Processing batch 4554/11884 - Loss: 30.7638\n",
      "Processing batch 4555/11884 - Loss: 29.3044\n",
      "Processing batch 4556/11884 - Loss: 30.6944\n",
      "Processing batch 4557/11884 - Loss: 29.3705\n",
      "Processing batch 4558/11884 - Loss: 29.7932\n",
      "Processing batch 4559/11884 - Loss: 29.7143\n",
      "Processing batch 4560/11884 - Loss: 30.8324\n",
      "Processing batch 4561/11884 - Loss: 30.0161\n",
      "Processing batch 4562/11884 - Loss: 30.9806\n",
      "Processing batch 4563/11884 - Loss: 30.5218\n",
      "Processing batch 4564/11884 - Loss: 28.4041\n",
      "Processing batch 4565/11884 - Loss: 29.6488\n",
      "Processing batch 4566/11884 - Loss: 30.6134\n",
      "Processing batch 4567/11884 - Loss: 30.0753\n",
      "Processing batch 4568/11884 - Loss: 29.3296\n",
      "Processing batch 4569/11884 - Loss: 30.0938\n",
      "Processing batch 4570/11884 - Loss: 29.7972\n",
      "Processing batch 4571/11884 - Loss: 29.5467\n",
      "Processing batch 4572/11884 - Loss: 30.8659\n",
      "Processing batch 4573/11884 - Loss: 30.2296\n",
      "Processing batch 4574/11884 - Loss: 29.6421\n",
      "Processing batch 4575/11884 - Loss: 29.7030\n",
      "Processing batch 4576/11884 - Loss: 31.7143\n",
      "Processing batch 4577/11884 - Loss: 29.1842\n",
      "Processing batch 4578/11884 - Loss: 29.4535\n",
      "Processing batch 4579/11884 - Loss: 29.5258\n",
      "Processing batch 4580/11884 - Loss: 30.3127\n",
      "Processing batch 4581/11884 - Loss: 31.5978\n",
      "Processing batch 4582/11884 - Loss: 29.2560\n",
      "Processing batch 4583/11884 - Loss: 30.0543\n",
      "Processing batch 4584/11884 - Loss: 31.0139\n",
      "Processing batch 4585/11884 - Loss: 29.8759\n",
      "Processing batch 4586/11884 - Loss: 29.4135\n",
      "Processing batch 4587/11884 - Loss: 30.0879\n",
      "Processing batch 4588/11884 - Loss: 30.9072\n",
      "Processing batch 4589/11884 - Loss: 30.4029\n",
      "Processing batch 4590/11884 - Loss: 30.1076\n",
      "Processing batch 4591/11884 - Loss: 29.4796\n",
      "Processing batch 4592/11884 - Loss: 29.2970\n",
      "Processing batch 4593/11884 - Loss: 31.4698\n",
      "Processing batch 4594/11884 - Loss: 29.3638\n",
      "Processing batch 4595/11884 - Loss: 30.6219\n",
      "Processing batch 4596/11884 - Loss: 30.3158\n",
      "Processing batch 4597/11884 - Loss: 29.6445\n",
      "Processing batch 4598/11884 - Loss: 29.3807\n",
      "Processing batch 4599/11884 - Loss: 31.7045\n",
      "Processing batch 4600/11884 - Loss: 29.4464\n",
      "Processing batch 4601/11884 - Loss: 29.9530\n",
      "Processing batch 4602/11884 - Loss: 29.3298\n",
      "Processing batch 4603/11884 - Loss: 31.0218\n",
      "Processing batch 4604/11884 - Loss: 30.0003\n",
      "Processing batch 4605/11884 - Loss: 29.2280\n",
      "Processing batch 4606/11884 - Loss: 29.3915\n",
      "Processing batch 4607/11884 - Loss: 29.5670\n",
      "Processing batch 4608/11884 - Loss: 30.6537\n",
      "Processing batch 4609/11884 - Loss: 31.4045\n",
      "Processing batch 4610/11884 - Loss: 28.8577\n",
      "Processing batch 4611/11884 - Loss: 31.5114\n",
      "Processing batch 4612/11884 - Loss: 30.4817\n",
      "Processing batch 4613/11884 - Loss: 29.0953\n",
      "Processing batch 4614/11884 - Loss: 29.4065\n",
      "Processing batch 4615/11884 - Loss: 29.4840\n",
      "Processing batch 4616/11884 - Loss: 30.0639\n",
      "Processing batch 4617/11884 - Loss: 29.5168\n",
      "Processing batch 4618/11884 - Loss: 29.7551\n",
      "Processing batch 4619/11884 - Loss: 29.2184\n",
      "Processing batch 4620/11884 - Loss: 30.6176\n",
      "Processing batch 4621/11884 - Loss: 30.9291\n",
      "Processing batch 4622/11884 - Loss: 31.8367\n",
      "Processing batch 4623/11884 - Loss: 30.5096\n",
      "Processing batch 4624/11884 - Loss: 30.6022\n",
      "Processing batch 4625/11884 - Loss: 30.4272\n",
      "Processing batch 4626/11884 - Loss: 30.6750\n",
      "Processing batch 4627/11884 - Loss: 30.0954\n",
      "Processing batch 4628/11884 - Loss: 30.0092\n",
      "Processing batch 4629/11884 - Loss: 29.7693\n",
      "Processing batch 4630/11884 - Loss: 29.6482\n",
      "Processing batch 4631/11884 - Loss: 30.8815\n",
      "Processing batch 4632/11884 - Loss: 29.8923\n",
      "Processing batch 4633/11884 - Loss: 30.1435\n",
      "Processing batch 4634/11884 - Loss: 29.8906\n",
      "Processing batch 4635/11884 - Loss: 29.7751\n",
      "Processing batch 4636/11884 - Loss: 29.6711\n",
      "Processing batch 4637/11884 - Loss: 29.1519\n",
      "Processing batch 4638/11884 - Loss: 29.3704\n",
      "Processing batch 4639/11884 - Loss: 29.8673\n",
      "Processing batch 4640/11884 - Loss: 30.5960\n",
      "Processing batch 4641/11884 - Loss: 30.4915\n",
      "Processing batch 4642/11884 - Loss: 29.5863\n",
      "Processing batch 4643/11884 - Loss: 31.8511\n",
      "Processing batch 4644/11884 - Loss: 30.8048\n",
      "Processing batch 4645/11884 - Loss: 29.8006\n",
      "Processing batch 4646/11884 - Loss: 30.6792\n",
      "Processing batch 4647/11884 - Loss: 29.7981\n",
      "Processing batch 4648/11884 - Loss: 30.8882\n",
      "Processing batch 4649/11884 - Loss: 29.9659\n",
      "Processing batch 4650/11884 - Loss: 30.4605\n",
      "Processing batch 4651/11884 - Loss: 29.7206\n",
      "Processing batch 4652/11884 - Loss: 30.5632\n",
      "Processing batch 4653/11884 - Loss: 29.8691\n",
      "Processing batch 4654/11884 - Loss: 31.0170\n",
      "Processing batch 4655/11884 - Loss: 31.3971\n",
      "Processing batch 4656/11884 - Loss: 30.4630\n",
      "Processing batch 4657/11884 - Loss: 30.0496\n",
      "Processing batch 4658/11884 - Loss: 31.5397\n",
      "Processing batch 4659/11884 - Loss: 30.6829\n",
      "Processing batch 4660/11884 - Loss: 29.1804\n",
      "Processing batch 4661/11884 - Loss: 28.3464\n",
      "Processing batch 4662/11884 - Loss: 28.9414\n",
      "Processing batch 4663/11884 - Loss: 30.0335\n",
      "Processing batch 4664/11884 - Loss: 29.4013\n",
      "Processing batch 4665/11884 - Loss: 29.8903\n",
      "Processing batch 4666/11884 - Loss: 30.3698\n",
      "Processing batch 4667/11884 - Loss: 29.0124\n",
      "Processing batch 4668/11884 - Loss: 31.5074\n",
      "Processing batch 4669/11884 - Loss: 28.8746\n",
      "Processing batch 4670/11884 - Loss: 30.1172\n",
      "Processing batch 4671/11884 - Loss: 29.2704\n",
      "Processing batch 4672/11884 - Loss: 28.5686\n",
      "Processing batch 4673/11884 - Loss: 29.9309\n",
      "Processing batch 4674/11884 - Loss: 29.3638\n",
      "Processing batch 4675/11884 - Loss: 31.1564\n",
      "Processing batch 4676/11884 - Loss: 29.7318\n",
      "Processing batch 4677/11884 - Loss: 29.5096\n",
      "Processing batch 4678/11884 - Loss: 28.2369\n",
      "Processing batch 4679/11884 - Loss: 29.0105\n",
      "Processing batch 4680/11884 - Loss: 30.3247\n",
      "Processing batch 4681/11884 - Loss: 30.6141\n",
      "Processing batch 4682/11884 - Loss: 29.4566\n",
      "Processing batch 4683/11884 - Loss: 28.6514\n",
      "Processing batch 4684/11884 - Loss: 28.8790\n",
      "Processing batch 4685/11884 - Loss: 30.1080\n",
      "Processing batch 4686/11884 - Loss: 30.2501\n",
      "Processing batch 4687/11884 - Loss: 29.0482\n",
      "Processing batch 4688/11884 - Loss: 30.2736\n",
      "Processing batch 4689/11884 - Loss: 29.5508\n",
      "Processing batch 4690/11884 - Loss: 29.1374\n",
      "Processing batch 4691/11884 - Loss: 29.2673\n",
      "Processing batch 4692/11884 - Loss: 28.7048\n",
      "Processing batch 4693/11884 - Loss: 29.9510\n",
      "Processing batch 4694/11884 - Loss: 30.0753\n",
      "Processing batch 4695/11884 - Loss: 28.8121\n",
      "Processing batch 4696/11884 - Loss: 29.2554\n",
      "Processing batch 4697/11884 - Loss: 29.8561\n",
      "Processing batch 4698/11884 - Loss: 30.5083\n",
      "Processing batch 4699/11884 - Loss: 30.2556\n",
      "Processing batch 4700/11884 - Loss: 30.2862\n",
      "Processing batch 4701/11884 - Loss: 30.5448\n",
      "Processing batch 4702/11884 - Loss: 30.1827\n",
      "Processing batch 4703/11884 - Loss: 30.1830\n",
      "Processing batch 4704/11884 - Loss: 30.1673\n",
      "Processing batch 4705/11884 - Loss: 29.4220\n",
      "Processing batch 4706/11884 - Loss: 30.6600\n",
      "Processing batch 4707/11884 - Loss: 30.9553\n",
      "Processing batch 4708/11884 - Loss: 30.8087\n",
      "Processing batch 4709/11884 - Loss: 29.8684\n",
      "Processing batch 4710/11884 - Loss: 29.9941\n",
      "Processing batch 4711/11884 - Loss: 30.8234\n",
      "Processing batch 4712/11884 - Loss: 29.0489\n",
      "Processing batch 4713/11884 - Loss: 29.7344\n",
      "Processing batch 4714/11884 - Loss: 29.6490\n",
      "Processing batch 4715/11884 - Loss: 31.4978\n",
      "Processing batch 4716/11884 - Loss: 30.4615\n",
      "Processing batch 4717/11884 - Loss: 30.7700\n",
      "Processing batch 4718/11884 - Loss: 28.9656\n",
      "Processing batch 4719/11884 - Loss: 29.6999\n",
      "Processing batch 4720/11884 - Loss: 29.3169\n",
      "Processing batch 4721/11884 - Loss: 30.2119\n",
      "Processing batch 4722/11884 - Loss: 28.2564\n",
      "Processing batch 4723/11884 - Loss: 30.3258\n",
      "Processing batch 4724/11884 - Loss: 29.9717\n",
      "Processing batch 4725/11884 - Loss: 30.3473\n",
      "Processing batch 4726/11884 - Loss: 29.0940\n",
      "Processing batch 4727/11884 - Loss: 29.5415\n",
      "Processing batch 4728/11884 - Loss: 29.7644\n",
      "Processing batch 4729/11884 - Loss: 31.3648\n",
      "Processing batch 4730/11884 - Loss: 29.8424\n",
      "Processing batch 4731/11884 - Loss: 31.0158\n",
      "Processing batch 4732/11884 - Loss: 29.9843\n",
      "Processing batch 4733/11884 - Loss: 30.0640\n",
      "Processing batch 4734/11884 - Loss: 30.1145\n",
      "Processing batch 4735/11884 - Loss: 31.2828\n",
      "Processing batch 4736/11884 - Loss: 29.8558\n",
      "Processing batch 4737/11884 - Loss: 30.3962\n",
      "Processing batch 4738/11884 - Loss: 30.8219\n",
      "Processing batch 4739/11884 - Loss: 28.7527\n",
      "Processing batch 4740/11884 - Loss: 29.6405\n",
      "Processing batch 4741/11884 - Loss: 30.1278\n",
      "Processing batch 4742/11884 - Loss: 29.5942\n",
      "Processing batch 4743/11884 - Loss: 29.6844\n",
      "Processing batch 4744/11884 - Loss: 30.4715\n",
      "Processing batch 4745/11884 - Loss: 30.3137\n",
      "Processing batch 4746/11884 - Loss: 30.5891\n",
      "Processing batch 4747/11884 - Loss: 30.7813\n",
      "Processing batch 4748/11884 - Loss: 29.7409\n",
      "Processing batch 4749/11884 - Loss: 30.7517\n",
      "Processing batch 4750/11884 - Loss: 29.8988\n",
      "Processing batch 4751/11884 - Loss: 29.3908\n",
      "Processing batch 4752/11884 - Loss: 28.9528\n",
      "Processing batch 4753/11884 - Loss: 31.3260\n",
      "Processing batch 4754/11884 - Loss: 30.1426\n",
      "Processing batch 4755/11884 - Loss: 29.9803\n",
      "Processing batch 4756/11884 - Loss: 30.3303\n",
      "Processing batch 4757/11884 - Loss: 29.5502\n",
      "Processing batch 4758/11884 - Loss: 29.7017\n",
      "Processing batch 4759/11884 - Loss: 30.6720\n",
      "Processing batch 4760/11884 - Loss: 30.5253\n",
      "Processing batch 4761/11884 - Loss: 30.4657\n",
      "Processing batch 4762/11884 - Loss: 28.7185\n",
      "Processing batch 4763/11884 - Loss: 29.6451\n",
      "Processing batch 4764/11884 - Loss: 28.3312\n",
      "Processing batch 4765/11884 - Loss: 29.4367\n",
      "Processing batch 4766/11884 - Loss: 29.7203\n",
      "Processing batch 4767/11884 - Loss: 28.8573\n",
      "Processing batch 4768/11884 - Loss: 31.1122\n",
      "Processing batch 4769/11884 - Loss: 30.4680\n",
      "Processing batch 4770/11884 - Loss: 29.3287\n",
      "Processing batch 4771/11884 - Loss: 30.5328\n",
      "Processing batch 4772/11884 - Loss: 30.5284\n",
      "Processing batch 4773/11884 - Loss: 31.8807\n",
      "Processing batch 4774/11884 - Loss: 30.2910\n",
      "Processing batch 4775/11884 - Loss: 30.4146\n",
      "Processing batch 4776/11884 - Loss: 31.1676\n",
      "Processing batch 4777/11884 - Loss: 30.4544\n",
      "Processing batch 4778/11884 - Loss: 29.6799\n",
      "Processing batch 4779/11884 - Loss: 29.4994\n",
      "Processing batch 4780/11884 - Loss: 30.9735\n",
      "Processing batch 4781/11884 - Loss: 30.4887\n",
      "Processing batch 4782/11884 - Loss: 29.7462\n",
      "Processing batch 4783/11884 - Loss: 29.6456\n",
      "Processing batch 4784/11884 - Loss: 30.3617\n",
      "Processing batch 4785/11884 - Loss: 30.7704\n",
      "Processing batch 4786/11884 - Loss: 29.8374\n",
      "Processing batch 4787/11884 - Loss: 31.4243\n",
      "Processing batch 4788/11884 - Loss: 29.3276\n",
      "Processing batch 4789/11884 - Loss: 30.8739\n",
      "Processing batch 4790/11884 - Loss: 29.4855\n",
      "Processing batch 4791/11884 - Loss: 30.2184\n",
      "Processing batch 4792/11884 - Loss: 32.0602\n",
      "Processing batch 4793/11884 - Loss: 28.8864\n",
      "Processing batch 4794/11884 - Loss: 28.8297\n",
      "Processing batch 4795/11884 - Loss: 29.7359\n",
      "Processing batch 4796/11884 - Loss: 30.5560\n",
      "Processing batch 4797/11884 - Loss: 29.4342\n",
      "Processing batch 4798/11884 - Loss: 30.7179\n",
      "Processing batch 4799/11884 - Loss: 30.3384\n",
      "Processing batch 4800/11884 - Loss: 29.3247\n",
      "Processing batch 4801/11884 - Loss: 29.5445\n",
      "Processing batch 4802/11884 - Loss: 31.0770\n",
      "Processing batch 4803/11884 - Loss: 29.9436\n",
      "Processing batch 4804/11884 - Loss: 31.0548\n",
      "Processing batch 4805/11884 - Loss: 29.1157\n",
      "Processing batch 4806/11884 - Loss: 30.9053\n",
      "Processing batch 4807/11884 - Loss: 30.5849\n",
      "Processing batch 4808/11884 - Loss: 29.4592\n",
      "Processing batch 4809/11884 - Loss: 31.4902\n",
      "Processing batch 4810/11884 - Loss: 30.6369\n",
      "Processing batch 4811/11884 - Loss: 29.8450\n",
      "Processing batch 4812/11884 - Loss: 30.4186\n",
      "Processing batch 4813/11884 - Loss: 30.6509\n",
      "Processing batch 4814/11884 - Loss: 31.8596\n",
      "Processing batch 4815/11884 - Loss: 30.2759\n",
      "Processing batch 4816/11884 - Loss: 31.6773\n",
      "Processing batch 4817/11884 - Loss: 30.3954\n",
      "Processing batch 4818/11884 - Loss: 30.8454\n",
      "Processing batch 4819/11884 - Loss: 29.9030\n",
      "Processing batch 4820/11884 - Loss: 30.0705\n",
      "Processing batch 4821/11884 - Loss: 28.3156\n",
      "Processing batch 4822/11884 - Loss: 29.9819\n",
      "Processing batch 4823/11884 - Loss: 31.5127\n",
      "Processing batch 4824/11884 - Loss: 29.1209\n",
      "Processing batch 4825/11884 - Loss: 28.8362\n",
      "Processing batch 4826/11884 - Loss: 28.9556\n",
      "Processing batch 4827/11884 - Loss: 29.2712\n",
      "Processing batch 4828/11884 - Loss: 29.2814\n",
      "Processing batch 4829/11884 - Loss: 30.1371\n",
      "Processing batch 4830/11884 - Loss: 29.5047\n",
      "Processing batch 4831/11884 - Loss: 30.4760\n",
      "Processing batch 4832/11884 - Loss: 30.3565\n",
      "Processing batch 4833/11884 - Loss: 29.8978\n",
      "Processing batch 4834/11884 - Loss: 29.6339\n",
      "Processing batch 4835/11884 - Loss: 28.8242\n",
      "Processing batch 4836/11884 - Loss: 29.3164\n",
      "Processing batch 4837/11884 - Loss: 30.1662\n",
      "Processing batch 4838/11884 - Loss: 30.0489\n",
      "Processing batch 4839/11884 - Loss: 29.7363\n",
      "Processing batch 4840/11884 - Loss: 30.6413\n",
      "Processing batch 4841/11884 - Loss: 31.5702\n",
      "Processing batch 4842/11884 - Loss: 30.2890\n",
      "Processing batch 4843/11884 - Loss: 30.0034\n",
      "Processing batch 4844/11884 - Loss: 30.4150\n",
      "Processing batch 4845/11884 - Loss: 29.2639\n",
      "Processing batch 4846/11884 - Loss: 30.4382\n",
      "Processing batch 4847/11884 - Loss: 30.2412\n",
      "Processing batch 4848/11884 - Loss: 29.5842\n",
      "Processing batch 4849/11884 - Loss: 29.7529\n",
      "Processing batch 4850/11884 - Loss: 30.0518\n",
      "Processing batch 4851/11884 - Loss: 30.3415\n",
      "Processing batch 4852/11884 - Loss: 29.3810\n",
      "Processing batch 4853/11884 - Loss: 31.3665\n",
      "Processing batch 4854/11884 - Loss: 29.2837\n",
      "Processing batch 4855/11884 - Loss: 29.1726\n",
      "Processing batch 4856/11884 - Loss: 29.6501\n",
      "Processing batch 4857/11884 - Loss: 30.2622\n",
      "Processing batch 4858/11884 - Loss: 30.1503\n",
      "Processing batch 4859/11884 - Loss: 31.0325\n",
      "Processing batch 4860/11884 - Loss: 30.9056\n",
      "Processing batch 4861/11884 - Loss: 29.5879\n",
      "Processing batch 4862/11884 - Loss: 30.5919\n",
      "Processing batch 4863/11884 - Loss: 29.3734\n",
      "Processing batch 4864/11884 - Loss: 30.5212\n",
      "Processing batch 4865/11884 - Loss: 31.1231\n",
      "Processing batch 4866/11884 - Loss: 28.9796\n",
      "Processing batch 4867/11884 - Loss: 30.7375\n",
      "Processing batch 4868/11884 - Loss: 29.5490\n",
      "Processing batch 4869/11884 - Loss: 30.6650\n",
      "Processing batch 4870/11884 - Loss: 29.5327\n",
      "Processing batch 4871/11884 - Loss: 29.7659\n",
      "Processing batch 4872/11884 - Loss: 29.2062\n",
      "Processing batch 4873/11884 - Loss: 30.2551\n",
      "Processing batch 4874/11884 - Loss: 31.5586\n",
      "Processing batch 4875/11884 - Loss: 30.3678\n",
      "Processing batch 4876/11884 - Loss: 29.6855\n",
      "Processing batch 4877/11884 - Loss: 30.7053\n",
      "Processing batch 4878/11884 - Loss: 30.4296\n",
      "Processing batch 4879/11884 - Loss: 30.6009\n",
      "Processing batch 4880/11884 - Loss: 30.4725\n",
      "Processing batch 4881/11884 - Loss: 30.4593\n",
      "Processing batch 4882/11884 - Loss: 31.9542\n",
      "Processing batch 4883/11884 - Loss: 28.7194\n",
      "Processing batch 4884/11884 - Loss: 31.7043\n",
      "Processing batch 4885/11884 - Loss: 30.5882\n",
      "Processing batch 4886/11884 - Loss: 30.5838\n",
      "Processing batch 4887/11884 - Loss: 29.6566\n",
      "Processing batch 4888/11884 - Loss: 31.2373\n",
      "Processing batch 4889/11884 - Loss: 30.7310\n",
      "Processing batch 4890/11884 - Loss: 31.2861\n",
      "Processing batch 4891/11884 - Loss: 30.3231\n",
      "Processing batch 4892/11884 - Loss: 29.4365\n",
      "Processing batch 4893/11884 - Loss: 29.5201\n",
      "Processing batch 4894/11884 - Loss: 31.4193\n",
      "Processing batch 4895/11884 - Loss: 30.0447\n",
      "Processing batch 4896/11884 - Loss: 29.4041\n",
      "Processing batch 4897/11884 - Loss: 31.5562\n",
      "Processing batch 4898/11884 - Loss: 29.0182\n",
      "Processing batch 4899/11884 - Loss: 29.3103\n",
      "Processing batch 4900/11884 - Loss: 28.6572\n",
      "Processing batch 4901/11884 - Loss: 29.7987\n",
      "Processing batch 4902/11884 - Loss: 29.9392\n",
      "Processing batch 4903/11884 - Loss: 30.6400\n",
      "Processing batch 4904/11884 - Loss: 30.1229\n",
      "Processing batch 4905/11884 - Loss: 30.2287\n",
      "Processing batch 4906/11884 - Loss: 30.0172\n",
      "Processing batch 4907/11884 - Loss: 31.2419\n",
      "Processing batch 4908/11884 - Loss: 29.7445\n",
      "Processing batch 4909/11884 - Loss: 30.5830\n",
      "Processing batch 4910/11884 - Loss: 29.8582\n",
      "Processing batch 4911/11884 - Loss: 31.2322\n",
      "Processing batch 4912/11884 - Loss: 29.7916\n",
      "Processing batch 4913/11884 - Loss: 29.3992\n",
      "Processing batch 4914/11884 - Loss: 31.1306\n",
      "Processing batch 4915/11884 - Loss: 31.3742\n",
      "Processing batch 4916/11884 - Loss: 30.5964\n",
      "Processing batch 4917/11884 - Loss: 30.8629\n",
      "Processing batch 4918/11884 - Loss: 31.5139\n",
      "Processing batch 4919/11884 - Loss: 29.5569\n",
      "Processing batch 4920/11884 - Loss: 29.8552\n",
      "Processing batch 4921/11884 - Loss: 29.6830\n",
      "Processing batch 4922/11884 - Loss: 31.5623\n",
      "Processing batch 4923/11884 - Loss: 30.1788\n",
      "Processing batch 4924/11884 - Loss: 30.4757\n",
      "Processing batch 4925/11884 - Loss: 29.8053\n",
      "Processing batch 4926/11884 - Loss: 29.3178\n",
      "Processing batch 4927/11884 - Loss: 30.3284\n",
      "Processing batch 4928/11884 - Loss: 31.0127\n",
      "Processing batch 4929/11884 - Loss: 30.2586\n",
      "Processing batch 4930/11884 - Loss: 29.0608\n",
      "Processing batch 4931/11884 - Loss: 30.4908\n",
      "Processing batch 4932/11884 - Loss: 31.2097\n",
      "Processing batch 4933/11884 - Loss: 30.0345\n",
      "Processing batch 4934/11884 - Loss: 30.1516\n",
      "Processing batch 4935/11884 - Loss: 30.3573\n",
      "Processing batch 4936/11884 - Loss: 30.6080\n",
      "Processing batch 4937/11884 - Loss: 29.9014\n",
      "Processing batch 4938/11884 - Loss: 30.8768\n",
      "Processing batch 4939/11884 - Loss: 30.8579\n",
      "Processing batch 4940/11884 - Loss: 29.6654\n",
      "Processing batch 4941/11884 - Loss: 30.5153\n",
      "Processing batch 4942/11884 - Loss: 29.4184\n",
      "Processing batch 4943/11884 - Loss: 30.3710\n",
      "Processing batch 4944/11884 - Loss: 31.1292\n",
      "Processing batch 4945/11884 - Loss: 30.9863\n",
      "Processing batch 4946/11884 - Loss: 29.0666\n",
      "Processing batch 4947/11884 - Loss: 29.3689\n",
      "Processing batch 4948/11884 - Loss: 28.7548\n",
      "Processing batch 4949/11884 - Loss: 30.3282\n",
      "Processing batch 4950/11884 - Loss: 31.6624\n",
      "Processing batch 4951/11884 - Loss: 29.2429\n",
      "Processing batch 4952/11884 - Loss: 28.9490\n",
      "Processing batch 4953/11884 - Loss: 30.7322\n",
      "Processing batch 4954/11884 - Loss: 30.3606\n",
      "Processing batch 4955/11884 - Loss: 29.8513\n",
      "Processing batch 4956/11884 - Loss: 29.7316\n",
      "Processing batch 4957/11884 - Loss: 30.5389\n",
      "Processing batch 4958/11884 - Loss: 30.4690\n",
      "Processing batch 4959/11884 - Loss: 30.4322\n",
      "Processing batch 4960/11884 - Loss: 30.3971\n",
      "Processing batch 4961/11884 - Loss: 30.5326\n",
      "Processing batch 4962/11884 - Loss: 30.1516\n",
      "Processing batch 4963/11884 - Loss: 31.1943\n",
      "Processing batch 4964/11884 - Loss: 30.2043\n",
      "Processing batch 4965/11884 - Loss: 29.7579\n",
      "Processing batch 4966/11884 - Loss: 29.2199\n",
      "Processing batch 4967/11884 - Loss: 30.9096\n",
      "Processing batch 4968/11884 - Loss: 29.6109\n",
      "Processing batch 4969/11884 - Loss: 30.6813\n",
      "Processing batch 4970/11884 - Loss: 30.4400\n",
      "Processing batch 4971/11884 - Loss: 29.5905\n",
      "Processing batch 4972/11884 - Loss: 30.2291\n",
      "Processing batch 4973/11884 - Loss: 30.8181\n",
      "Processing batch 4974/11884 - Loss: 29.8706\n",
      "Processing batch 4975/11884 - Loss: 30.7421\n",
      "Processing batch 4976/11884 - Loss: 31.5033\n",
      "Processing batch 4977/11884 - Loss: 31.9582\n",
      "Processing batch 4978/11884 - Loss: 30.1220\n",
      "Processing batch 4979/11884 - Loss: 30.2209\n",
      "Processing batch 4980/11884 - Loss: 29.7500\n",
      "Processing batch 4981/11884 - Loss: 29.6649\n",
      "Processing batch 4982/11884 - Loss: 31.1522\n",
      "Processing batch 4983/11884 - Loss: 29.1230\n",
      "Processing batch 4984/11884 - Loss: 29.3824\n",
      "Processing batch 4985/11884 - Loss: 29.7965\n",
      "Processing batch 4986/11884 - Loss: 29.6762\n",
      "Processing batch 4987/11884 - Loss: 29.9381\n",
      "Processing batch 4988/11884 - Loss: 30.4616\n",
      "Processing batch 4989/11884 - Loss: 28.5644\n",
      "Processing batch 4990/11884 - Loss: 30.2268\n",
      "Processing batch 4991/11884 - Loss: 29.0117\n",
      "Processing batch 4992/11884 - Loss: 29.8439\n",
      "Processing batch 4993/11884 - Loss: 30.0851\n",
      "Processing batch 4994/11884 - Loss: 31.0685\n",
      "Processing batch 4995/11884 - Loss: 30.0961\n",
      "Processing batch 4996/11884 - Loss: 30.7947\n",
      "Processing batch 4997/11884 - Loss: 29.5809\n",
      "Processing batch 4998/11884 - Loss: 30.9577\n",
      "Processing batch 4999/11884 - Loss: 31.2399\n",
      "Processing batch 5000/11884 - Loss: 29.8722\n",
      "Processing batch 5001/11884 - Loss: 29.5120\n",
      "Processing batch 5002/11884 - Loss: 30.6034\n",
      "Processing batch 5003/11884 - Loss: 29.5299\n",
      "Processing batch 5004/11884 - Loss: 30.9124\n",
      "Processing batch 5005/11884 - Loss: 31.1702\n",
      "Processing batch 5006/11884 - Loss: 30.6520\n",
      "Processing batch 5007/11884 - Loss: 30.0586\n",
      "Processing batch 5008/11884 - Loss: 30.5127\n",
      "Processing batch 5009/11884 - Loss: 30.0869\n",
      "Processing batch 5010/11884 - Loss: 29.9276\n",
      "Processing batch 5011/11884 - Loss: 29.9455\n",
      "Processing batch 5012/11884 - Loss: 29.9563\n",
      "Processing batch 5013/11884 - Loss: 29.4669\n",
      "Processing batch 5014/11884 - Loss: 28.8172\n",
      "Processing batch 5015/11884 - Loss: 30.4534\n",
      "Processing batch 5016/11884 - Loss: 31.5752\n",
      "Processing batch 5017/11884 - Loss: 29.4401\n",
      "Processing batch 5018/11884 - Loss: 29.2559\n",
      "Processing batch 5019/11884 - Loss: 29.8617\n",
      "Processing batch 5020/11884 - Loss: 29.0646\n",
      "Processing batch 5021/11884 - Loss: 29.5096\n",
      "Processing batch 5022/11884 - Loss: 31.1177\n",
      "Processing batch 5023/11884 - Loss: 29.5976\n",
      "Processing batch 5024/11884 - Loss: 28.7715\n",
      "Processing batch 5025/11884 - Loss: 30.3902\n",
      "Processing batch 5026/11884 - Loss: 31.2588\n",
      "Processing batch 5027/11884 - Loss: 31.1103\n",
      "Processing batch 5028/11884 - Loss: 29.8974\n",
      "Processing batch 5029/11884 - Loss: 30.7040\n",
      "Processing batch 5030/11884 - Loss: 30.2979\n",
      "Processing batch 5031/11884 - Loss: 29.4367\n",
      "Processing batch 5032/11884 - Loss: 29.4293\n",
      "Processing batch 5033/11884 - Loss: 30.7645\n",
      "Processing batch 5034/11884 - Loss: 31.0280\n",
      "Processing batch 5035/11884 - Loss: 29.0636\n",
      "Processing batch 5036/11884 - Loss: 30.0062\n",
      "Processing batch 5037/11884 - Loss: 28.7865\n",
      "Processing batch 5038/11884 - Loss: 30.0481\n",
      "Processing batch 5039/11884 - Loss: 30.7885\n",
      "Processing batch 5040/11884 - Loss: 30.0398\n",
      "Processing batch 5041/11884 - Loss: 29.9872\n",
      "Processing batch 5042/11884 - Loss: 30.8762\n",
      "Processing batch 5043/11884 - Loss: 29.5126\n",
      "Processing batch 5044/11884 - Loss: 29.7360\n",
      "Processing batch 5045/11884 - Loss: 29.5372\n",
      "Processing batch 5046/11884 - Loss: 30.7687\n",
      "Processing batch 5047/11884 - Loss: 30.1016\n",
      "Processing batch 5048/11884 - Loss: 30.2666\n",
      "Processing batch 5049/11884 - Loss: 30.6665\n",
      "Processing batch 5050/11884 - Loss: 29.4425\n",
      "Processing batch 5051/11884 - Loss: 30.4704\n",
      "Processing batch 5052/11884 - Loss: 29.9860\n",
      "Processing batch 5053/11884 - Loss: 29.4072\n",
      "Processing batch 5054/11884 - Loss: 29.7735\n",
      "Processing batch 5055/11884 - Loss: 28.3659\n",
      "Processing batch 5056/11884 - Loss: 31.0289\n",
      "Processing batch 5057/11884 - Loss: 29.5041\n",
      "Processing batch 5058/11884 - Loss: 30.9979\n",
      "Processing batch 5059/11884 - Loss: 29.0573\n",
      "Processing batch 5060/11884 - Loss: 28.9742\n",
      "Processing batch 5061/11884 - Loss: 31.2217\n",
      "Processing batch 5062/11884 - Loss: 30.2572\n",
      "Processing batch 5063/11884 - Loss: 29.7707\n",
      "Processing batch 5064/11884 - Loss: 30.5195\n",
      "Processing batch 5065/11884 - Loss: 30.0481\n",
      "Processing batch 5066/11884 - Loss: 30.2581\n",
      "Processing batch 5067/11884 - Loss: 30.9103\n",
      "Processing batch 5068/11884 - Loss: 30.0481\n",
      "Processing batch 5069/11884 - Loss: 29.6702\n",
      "Processing batch 5070/11884 - Loss: 31.1503\n",
      "Processing batch 5071/11884 - Loss: 31.5588\n",
      "Processing batch 5072/11884 - Loss: 31.0607\n",
      "Processing batch 5073/11884 - Loss: 32.0909\n",
      "Processing batch 5074/11884 - Loss: 29.8800\n",
      "Processing batch 5075/11884 - Loss: 30.9390\n",
      "Processing batch 5076/11884 - Loss: 30.6608\n",
      "Processing batch 5077/11884 - Loss: 29.2579\n",
      "Processing batch 5078/11884 - Loss: 31.1620\n",
      "Processing batch 5079/11884 - Loss: 28.5038\n",
      "Processing batch 5080/11884 - Loss: 30.5985\n",
      "Processing batch 5081/11884 - Loss: 30.0570\n",
      "Processing batch 5082/11884 - Loss: 30.8264\n",
      "Processing batch 5083/11884 - Loss: 30.4644\n",
      "Processing batch 5084/11884 - Loss: 30.4896\n",
      "Processing batch 5085/11884 - Loss: 28.8311\n",
      "Processing batch 5086/11884 - Loss: 29.6665\n",
      "Processing batch 5087/11884 - Loss: 29.9951\n",
      "Processing batch 5088/11884 - Loss: 30.9869\n",
      "Processing batch 5089/11884 - Loss: 31.0577\n",
      "Processing batch 5090/11884 - Loss: 28.8892\n",
      "Processing batch 5091/11884 - Loss: 29.7196\n",
      "Processing batch 5092/11884 - Loss: 30.2369\n",
      "Processing batch 5093/11884 - Loss: 31.4376\n",
      "Processing batch 5094/11884 - Loss: 30.8053\n",
      "Processing batch 5095/11884 - Loss: 30.4102\n",
      "Processing batch 5096/11884 - Loss: 29.7348\n",
      "Processing batch 5097/11884 - Loss: 30.2868\n",
      "Processing batch 5098/11884 - Loss: 29.2120\n",
      "Processing batch 5099/11884 - Loss: 31.2129\n",
      "Processing batch 5100/11884 - Loss: 29.7920\n",
      "Processing batch 5101/11884 - Loss: 29.6610\n",
      "Processing batch 5102/11884 - Loss: 31.6228\n",
      "Processing batch 5103/11884 - Loss: 29.0100\n",
      "Processing batch 5104/11884 - Loss: 29.7687\n",
      "Processing batch 5105/11884 - Loss: 30.2004\n",
      "Processing batch 5106/11884 - Loss: 29.8328\n",
      "Processing batch 5107/11884 - Loss: 29.2911\n",
      "Processing batch 5108/11884 - Loss: 28.9935\n",
      "Processing batch 5109/11884 - Loss: 31.0938\n",
      "Processing batch 5110/11884 - Loss: 31.3001\n",
      "Processing batch 5111/11884 - Loss: 29.9156\n",
      "Processing batch 5112/11884 - Loss: 30.0687\n",
      "Processing batch 5113/11884 - Loss: 30.9058\n",
      "Processing batch 5114/11884 - Loss: 29.2378\n",
      "Processing batch 5115/11884 - Loss: 29.4685\n",
      "Processing batch 5116/11884 - Loss: 30.8020\n",
      "Processing batch 5117/11884 - Loss: 30.4968\n",
      "Processing batch 5118/11884 - Loss: 30.6923\n",
      "Processing batch 5119/11884 - Loss: 30.6465\n",
      "Processing batch 5120/11884 - Loss: 29.0443\n",
      "Processing batch 5121/11884 - Loss: 30.9040\n",
      "Processing batch 5122/11884 - Loss: 29.5014\n",
      "Processing batch 5123/11884 - Loss: 29.0089\n",
      "Processing batch 5124/11884 - Loss: 30.5542\n",
      "Processing batch 5125/11884 - Loss: 29.7434\n",
      "Processing batch 5126/11884 - Loss: 29.9066\n",
      "Processing batch 5127/11884 - Loss: 30.8434\n",
      "Processing batch 5128/11884 - Loss: 30.0898\n",
      "Processing batch 5129/11884 - Loss: 31.1329\n",
      "Processing batch 5130/11884 - Loss: 29.7528\n",
      "Processing batch 5131/11884 - Loss: 31.0265\n",
      "Processing batch 5132/11884 - Loss: 29.2808\n",
      "Processing batch 5133/11884 - Loss: 29.3948\n",
      "Processing batch 5134/11884 - Loss: 31.5390\n",
      "Processing batch 5135/11884 - Loss: 29.7200\n",
      "Processing batch 5136/11884 - Loss: 29.0077\n",
      "Processing batch 5137/11884 - Loss: 30.0482\n",
      "Processing batch 5138/11884 - Loss: 28.9072\n",
      "Processing batch 5139/11884 - Loss: 30.0771\n",
      "Processing batch 5140/11884 - Loss: 30.5459\n",
      "Processing batch 5141/11884 - Loss: 29.8191\n",
      "Processing batch 5142/11884 - Loss: 30.0049\n",
      "Processing batch 5143/11884 - Loss: 29.6944\n",
      "Processing batch 5144/11884 - Loss: 28.9695\n",
      "Processing batch 5145/11884 - Loss: 30.1768\n",
      "Processing batch 5146/11884 - Loss: 29.3921\n",
      "Processing batch 5147/11884 - Loss: 29.5734\n",
      "Processing batch 5148/11884 - Loss: 29.3882\n",
      "Processing batch 5149/11884 - Loss: 29.5590\n",
      "Processing batch 5150/11884 - Loss: 30.0357\n",
      "Processing batch 5151/11884 - Loss: 29.2400\n",
      "Processing batch 5152/11884 - Loss: 29.4951\n",
      "Processing batch 5153/11884 - Loss: 30.1737\n",
      "Processing batch 5154/11884 - Loss: 31.4508\n",
      "Processing batch 5155/11884 - Loss: 29.6639\n",
      "Processing batch 5156/11884 - Loss: 29.8177\n",
      "Processing batch 5157/11884 - Loss: 30.9988\n",
      "Processing batch 5158/11884 - Loss: 30.6198\n",
      "Processing batch 5159/11884 - Loss: 31.2123\n",
      "Processing batch 5160/11884 - Loss: 29.4000\n",
      "Processing batch 5161/11884 - Loss: 29.0431\n",
      "Processing batch 5162/11884 - Loss: 29.6714\n",
      "Processing batch 5163/11884 - Loss: 28.6037\n",
      "Processing batch 5164/11884 - Loss: 30.3960\n",
      "Processing batch 5165/11884 - Loss: 30.6320\n",
      "Processing batch 5166/11884 - Loss: 30.0457\n",
      "Processing batch 5167/11884 - Loss: 30.4473\n",
      "Processing batch 5168/11884 - Loss: 29.3199\n",
      "Processing batch 5169/11884 - Loss: 30.1097\n",
      "Processing batch 5170/11884 - Loss: 31.3163\n",
      "Processing batch 5171/11884 - Loss: 29.1449\n",
      "Processing batch 5172/11884 - Loss: 30.7615\n",
      "Processing batch 5173/11884 - Loss: 29.7993\n",
      "Processing batch 5174/11884 - Loss: 31.0053\n",
      "Processing batch 5175/11884 - Loss: 30.1009\n",
      "Processing batch 5176/11884 - Loss: 30.1991\n",
      "Processing batch 5177/11884 - Loss: 29.7467\n",
      "Processing batch 5178/11884 - Loss: 30.1806\n",
      "Processing batch 5179/11884 - Loss: 30.6063\n",
      "Processing batch 5180/11884 - Loss: 30.5498\n",
      "Processing batch 5181/11884 - Loss: 30.2545\n",
      "Processing batch 5182/11884 - Loss: 29.7368\n",
      "Processing batch 5183/11884 - Loss: 28.9318\n",
      "Processing batch 5184/11884 - Loss: 29.7749\n",
      "Processing batch 5185/11884 - Loss: 28.7300\n",
      "Processing batch 5186/11884 - Loss: 29.8455\n",
      "Processing batch 5187/11884 - Loss: 31.4934\n",
      "Processing batch 5188/11884 - Loss: 30.2191\n",
      "Processing batch 5189/11884 - Loss: 29.3857\n",
      "Processing batch 5190/11884 - Loss: 30.0405\n",
      "Processing batch 5191/11884 - Loss: 30.0662\n",
      "Processing batch 5192/11884 - Loss: 30.1663\n",
      "Processing batch 5193/11884 - Loss: 29.6218\n",
      "Processing batch 5194/11884 - Loss: 30.5460\n",
      "Processing batch 5195/11884 - Loss: 29.5194\n",
      "Processing batch 5196/11884 - Loss: 30.0400\n",
      "Processing batch 5197/11884 - Loss: 30.8920\n",
      "Processing batch 5198/11884 - Loss: 31.1014\n",
      "Processing batch 5199/11884 - Loss: 30.3712\n",
      "Processing batch 5200/11884 - Loss: 30.4406\n",
      "Processing batch 5201/11884 - Loss: 30.3076\n",
      "Processing batch 5202/11884 - Loss: 31.0167\n",
      "Processing batch 5203/11884 - Loss: 31.5309\n",
      "Processing batch 5204/11884 - Loss: 29.2058\n",
      "Processing batch 5205/11884 - Loss: 29.3152\n",
      "Processing batch 5206/11884 - Loss: 29.3190\n",
      "Processing batch 5207/11884 - Loss: 30.1043\n",
      "Processing batch 5208/11884 - Loss: 31.1685\n",
      "Processing batch 5209/11884 - Loss: 29.7219\n",
      "Processing batch 5210/11884 - Loss: 31.3237\n",
      "Processing batch 5211/11884 - Loss: 28.7238\n",
      "Processing batch 5212/11884 - Loss: 31.2195\n",
      "Processing batch 5213/11884 - Loss: 28.8427\n",
      "Processing batch 5214/11884 - Loss: 30.4295\n",
      "Processing batch 5215/11884 - Loss: 30.8101\n",
      "Processing batch 5216/11884 - Loss: 29.6842\n",
      "Processing batch 5217/11884 - Loss: 30.0152\n",
      "Processing batch 5218/11884 - Loss: 29.9417\n",
      "Processing batch 5219/11884 - Loss: 30.4189\n",
      "Processing batch 5220/11884 - Loss: 28.8352\n",
      "Processing batch 5221/11884 - Loss: 30.4254\n",
      "Processing batch 5222/11884 - Loss: 29.7969\n",
      "Processing batch 5223/11884 - Loss: 30.9049\n",
      "Processing batch 5224/11884 - Loss: 29.8508\n",
      "Processing batch 5225/11884 - Loss: 30.5570\n",
      "Processing batch 5226/11884 - Loss: 31.1916\n",
      "Processing batch 5227/11884 - Loss: 30.6832\n",
      "Processing batch 5228/11884 - Loss: 29.3081\n",
      "Processing batch 5229/11884 - Loss: 29.0702\n",
      "Processing batch 5230/11884 - Loss: 29.8088\n",
      "Processing batch 5231/11884 - Loss: 29.5385\n",
      "Processing batch 5232/11884 - Loss: 30.2799\n",
      "Processing batch 5233/11884 - Loss: 30.0489\n",
      "Processing batch 5234/11884 - Loss: 29.7977\n",
      "Processing batch 5235/11884 - Loss: 29.1138\n",
      "Processing batch 5236/11884 - Loss: 31.4384\n",
      "Processing batch 5237/11884 - Loss: 30.7404\n",
      "Processing batch 5238/11884 - Loss: 30.3207\n",
      "Processing batch 5239/11884 - Loss: 29.1902\n",
      "Processing batch 5240/11884 - Loss: 30.4645\n",
      "Processing batch 5241/11884 - Loss: 29.5401\n",
      "Processing batch 5242/11884 - Loss: 29.3591\n",
      "Processing batch 5243/11884 - Loss: 29.2978\n",
      "Processing batch 5244/11884 - Loss: 30.1393\n",
      "Processing batch 5245/11884 - Loss: 30.7088\n",
      "Processing batch 5246/11884 - Loss: 31.3411\n",
      "Processing batch 5247/11884 - Loss: 30.5693\n",
      "Processing batch 5248/11884 - Loss: 30.0689\n",
      "Processing batch 5249/11884 - Loss: 30.3795\n",
      "Processing batch 5250/11884 - Loss: 30.3702\n",
      "Processing batch 5251/11884 - Loss: 29.9808\n",
      "Processing batch 5252/11884 - Loss: 30.9264\n",
      "Processing batch 5253/11884 - Loss: 30.7864\n",
      "Processing batch 5254/11884 - Loss: 31.5343\n",
      "Processing batch 5255/11884 - Loss: 29.7126\n",
      "Processing batch 5256/11884 - Loss: 30.4888\n",
      "Processing batch 5257/11884 - Loss: 30.9378\n",
      "Processing batch 5258/11884 - Loss: 29.2944\n",
      "Processing batch 5259/11884 - Loss: 31.5576\n",
      "Processing batch 5260/11884 - Loss: 29.9374\n",
      "Processing batch 5261/11884 - Loss: 30.4370\n",
      "Processing batch 5262/11884 - Loss: 30.1675\n",
      "Processing batch 5263/11884 - Loss: 30.3792\n",
      "Processing batch 5264/11884 - Loss: 30.9528\n",
      "Processing batch 5265/11884 - Loss: 30.4271\n",
      "Processing batch 5266/11884 - Loss: 29.9051\n",
      "Processing batch 5267/11884 - Loss: 30.2677\n",
      "Processing batch 5268/11884 - Loss: 29.4106\n",
      "Processing batch 5269/11884 - Loss: 30.6210\n",
      "Processing batch 5270/11884 - Loss: 29.2572\n",
      "Processing batch 5271/11884 - Loss: 31.5517\n",
      "Processing batch 5272/11884 - Loss: 29.3255\n",
      "Processing batch 5273/11884 - Loss: 29.7389\n",
      "Processing batch 5274/11884 - Loss: 30.9018\n",
      "Processing batch 5275/11884 - Loss: 29.5449\n",
      "Processing batch 5276/11884 - Loss: 29.3296\n",
      "Processing batch 5277/11884 - Loss: 30.2665\n",
      "Processing batch 5278/11884 - Loss: 30.2191\n",
      "Processing batch 5279/11884 - Loss: 30.5956\n",
      "Processing batch 5280/11884 - Loss: 31.6779\n",
      "Processing batch 5281/11884 - Loss: 29.6768\n",
      "Processing batch 5282/11884 - Loss: 29.8099\n",
      "Processing batch 5283/11884 - Loss: 29.4198\n",
      "Processing batch 5284/11884 - Loss: 30.5107\n",
      "Processing batch 5285/11884 - Loss: 29.8272\n",
      "Processing batch 5286/11884 - Loss: 30.2588\n",
      "Processing batch 5287/11884 - Loss: 30.2481\n",
      "Processing batch 5288/11884 - Loss: 29.5705\n",
      "Processing batch 5289/11884 - Loss: 30.6069\n",
      "Processing batch 5290/11884 - Loss: 29.6549\n",
      "Processing batch 5291/11884 - Loss: 30.2020\n",
      "Processing batch 5292/11884 - Loss: 29.3936\n",
      "Processing batch 5293/11884 - Loss: 28.6823\n",
      "Processing batch 5294/11884 - Loss: 31.2325\n",
      "Processing batch 5295/11884 - Loss: 29.2804\n",
      "Processing batch 5296/11884 - Loss: 30.5088\n",
      "Processing batch 5297/11884 - Loss: 30.8592\n",
      "Processing batch 5298/11884 - Loss: 29.9034\n",
      "Processing batch 5299/11884 - Loss: 30.0421\n",
      "Processing batch 5300/11884 - Loss: 30.7631\n",
      "Processing batch 5301/11884 - Loss: 31.8052\n",
      "Processing batch 5302/11884 - Loss: 30.5873\n",
      "Processing batch 5303/11884 - Loss: 29.5080\n",
      "Processing batch 5304/11884 - Loss: 29.8877\n",
      "Processing batch 5305/11884 - Loss: 29.5485\n",
      "Processing batch 5306/11884 - Loss: 28.6260\n",
      "Processing batch 5307/11884 - Loss: 29.7736\n",
      "Processing batch 5308/11884 - Loss: 30.0205\n",
      "Processing batch 5309/11884 - Loss: 31.4616\n",
      "Processing batch 5310/11884 - Loss: 30.7223\n",
      "Processing batch 5311/11884 - Loss: 29.1577\n",
      "Processing batch 5312/11884 - Loss: 29.9167\n",
      "Processing batch 5313/11884 - Loss: 29.9018\n",
      "Processing batch 5314/11884 - Loss: 30.5065\n",
      "Processing batch 5315/11884 - Loss: 30.5782\n",
      "Processing batch 5316/11884 - Loss: 31.6017\n",
      "Processing batch 5317/11884 - Loss: 30.6586\n",
      "Processing batch 5318/11884 - Loss: 31.0953\n",
      "Processing batch 5319/11884 - Loss: 30.4014\n",
      "Processing batch 5320/11884 - Loss: 32.1081\n",
      "Processing batch 5321/11884 - Loss: 30.0005\n",
      "Processing batch 5322/11884 - Loss: 30.2978\n",
      "Processing batch 5323/11884 - Loss: 30.0231\n",
      "Processing batch 5324/11884 - Loss: 30.6475\n",
      "Processing batch 5325/11884 - Loss: 29.4501\n",
      "Processing batch 5326/11884 - Loss: 31.4474\n",
      "Processing batch 5327/11884 - Loss: 29.8008\n",
      "Processing batch 5328/11884 - Loss: 30.6606\n",
      "Processing batch 5329/11884 - Loss: 29.7946\n",
      "Processing batch 5330/11884 - Loss: 29.3212\n",
      "Processing batch 5331/11884 - Loss: 29.0848\n",
      "Processing batch 5332/11884 - Loss: 28.3971\n",
      "Processing batch 5333/11884 - Loss: 30.6019\n",
      "Processing batch 5334/11884 - Loss: 31.2182\n",
      "Processing batch 5335/11884 - Loss: 30.0648\n",
      "Processing batch 5336/11884 - Loss: 30.2500\n",
      "Processing batch 5337/11884 - Loss: 31.0020\n",
      "Processing batch 5338/11884 - Loss: 30.6679\n",
      "Processing batch 5339/11884 - Loss: 30.9388\n",
      "Processing batch 5340/11884 - Loss: 30.7101\n",
      "Processing batch 5341/11884 - Loss: 29.4934\n",
      "Processing batch 5342/11884 - Loss: 30.8145\n",
      "Processing batch 5343/11884 - Loss: 31.7463\n",
      "Processing batch 5344/11884 - Loss: 29.9374\n",
      "Processing batch 5345/11884 - Loss: 29.3524\n",
      "Processing batch 5346/11884 - Loss: 29.5882\n",
      "Processing batch 5347/11884 - Loss: 30.3894\n",
      "Processing batch 5348/11884 - Loss: 30.4180\n",
      "Processing batch 5349/11884 - Loss: 29.8273\n",
      "Processing batch 5350/11884 - Loss: 30.3446\n",
      "Processing batch 5351/11884 - Loss: 30.7202\n",
      "Processing batch 5352/11884 - Loss: 30.6608\n",
      "Processing batch 5353/11884 - Loss: 29.6767\n",
      "Processing batch 5354/11884 - Loss: 30.1455\n",
      "Processing batch 5355/11884 - Loss: 29.9632\n",
      "Processing batch 5356/11884 - Loss: 30.8224\n",
      "Processing batch 5357/11884 - Loss: 29.9889\n",
      "Processing batch 5358/11884 - Loss: 30.3282\n",
      "Processing batch 5359/11884 - Loss: 30.8799\n",
      "Processing batch 5360/11884 - Loss: 30.2978\n",
      "Processing batch 5361/11884 - Loss: 29.0373\n",
      "Processing batch 5362/11884 - Loss: 28.7755\n",
      "Processing batch 5363/11884 - Loss: 31.1228\n",
      "Processing batch 5364/11884 - Loss: 29.7090\n",
      "Processing batch 5365/11884 - Loss: 32.0187\n",
      "Processing batch 5366/11884 - Loss: 29.5891\n",
      "Processing batch 5367/11884 - Loss: 31.3743\n",
      "Processing batch 5368/11884 - Loss: 29.6568\n",
      "Processing batch 5369/11884 - Loss: 30.4383\n",
      "Processing batch 5370/11884 - Loss: 30.8751\n",
      "Processing batch 5371/11884 - Loss: 29.6254\n",
      "Processing batch 5372/11884 - Loss: 29.2281\n",
      "Processing batch 5373/11884 - Loss: 30.1565\n",
      "Processing batch 5374/11884 - Loss: 29.4565\n",
      "Processing batch 5375/11884 - Loss: 28.5064\n",
      "Processing batch 5376/11884 - Loss: 30.8868\n",
      "Processing batch 5377/11884 - Loss: 30.3516\n",
      "Processing batch 5378/11884 - Loss: 30.6197\n",
      "Processing batch 5379/11884 - Loss: 31.5270\n",
      "Processing batch 5380/11884 - Loss: 30.0207\n",
      "Processing batch 5381/11884 - Loss: 31.8416\n",
      "Processing batch 5382/11884 - Loss: 30.6539\n",
      "Processing batch 5383/11884 - Loss: 30.8824\n",
      "Processing batch 5384/11884 - Loss: 30.0600\n",
      "Processing batch 5385/11884 - Loss: 30.7184\n",
      "Processing batch 5386/11884 - Loss: 29.7650\n",
      "Processing batch 5387/11884 - Loss: 29.5624\n",
      "Processing batch 5388/11884 - Loss: 29.6551\n",
      "Processing batch 5389/11884 - Loss: 29.7455\n",
      "Processing batch 5390/11884 - Loss: 30.7868\n",
      "Processing batch 5391/11884 - Loss: 29.9115\n",
      "Processing batch 5392/11884 - Loss: 29.9755\n",
      "Processing batch 5393/11884 - Loss: 28.8204\n",
      "Processing batch 5394/11884 - Loss: 29.9609\n",
      "Processing batch 5395/11884 - Loss: 31.2849\n",
      "Processing batch 5396/11884 - Loss: 29.5442\n",
      "Processing batch 5397/11884 - Loss: 30.4844\n",
      "Processing batch 5398/11884 - Loss: 30.9490\n",
      "Processing batch 5399/11884 - Loss: 29.4092\n",
      "Processing batch 5400/11884 - Loss: 29.7823\n",
      "Processing batch 5401/11884 - Loss: 30.1289\n",
      "Processing batch 5402/11884 - Loss: 30.0792\n",
      "Processing batch 5403/11884 - Loss: 31.3615\n",
      "Processing batch 5404/11884 - Loss: 30.0083\n",
      "Processing batch 5405/11884 - Loss: 30.6081\n",
      "Processing batch 5406/11884 - Loss: 29.5393\n",
      "Processing batch 5407/11884 - Loss: 30.4817\n",
      "Processing batch 5408/11884 - Loss: 30.4602\n",
      "Processing batch 5409/11884 - Loss: 29.6681\n",
      "Processing batch 5410/11884 - Loss: 29.6189\n",
      "Processing batch 5411/11884 - Loss: 28.6940\n",
      "Processing batch 5412/11884 - Loss: 30.0616\n",
      "Processing batch 5413/11884 - Loss: 29.9948\n",
      "Processing batch 5414/11884 - Loss: 29.8497\n",
      "Processing batch 5415/11884 - Loss: 30.1816\n",
      "Processing batch 5416/11884 - Loss: 30.6546\n",
      "Processing batch 5417/11884 - Loss: 30.5719\n",
      "Processing batch 5418/11884 - Loss: 31.2781\n",
      "Processing batch 5419/11884 - Loss: 29.4881\n",
      "Processing batch 5420/11884 - Loss: 30.3126\n",
      "Processing batch 5421/11884 - Loss: 29.1860\n",
      "Processing batch 5422/11884 - Loss: 30.4167\n",
      "Processing batch 5423/11884 - Loss: 29.0964\n",
      "Processing batch 5424/11884 - Loss: 29.4224\n",
      "Processing batch 5425/11884 - Loss: 30.0467\n",
      "Processing batch 5426/11884 - Loss: 29.6696\n",
      "Processing batch 5427/11884 - Loss: 29.4228\n",
      "Processing batch 5428/11884 - Loss: 31.4958\n",
      "Processing batch 5429/11884 - Loss: 30.8573\n",
      "Processing batch 5430/11884 - Loss: 31.1131\n",
      "Processing batch 5431/11884 - Loss: 31.1898\n",
      "Processing batch 5432/11884 - Loss: 31.0241\n",
      "Processing batch 5433/11884 - Loss: 29.8148\n",
      "Processing batch 5434/11884 - Loss: 29.6551\n",
      "Processing batch 5435/11884 - Loss: 31.6774\n",
      "Processing batch 5436/11884 - Loss: 28.3881\n",
      "Processing batch 5437/11884 - Loss: 30.7058\n",
      "Processing batch 5438/11884 - Loss: 29.8099\n",
      "Processing batch 5439/11884 - Loss: 29.6091\n",
      "Processing batch 5440/11884 - Loss: 30.1603\n",
      "Processing batch 5441/11884 - Loss: 29.8716\n",
      "Processing batch 5442/11884 - Loss: 31.6545\n",
      "Processing batch 5443/11884 - Loss: 29.9104\n",
      "Processing batch 5444/11884 - Loss: 27.7286\n",
      "Processing batch 5445/11884 - Loss: 28.4084\n",
      "Processing batch 5446/11884 - Loss: 30.0474\n",
      "Processing batch 5447/11884 - Loss: 29.2763\n",
      "Processing batch 5448/11884 - Loss: 31.3547\n",
      "Processing batch 5449/11884 - Loss: 30.2558\n",
      "Processing batch 5450/11884 - Loss: 28.2394\n",
      "Processing batch 5451/11884 - Loss: 29.6631\n",
      "Processing batch 5452/11884 - Loss: 29.6584\n",
      "Processing batch 5453/11884 - Loss: 30.4984\n",
      "Processing batch 5454/11884 - Loss: 30.8234\n",
      "Processing batch 5455/11884 - Loss: 29.1071\n",
      "Processing batch 5456/11884 - Loss: 29.7045\n",
      "Processing batch 5457/11884 - Loss: 29.5520\n",
      "Processing batch 5458/11884 - Loss: 29.3452\n",
      "Processing batch 5459/11884 - Loss: 31.1973\n",
      "Processing batch 5460/11884 - Loss: 31.0643\n",
      "Processing batch 5461/11884 - Loss: 29.6300\n",
      "Processing batch 5462/11884 - Loss: 30.4677\n",
      "Processing batch 5463/11884 - Loss: 30.6309\n",
      "Processing batch 5464/11884 - Loss: 30.4803\n",
      "Processing batch 5465/11884 - Loss: 29.5400\n",
      "Processing batch 5466/11884 - Loss: 28.9351\n",
      "Processing batch 5467/11884 - Loss: 31.1169\n",
      "Processing batch 5468/11884 - Loss: 32.0639\n",
      "Processing batch 5469/11884 - Loss: 30.4330\n",
      "Processing batch 5470/11884 - Loss: 29.8716\n",
      "Processing batch 5471/11884 - Loss: 28.5759\n",
      "Processing batch 5472/11884 - Loss: 29.3885\n",
      "Processing batch 5473/11884 - Loss: 30.7873\n",
      "Processing batch 5474/11884 - Loss: 30.2053\n",
      "Processing batch 5475/11884 - Loss: 29.5195\n",
      "Processing batch 5476/11884 - Loss: 30.1096\n",
      "Processing batch 5477/11884 - Loss: 30.0191\n",
      "Processing batch 5478/11884 - Loss: 31.7130\n",
      "Processing batch 5479/11884 - Loss: 29.5246\n",
      "Processing batch 5480/11884 - Loss: 29.6275\n",
      "Processing batch 5481/11884 - Loss: 30.3607\n",
      "Processing batch 5482/11884 - Loss: 29.8214\n",
      "Processing batch 5483/11884 - Loss: 29.8110\n",
      "Processing batch 5484/11884 - Loss: 28.9643\n",
      "Processing batch 5485/11884 - Loss: 28.9094\n",
      "Processing batch 5486/11884 - Loss: 28.5891\n",
      "Processing batch 5487/11884 - Loss: 28.2598\n",
      "Processing batch 5488/11884 - Loss: 30.9095\n",
      "Processing batch 5489/11884 - Loss: 29.7604\n",
      "Processing batch 5490/11884 - Loss: 31.5021\n",
      "Processing batch 5491/11884 - Loss: 29.1275\n",
      "Processing batch 5492/11884 - Loss: 29.6266\n",
      "Processing batch 5493/11884 - Loss: 30.7519\n",
      "Processing batch 5494/11884 - Loss: 30.3850\n",
      "Processing batch 5495/11884 - Loss: 29.4483\n",
      "Processing batch 5496/11884 - Loss: 29.2744\n",
      "Processing batch 5497/11884 - Loss: 29.7234\n",
      "Processing batch 5498/11884 - Loss: 29.9555\n",
      "Processing batch 5499/11884 - Loss: 30.5526\n",
      "Processing batch 5500/11884 - Loss: 30.0091\n",
      "Processing batch 5501/11884 - Loss: 30.1912\n",
      "Processing batch 5502/11884 - Loss: 30.0340\n",
      "Processing batch 5503/11884 - Loss: 30.6493\n",
      "Processing batch 5504/11884 - Loss: 29.9346\n",
      "Processing batch 5505/11884 - Loss: 30.6704\n",
      "Processing batch 5506/11884 - Loss: 30.8836\n",
      "Processing batch 5507/11884 - Loss: 29.7109\n",
      "Processing batch 5508/11884 - Loss: 29.9088\n",
      "Processing batch 5509/11884 - Loss: 29.4914\n",
      "Processing batch 5510/11884 - Loss: 30.7879\n",
      "Processing batch 5511/11884 - Loss: 31.5128\n",
      "Processing batch 5512/11884 - Loss: 31.5727\n",
      "Processing batch 5513/11884 - Loss: 28.3248\n",
      "Processing batch 5514/11884 - Loss: 30.1436\n",
      "Processing batch 5515/11884 - Loss: 29.0145\n",
      "Processing batch 5516/11884 - Loss: 29.4479\n",
      "Processing batch 5517/11884 - Loss: 30.4833\n",
      "Processing batch 5518/11884 - Loss: 30.1311\n",
      "Processing batch 5519/11884 - Loss: 31.3023\n",
      "Processing batch 5520/11884 - Loss: 31.8800\n",
      "Processing batch 5521/11884 - Loss: 29.7274\n",
      "Processing batch 5522/11884 - Loss: 29.6391\n",
      "Processing batch 5523/11884 - Loss: 29.5338\n",
      "Processing batch 5524/11884 - Loss: 28.8751\n",
      "Processing batch 5525/11884 - Loss: 29.4708\n",
      "Processing batch 5526/11884 - Loss: 30.0385\n",
      "Processing batch 5527/11884 - Loss: 29.8885\n",
      "Processing batch 5528/11884 - Loss: 29.4760\n",
      "Processing batch 5529/11884 - Loss: 29.7943\n",
      "Processing batch 5530/11884 - Loss: 30.4692\n",
      "Processing batch 5531/11884 - Loss: 28.7392\n",
      "Processing batch 5532/11884 - Loss: 29.5701\n",
      "Processing batch 5533/11884 - Loss: 30.2204\n",
      "Processing batch 5534/11884 - Loss: 29.9680\n",
      "Processing batch 5535/11884 - Loss: 29.5806\n",
      "Processing batch 5536/11884 - Loss: 29.3815\n",
      "Processing batch 5537/11884 - Loss: 28.4118\n",
      "Processing batch 5538/11884 - Loss: 31.1548\n",
      "Processing batch 5539/11884 - Loss: 30.5171\n",
      "Processing batch 5540/11884 - Loss: 32.1673\n",
      "Processing batch 5541/11884 - Loss: 29.5682\n",
      "Processing batch 5542/11884 - Loss: 28.5777\n",
      "Processing batch 5543/11884 - Loss: 31.2055\n",
      "Processing batch 5544/11884 - Loss: 32.8201\n",
      "Processing batch 5545/11884 - Loss: 30.0979\n",
      "Processing batch 5546/11884 - Loss: 29.4215\n",
      "Processing batch 5547/11884 - Loss: 30.1423\n",
      "Processing batch 5548/11884 - Loss: 30.1584\n",
      "Processing batch 5549/11884 - Loss: 28.8165\n",
      "Processing batch 5550/11884 - Loss: 30.7982\n",
      "Processing batch 5551/11884 - Loss: 29.4830\n",
      "Processing batch 5552/11884 - Loss: 30.1914\n",
      "Processing batch 5553/11884 - Loss: 30.7670\n",
      "Processing batch 5554/11884 - Loss: 31.3192\n",
      "Processing batch 5555/11884 - Loss: 31.0291\n",
      "Processing batch 5556/11884 - Loss: 30.4806\n",
      "Processing batch 5557/11884 - Loss: 29.9495\n",
      "Processing batch 5558/11884 - Loss: 30.8338\n",
      "Processing batch 5559/11884 - Loss: 28.7191\n",
      "Processing batch 5560/11884 - Loss: 29.7840\n",
      "Processing batch 5561/11884 - Loss: 31.6577\n",
      "Processing batch 5562/11884 - Loss: 29.8583\n",
      "Processing batch 5563/11884 - Loss: 29.8556\n",
      "Processing batch 5564/11884 - Loss: 30.8975\n",
      "Processing batch 5565/11884 - Loss: 30.3511\n",
      "Processing batch 5566/11884 - Loss: 31.4023\n",
      "Processing batch 5567/11884 - Loss: 29.7229\n",
      "Processing batch 5568/11884 - Loss: 28.8893\n",
      "Processing batch 5569/11884 - Loss: 30.4547\n",
      "Processing batch 5570/11884 - Loss: 29.3867\n",
      "Processing batch 5571/11884 - Loss: 29.8901\n",
      "Processing batch 5572/11884 - Loss: 29.5770\n",
      "Processing batch 5573/11884 - Loss: 29.8766\n",
      "Processing batch 5574/11884 - Loss: 30.4654\n",
      "Processing batch 5575/11884 - Loss: 31.0483\n",
      "Processing batch 5576/11884 - Loss: 29.4436\n",
      "Processing batch 5577/11884 - Loss: 29.2468\n",
      "Processing batch 5578/11884 - Loss: 31.5409\n",
      "Processing batch 5579/11884 - Loss: 31.2724\n",
      "Processing batch 5580/11884 - Loss: 31.5689\n",
      "Processing batch 5581/11884 - Loss: 29.4041\n",
      "Processing batch 5582/11884 - Loss: 30.3417\n",
      "Processing batch 5583/11884 - Loss: 29.0914\n",
      "Processing batch 5584/11884 - Loss: 30.8832\n",
      "Processing batch 5585/11884 - Loss: 29.4640\n",
      "Processing batch 5586/11884 - Loss: 30.0866\n",
      "Processing batch 5587/11884 - Loss: 29.2761\n",
      "Processing batch 5588/11884 - Loss: 31.3033\n",
      "Processing batch 5589/11884 - Loss: 31.7083\n",
      "Processing batch 5590/11884 - Loss: 29.9477\n",
      "Processing batch 5591/11884 - Loss: 29.5426\n",
      "Processing batch 5592/11884 - Loss: 30.8732\n",
      "Processing batch 5593/11884 - Loss: 30.4897\n",
      "Processing batch 5594/11884 - Loss: 30.2466\n",
      "Processing batch 5595/11884 - Loss: 29.9779\n",
      "Processing batch 5596/11884 - Loss: 30.8161\n",
      "Processing batch 5597/11884 - Loss: 29.9594\n",
      "Processing batch 5598/11884 - Loss: 30.3901\n",
      "Processing batch 5599/11884 - Loss: 31.0240\n",
      "Processing batch 5600/11884 - Loss: 30.3344\n",
      "Processing batch 5601/11884 - Loss: 29.6731\n",
      "Processing batch 5602/11884 - Loss: 29.7782\n",
      "Processing batch 5603/11884 - Loss: 30.8952\n",
      "Processing batch 5604/11884 - Loss: 28.8032\n",
      "Processing batch 5605/11884 - Loss: 30.0631\n",
      "Processing batch 5606/11884 - Loss: 28.6917\n",
      "Processing batch 5607/11884 - Loss: 30.8922\n",
      "Processing batch 5608/11884 - Loss: 29.8957\n",
      "Processing batch 5609/11884 - Loss: 31.4225\n",
      "Processing batch 5610/11884 - Loss: 30.5360\n",
      "Processing batch 5611/11884 - Loss: 30.0057\n",
      "Processing batch 5612/11884 - Loss: 30.3547\n",
      "Processing batch 5613/11884 - Loss: 30.1687\n",
      "Processing batch 5614/11884 - Loss: 30.3329\n",
      "Processing batch 5615/11884 - Loss: 29.8429\n",
      "Processing batch 5616/11884 - Loss: 30.5519\n",
      "Processing batch 5617/11884 - Loss: 30.4506\n",
      "Processing batch 5618/11884 - Loss: 30.2980\n",
      "Processing batch 5619/11884 - Loss: 29.5509\n",
      "Processing batch 5620/11884 - Loss: 29.5636\n",
      "Processing batch 5621/11884 - Loss: 30.7439\n",
      "Processing batch 5622/11884 - Loss: 29.7106\n",
      "Processing batch 5623/11884 - Loss: 30.0980\n",
      "Processing batch 5624/11884 - Loss: 29.7206\n",
      "Processing batch 5625/11884 - Loss: 30.8719\n",
      "Processing batch 5626/11884 - Loss: 30.5706\n",
      "Processing batch 5627/11884 - Loss: 29.8660\n",
      "Processing batch 5628/11884 - Loss: 30.0898\n",
      "Processing batch 5629/11884 - Loss: 29.1898\n",
      "Processing batch 5630/11884 - Loss: 30.5062\n",
      "Processing batch 5631/11884 - Loss: 27.9606\n",
      "Processing batch 5632/11884 - Loss: 30.1921\n",
      "Processing batch 5633/11884 - Loss: 30.7443\n",
      "Processing batch 5634/11884 - Loss: 29.7890\n",
      "Processing batch 5635/11884 - Loss: 29.3163\n",
      "Processing batch 5636/11884 - Loss: 30.1876\n",
      "Processing batch 5637/11884 - Loss: 30.0315\n",
      "Processing batch 5638/11884 - Loss: 31.2391\n",
      "Processing batch 5639/11884 - Loss: 30.3151\n",
      "Processing batch 5640/11884 - Loss: 30.3109\n",
      "Processing batch 5641/11884 - Loss: 29.8507\n",
      "Processing batch 5642/11884 - Loss: 30.7041\n",
      "Processing batch 5643/11884 - Loss: 30.9944\n",
      "Processing batch 5644/11884 - Loss: 29.3927\n",
      "Processing batch 5645/11884 - Loss: 29.5009\n",
      "Processing batch 5646/11884 - Loss: 30.3141\n",
      "Processing batch 5647/11884 - Loss: 29.4028\n",
      "Processing batch 5648/11884 - Loss: 30.2137\n",
      "Processing batch 5649/11884 - Loss: 28.1038\n",
      "Processing batch 5650/11884 - Loss: 29.9154\n",
      "Processing batch 5651/11884 - Loss: 29.8217\n",
      "Processing batch 5652/11884 - Loss: 29.8748\n",
      "Processing batch 5653/11884 - Loss: 30.1641\n",
      "Processing batch 5654/11884 - Loss: 30.1282\n",
      "Processing batch 5655/11884 - Loss: 29.6408\n",
      "Processing batch 5656/11884 - Loss: 30.0763\n",
      "Processing batch 5657/11884 - Loss: 28.7828\n",
      "Processing batch 5658/11884 - Loss: 29.1145\n",
      "Processing batch 5659/11884 - Loss: 29.3227\n",
      "Processing batch 5660/11884 - Loss: 30.7229\n",
      "Processing batch 5661/11884 - Loss: 31.2123\n",
      "Processing batch 5662/11884 - Loss: 29.9737\n",
      "Processing batch 5663/11884 - Loss: 29.6140\n",
      "Processing batch 5664/11884 - Loss: 29.7224\n",
      "Processing batch 5665/11884 - Loss: 29.4313\n",
      "Processing batch 5666/11884 - Loss: 31.7140\n",
      "Processing batch 5667/11884 - Loss: 30.3300\n",
      "Processing batch 5668/11884 - Loss: 31.4349\n",
      "Processing batch 5669/11884 - Loss: 29.9492\n",
      "Processing batch 5670/11884 - Loss: 30.2672\n",
      "Processing batch 5671/11884 - Loss: 30.1004\n",
      "Processing batch 5672/11884 - Loss: 29.3069\n",
      "Processing batch 5673/11884 - Loss: 29.6443\n",
      "Processing batch 5674/11884 - Loss: 30.5364\n",
      "Processing batch 5675/11884 - Loss: 30.9671\n",
      "Processing batch 5676/11884 - Loss: 30.7648\n",
      "Processing batch 5677/11884 - Loss: 31.4233\n",
      "Processing batch 5678/11884 - Loss: 30.0470\n",
      "Processing batch 5679/11884 - Loss: 30.0756\n",
      "Processing batch 5680/11884 - Loss: 28.9746\n",
      "Processing batch 5681/11884 - Loss: 29.5123\n",
      "Processing batch 5682/11884 - Loss: 30.2626\n",
      "Processing batch 5683/11884 - Loss: 29.2870\n",
      "Processing batch 5684/11884 - Loss: 29.7347\n",
      "Processing batch 5685/11884 - Loss: 30.2056\n",
      "Processing batch 5686/11884 - Loss: 30.9439\n",
      "Processing batch 5687/11884 - Loss: 29.6373\n",
      "Processing batch 5688/11884 - Loss: 29.0914\n",
      "Processing batch 5689/11884 - Loss: 29.7868\n",
      "Processing batch 5690/11884 - Loss: 29.1834\n",
      "Processing batch 5691/11884 - Loss: 29.8160\n",
      "Processing batch 5692/11884 - Loss: 29.7812\n",
      "Processing batch 5693/11884 - Loss: 31.0537\n",
      "Processing batch 5694/11884 - Loss: 30.7874\n",
      "Processing batch 5695/11884 - Loss: 29.7482\n",
      "Processing batch 5696/11884 - Loss: 29.8060\n",
      "Processing batch 5697/11884 - Loss: 31.0329\n",
      "Processing batch 5698/11884 - Loss: 30.8372\n",
      "Processing batch 5699/11884 - Loss: 29.1702\n",
      "Processing batch 5700/11884 - Loss: 28.6809\n",
      "Processing batch 5701/11884 - Loss: 30.1776\n",
      "Processing batch 5702/11884 - Loss: 29.8219\n",
      "Processing batch 5703/11884 - Loss: 30.6646\n",
      "Processing batch 5704/11884 - Loss: 29.5764\n",
      "Processing batch 5705/11884 - Loss: 30.4144\n",
      "Processing batch 5706/11884 - Loss: 30.5147\n",
      "Processing batch 5707/11884 - Loss: 30.8784\n",
      "Processing batch 5708/11884 - Loss: 28.6444\n",
      "Processing batch 5709/11884 - Loss: 30.6956\n",
      "Processing batch 5710/11884 - Loss: 30.5167\n",
      "Processing batch 5711/11884 - Loss: 29.4465\n",
      "Processing batch 5712/11884 - Loss: 29.2520\n",
      "Processing batch 5713/11884 - Loss: 31.1408\n",
      "Processing batch 5714/11884 - Loss: 29.8614\n",
      "Processing batch 5715/11884 - Loss: 29.1272\n",
      "Processing batch 5716/11884 - Loss: 29.5998\n",
      "Processing batch 5717/11884 - Loss: 29.4828\n",
      "Processing batch 5718/11884 - Loss: 27.9529\n",
      "Processing batch 5719/11884 - Loss: 30.6704\n",
      "Processing batch 5720/11884 - Loss: 30.6353\n",
      "Processing batch 5721/11884 - Loss: 29.6118\n",
      "Processing batch 5722/11884 - Loss: 29.0337\n",
      "Processing batch 5723/11884 - Loss: 31.5471\n",
      "Processing batch 5724/11884 - Loss: 28.9383\n",
      "Processing batch 5725/11884 - Loss: 31.5880\n",
      "Processing batch 5726/11884 - Loss: 30.5869\n",
      "Processing batch 5727/11884 - Loss: 29.8282\n",
      "Processing batch 5728/11884 - Loss: 29.2302\n",
      "Processing batch 5729/11884 - Loss: 30.3342\n",
      "Processing batch 5730/11884 - Loss: 29.8467\n",
      "Processing batch 5731/11884 - Loss: 30.4246\n",
      "Processing batch 5732/11884 - Loss: 28.5829\n",
      "Processing batch 5733/11884 - Loss: 31.0791\n",
      "Processing batch 5734/11884 - Loss: 30.7248\n",
      "Processing batch 5735/11884 - Loss: 29.6188\n",
      "Processing batch 5736/11884 - Loss: 31.2622\n",
      "Processing batch 5737/11884 - Loss: 30.8775\n",
      "Processing batch 5738/11884 - Loss: 30.6275\n",
      "Processing batch 5739/11884 - Loss: 31.0202\n",
      "Processing batch 5740/11884 - Loss: 30.2111\n",
      "Processing batch 5741/11884 - Loss: 29.5888\n",
      "Processing batch 5742/11884 - Loss: 30.0196\n",
      "Processing batch 5743/11884 - Loss: 29.4676\n",
      "Processing batch 5744/11884 - Loss: 31.9545\n",
      "Processing batch 5745/11884 - Loss: 30.9359\n",
      "Processing batch 5746/11884 - Loss: 29.2755\n",
      "Processing batch 5747/11884 - Loss: 29.6701\n",
      "Processing batch 5748/11884 - Loss: 30.2697\n",
      "Processing batch 5749/11884 - Loss: 30.9608\n",
      "Processing batch 5750/11884 - Loss: 30.2773\n",
      "Processing batch 5751/11884 - Loss: 30.6146\n",
      "Processing batch 5752/11884 - Loss: 29.4783\n",
      "Processing batch 5753/11884 - Loss: 29.5381\n",
      "Processing batch 5754/11884 - Loss: 29.3455\n",
      "Processing batch 5755/11884 - Loss: 30.0833\n",
      "Processing batch 5756/11884 - Loss: 30.6629\n",
      "Processing batch 5757/11884 - Loss: 29.3774\n",
      "Processing batch 5758/11884 - Loss: 29.8180\n",
      "Processing batch 5759/11884 - Loss: 30.0397\n",
      "Processing batch 5760/11884 - Loss: 29.6542\n",
      "Processing batch 5761/11884 - Loss: 31.1468\n",
      "Processing batch 5762/11884 - Loss: 29.5301\n",
      "Processing batch 5763/11884 - Loss: 29.8424\n",
      "Processing batch 5764/11884 - Loss: 29.0909\n",
      "Processing batch 5765/11884 - Loss: 30.6452\n",
      "Processing batch 5766/11884 - Loss: 29.1608\n",
      "Processing batch 5767/11884 - Loss: 28.9011\n",
      "Processing batch 5768/11884 - Loss: 30.7794\n",
      "Processing batch 5769/11884 - Loss: 29.7172\n",
      "Processing batch 5770/11884 - Loss: 30.4629\n",
      "Processing batch 5771/11884 - Loss: 28.5617\n",
      "Processing batch 5772/11884 - Loss: 30.4288\n",
      "Processing batch 5773/11884 - Loss: 29.5174\n",
      "Processing batch 5774/11884 - Loss: 30.9783\n",
      "Processing batch 5775/11884 - Loss: 30.2554\n",
      "Processing batch 5776/11884 - Loss: 30.0068\n",
      "Processing batch 5777/11884 - Loss: 30.8591\n",
      "Processing batch 5778/11884 - Loss: 32.0515\n",
      "Processing batch 5779/11884 - Loss: 31.1996\n",
      "Processing batch 5780/11884 - Loss: 29.5073\n",
      "Processing batch 5781/11884 - Loss: 30.6878\n",
      "Processing batch 5782/11884 - Loss: 30.8286\n",
      "Processing batch 5783/11884 - Loss: 31.0130\n",
      "Processing batch 5784/11884 - Loss: 29.6234\n",
      "Processing batch 5785/11884 - Loss: 30.4740\n",
      "Processing batch 5786/11884 - Loss: 30.7999\n",
      "Processing batch 5787/11884 - Loss: 30.1156\n",
      "Processing batch 5788/11884 - Loss: 30.7474\n",
      "Processing batch 5789/11884 - Loss: 29.9145\n",
      "Processing batch 5790/11884 - Loss: 29.4391\n",
      "Processing batch 5791/11884 - Loss: 30.3730\n",
      "Processing batch 5792/11884 - Loss: 30.2927\n",
      "Processing batch 5793/11884 - Loss: 31.0301\n",
      "Processing batch 5794/11884 - Loss: 30.3067\n",
      "Processing batch 5795/11884 - Loss: 31.0524\n",
      "Processing batch 5796/11884 - Loss: 30.7716\n",
      "Processing batch 5797/11884 - Loss: 30.0111\n",
      "Processing batch 5798/11884 - Loss: 30.1231\n",
      "Processing batch 5799/11884 - Loss: 31.2366\n",
      "Processing batch 5800/11884 - Loss: 30.5461\n",
      "Processing batch 5801/11884 - Loss: 29.4384\n",
      "Processing batch 5802/11884 - Loss: 29.6821\n",
      "Processing batch 5803/11884 - Loss: 30.6119\n",
      "Processing batch 5804/11884 - Loss: 30.5745\n",
      "Processing batch 5805/11884 - Loss: 30.8682\n",
      "Processing batch 5806/11884 - Loss: 30.9887\n",
      "Processing batch 5807/11884 - Loss: 30.1281\n",
      "Processing batch 5808/11884 - Loss: 31.0122\n",
      "Processing batch 5809/11884 - Loss: 30.9563\n",
      "Processing batch 5810/11884 - Loss: 29.8587\n",
      "Processing batch 5811/11884 - Loss: 29.7604\n",
      "Processing batch 5812/11884 - Loss: 28.4774\n",
      "Processing batch 5813/11884 - Loss: 29.9962\n",
      "Processing batch 5814/11884 - Loss: 29.8952\n",
      "Processing batch 5815/11884 - Loss: 29.5375\n",
      "Processing batch 5816/11884 - Loss: 30.5742\n",
      "Processing batch 5817/11884 - Loss: 29.5050\n",
      "Processing batch 5818/11884 - Loss: 30.3782\n",
      "Processing batch 5819/11884 - Loss: 29.3315\n",
      "Processing batch 5820/11884 - Loss: 29.9998\n",
      "Processing batch 5821/11884 - Loss: 29.5071\n",
      "Processing batch 5822/11884 - Loss: 28.7154\n",
      "Processing batch 5823/11884 - Loss: 30.0138\n",
      "Processing batch 5824/11884 - Loss: 29.9025\n",
      "Processing batch 5825/11884 - Loss: 30.1056\n",
      "Processing batch 5826/11884 - Loss: 29.1513\n",
      "Processing batch 5827/11884 - Loss: 30.7814\n",
      "Processing batch 5828/11884 - Loss: 27.8454\n",
      "Processing batch 5829/11884 - Loss: 30.8102\n",
      "Processing batch 5830/11884 - Loss: 28.6996\n",
      "Processing batch 5831/11884 - Loss: 30.5643\n",
      "Processing batch 5832/11884 - Loss: 29.5655\n",
      "Processing batch 5833/11884 - Loss: 30.3394\n",
      "Processing batch 5834/11884 - Loss: 31.0182\n",
      "Processing batch 5835/11884 - Loss: 31.0289\n",
      "Processing batch 5836/11884 - Loss: 29.1308\n",
      "Processing batch 5837/11884 - Loss: 30.7031\n",
      "Processing batch 5838/11884 - Loss: 30.0891\n",
      "Processing batch 5839/11884 - Loss: 30.6829\n",
      "Processing batch 5840/11884 - Loss: 29.8880\n",
      "Processing batch 5841/11884 - Loss: 31.1878\n",
      "Processing batch 5842/11884 - Loss: 31.0671\n",
      "Processing batch 5843/11884 - Loss: 29.6721\n",
      "Processing batch 5844/11884 - Loss: 29.8298\n",
      "Processing batch 5845/11884 - Loss: 29.8341\n",
      "Processing batch 5846/11884 - Loss: 29.5091\n",
      "Processing batch 5847/11884 - Loss: 31.7825\n",
      "Processing batch 5848/11884 - Loss: 30.4004\n",
      "Processing batch 5849/11884 - Loss: 30.7669\n",
      "Processing batch 5850/11884 - Loss: 29.7166\n",
      "Processing batch 5851/11884 - Loss: 31.1765\n",
      "Processing batch 5852/11884 - Loss: 29.3323\n",
      "Processing batch 5853/11884 - Loss: 30.7905\n",
      "Processing batch 5854/11884 - Loss: 29.6039\n",
      "Processing batch 5855/11884 - Loss: 29.3697\n",
      "Processing batch 5856/11884 - Loss: 31.0068\n",
      "Processing batch 5857/11884 - Loss: 30.2728\n",
      "Processing batch 5858/11884 - Loss: 31.2422\n",
      "Processing batch 5859/11884 - Loss: 30.3785\n",
      "Processing batch 5860/11884 - Loss: 29.8397\n",
      "Processing batch 5861/11884 - Loss: 30.4088\n",
      "Processing batch 5862/11884 - Loss: 29.1136\n",
      "Processing batch 5863/11884 - Loss: 30.5441\n",
      "Processing batch 5864/11884 - Loss: 30.3654\n",
      "Processing batch 5865/11884 - Loss: 28.7271\n",
      "Processing batch 5866/11884 - Loss: 29.7958\n",
      "Processing batch 5867/11884 - Loss: 30.1899\n",
      "Processing batch 5868/11884 - Loss: 30.4788\n",
      "Processing batch 5869/11884 - Loss: 29.5517\n",
      "Processing batch 5870/11884 - Loss: 29.9943\n",
      "Processing batch 5871/11884 - Loss: 29.8044\n",
      "Processing batch 5872/11884 - Loss: 30.8387\n",
      "Processing batch 5873/11884 - Loss: 29.8575\n",
      "Processing batch 5874/11884 - Loss: 31.0013\n",
      "Processing batch 5875/11884 - Loss: 30.7150\n",
      "Processing batch 5876/11884 - Loss: 29.5033\n",
      "Processing batch 5877/11884 - Loss: 30.7363\n",
      "Processing batch 5878/11884 - Loss: 29.9880\n",
      "Processing batch 5879/11884 - Loss: 30.4143\n",
      "Processing batch 5880/11884 - Loss: 28.6702\n",
      "Processing batch 5881/11884 - Loss: 28.8030\n",
      "Processing batch 5882/11884 - Loss: 31.0031\n",
      "Processing batch 5883/11884 - Loss: 31.1080\n",
      "Processing batch 5884/11884 - Loss: 32.0454\n",
      "Processing batch 5885/11884 - Loss: 30.0514\n",
      "Processing batch 5886/11884 - Loss: 30.6823\n",
      "Processing batch 5887/11884 - Loss: 30.5665\n",
      "Processing batch 5888/11884 - Loss: 31.6976\n",
      "Processing batch 5889/11884 - Loss: 29.4834\n",
      "Processing batch 5890/11884 - Loss: 29.5749\n",
      "Processing batch 5891/11884 - Loss: 29.1533\n",
      "Processing batch 5892/11884 - Loss: 29.7234\n",
      "Processing batch 5893/11884 - Loss: 30.4656\n",
      "Processing batch 5894/11884 - Loss: 28.8044\n",
      "Processing batch 5895/11884 - Loss: 29.0389\n",
      "Processing batch 5896/11884 - Loss: 29.3638\n",
      "Processing batch 5897/11884 - Loss: 29.4182\n",
      "Processing batch 5898/11884 - Loss: 31.6985\n",
      "Processing batch 5899/11884 - Loss: 29.6983\n",
      "Processing batch 5900/11884 - Loss: 31.5570\n",
      "Processing batch 5901/11884 - Loss: 29.0199\n",
      "Processing batch 5902/11884 - Loss: 29.4983\n",
      "Processing batch 5903/11884 - Loss: 30.1770\n",
      "Processing batch 5904/11884 - Loss: 30.2450\n",
      "Processing batch 5905/11884 - Loss: 29.3388\n",
      "Processing batch 5906/11884 - Loss: 30.3268\n",
      "Processing batch 5907/11884 - Loss: 31.4531\n",
      "Processing batch 5908/11884 - Loss: 28.8498\n",
      "Processing batch 5909/11884 - Loss: 31.4484\n",
      "Processing batch 5910/11884 - Loss: 31.2086\n",
      "Processing batch 5911/11884 - Loss: 31.3967\n",
      "Processing batch 5912/11884 - Loss: 29.7727\n",
      "Processing batch 5913/11884 - Loss: 30.1767\n",
      "Processing batch 5914/11884 - Loss: 28.7617\n",
      "Processing batch 5915/11884 - Loss: 30.8052\n",
      "Processing batch 5916/11884 - Loss: 30.6046\n",
      "Processing batch 5917/11884 - Loss: 31.4712\n",
      "Processing batch 5918/11884 - Loss: 31.3281\n",
      "Processing batch 5919/11884 - Loss: 30.1767\n",
      "Processing batch 5920/11884 - Loss: 29.8614\n",
      "Processing batch 5921/11884 - Loss: 30.0060\n",
      "Processing batch 5922/11884 - Loss: 30.2528\n",
      "Processing batch 5923/11884 - Loss: 30.7924\n",
      "Processing batch 5924/11884 - Loss: 30.6973\n",
      "Processing batch 5925/11884 - Loss: 29.9170\n",
      "Processing batch 5926/11884 - Loss: 30.2565\n",
      "Processing batch 5927/11884 - Loss: 29.7949\n",
      "Processing batch 5928/11884 - Loss: 29.2471\n",
      "Processing batch 5929/11884 - Loss: 29.8886\n",
      "Processing batch 5930/11884 - Loss: 29.7798\n",
      "Processing batch 5931/11884 - Loss: 29.9456\n",
      "Processing batch 5932/11884 - Loss: 29.9336\n",
      "Processing batch 5933/11884 - Loss: 29.9073\n",
      "Processing batch 5934/11884 - Loss: 31.8468\n",
      "Processing batch 5935/11884 - Loss: 29.6800\n",
      "Processing batch 5936/11884 - Loss: 30.0878\n",
      "Processing batch 5937/11884 - Loss: 31.1696\n",
      "Processing batch 5938/11884 - Loss: 28.5772\n",
      "Processing batch 5939/11884 - Loss: 29.1098\n",
      "Processing batch 5940/11884 - Loss: 31.1576\n",
      "Processing batch 5941/11884 - Loss: 29.7428\n",
      "Processing batch 5942/11884 - Loss: 29.4568\n",
      "Processing batch 5943/11884 - Loss: 29.1985\n",
      "Processing batch 5944/11884 - Loss: 29.7504\n",
      "Processing batch 5945/11884 - Loss: 31.0580\n",
      "Processing batch 5946/11884 - Loss: 30.4892\n",
      "Processing batch 5947/11884 - Loss: 30.4472\n",
      "Processing batch 5948/11884 - Loss: 28.8567\n",
      "Processing batch 5949/11884 - Loss: 29.4321\n",
      "Processing batch 5950/11884 - Loss: 29.1430\n",
      "Processing batch 5951/11884 - Loss: 30.9490\n",
      "Processing batch 5952/11884 - Loss: 29.6010\n",
      "Processing batch 5953/11884 - Loss: 31.1240\n",
      "Processing batch 5954/11884 - Loss: 30.7492\n",
      "Processing batch 5955/11884 - Loss: 30.3654\n",
      "Processing batch 5956/11884 - Loss: 29.3440\n",
      "Processing batch 5957/11884 - Loss: 30.8991\n",
      "Processing batch 5958/11884 - Loss: 30.1264\n",
      "Processing batch 5959/11884 - Loss: 31.7690\n",
      "Processing batch 5960/11884 - Loss: 30.9420\n",
      "Processing batch 5961/11884 - Loss: 28.4816\n",
      "Processing batch 5962/11884 - Loss: 29.7956\n",
      "Processing batch 5963/11884 - Loss: 32.0622\n",
      "Processing batch 5964/11884 - Loss: 29.3755\n",
      "Processing batch 5965/11884 - Loss: 29.6051\n",
      "Processing batch 5966/11884 - Loss: 28.7002\n",
      "Processing batch 5967/11884 - Loss: 31.1579\n",
      "Processing batch 5968/11884 - Loss: 30.2289\n",
      "Processing batch 5969/11884 - Loss: 29.7984\n",
      "Processing batch 5970/11884 - Loss: 29.6910\n",
      "Processing batch 5971/11884 - Loss: 30.3382\n",
      "Processing batch 5972/11884 - Loss: 30.1390\n",
      "Processing batch 5973/11884 - Loss: 30.2459\n",
      "Processing batch 5974/11884 - Loss: 29.7596\n",
      "Processing batch 5975/11884 - Loss: 30.5534\n",
      "Processing batch 5976/11884 - Loss: 29.5261\n",
      "Processing batch 5977/11884 - Loss: 29.1147\n",
      "Processing batch 5978/11884 - Loss: 28.8821\n",
      "Processing batch 5979/11884 - Loss: 30.4442\n",
      "Processing batch 5980/11884 - Loss: 29.4836\n",
      "Processing batch 5981/11884 - Loss: 29.8202\n",
      "Processing batch 5982/11884 - Loss: 30.0439\n",
      "Processing batch 5983/11884 - Loss: 30.0614\n",
      "Processing batch 5984/11884 - Loss: 30.3393\n",
      "Processing batch 5985/11884 - Loss: 30.6015\n",
      "Processing batch 5986/11884 - Loss: 31.0266\n",
      "Processing batch 5987/11884 - Loss: 30.3452\n",
      "Processing batch 5988/11884 - Loss: 30.2382\n",
      "Processing batch 5989/11884 - Loss: 29.2245\n",
      "Processing batch 5990/11884 - Loss: 30.6819\n",
      "Processing batch 5991/11884 - Loss: 29.9760\n",
      "Processing batch 5992/11884 - Loss: 32.0545\n",
      "Processing batch 5993/11884 - Loss: 30.2212\n",
      "Processing batch 5994/11884 - Loss: 31.0268\n",
      "Processing batch 5995/11884 - Loss: 30.1678\n",
      "Processing batch 5996/11884 - Loss: 30.3094\n",
      "Processing batch 5997/11884 - Loss: 28.9972\n",
      "Processing batch 5998/11884 - Loss: 31.9465\n",
      "Processing batch 5999/11884 - Loss: 30.7193\n",
      "Processing batch 6000/11884 - Loss: 29.9532\n",
      "Processing batch 6001/11884 - Loss: 29.6455\n",
      "Processing batch 6002/11884 - Loss: 30.8800\n",
      "Processing batch 6003/11884 - Loss: 30.9910\n",
      "Processing batch 6004/11884 - Loss: 29.3254\n",
      "Processing batch 6005/11884 - Loss: 29.8761\n",
      "Processing batch 6006/11884 - Loss: 30.4708\n",
      "Processing batch 6007/11884 - Loss: 31.0537\n",
      "Processing batch 6008/11884 - Loss: 31.8307\n",
      "Processing batch 6009/11884 - Loss: 31.6145\n",
      "Processing batch 6010/11884 - Loss: 29.9829\n",
      "Processing batch 6011/11884 - Loss: 29.4192\n",
      "Processing batch 6012/11884 - Loss: 29.7308\n",
      "Processing batch 6013/11884 - Loss: 28.7725\n",
      "Processing batch 6014/11884 - Loss: 30.5622\n",
      "Processing batch 6015/11884 - Loss: 30.4210\n",
      "Processing batch 6016/11884 - Loss: 30.7734\n",
      "Processing batch 6017/11884 - Loss: 29.7889\n",
      "Processing batch 6018/11884 - Loss: 29.0334\n",
      "Processing batch 6019/11884 - Loss: 29.5145\n",
      "Processing batch 6020/11884 - Loss: 30.2971\n",
      "Processing batch 6021/11884 - Loss: 30.7934\n",
      "Processing batch 6022/11884 - Loss: 29.5275\n",
      "Processing batch 6023/11884 - Loss: 29.3619\n",
      "Processing batch 6024/11884 - Loss: 29.7199\n",
      "Processing batch 6025/11884 - Loss: 29.6613\n",
      "Processing batch 6026/11884 - Loss: 30.7554\n",
      "Processing batch 6027/11884 - Loss: 30.7048\n",
      "Processing batch 6028/11884 - Loss: 30.8795\n",
      "Processing batch 6029/11884 - Loss: 29.3885\n",
      "Processing batch 6030/11884 - Loss: 31.3738\n",
      "Processing batch 6031/11884 - Loss: 31.5338\n",
      "Processing batch 6032/11884 - Loss: 30.3693\n",
      "Processing batch 6033/11884 - Loss: 29.5523\n",
      "Processing batch 6034/11884 - Loss: 29.5244\n",
      "Processing batch 6035/11884 - Loss: 30.4096\n",
      "Processing batch 6036/11884 - Loss: 28.9763\n",
      "Processing batch 6037/11884 - Loss: 29.8229\n",
      "Processing batch 6038/11884 - Loss: 30.9408\n",
      "Processing batch 6039/11884 - Loss: 30.6156\n",
      "Processing batch 6040/11884 - Loss: 29.4182\n",
      "Processing batch 6041/11884 - Loss: 30.5562\n",
      "Processing batch 6042/11884 - Loss: 30.0873\n",
      "Processing batch 6043/11884 - Loss: 30.8827\n",
      "Processing batch 6044/11884 - Loss: 31.5522\n",
      "Processing batch 6045/11884 - Loss: 30.1791\n",
      "Processing batch 6046/11884 - Loss: 31.6904\n",
      "Processing batch 6047/11884 - Loss: 30.2839\n",
      "Processing batch 6048/11884 - Loss: 30.0228\n",
      "Processing batch 6049/11884 - Loss: 29.7773\n",
      "Processing batch 6050/11884 - Loss: 29.6640\n",
      "Processing batch 6051/11884 - Loss: 29.1934\n",
      "Processing batch 6052/11884 - Loss: 29.5997\n",
      "Processing batch 6053/11884 - Loss: 30.1568\n",
      "Processing batch 6054/11884 - Loss: 30.0904\n",
      "Processing batch 6055/11884 - Loss: 29.4840\n",
      "Processing batch 6056/11884 - Loss: 29.1886\n",
      "Processing batch 6057/11884 - Loss: 29.5608\n",
      "Processing batch 6058/11884 - Loss: 29.8332\n",
      "Processing batch 6059/11884 - Loss: 29.7929\n",
      "Processing batch 6060/11884 - Loss: 30.4618\n",
      "Processing batch 6061/11884 - Loss: 29.7541\n",
      "Processing batch 6062/11884 - Loss: 29.4752\n",
      "Processing batch 6063/11884 - Loss: 30.9676\n",
      "Processing batch 6064/11884 - Loss: 31.3609\n",
      "Processing batch 6065/11884 - Loss: 29.6649\n",
      "Processing batch 6066/11884 - Loss: 30.8040\n",
      "Processing batch 6067/11884 - Loss: 30.1096\n",
      "Processing batch 6068/11884 - Loss: 29.8664\n",
      "Processing batch 6069/11884 - Loss: 30.8525\n",
      "Processing batch 6070/11884 - Loss: 31.3168\n",
      "Processing batch 6071/11884 - Loss: 30.9869\n",
      "Processing batch 6072/11884 - Loss: 30.9561\n",
      "Processing batch 6073/11884 - Loss: 30.7755\n",
      "Processing batch 6074/11884 - Loss: 31.0500\n",
      "Processing batch 6075/11884 - Loss: 29.6108\n",
      "Processing batch 6076/11884 - Loss: 30.7755\n",
      "Processing batch 6077/11884 - Loss: 30.3295\n",
      "Processing batch 6078/11884 - Loss: 29.6370\n",
      "Processing batch 6079/11884 - Loss: 30.2193\n",
      "Processing batch 6080/11884 - Loss: 29.8801\n",
      "Processing batch 6081/11884 - Loss: 28.8103\n",
      "Processing batch 6082/11884 - Loss: 29.8238\n",
      "Processing batch 6083/11884 - Loss: 30.5493\n",
      "Processing batch 6084/11884 - Loss: 30.1656\n",
      "Processing batch 6085/11884 - Loss: 30.0902\n",
      "Processing batch 6086/11884 - Loss: 31.2422\n",
      "Processing batch 6087/11884 - Loss: 30.9155\n",
      "Processing batch 6088/11884 - Loss: 30.7181\n",
      "Processing batch 6089/11884 - Loss: 30.5530\n",
      "Processing batch 6090/11884 - Loss: 29.3644\n",
      "Processing batch 6091/11884 - Loss: 29.3150\n",
      "Processing batch 6092/11884 - Loss: 29.5718\n",
      "Processing batch 6093/11884 - Loss: 29.0598\n",
      "Processing batch 6094/11884 - Loss: 30.8658\n",
      "Processing batch 6095/11884 - Loss: 30.6524\n",
      "Processing batch 6096/11884 - Loss: 31.5812\n",
      "Processing batch 6097/11884 - Loss: 30.4483\n",
      "Processing batch 6098/11884 - Loss: 30.1995\n",
      "Processing batch 6099/11884 - Loss: 30.1492\n",
      "Processing batch 6100/11884 - Loss: 29.8729\n",
      "Processing batch 6101/11884 - Loss: 29.1711\n",
      "Processing batch 6102/11884 - Loss: 30.2847\n",
      "Processing batch 6103/11884 - Loss: 29.9175\n",
      "Processing batch 6104/11884 - Loss: 30.1109\n",
      "Processing batch 6105/11884 - Loss: 29.3530\n",
      "Processing batch 6106/11884 - Loss: 29.0023\n",
      "Processing batch 6107/11884 - Loss: 31.3163\n",
      "Processing batch 6108/11884 - Loss: 29.4916\n",
      "Processing batch 6109/11884 - Loss: 30.7443\n",
      "Processing batch 6110/11884 - Loss: 29.9641\n",
      "Processing batch 6111/11884 - Loss: 28.2837\n",
      "Processing batch 6112/11884 - Loss: 31.8476\n",
      "Processing batch 6113/11884 - Loss: 29.9400\n",
      "Processing batch 6114/11884 - Loss: 29.8341\n",
      "Processing batch 6115/11884 - Loss: 30.2794\n",
      "Processing batch 6116/11884 - Loss: 30.6816\n",
      "Processing batch 6117/11884 - Loss: 31.0367\n",
      "Processing batch 6118/11884 - Loss: 29.9897\n",
      "Processing batch 6119/11884 - Loss: 30.0154\n",
      "Processing batch 6120/11884 - Loss: 30.0778\n",
      "Processing batch 6121/11884 - Loss: 29.7507\n",
      "Processing batch 6122/11884 - Loss: 30.5414\n",
      "Processing batch 6123/11884 - Loss: 30.9549\n",
      "Processing batch 6124/11884 - Loss: 32.0809\n",
      "Processing batch 6125/11884 - Loss: 30.2017\n",
      "Processing batch 6126/11884 - Loss: 28.9789\n",
      "Processing batch 6127/11884 - Loss: 30.0196\n",
      "Processing batch 6128/11884 - Loss: 29.7259\n",
      "Processing batch 6129/11884 - Loss: 30.2882\n",
      "Processing batch 6130/11884 - Loss: 30.6217\n",
      "Processing batch 6131/11884 - Loss: 30.5224\n",
      "Processing batch 6132/11884 - Loss: 29.7547\n",
      "Processing batch 6133/11884 - Loss: 30.1029\n",
      "Processing batch 6134/11884 - Loss: 30.7984\n",
      "Processing batch 6135/11884 - Loss: 30.2370\n",
      "Processing batch 6136/11884 - Loss: 30.5663\n",
      "Processing batch 6137/11884 - Loss: 30.1273\n",
      "Processing batch 6138/11884 - Loss: 30.1624\n",
      "Processing batch 6139/11884 - Loss: 30.7790\n",
      "Processing batch 6140/11884 - Loss: 28.7303\n",
      "Processing batch 6141/11884 - Loss: 30.5090\n",
      "Processing batch 6142/11884 - Loss: 29.9887\n",
      "Processing batch 6143/11884 - Loss: 29.9224\n",
      "Processing batch 6144/11884 - Loss: 29.5354\n",
      "Processing batch 6145/11884 - Loss: 30.6168\n",
      "Processing batch 6146/11884 - Loss: 30.3193\n",
      "Processing batch 6147/11884 - Loss: 30.4749\n",
      "Processing batch 6148/11884 - Loss: 31.3971\n",
      "Processing batch 6149/11884 - Loss: 29.3229\n",
      "Processing batch 6150/11884 - Loss: 30.4995\n",
      "Processing batch 6151/11884 - Loss: 30.1755\n",
      "Processing batch 6152/11884 - Loss: 30.3088\n",
      "Processing batch 6153/11884 - Loss: 31.2872\n",
      "Processing batch 6154/11884 - Loss: 28.9842\n",
      "Processing batch 6155/11884 - Loss: 30.1088\n",
      "Processing batch 6156/11884 - Loss: 30.2920\n",
      "Processing batch 6157/11884 - Loss: 30.3754\n",
      "Processing batch 6158/11884 - Loss: 31.0939\n",
      "Processing batch 6159/11884 - Loss: 30.6825\n",
      "Processing batch 6160/11884 - Loss: 31.0185\n",
      "Processing batch 6161/11884 - Loss: 30.8734\n",
      "Processing batch 6162/11884 - Loss: 30.2549\n",
      "Processing batch 6163/11884 - Loss: 30.1240\n",
      "Processing batch 6164/11884 - Loss: 29.2405\n",
      "Processing batch 6165/11884 - Loss: 29.6561\n",
      "Processing batch 6166/11884 - Loss: 31.1401\n",
      "Processing batch 6167/11884 - Loss: 28.3828\n",
      "Processing batch 6168/11884 - Loss: 30.6281\n",
      "Processing batch 6169/11884 - Loss: 30.3408\n",
      "Processing batch 6170/11884 - Loss: 30.7973\n",
      "Processing batch 6171/11884 - Loss: 30.4819\n",
      "Processing batch 6172/11884 - Loss: 29.8096\n",
      "Processing batch 6173/11884 - Loss: 30.0319\n",
      "Processing batch 6174/11884 - Loss: 30.2111\n",
      "Processing batch 6175/11884 - Loss: 29.6348\n",
      "Processing batch 6176/11884 - Loss: 30.0758\n",
      "Processing batch 6177/11884 - Loss: 29.2870\n",
      "Processing batch 6178/11884 - Loss: 31.1781\n",
      "Processing batch 6179/11884 - Loss: 28.0807\n",
      "Processing batch 6180/11884 - Loss: 29.9849\n",
      "Processing batch 6181/11884 - Loss: 31.1130\n",
      "Processing batch 6182/11884 - Loss: 31.1502\n",
      "Processing batch 6183/11884 - Loss: 30.2880\n",
      "Processing batch 6184/11884 - Loss: 28.7138\n",
      "Processing batch 6185/11884 - Loss: 30.6694\n",
      "Processing batch 6186/11884 - Loss: 30.3180\n",
      "Processing batch 6187/11884 - Loss: 30.6850\n",
      "Processing batch 6188/11884 - Loss: 30.3668\n",
      "Processing batch 6189/11884 - Loss: 30.7366\n",
      "Processing batch 6190/11884 - Loss: 31.1454\n",
      "Processing batch 6191/11884 - Loss: 30.4544\n",
      "Processing batch 6192/11884 - Loss: 29.7472\n",
      "Processing batch 6193/11884 - Loss: 28.4580\n",
      "Processing batch 6194/11884 - Loss: 29.0498\n",
      "Processing batch 6195/11884 - Loss: 30.1259\n",
      "Processing batch 6196/11884 - Loss: 29.5834\n",
      "Processing batch 6197/11884 - Loss: 30.9028\n",
      "Processing batch 6198/11884 - Loss: 30.2336\n",
      "Processing batch 6199/11884 - Loss: 31.7255\n",
      "Processing batch 6200/11884 - Loss: 29.7251\n",
      "Processing batch 6201/11884 - Loss: 30.5528\n",
      "Processing batch 6202/11884 - Loss: 29.4669\n",
      "Processing batch 6203/11884 - Loss: 29.7740\n",
      "Processing batch 6204/11884 - Loss: 30.3636\n",
      "Processing batch 6205/11884 - Loss: 31.0893\n",
      "Processing batch 6206/11884 - Loss: 29.2034\n",
      "Processing batch 6207/11884 - Loss: 30.2940\n",
      "Processing batch 6208/11884 - Loss: 29.2838\n",
      "Processing batch 6209/11884 - Loss: 30.9856\n",
      "Processing batch 6210/11884 - Loss: 29.4191\n",
      "Processing batch 6211/11884 - Loss: 31.0272\n",
      "Processing batch 6212/11884 - Loss: 30.5094\n",
      "Processing batch 6213/11884 - Loss: 29.5928\n",
      "Processing batch 6214/11884 - Loss: 29.6594\n",
      "Processing batch 6215/11884 - Loss: 30.5154\n",
      "Processing batch 6216/11884 - Loss: 30.6182\n",
      "Processing batch 6217/11884 - Loss: 29.2724\n",
      "Processing batch 6218/11884 - Loss: 29.8049\n",
      "Processing batch 6219/11884 - Loss: 30.3064\n",
      "Processing batch 6220/11884 - Loss: 30.2308\n",
      "Processing batch 6221/11884 - Loss: 28.9246\n",
      "Processing batch 6222/11884 - Loss: 28.2280\n",
      "Processing batch 6223/11884 - Loss: 30.0860\n",
      "Processing batch 6224/11884 - Loss: 30.4870\n",
      "Processing batch 6225/11884 - Loss: 29.7827\n",
      "Processing batch 6226/11884 - Loss: 31.1822\n",
      "Processing batch 6227/11884 - Loss: 29.5329\n",
      "Processing batch 6228/11884 - Loss: 29.8140\n",
      "Processing batch 6229/11884 - Loss: 31.4164\n",
      "Processing batch 6230/11884 - Loss: 30.1404\n",
      "Processing batch 6231/11884 - Loss: 30.3355\n",
      "Processing batch 6232/11884 - Loss: 30.2969\n",
      "Processing batch 6233/11884 - Loss: 29.5478\n",
      "Processing batch 6234/11884 - Loss: 29.9478\n",
      "Processing batch 6235/11884 - Loss: 30.1726\n",
      "Processing batch 6236/11884 - Loss: 31.0466\n",
      "Processing batch 6237/11884 - Loss: 28.8672\n",
      "Processing batch 6238/11884 - Loss: 29.9051\n",
      "Processing batch 6239/11884 - Loss: 30.2447\n",
      "Processing batch 6240/11884 - Loss: 29.9630\n",
      "Processing batch 6241/11884 - Loss: 29.9292\n",
      "Processing batch 6242/11884 - Loss: 29.5238\n",
      "Processing batch 6243/11884 - Loss: 29.9410\n",
      "Processing batch 6244/11884 - Loss: 30.7149\n",
      "Processing batch 6245/11884 - Loss: 30.3431\n",
      "Processing batch 6246/11884 - Loss: 30.3568\n",
      "Processing batch 6247/11884 - Loss: 30.2846\n",
      "Processing batch 6248/11884 - Loss: 29.4991\n",
      "Processing batch 6249/11884 - Loss: 28.4763\n",
      "Processing batch 6250/11884 - Loss: 30.0072\n",
      "Processing batch 6251/11884 - Loss: 30.6189\n",
      "Processing batch 6252/11884 - Loss: 28.7833\n",
      "Processing batch 6253/11884 - Loss: 30.4165\n",
      "Processing batch 6254/11884 - Loss: 29.4752\n",
      "Processing batch 6255/11884 - Loss: 31.0863\n",
      "Processing batch 6256/11884 - Loss: 30.2390\n",
      "Processing batch 6257/11884 - Loss: 31.3487\n",
      "Processing batch 6258/11884 - Loss: 29.8012\n",
      "Processing batch 6259/11884 - Loss: 30.8125\n",
      "Processing batch 6260/11884 - Loss: 28.9026\n",
      "Processing batch 6261/11884 - Loss: 29.5555\n",
      "Processing batch 6262/11884 - Loss: 30.0927\n",
      "Processing batch 6263/11884 - Loss: 30.0985\n",
      "Processing batch 6264/11884 - Loss: 30.7325\n",
      "Processing batch 6265/11884 - Loss: 30.7442\n",
      "Processing batch 6266/11884 - Loss: 30.2450\n",
      "Processing batch 6267/11884 - Loss: 30.8009\n",
      "Processing batch 6268/11884 - Loss: 31.1600\n",
      "Processing batch 6269/11884 - Loss: 30.1436\n",
      "Processing batch 6270/11884 - Loss: 30.5760\n",
      "Processing batch 6271/11884 - Loss: 29.3249\n",
      "Processing batch 6272/11884 - Loss: 30.5966\n",
      "Processing batch 6273/11884 - Loss: 30.0751\n",
      "Processing batch 6274/11884 - Loss: 29.9961\n",
      "Processing batch 6275/11884 - Loss: 29.5977\n",
      "Processing batch 6276/11884 - Loss: 29.1328\n",
      "Processing batch 6277/11884 - Loss: 32.2427\n",
      "Processing batch 6278/11884 - Loss: 31.8040\n",
      "Processing batch 6279/11884 - Loss: 30.0917\n",
      "Processing batch 6280/11884 - Loss: 29.5552\n",
      "Processing batch 6281/11884 - Loss: 31.2011\n",
      "Processing batch 6282/11884 - Loss: 29.6338\n",
      "Processing batch 6283/11884 - Loss: 30.6273\n",
      "Processing batch 6284/11884 - Loss: 31.1622\n",
      "Processing batch 6285/11884 - Loss: 30.0237\n",
      "Processing batch 6286/11884 - Loss: 30.2849\n",
      "Processing batch 6287/11884 - Loss: 31.0757\n",
      "Processing batch 6288/11884 - Loss: 30.2119\n",
      "Processing batch 6289/11884 - Loss: 30.5277\n",
      "Processing batch 6290/11884 - Loss: 30.6897\n",
      "Processing batch 6291/11884 - Loss: 29.4964\n",
      "Processing batch 6292/11884 - Loss: 29.3575\n",
      "Processing batch 6293/11884 - Loss: 29.2892\n",
      "Processing batch 6294/11884 - Loss: 30.6744\n",
      "Processing batch 6295/11884 - Loss: 31.9519\n",
      "Processing batch 6296/11884 - Loss: 30.2845\n",
      "Processing batch 6297/11884 - Loss: 28.5909\n",
      "Processing batch 6298/11884 - Loss: 29.1490\n",
      "Processing batch 6299/11884 - Loss: 30.8337\n",
      "Processing batch 6300/11884 - Loss: 29.9989\n",
      "Processing batch 6301/11884 - Loss: 30.2795\n",
      "Processing batch 6302/11884 - Loss: 31.9944\n",
      "Processing batch 6303/11884 - Loss: 28.7303\n",
      "Processing batch 6304/11884 - Loss: 31.3667\n",
      "Processing batch 6305/11884 - Loss: 29.8985\n",
      "Processing batch 6306/11884 - Loss: 29.3349\n",
      "Processing batch 6307/11884 - Loss: 30.5061\n",
      "Processing batch 6308/11884 - Loss: 28.3257\n",
      "Processing batch 6309/11884 - Loss: 28.9556\n",
      "Processing batch 6310/11884 - Loss: 30.0134\n",
      "Processing batch 6311/11884 - Loss: 30.4612\n",
      "Processing batch 6312/11884 - Loss: 30.6463\n",
      "Processing batch 6313/11884 - Loss: 28.8417\n",
      "Processing batch 6314/11884 - Loss: 28.9964\n",
      "Processing batch 6315/11884 - Loss: 29.7722\n",
      "Processing batch 6316/11884 - Loss: 30.2014\n",
      "Processing batch 6317/11884 - Loss: 29.3230\n",
      "Processing batch 6318/11884 - Loss: 31.1784\n",
      "Processing batch 6319/11884 - Loss: 30.0904\n",
      "Processing batch 6320/11884 - Loss: 31.3823\n",
      "Processing batch 6321/11884 - Loss: 29.7483\n",
      "Processing batch 6322/11884 - Loss: 29.7629\n",
      "Processing batch 6323/11884 - Loss: 29.1431\n",
      "Processing batch 6324/11884 - Loss: 28.4746\n",
      "Processing batch 6325/11884 - Loss: 30.1275\n",
      "Processing batch 6326/11884 - Loss: 30.6979\n",
      "Processing batch 6327/11884 - Loss: 31.1111\n",
      "Processing batch 6328/11884 - Loss: 30.5052\n",
      "Processing batch 6329/11884 - Loss: 29.3831\n",
      "Processing batch 6330/11884 - Loss: 29.3319\n",
      "Processing batch 6331/11884 - Loss: 29.6367\n",
      "Processing batch 6332/11884 - Loss: 30.4014\n",
      "Processing batch 6333/11884 - Loss: 30.0087\n",
      "Processing batch 6334/11884 - Loss: 30.4960\n",
      "Processing batch 6335/11884 - Loss: 30.8531\n",
      "Processing batch 6336/11884 - Loss: 28.5909\n",
      "Processing batch 6337/11884 - Loss: 29.7473\n",
      "Processing batch 6338/11884 - Loss: 30.1922\n",
      "Processing batch 6339/11884 - Loss: 30.7551\n",
      "Processing batch 6340/11884 - Loss: 31.9857\n",
      "Processing batch 6341/11884 - Loss: 29.8743\n",
      "Processing batch 6342/11884 - Loss: 29.5614\n",
      "Processing batch 6343/11884 - Loss: 30.1119\n",
      "Processing batch 6344/11884 - Loss: 29.5411\n",
      "Processing batch 6345/11884 - Loss: 31.8378\n",
      "Processing batch 6346/11884 - Loss: 30.2338\n",
      "Processing batch 6347/11884 - Loss: 29.4698\n",
      "Processing batch 6348/11884 - Loss: 29.6901\n",
      "Processing batch 6349/11884 - Loss: 30.1610\n",
      "Processing batch 6350/11884 - Loss: 30.7970\n",
      "Processing batch 6351/11884 - Loss: 30.0914\n",
      "Processing batch 6352/11884 - Loss: 28.8290\n",
      "Processing batch 6353/11884 - Loss: 29.8879\n",
      "Processing batch 6354/11884 - Loss: 30.0532\n",
      "Processing batch 6355/11884 - Loss: 29.0380\n",
      "Processing batch 6356/11884 - Loss: 31.1724\n",
      "Processing batch 6357/11884 - Loss: 29.8335\n",
      "Processing batch 6358/11884 - Loss: 28.6184\n",
      "Processing batch 6359/11884 - Loss: 30.8307\n",
      "Processing batch 6360/11884 - Loss: 30.0550\n",
      "Processing batch 6361/11884 - Loss: 30.4846\n",
      "Processing batch 6362/11884 - Loss: 30.2865\n",
      "Processing batch 6363/11884 - Loss: 28.3600\n",
      "Processing batch 6364/11884 - Loss: 31.8811\n",
      "Processing batch 6365/11884 - Loss: 29.2046\n",
      "Processing batch 6366/11884 - Loss: 29.7255\n",
      "Processing batch 6367/11884 - Loss: 30.9080\n",
      "Processing batch 6368/11884 - Loss: 31.1239\n",
      "Processing batch 6369/11884 - Loss: 32.1973\n",
      "Processing batch 6370/11884 - Loss: 30.1493\n",
      "Processing batch 6371/11884 - Loss: 29.9236\n",
      "Processing batch 6372/11884 - Loss: 29.4608\n",
      "Processing batch 6373/11884 - Loss: 28.6877\n",
      "Processing batch 6374/11884 - Loss: 31.0157\n",
      "Processing batch 6375/11884 - Loss: 29.6215\n",
      "Processing batch 6376/11884 - Loss: 30.7963\n",
      "Processing batch 6377/11884 - Loss: 30.2054\n",
      "Processing batch 6378/11884 - Loss: 30.0792\n",
      "Processing batch 6379/11884 - Loss: 29.5917\n",
      "Processing batch 6380/11884 - Loss: 29.6387\n",
      "Processing batch 6381/11884 - Loss: 30.0337\n",
      "Processing batch 6382/11884 - Loss: 30.0120\n",
      "Processing batch 6383/11884 - Loss: 31.3587\n",
      "Processing batch 6384/11884 - Loss: 29.9763\n",
      "Processing batch 6385/11884 - Loss: 30.2848\n",
      "Processing batch 6386/11884 - Loss: 30.6905\n",
      "Processing batch 6387/11884 - Loss: 28.9184\n",
      "Processing batch 6388/11884 - Loss: 30.5908\n",
      "Processing batch 6389/11884 - Loss: 28.9400\n",
      "Processing batch 6390/11884 - Loss: 30.4887\n",
      "Processing batch 6391/11884 - Loss: 31.3766\n",
      "Processing batch 6392/11884 - Loss: 30.2474\n",
      "Processing batch 6393/11884 - Loss: 30.5372\n",
      "Processing batch 6394/11884 - Loss: 30.4273\n",
      "Processing batch 6395/11884 - Loss: 29.6815\n",
      "Processing batch 6396/11884 - Loss: 30.6099\n",
      "Processing batch 6397/11884 - Loss: 29.7463\n",
      "Processing batch 6398/11884 - Loss: 28.0165\n",
      "Processing batch 6399/11884 - Loss: 31.5804\n",
      "Processing batch 6400/11884 - Loss: 29.7114\n",
      "Processing batch 6401/11884 - Loss: 29.0007\n",
      "Processing batch 6402/11884 - Loss: 30.1578\n",
      "Processing batch 6403/11884 - Loss: 29.9601\n",
      "Processing batch 6404/11884 - Loss: 30.7969\n",
      "Processing batch 6405/11884 - Loss: 29.7574\n",
      "Processing batch 6406/11884 - Loss: 31.9845\n",
      "Processing batch 6407/11884 - Loss: 29.1867\n",
      "Processing batch 6408/11884 - Loss: 29.6365\n",
      "Processing batch 6409/11884 - Loss: 31.2439\n",
      "Processing batch 6410/11884 - Loss: 29.5437\n",
      "Processing batch 6411/11884 - Loss: 30.6620\n",
      "Processing batch 6412/11884 - Loss: 29.7962\n",
      "Processing batch 6413/11884 - Loss: 30.0714\n",
      "Processing batch 6414/11884 - Loss: 29.0061\n",
      "Processing batch 6415/11884 - Loss: 31.1607\n",
      "Processing batch 6416/11884 - Loss: 29.5685\n",
      "Processing batch 6417/11884 - Loss: 28.9595\n",
      "Processing batch 6418/11884 - Loss: 29.6249\n",
      "Processing batch 6419/11884 - Loss: 30.3759\n",
      "Processing batch 6420/11884 - Loss: 29.9363\n",
      "Processing batch 6421/11884 - Loss: 30.1739\n",
      "Processing batch 6422/11884 - Loss: 29.3481\n",
      "Processing batch 6423/11884 - Loss: 30.5519\n",
      "Processing batch 6424/11884 - Loss: 29.0683\n",
      "Processing batch 6425/11884 - Loss: 30.4154\n",
      "Processing batch 6426/11884 - Loss: 29.9087\n",
      "Processing batch 6427/11884 - Loss: 29.5722\n",
      "Processing batch 6428/11884 - Loss: 29.2532\n",
      "Processing batch 6429/11884 - Loss: 30.4029\n",
      "Processing batch 6430/11884 - Loss: 30.2676\n",
      "Processing batch 6431/11884 - Loss: 29.7193\n",
      "Processing batch 6432/11884 - Loss: 30.9392\n",
      "Processing batch 6433/11884 - Loss: 31.4237\n",
      "Processing batch 6434/11884 - Loss: 30.1157\n",
      "Processing batch 6435/11884 - Loss: 28.8140\n",
      "Processing batch 6436/11884 - Loss: 31.2372\n",
      "Processing batch 6437/11884 - Loss: 30.5887\n",
      "Processing batch 6438/11884 - Loss: 31.3410\n",
      "Processing batch 6439/11884 - Loss: 30.7974\n",
      "Processing batch 6440/11884 - Loss: 30.1586\n",
      "Processing batch 6441/11884 - Loss: 30.5730\n",
      "Processing batch 6442/11884 - Loss: 30.4909\n",
      "Processing batch 6443/11884 - Loss: 28.7598\n",
      "Processing batch 6444/11884 - Loss: 30.4483\n",
      "Processing batch 6445/11884 - Loss: 29.9361\n",
      "Processing batch 6446/11884 - Loss: 30.8307\n",
      "Processing batch 6447/11884 - Loss: 29.8540\n",
      "Processing batch 6448/11884 - Loss: 29.2838\n",
      "Processing batch 6449/11884 - Loss: 29.8395\n",
      "Processing batch 6450/11884 - Loss: 30.6664\n",
      "Processing batch 6451/11884 - Loss: 29.9198\n",
      "Processing batch 6452/11884 - Loss: 30.0399\n",
      "Processing batch 6453/11884 - Loss: 31.2034\n",
      "Processing batch 6454/11884 - Loss: 30.5600\n",
      "Processing batch 6455/11884 - Loss: 29.5153\n",
      "Processing batch 6456/11884 - Loss: 29.4254\n",
      "Processing batch 6457/11884 - Loss: 30.6827\n",
      "Processing batch 6458/11884 - Loss: 30.5182\n",
      "Processing batch 6459/11884 - Loss: 30.4684\n",
      "Processing batch 6460/11884 - Loss: 29.4364\n",
      "Processing batch 6461/11884 - Loss: 30.5215\n",
      "Processing batch 6462/11884 - Loss: 31.1199\n",
      "Processing batch 6463/11884 - Loss: 30.1164\n",
      "Processing batch 6464/11884 - Loss: 30.9633\n",
      "Processing batch 6465/11884 - Loss: 29.6550\n",
      "Processing batch 6466/11884 - Loss: 30.4966\n",
      "Processing batch 6467/11884 - Loss: 29.9086\n",
      "Processing batch 6468/11884 - Loss: 31.4655\n",
      "Processing batch 6469/11884 - Loss: 30.2487\n",
      "Processing batch 6470/11884 - Loss: 28.9612\n",
      "Processing batch 6471/11884 - Loss: 30.0643\n",
      "Processing batch 6472/11884 - Loss: 31.6713\n",
      "Processing batch 6473/11884 - Loss: 30.6269\n",
      "Processing batch 6474/11884 - Loss: 30.4662\n",
      "Processing batch 6475/11884 - Loss: 29.3495\n",
      "Processing batch 6476/11884 - Loss: 32.3966\n",
      "Processing batch 6477/11884 - Loss: 30.7321\n",
      "Processing batch 6478/11884 - Loss: 28.8550\n",
      "Processing batch 6479/11884 - Loss: 29.5227\n",
      "Processing batch 6480/11884 - Loss: 29.6999\n",
      "Processing batch 6481/11884 - Loss: 29.3628\n",
      "Processing batch 6482/11884 - Loss: 29.6939\n",
      "Processing batch 6483/11884 - Loss: 28.9433\n",
      "Processing batch 6484/11884 - Loss: 31.2367\n",
      "Processing batch 6485/11884 - Loss: 30.1389\n",
      "Processing batch 6486/11884 - Loss: 29.8771\n",
      "Processing batch 6487/11884 - Loss: 29.7516\n",
      "Processing batch 6488/11884 - Loss: 29.8378\n",
      "Processing batch 6489/11884 - Loss: 30.8865\n",
      "Processing batch 6490/11884 - Loss: 29.8886\n",
      "Processing batch 6491/11884 - Loss: 30.9745\n",
      "Processing batch 6492/11884 - Loss: 28.3563\n",
      "Processing batch 6493/11884 - Loss: 30.6560\n",
      "Processing batch 6494/11884 - Loss: 29.0517\n",
      "Processing batch 6495/11884 - Loss: 29.7726\n",
      "Processing batch 6496/11884 - Loss: 30.5040\n",
      "Processing batch 6497/11884 - Loss: 30.7102\n",
      "Processing batch 6498/11884 - Loss: 30.9671\n",
      "Processing batch 6499/11884 - Loss: 30.1687\n",
      "Processing batch 6500/11884 - Loss: 28.7820\n",
      "Processing batch 6501/11884 - Loss: 28.8264\n",
      "Processing batch 6502/11884 - Loss: 29.7461\n",
      "Processing batch 6503/11884 - Loss: 30.4068\n",
      "Processing batch 6504/11884 - Loss: 29.1635\n",
      "Processing batch 6505/11884 - Loss: 29.8465\n",
      "Processing batch 6506/11884 - Loss: 29.8420\n",
      "Processing batch 6507/11884 - Loss: 30.9935\n",
      "Processing batch 6508/11884 - Loss: 30.0513\n",
      "Processing batch 6509/11884 - Loss: 30.9615\n",
      "Processing batch 6510/11884 - Loss: 29.8159\n",
      "Processing batch 6511/11884 - Loss: 30.5247\n",
      "Processing batch 6512/11884 - Loss: 29.8150\n",
      "Processing batch 6513/11884 - Loss: 29.2055\n",
      "Processing batch 6514/11884 - Loss: 29.6748\n",
      "Processing batch 6515/11884 - Loss: 31.2808\n",
      "Processing batch 6516/11884 - Loss: 29.8929\n",
      "Processing batch 6517/11884 - Loss: 30.2973\n",
      "Processing batch 6518/11884 - Loss: 31.5112\n",
      "Processing batch 6519/11884 - Loss: 29.5114\n",
      "Processing batch 6520/11884 - Loss: 30.3965\n",
      "Processing batch 6521/11884 - Loss: 30.0183\n",
      "Processing batch 6522/11884 - Loss: 29.9650\n",
      "Processing batch 6523/11884 - Loss: 29.5659\n",
      "Processing batch 6524/11884 - Loss: 30.4807\n",
      "Processing batch 6525/11884 - Loss: 30.8610\n",
      "Processing batch 6526/11884 - Loss: 29.4323\n",
      "Processing batch 6527/11884 - Loss: 30.7355\n",
      "Processing batch 6528/11884 - Loss: 29.9313\n",
      "Processing batch 6529/11884 - Loss: 29.9077\n",
      "Processing batch 6530/11884 - Loss: 30.0889\n",
      "Processing batch 6531/11884 - Loss: 29.0871\n",
      "Processing batch 6532/11884 - Loss: 31.6250\n",
      "Processing batch 6533/11884 - Loss: 29.1756\n",
      "Processing batch 6534/11884 - Loss: 30.4397\n",
      "Processing batch 6535/11884 - Loss: 30.7112\n",
      "Processing batch 6536/11884 - Loss: 30.3279\n",
      "Processing batch 6537/11884 - Loss: 29.9090\n",
      "Processing batch 6538/11884 - Loss: 31.4916\n",
      "Processing batch 6539/11884 - Loss: 30.3534\n",
      "Processing batch 6540/11884 - Loss: 31.1268\n",
      "Processing batch 6541/11884 - Loss: 28.8300\n",
      "Processing batch 6542/11884 - Loss: 29.5650\n",
      "Processing batch 6543/11884 - Loss: 29.4594\n",
      "Processing batch 6544/11884 - Loss: 31.2118\n",
      "Processing batch 6545/11884 - Loss: 29.5082\n",
      "Processing batch 6546/11884 - Loss: 29.7535\n",
      "Processing batch 6547/11884 - Loss: 29.9940\n",
      "Processing batch 6548/11884 - Loss: 30.7219\n",
      "Processing batch 6549/11884 - Loss: 31.1357\n",
      "Processing batch 6550/11884 - Loss: 29.9059\n",
      "Processing batch 6551/11884 - Loss: 30.0282\n",
      "Processing batch 6552/11884 - Loss: 30.9354\n",
      "Processing batch 6553/11884 - Loss: 31.2967\n",
      "Processing batch 6554/11884 - Loss: 30.6521\n",
      "Processing batch 6555/11884 - Loss: 30.5328\n",
      "Processing batch 6556/11884 - Loss: 30.2812\n",
      "Processing batch 6557/11884 - Loss: 30.5073\n",
      "Processing batch 6558/11884 - Loss: 30.5580\n",
      "Processing batch 6559/11884 - Loss: 30.0389\n",
      "Processing batch 6560/11884 - Loss: 30.5225\n",
      "Processing batch 6561/11884 - Loss: 29.5920\n",
      "Processing batch 6562/11884 - Loss: 30.0734\n",
      "Processing batch 6563/11884 - Loss: 29.9255\n",
      "Processing batch 6564/11884 - Loss: 30.0449\n",
      "Processing batch 6565/11884 - Loss: 30.7177\n",
      "Processing batch 6566/11884 - Loss: 29.7765\n",
      "Processing batch 6567/11884 - Loss: 29.6205\n",
      "Processing batch 6568/11884 - Loss: 29.1672\n",
      "Processing batch 6569/11884 - Loss: 30.2084\n",
      "Processing batch 6570/11884 - Loss: 29.7232\n",
      "Processing batch 6571/11884 - Loss: 30.4694\n",
      "Processing batch 6572/11884 - Loss: 30.3755\n",
      "Processing batch 6573/11884 - Loss: 29.8392\n",
      "Processing batch 6574/11884 - Loss: 30.6875\n",
      "Processing batch 6575/11884 - Loss: 29.8801\n",
      "Processing batch 6576/11884 - Loss: 30.7493\n",
      "Processing batch 6577/11884 - Loss: 29.9866\n",
      "Processing batch 6578/11884 - Loss: 28.7334\n",
      "Processing batch 6579/11884 - Loss: 29.3871\n",
      "Processing batch 6580/11884 - Loss: 30.3395\n",
      "Processing batch 6581/11884 - Loss: 28.3484\n",
      "Processing batch 6582/11884 - Loss: 29.9581\n",
      "Processing batch 6583/11884 - Loss: 30.4150\n",
      "Processing batch 6584/11884 - Loss: 30.5167\n",
      "Processing batch 6585/11884 - Loss: 29.3751\n",
      "Processing batch 6586/11884 - Loss: 31.0963\n",
      "Processing batch 6587/11884 - Loss: 30.4973\n",
      "Processing batch 6588/11884 - Loss: 30.3501\n",
      "Processing batch 6589/11884 - Loss: 29.6999\n",
      "Processing batch 6590/11884 - Loss: 30.0618\n",
      "Processing batch 6591/11884 - Loss: 30.3746\n",
      "Processing batch 6592/11884 - Loss: 30.6907\n",
      "Processing batch 6593/11884 - Loss: 31.3054\n",
      "Processing batch 6594/11884 - Loss: 31.5124\n",
      "Processing batch 6595/11884 - Loss: 30.0793\n",
      "Processing batch 6596/11884 - Loss: 29.1380\n",
      "Processing batch 6597/11884 - Loss: 30.4641\n",
      "Processing batch 6598/11884 - Loss: 30.8909\n",
      "Processing batch 6599/11884 - Loss: 30.5870\n",
      "Processing batch 6600/11884 - Loss: 31.2499\n",
      "Processing batch 6601/11884 - Loss: 30.3625\n",
      "Processing batch 6602/11884 - Loss: 31.3786\n",
      "Processing batch 6603/11884 - Loss: 30.6962\n",
      "Processing batch 6604/11884 - Loss: 30.3656\n",
      "Processing batch 6605/11884 - Loss: 28.2690\n",
      "Processing batch 6606/11884 - Loss: 31.0410\n",
      "Processing batch 6607/11884 - Loss: 29.9045\n",
      "Processing batch 6608/11884 - Loss: 30.9924\n",
      "Processing batch 6609/11884 - Loss: 29.0525\n",
      "Processing batch 6610/11884 - Loss: 29.4074\n",
      "Processing batch 6611/11884 - Loss: 29.5244\n",
      "Processing batch 6612/11884 - Loss: 30.5819\n",
      "Processing batch 6613/11884 - Loss: 29.7511\n",
      "Processing batch 6614/11884 - Loss: 30.1066\n",
      "Processing batch 6615/11884 - Loss: 29.2130\n",
      "Processing batch 6616/11884 - Loss: 31.0885\n",
      "Processing batch 6617/11884 - Loss: 28.2590\n",
      "Processing batch 6618/11884 - Loss: 31.3295\n",
      "Processing batch 6619/11884 - Loss: 30.2362\n",
      "Processing batch 6620/11884 - Loss: 31.2451\n",
      "Processing batch 6621/11884 - Loss: 30.5520\n",
      "Processing batch 6622/11884 - Loss: 29.2190\n",
      "Processing batch 6623/11884 - Loss: 30.3097\n",
      "Processing batch 6624/11884 - Loss: 29.5831\n",
      "Processing batch 6625/11884 - Loss: 30.6126\n",
      "Processing batch 6626/11884 - Loss: 30.2571\n",
      "Processing batch 6627/11884 - Loss: 29.8035\n",
      "Processing batch 6628/11884 - Loss: 30.7168\n",
      "Processing batch 6629/11884 - Loss: 30.1032\n",
      "Processing batch 6630/11884 - Loss: 29.2723\n",
      "Processing batch 6631/11884 - Loss: 30.6726\n",
      "Processing batch 6632/11884 - Loss: 29.3229\n",
      "Processing batch 6633/11884 - Loss: 28.8499\n",
      "Processing batch 6634/11884 - Loss: 29.9072\n",
      "Processing batch 6635/11884 - Loss: 30.3457\n",
      "Processing batch 6636/11884 - Loss: 30.8834\n",
      "Processing batch 6637/11884 - Loss: 28.6351\n",
      "Processing batch 6638/11884 - Loss: 31.0841\n",
      "Processing batch 6639/11884 - Loss: 30.5877\n",
      "Processing batch 6640/11884 - Loss: 29.9376\n",
      "Processing batch 6641/11884 - Loss: 31.0219\n",
      "Processing batch 6642/11884 - Loss: 31.1851\n",
      "Processing batch 6643/11884 - Loss: 29.0148\n",
      "Processing batch 6644/11884 - Loss: 30.6682\n",
      "Processing batch 6645/11884 - Loss: 30.1413\n",
      "Processing batch 6646/11884 - Loss: 29.9635\n",
      "Processing batch 6647/11884 - Loss: 29.4097\n",
      "Processing batch 6648/11884 - Loss: 29.6152\n",
      "Processing batch 6649/11884 - Loss: 30.0196\n",
      "Processing batch 6650/11884 - Loss: 30.8761\n",
      "Processing batch 6651/11884 - Loss: 29.8373\n",
      "Processing batch 6652/11884 - Loss: 29.2997\n",
      "Processing batch 6653/11884 - Loss: 30.6781\n",
      "Processing batch 6654/11884 - Loss: 30.3700\n",
      "Processing batch 6655/11884 - Loss: 30.9005\n",
      "Processing batch 6656/11884 - Loss: 30.7466\n",
      "Processing batch 6657/11884 - Loss: 30.3821\n",
      "Processing batch 6658/11884 - Loss: 31.0035\n",
      "Processing batch 6659/11884 - Loss: 31.0342\n",
      "Processing batch 6660/11884 - Loss: 29.1942\n",
      "Processing batch 6661/11884 - Loss: 30.0984\n",
      "Processing batch 6662/11884 - Loss: 29.8353\n",
      "Processing batch 6663/11884 - Loss: 29.7835\n",
      "Processing batch 6664/11884 - Loss: 29.8647\n",
      "Processing batch 6665/11884 - Loss: 29.4726\n",
      "Processing batch 6666/11884 - Loss: 28.9718\n",
      "Processing batch 6667/11884 - Loss: 30.4679\n",
      "Processing batch 6668/11884 - Loss: 31.8995\n",
      "Processing batch 6669/11884 - Loss: 30.8782\n",
      "Processing batch 6670/11884 - Loss: 29.6999\n",
      "Processing batch 6671/11884 - Loss: 31.3462\n",
      "Processing batch 6672/11884 - Loss: 29.6694\n",
      "Processing batch 6673/11884 - Loss: 28.2315\n",
      "Processing batch 6674/11884 - Loss: 29.1331\n",
      "Processing batch 6675/11884 - Loss: 30.3657\n",
      "Processing batch 6676/11884 - Loss: 30.8069\n",
      "Processing batch 6677/11884 - Loss: 29.6815\n",
      "Processing batch 6678/11884 - Loss: 29.2493\n",
      "Processing batch 6679/11884 - Loss: 30.2558\n",
      "Processing batch 6680/11884 - Loss: 30.7222\n",
      "Processing batch 6681/11884 - Loss: 29.5670\n",
      "Processing batch 6682/11884 - Loss: 29.5863\n",
      "Processing batch 6683/11884 - Loss: 29.6863\n",
      "Processing batch 6684/11884 - Loss: 31.2801\n",
      "Processing batch 6685/11884 - Loss: 30.4080\n",
      "Processing batch 6686/11884 - Loss: 30.1093\n",
      "Processing batch 6687/11884 - Loss: 30.2480\n",
      "Processing batch 6688/11884 - Loss: 30.2153\n",
      "Processing batch 6689/11884 - Loss: 31.0386\n",
      "Processing batch 6690/11884 - Loss: 31.2004\n",
      "Processing batch 6691/11884 - Loss: 30.0519\n",
      "Processing batch 6692/11884 - Loss: 28.9549\n",
      "Processing batch 6693/11884 - Loss: 31.6803\n",
      "Processing batch 6694/11884 - Loss: 30.0499\n",
      "Processing batch 6695/11884 - Loss: 28.6362\n",
      "Processing batch 6696/11884 - Loss: 29.9289\n",
      "Processing batch 6697/11884 - Loss: 31.2448\n",
      "Processing batch 6698/11884 - Loss: 29.7664\n",
      "Processing batch 6699/11884 - Loss: 30.7107\n",
      "Processing batch 6700/11884 - Loss: 31.5245\n",
      "Processing batch 6701/11884 - Loss: 30.4014\n",
      "Processing batch 6702/11884 - Loss: 29.9227\n",
      "Processing batch 6703/11884 - Loss: 30.2668\n",
      "Processing batch 6704/11884 - Loss: 29.4074\n",
      "Processing batch 6705/11884 - Loss: 30.1679\n",
      "Processing batch 6706/11884 - Loss: 30.6026\n",
      "Processing batch 6707/11884 - Loss: 30.2297\n",
      "Processing batch 6708/11884 - Loss: 31.2534\n",
      "Processing batch 6709/11884 - Loss: 30.2714\n",
      "Processing batch 6710/11884 - Loss: 30.4579\n",
      "Processing batch 6711/11884 - Loss: 30.7912\n",
      "Processing batch 6712/11884 - Loss: 29.6205\n",
      "Processing batch 6713/11884 - Loss: 30.9261\n",
      "Processing batch 6714/11884 - Loss: 30.8456\n",
      "Processing batch 6715/11884 - Loss: 29.7459\n",
      "Processing batch 6716/11884 - Loss: 29.7198\n",
      "Processing batch 6717/11884 - Loss: 30.8096\n",
      "Processing batch 6718/11884 - Loss: 28.3724\n",
      "Processing batch 6719/11884 - Loss: 30.1341\n",
      "Processing batch 6720/11884 - Loss: 29.7868\n",
      "Processing batch 6721/11884 - Loss: 30.0218\n",
      "Processing batch 6722/11884 - Loss: 31.8003\n",
      "Processing batch 6723/11884 - Loss: 29.8992\n",
      "Processing batch 6724/11884 - Loss: 30.5923\n",
      "Processing batch 6725/11884 - Loss: 29.8251\n",
      "Processing batch 6726/11884 - Loss: 30.1567\n",
      "Processing batch 6727/11884 - Loss: 31.0135\n",
      "Processing batch 6728/11884 - Loss: 31.1795\n",
      "Processing batch 6729/11884 - Loss: 31.9959\n",
      "Processing batch 6730/11884 - Loss: 30.2648\n",
      "Processing batch 6731/11884 - Loss: 30.3190\n",
      "Processing batch 6732/11884 - Loss: 30.2557\n",
      "Processing batch 6733/11884 - Loss: 28.5773\n",
      "Processing batch 6734/11884 - Loss: 30.2279\n",
      "Processing batch 6735/11884 - Loss: 31.8906\n",
      "Processing batch 6736/11884 - Loss: 28.2814\n",
      "Processing batch 6737/11884 - Loss: 29.7150\n",
      "Processing batch 6738/11884 - Loss: 29.8706\n",
      "Processing batch 6739/11884 - Loss: 28.7612\n",
      "Processing batch 6740/11884 - Loss: 30.7802\n",
      "Processing batch 6741/11884 - Loss: 30.9402\n",
      "Processing batch 6742/11884 - Loss: 30.0874\n",
      "Processing batch 6743/11884 - Loss: 30.3910\n",
      "Processing batch 6744/11884 - Loss: 29.0192\n",
      "Processing batch 6745/11884 - Loss: 30.4101\n",
      "Processing batch 6746/11884 - Loss: 29.9958\n",
      "Processing batch 6747/11884 - Loss: 30.0028\n",
      "Processing batch 6748/11884 - Loss: 30.4266\n",
      "Processing batch 6749/11884 - Loss: 29.8704\n",
      "Processing batch 6750/11884 - Loss: 30.0290\n",
      "Processing batch 6751/11884 - Loss: 29.5233\n",
      "Processing batch 6752/11884 - Loss: 28.3392\n",
      "Processing batch 6753/11884 - Loss: 30.5117\n",
      "Processing batch 6754/11884 - Loss: 29.3405\n",
      "Processing batch 6755/11884 - Loss: 30.4521\n",
      "Processing batch 6756/11884 - Loss: 30.1997\n",
      "Processing batch 6757/11884 - Loss: 29.7044\n",
      "Processing batch 6758/11884 - Loss: 30.0998\n",
      "Processing batch 6759/11884 - Loss: 29.8075\n",
      "Processing batch 6760/11884 - Loss: 29.9524\n",
      "Processing batch 6761/11884 - Loss: 29.5630\n",
      "Processing batch 6762/11884 - Loss: 28.8557\n",
      "Processing batch 6763/11884 - Loss: 30.2063\n",
      "Processing batch 6764/11884 - Loss: 30.7406\n",
      "Processing batch 6765/11884 - Loss: 30.8702\n",
      "Processing batch 6766/11884 - Loss: 31.1917\n",
      "Processing batch 6767/11884 - Loss: 30.5088\n",
      "Processing batch 6768/11884 - Loss: 28.8568\n",
      "Processing batch 6769/11884 - Loss: 29.3126\n",
      "Processing batch 6770/11884 - Loss: 30.4476\n",
      "Processing batch 6771/11884 - Loss: 30.0977\n",
      "Processing batch 6772/11884 - Loss: 29.2869\n",
      "Processing batch 6773/11884 - Loss: 29.6374\n",
      "Processing batch 6774/11884 - Loss: 29.3883\n",
      "Processing batch 6775/11884 - Loss: 29.5722\n",
      "Processing batch 6776/11884 - Loss: 30.4331\n",
      "Processing batch 6777/11884 - Loss: 29.9259\n",
      "Processing batch 6778/11884 - Loss: 31.3698\n",
      "Processing batch 6779/11884 - Loss: 30.8823\n",
      "Processing batch 6780/11884 - Loss: 31.0892\n",
      "Processing batch 6781/11884 - Loss: 28.9402\n",
      "Processing batch 6782/11884 - Loss: 29.2563\n",
      "Processing batch 6783/11884 - Loss: 30.5042\n",
      "Processing batch 6784/11884 - Loss: 30.3642\n",
      "Processing batch 6785/11884 - Loss: 28.9249\n",
      "Processing batch 6786/11884 - Loss: 29.6788\n",
      "Processing batch 6787/11884 - Loss: 30.7020\n",
      "Processing batch 6788/11884 - Loss: 29.3555\n",
      "Processing batch 6789/11884 - Loss: 29.6293\n",
      "Processing batch 6790/11884 - Loss: 30.1841\n",
      "Processing batch 6791/11884 - Loss: 30.5599\n",
      "Processing batch 6792/11884 - Loss: 28.8236\n",
      "Processing batch 6793/11884 - Loss: 29.8691\n",
      "Processing batch 6794/11884 - Loss: 30.1693\n",
      "Processing batch 6795/11884 - Loss: 30.0401\n",
      "Processing batch 6796/11884 - Loss: 30.3399\n",
      "Processing batch 6797/11884 - Loss: 31.1834\n",
      "Processing batch 6798/11884 - Loss: 30.1609\n",
      "Processing batch 6799/11884 - Loss: 30.7750\n",
      "Processing batch 6800/11884 - Loss: 31.4105\n",
      "Processing batch 6801/11884 - Loss: 30.5117\n",
      "Processing batch 6802/11884 - Loss: 29.7084\n",
      "Processing batch 6803/11884 - Loss: 29.6943\n",
      "Processing batch 6804/11884 - Loss: 31.0771\n",
      "Processing batch 6805/11884 - Loss: 29.6447\n",
      "Processing batch 6806/11884 - Loss: 30.5592\n",
      "Processing batch 6807/11884 - Loss: 30.0930\n",
      "Processing batch 6808/11884 - Loss: 31.2806\n",
      "Processing batch 6809/11884 - Loss: 30.7333\n",
      "Processing batch 6810/11884 - Loss: 28.3004\n",
      "Processing batch 6811/11884 - Loss: 30.4135\n",
      "Processing batch 6812/11884 - Loss: 29.8114\n",
      "Processing batch 6813/11884 - Loss: 29.8664\n",
      "Processing batch 6814/11884 - Loss: 28.8803\n",
      "Processing batch 6815/11884 - Loss: 30.4178\n",
      "Processing batch 6816/11884 - Loss: 30.5719\n",
      "Processing batch 6817/11884 - Loss: 29.8983\n",
      "Processing batch 6818/11884 - Loss: 30.9384\n",
      "Processing batch 6819/11884 - Loss: 30.6095\n",
      "Processing batch 6820/11884 - Loss: 30.6835\n",
      "Processing batch 6821/11884 - Loss: 29.0373\n",
      "Processing batch 6822/11884 - Loss: 30.0498\n",
      "Processing batch 6823/11884 - Loss: 30.2271\n",
      "Processing batch 6824/11884 - Loss: 29.8966\n",
      "Processing batch 6825/11884 - Loss: 29.5981\n",
      "Processing batch 6826/11884 - Loss: 30.1799\n",
      "Processing batch 6827/11884 - Loss: 29.9539\n",
      "Processing batch 6828/11884 - Loss: 29.5165\n",
      "Processing batch 6829/11884 - Loss: 29.6028\n",
      "Processing batch 6830/11884 - Loss: 29.8876\n",
      "Processing batch 6831/11884 - Loss: 30.4255\n",
      "Processing batch 6832/11884 - Loss: 30.1319\n",
      "Processing batch 6833/11884 - Loss: 29.3013\n",
      "Processing batch 6834/11884 - Loss: 29.9674\n",
      "Processing batch 6835/11884 - Loss: 28.6165\n",
      "Processing batch 6836/11884 - Loss: 28.7794\n",
      "Processing batch 6837/11884 - Loss: 29.1684\n",
      "Processing batch 6838/11884 - Loss: 28.9445\n",
      "Processing batch 6839/11884 - Loss: 29.2144\n",
      "Processing batch 6840/11884 - Loss: 29.6675\n",
      "Processing batch 6841/11884 - Loss: 30.2673\n",
      "Processing batch 6842/11884 - Loss: 31.0750\n",
      "Processing batch 6843/11884 - Loss: 31.0009\n",
      "Processing batch 6844/11884 - Loss: 29.2843\n",
      "Processing batch 6845/11884 - Loss: 31.2882\n",
      "Processing batch 6846/11884 - Loss: 29.2377\n",
      "Processing batch 6847/11884 - Loss: 30.5144\n",
      "Processing batch 6848/11884 - Loss: 29.4812\n",
      "Processing batch 6849/11884 - Loss: 29.8087\n",
      "Processing batch 6850/11884 - Loss: 29.0160\n",
      "Processing batch 6851/11884 - Loss: 29.0073\n",
      "Processing batch 6852/11884 - Loss: 30.0021\n",
      "Processing batch 6853/11884 - Loss: 30.4527\n",
      "Processing batch 6854/11884 - Loss: 31.5375\n",
      "Processing batch 6855/11884 - Loss: 30.0973\n",
      "Processing batch 6856/11884 - Loss: 28.9179\n",
      "Processing batch 6857/11884 - Loss: 29.6146\n",
      "Processing batch 6858/11884 - Loss: 30.8639\n",
      "Processing batch 6859/11884 - Loss: 30.9771\n",
      "Processing batch 6860/11884 - Loss: 29.8658\n",
      "Processing batch 6861/11884 - Loss: 30.6347\n",
      "Processing batch 6862/11884 - Loss: 30.4344\n",
      "Processing batch 6863/11884 - Loss: 28.7716\n",
      "Processing batch 6864/11884 - Loss: 29.2542\n",
      "Processing batch 6865/11884 - Loss: 29.8625\n",
      "Processing batch 6866/11884 - Loss: 29.8669\n",
      "Processing batch 6867/11884 - Loss: 29.8414\n",
      "Processing batch 6868/11884 - Loss: 28.2346\n",
      "Processing batch 6869/11884 - Loss: 28.8755\n",
      "Processing batch 6870/11884 - Loss: 29.4503\n",
      "Processing batch 6871/11884 - Loss: 29.0177\n",
      "Processing batch 6872/11884 - Loss: 30.8239\n",
      "Processing batch 6873/11884 - Loss: 31.3742\n",
      "Processing batch 6874/11884 - Loss: 29.9440\n",
      "Processing batch 6875/11884 - Loss: 30.6978\n",
      "Processing batch 6876/11884 - Loss: 30.4552\n",
      "Processing batch 6877/11884 - Loss: 30.1786\n",
      "Processing batch 6878/11884 - Loss: 29.7496\n",
      "Processing batch 6879/11884 - Loss: 29.7982\n",
      "Processing batch 6880/11884 - Loss: 29.6757\n",
      "Processing batch 6881/11884 - Loss: 30.4645\n",
      "Processing batch 6882/11884 - Loss: 30.2680\n",
      "Processing batch 6883/11884 - Loss: 30.8498\n",
      "Processing batch 6884/11884 - Loss: 30.3606\n",
      "Processing batch 6885/11884 - Loss: 31.0324\n",
      "Processing batch 6886/11884 - Loss: 29.8171\n",
      "Processing batch 6887/11884 - Loss: 29.2650\n",
      "Processing batch 6888/11884 - Loss: 30.3695\n",
      "Processing batch 6889/11884 - Loss: 30.0788\n",
      "Processing batch 6890/11884 - Loss: 30.5883\n",
      "Processing batch 6891/11884 - Loss: 29.5739\n",
      "Processing batch 6892/11884 - Loss: 30.4787\n",
      "Processing batch 6893/11884 - Loss: 31.0683\n",
      "Processing batch 6894/11884 - Loss: 31.5106\n",
      "Processing batch 6895/11884 - Loss: 30.1775\n",
      "Processing batch 6896/11884 - Loss: 29.2493\n",
      "Processing batch 6897/11884 - Loss: 31.7016\n",
      "Processing batch 6898/11884 - Loss: 31.0453\n",
      "Processing batch 6899/11884 - Loss: 29.7412\n",
      "Processing batch 6900/11884 - Loss: 28.5574\n",
      "Processing batch 6901/11884 - Loss: 30.3510\n",
      "Processing batch 6902/11884 - Loss: 32.6496\n",
      "Processing batch 6903/11884 - Loss: 29.8138\n",
      "Processing batch 6904/11884 - Loss: 31.5934\n",
      "Processing batch 6905/11884 - Loss: 30.2026\n",
      "Processing batch 6906/11884 - Loss: 29.0256\n",
      "Processing batch 6907/11884 - Loss: 30.1836\n",
      "Processing batch 6908/11884 - Loss: 28.7490\n",
      "Processing batch 6909/11884 - Loss: 31.3311\n",
      "Processing batch 6910/11884 - Loss: 30.6832\n",
      "Processing batch 6911/11884 - Loss: 29.9744\n",
      "Processing batch 6912/11884 - Loss: 29.7409\n",
      "Processing batch 6913/11884 - Loss: 30.4022\n",
      "Processing batch 6914/11884 - Loss: 30.4444\n",
      "Processing batch 6915/11884 - Loss: 28.9172\n",
      "Processing batch 6916/11884 - Loss: 30.5542\n",
      "Processing batch 6917/11884 - Loss: 29.4273\n",
      "Processing batch 6918/11884 - Loss: 31.2672\n",
      "Processing batch 6919/11884 - Loss: 29.6350\n",
      "Processing batch 6920/11884 - Loss: 29.4733\n",
      "Processing batch 6921/11884 - Loss: 30.2865\n",
      "Processing batch 6922/11884 - Loss: 30.2899\n",
      "Processing batch 6923/11884 - Loss: 29.3933\n",
      "Processing batch 6924/11884 - Loss: 29.8912\n",
      "Processing batch 6925/11884 - Loss: 29.9770\n",
      "Processing batch 6926/11884 - Loss: 30.4312\n",
      "Processing batch 6927/11884 - Loss: 31.0671\n",
      "Processing batch 6928/11884 - Loss: 29.9660\n",
      "Processing batch 6929/11884 - Loss: 30.7749\n",
      "Processing batch 6930/11884 - Loss: 31.0593\n",
      "Processing batch 6931/11884 - Loss: 30.6427\n",
      "Processing batch 6932/11884 - Loss: 29.3900\n",
      "Processing batch 6933/11884 - Loss: 29.2775\n",
      "Processing batch 6934/11884 - Loss: 29.9878\n",
      "Processing batch 6935/11884 - Loss: 31.3179\n",
      "Processing batch 6936/11884 - Loss: 29.7158\n",
      "Processing batch 6937/11884 - Loss: 30.8476\n",
      "Processing batch 6938/11884 - Loss: 28.5847\n",
      "Processing batch 6939/11884 - Loss: 30.2564\n",
      "Processing batch 6940/11884 - Loss: 30.5971\n",
      "Processing batch 6941/11884 - Loss: 31.1794\n",
      "Processing batch 6942/11884 - Loss: 30.2976\n",
      "Processing batch 6943/11884 - Loss: 30.8395\n",
      "Processing batch 6944/11884 - Loss: 30.0366\n",
      "Processing batch 6945/11884 - Loss: 31.2539\n",
      "Processing batch 6946/11884 - Loss: 29.5681\n",
      "Processing batch 6947/11884 - Loss: 30.8242\n",
      "Processing batch 6948/11884 - Loss: 30.4171\n",
      "Processing batch 6949/11884 - Loss: 29.6131\n",
      "Processing batch 6950/11884 - Loss: 29.9455\n",
      "Processing batch 6951/11884 - Loss: 30.4653\n",
      "Processing batch 6952/11884 - Loss: 30.6001\n",
      "Processing batch 6953/11884 - Loss: 28.4869\n",
      "Processing batch 6954/11884 - Loss: 28.8418\n",
      "Processing batch 6955/11884 - Loss: 29.3676\n",
      "Processing batch 6956/11884 - Loss: 30.5457\n",
      "Processing batch 6957/11884 - Loss: 30.3616\n",
      "Processing batch 6958/11884 - Loss: 30.3878\n",
      "Processing batch 6959/11884 - Loss: 30.7501\n",
      "Processing batch 6960/11884 - Loss: 30.9927\n",
      "Processing batch 6961/11884 - Loss: 28.8994\n",
      "Processing batch 6962/11884 - Loss: 29.5975\n",
      "Processing batch 6963/11884 - Loss: 30.0621\n",
      "Processing batch 6964/11884 - Loss: 28.9709\n",
      "Processing batch 6965/11884 - Loss: 29.2524\n",
      "Processing batch 6966/11884 - Loss: 30.0697\n",
      "Processing batch 6967/11884 - Loss: 31.0277\n",
      "Processing batch 6968/11884 - Loss: 32.1955\n",
      "Processing batch 6969/11884 - Loss: 30.1505\n",
      "Processing batch 6970/11884 - Loss: 30.1519\n",
      "Processing batch 6971/11884 - Loss: 29.4392\n",
      "Processing batch 6972/11884 - Loss: 30.4707\n",
      "Processing batch 6973/11884 - Loss: 30.8762\n",
      "Processing batch 6974/11884 - Loss: 30.6919\n",
      "Processing batch 6975/11884 - Loss: 30.3433\n",
      "Processing batch 6976/11884 - Loss: 29.4476\n",
      "Processing batch 6977/11884 - Loss: 30.2798\n",
      "Processing batch 6978/11884 - Loss: 29.9011\n",
      "Processing batch 6979/11884 - Loss: 29.9801\n",
      "Processing batch 6980/11884 - Loss: 29.1307\n",
      "Processing batch 6981/11884 - Loss: 29.9661\n",
      "Processing batch 6982/11884 - Loss: 29.7942\n",
      "Processing batch 6983/11884 - Loss: 30.0124\n",
      "Processing batch 6984/11884 - Loss: 30.4223\n",
      "Processing batch 6985/11884 - Loss: 29.7707\n",
      "Processing batch 6986/11884 - Loss: 29.7122\n",
      "Processing batch 6987/11884 - Loss: 30.2432\n",
      "Processing batch 6988/11884 - Loss: 29.9926\n",
      "Processing batch 6989/11884 - Loss: 30.3561\n",
      "Processing batch 6990/11884 - Loss: 30.2537\n",
      "Processing batch 6991/11884 - Loss: 30.1354\n",
      "Processing batch 6992/11884 - Loss: 30.8903\n",
      "Processing batch 6993/11884 - Loss: 29.7815\n",
      "Processing batch 6994/11884 - Loss: 31.0250\n",
      "Processing batch 6995/11884 - Loss: 29.4591\n",
      "Processing batch 6996/11884 - Loss: 29.2335\n",
      "Processing batch 6997/11884 - Loss: 29.1794\n",
      "Processing batch 6998/11884 - Loss: 29.3410\n",
      "Processing batch 6999/11884 - Loss: 30.2653\n",
      "Processing batch 7000/11884 - Loss: 29.3473\n",
      "Processing batch 7001/11884 - Loss: 30.9003\n",
      "Processing batch 7002/11884 - Loss: 31.5026\n",
      "Processing batch 7003/11884 - Loss: 31.8230\n",
      "Processing batch 7004/11884 - Loss: 30.8917\n",
      "Processing batch 7005/11884 - Loss: 29.9596\n",
      "Processing batch 7006/11884 - Loss: 30.3572\n",
      "Processing batch 7007/11884 - Loss: 30.1989\n",
      "Processing batch 7008/11884 - Loss: 29.5444\n",
      "Processing batch 7009/11884 - Loss: 29.2253\n",
      "Processing batch 7010/11884 - Loss: 30.2586\n",
      "Processing batch 7011/11884 - Loss: 29.8945\n",
      "Processing batch 7012/11884 - Loss: 29.7407\n",
      "Processing batch 7013/11884 - Loss: 30.0959\n",
      "Processing batch 7014/11884 - Loss: 28.9273\n",
      "Processing batch 7015/11884 - Loss: 30.5542\n",
      "Processing batch 7016/11884 - Loss: 29.6937\n",
      "Processing batch 7017/11884 - Loss: 31.1675\n",
      "Processing batch 7018/11884 - Loss: 30.2580\n",
      "Processing batch 7019/11884 - Loss: 29.1592\n",
      "Processing batch 7020/11884 - Loss: 29.2962\n",
      "Processing batch 7021/11884 - Loss: 29.8169\n",
      "Processing batch 7022/11884 - Loss: 30.1391\n",
      "Processing batch 7023/11884 - Loss: 30.1582\n",
      "Processing batch 7024/11884 - Loss: 29.9979\n",
      "Processing batch 7025/11884 - Loss: 30.9207\n",
      "Processing batch 7026/11884 - Loss: 29.6902\n",
      "Processing batch 7027/11884 - Loss: 29.6762\n",
      "Processing batch 7028/11884 - Loss: 29.7067\n",
      "Processing batch 7029/11884 - Loss: 29.8588\n",
      "Processing batch 7030/11884 - Loss: 30.1268\n",
      "Processing batch 7031/11884 - Loss: 28.8299\n",
      "Processing batch 7032/11884 - Loss: 30.7904\n",
      "Processing batch 7033/11884 - Loss: 29.8986\n",
      "Processing batch 7034/11884 - Loss: 30.6155\n",
      "Processing batch 7035/11884 - Loss: 30.2782\n",
      "Processing batch 7036/11884 - Loss: 29.8097\n",
      "Processing batch 7037/11884 - Loss: 30.8623\n",
      "Processing batch 7038/11884 - Loss: 29.6266\n",
      "Processing batch 7039/11884 - Loss: 30.2000\n",
      "Processing batch 7040/11884 - Loss: 29.5046\n",
      "Processing batch 7041/11884 - Loss: 29.5160\n",
      "Processing batch 7042/11884 - Loss: 30.5753\n",
      "Processing batch 7043/11884 - Loss: 29.8179\n",
      "Processing batch 7044/11884 - Loss: 30.0035\n",
      "Processing batch 7045/11884 - Loss: 30.9163\n",
      "Processing batch 7046/11884 - Loss: 29.7276\n",
      "Processing batch 7047/11884 - Loss: 30.4489\n",
      "Processing batch 7048/11884 - Loss: 29.6013\n",
      "Processing batch 7049/11884 - Loss: 28.6055\n",
      "Processing batch 7050/11884 - Loss: 31.6102\n",
      "Processing batch 7051/11884 - Loss: 30.2528\n",
      "Processing batch 7052/11884 - Loss: 29.8665\n",
      "Processing batch 7053/11884 - Loss: 29.4119\n",
      "Processing batch 7054/11884 - Loss: 30.9109\n",
      "Processing batch 7055/11884 - Loss: 29.8403\n",
      "Processing batch 7056/11884 - Loss: 30.4365\n",
      "Processing batch 7057/11884 - Loss: 30.0436\n",
      "Processing batch 7058/11884 - Loss: 29.4606\n",
      "Processing batch 7059/11884 - Loss: 29.6879\n",
      "Processing batch 7060/11884 - Loss: 30.5600\n",
      "Processing batch 7061/11884 - Loss: 30.1922\n",
      "Processing batch 7062/11884 - Loss: 29.8685\n",
      "Processing batch 7063/11884 - Loss: 30.2266\n",
      "Processing batch 7064/11884 - Loss: 31.0262\n",
      "Processing batch 7065/11884 - Loss: 29.4417\n",
      "Processing batch 7066/11884 - Loss: 30.9965\n",
      "Processing batch 7067/11884 - Loss: 30.6356\n",
      "Processing batch 7068/11884 - Loss: 30.7168\n",
      "Processing batch 7069/11884 - Loss: 30.0803\n",
      "Processing batch 7070/11884 - Loss: 30.5889\n",
      "Processing batch 7071/11884 - Loss: 30.1177\n",
      "Processing batch 7072/11884 - Loss: 29.0699\n",
      "Processing batch 7073/11884 - Loss: 30.1918\n",
      "Processing batch 7074/11884 - Loss: 30.5339\n",
      "Processing batch 7075/11884 - Loss: 29.7410\n",
      "Processing batch 7076/11884 - Loss: 28.7890\n",
      "Processing batch 7077/11884 - Loss: 29.5002\n",
      "Processing batch 7078/11884 - Loss: 29.0442\n",
      "Processing batch 7079/11884 - Loss: 28.4842\n",
      "Processing batch 7080/11884 - Loss: 31.9407\n",
      "Processing batch 7081/11884 - Loss: 30.8460\n",
      "Processing batch 7082/11884 - Loss: 30.7230\n",
      "Processing batch 7083/11884 - Loss: 28.8025\n",
      "Processing batch 7084/11884 - Loss: 29.8085\n",
      "Processing batch 7085/11884 - Loss: 30.0843\n",
      "Processing batch 7086/11884 - Loss: 31.2584\n",
      "Processing batch 7087/11884 - Loss: 30.1604\n",
      "Processing batch 7088/11884 - Loss: 31.1247\n",
      "Processing batch 7089/11884 - Loss: 29.6881\n",
      "Processing batch 7090/11884 - Loss: 30.2078\n",
      "Processing batch 7091/11884 - Loss: 30.3553\n",
      "Processing batch 7092/11884 - Loss: 31.1694\n",
      "Processing batch 7093/11884 - Loss: 29.3548\n",
      "Processing batch 7094/11884 - Loss: 30.0240\n",
      "Processing batch 7095/11884 - Loss: 30.5022\n",
      "Processing batch 7096/11884 - Loss: 29.3511\n",
      "Processing batch 7097/11884 - Loss: 31.1008\n",
      "Processing batch 7098/11884 - Loss: 29.7917\n",
      "Processing batch 7099/11884 - Loss: 29.2795\n",
      "Processing batch 7100/11884 - Loss: 29.5028\n",
      "Processing batch 7101/11884 - Loss: 30.2823\n",
      "Processing batch 7102/11884 - Loss: 29.1984\n",
      "Processing batch 7103/11884 - Loss: 30.0375\n",
      "Processing batch 7104/11884 - Loss: 30.7170\n",
      "Processing batch 7105/11884 - Loss: 29.5424\n",
      "Processing batch 7106/11884 - Loss: 29.2713\n",
      "Processing batch 7107/11884 - Loss: 29.9009\n",
      "Processing batch 7108/11884 - Loss: 30.3947\n",
      "Processing batch 7109/11884 - Loss: 29.4253\n",
      "Processing batch 7110/11884 - Loss: 29.9724\n",
      "Processing batch 7111/11884 - Loss: 29.0887\n",
      "Processing batch 7112/11884 - Loss: 30.2589\n",
      "Processing batch 7113/11884 - Loss: 29.7344\n",
      "Processing batch 7114/11884 - Loss: 30.2953\n",
      "Processing batch 7115/11884 - Loss: 30.0925\n",
      "Processing batch 7116/11884 - Loss: 30.0811\n",
      "Processing batch 7117/11884 - Loss: 28.8921\n",
      "Processing batch 7118/11884 - Loss: 32.6335\n",
      "Processing batch 7119/11884 - Loss: 30.0304\n",
      "Processing batch 7120/11884 - Loss: 30.6896\n",
      "Processing batch 7121/11884 - Loss: 30.8385\n",
      "Processing batch 7122/11884 - Loss: 31.6305\n",
      "Processing batch 7123/11884 - Loss: 28.8518\n",
      "Processing batch 7124/11884 - Loss: 31.6395\n",
      "Processing batch 7125/11884 - Loss: 29.4543\n",
      "Processing batch 7126/11884 - Loss: 30.7862\n",
      "Processing batch 7127/11884 - Loss: 31.2555\n",
      "Processing batch 7128/11884 - Loss: 29.3768\n",
      "Processing batch 7129/11884 - Loss: 30.4764\n",
      "Processing batch 7130/11884 - Loss: 29.9622\n",
      "Processing batch 7131/11884 - Loss: 30.3459\n",
      "Processing batch 7132/11884 - Loss: 30.5051\n",
      "Processing batch 7133/11884 - Loss: 30.0893\n",
      "Processing batch 7134/11884 - Loss: 29.9556\n",
      "Processing batch 7135/11884 - Loss: 28.9706\n",
      "Processing batch 7136/11884 - Loss: 29.9919\n",
      "Processing batch 7137/11884 - Loss: 30.5015\n",
      "Processing batch 7138/11884 - Loss: 30.0219\n",
      "Processing batch 7139/11884 - Loss: 30.6434\n",
      "Processing batch 7140/11884 - Loss: 29.0108\n",
      "Processing batch 7141/11884 - Loss: 30.1642\n",
      "Processing batch 7142/11884 - Loss: 30.4201\n",
      "Processing batch 7143/11884 - Loss: 29.8284\n",
      "Processing batch 7144/11884 - Loss: 29.9742\n",
      "Processing batch 7145/11884 - Loss: 29.6545\n",
      "Processing batch 7146/11884 - Loss: 28.3072\n",
      "Processing batch 7147/11884 - Loss: 30.1852\n",
      "Processing batch 7148/11884 - Loss: 31.0049\n",
      "Processing batch 7149/11884 - Loss: 30.1345\n",
      "Processing batch 7150/11884 - Loss: 30.6245\n",
      "Processing batch 7151/11884 - Loss: 29.4942\n",
      "Processing batch 7152/11884 - Loss: 29.4301\n",
      "Processing batch 7153/11884 - Loss: 29.8325\n",
      "Processing batch 7154/11884 - Loss: 30.5716\n",
      "Processing batch 7155/11884 - Loss: 29.6958\n",
      "Processing batch 7156/11884 - Loss: 31.7788\n",
      "Processing batch 7157/11884 - Loss: 29.9245\n",
      "Processing batch 7158/11884 - Loss: 30.9415\n",
      "Processing batch 7159/11884 - Loss: 29.2214\n",
      "Processing batch 7160/11884 - Loss: 31.2062\n",
      "Processing batch 7161/11884 - Loss: 31.1428\n",
      "Processing batch 7162/11884 - Loss: 30.7578\n",
      "Processing batch 7163/11884 - Loss: 30.1356\n",
      "Processing batch 7164/11884 - Loss: 29.4460\n",
      "Processing batch 7165/11884 - Loss: 30.9290\n",
      "Processing batch 7166/11884 - Loss: 29.9610\n",
      "Processing batch 7167/11884 - Loss: 30.2705\n",
      "Processing batch 7168/11884 - Loss: 29.5343\n",
      "Processing batch 7169/11884 - Loss: 30.8316\n",
      "Processing batch 7170/11884 - Loss: 29.5796\n",
      "Processing batch 7171/11884 - Loss: 30.0446\n",
      "Processing batch 7172/11884 - Loss: 29.5276\n",
      "Processing batch 7173/11884 - Loss: 29.7616\n",
      "Processing batch 7174/11884 - Loss: 28.6814\n",
      "Processing batch 7175/11884 - Loss: 29.6072\n",
      "Processing batch 7176/11884 - Loss: 29.5650\n",
      "Processing batch 7177/11884 - Loss: 30.9771\n",
      "Processing batch 7178/11884 - Loss: 29.7473\n",
      "Processing batch 7179/11884 - Loss: 29.9450\n",
      "Processing batch 7180/11884 - Loss: 28.9941\n",
      "Processing batch 7181/11884 - Loss: 29.5777\n",
      "Processing batch 7182/11884 - Loss: 29.6875\n",
      "Processing batch 7183/11884 - Loss: 29.1360\n",
      "Processing batch 7184/11884 - Loss: 29.3853\n",
      "Processing batch 7185/11884 - Loss: 28.6907\n",
      "Processing batch 7186/11884 - Loss: 30.7496\n",
      "Processing batch 7187/11884 - Loss: 30.1513\n",
      "Processing batch 7188/11884 - Loss: 30.4157\n",
      "Processing batch 7189/11884 - Loss: 29.7318\n",
      "Processing batch 7190/11884 - Loss: 29.5428\n",
      "Processing batch 7191/11884 - Loss: 29.1984\n",
      "Processing batch 7192/11884 - Loss: 31.2182\n",
      "Processing batch 7193/11884 - Loss: 29.4812\n",
      "Processing batch 7194/11884 - Loss: 31.4644\n",
      "Processing batch 7195/11884 - Loss: 30.7530\n",
      "Processing batch 7196/11884 - Loss: 31.0837\n",
      "Processing batch 7197/11884 - Loss: 29.9235\n",
      "Processing batch 7198/11884 - Loss: 29.2975\n",
      "Processing batch 7199/11884 - Loss: 29.3995\n",
      "Processing batch 7200/11884 - Loss: 30.9066\n",
      "Processing batch 7201/11884 - Loss: 31.2787\n",
      "Processing batch 7202/11884 - Loss: 30.3804\n",
      "Processing batch 7203/11884 - Loss: 30.6151\n",
      "Processing batch 7204/11884 - Loss: 30.6877\n",
      "Processing batch 7205/11884 - Loss: 28.3326\n",
      "Processing batch 7206/11884 - Loss: 29.7841\n",
      "Processing batch 7207/11884 - Loss: 31.1894\n",
      "Processing batch 7208/11884 - Loss: 29.9323\n",
      "Processing batch 7209/11884 - Loss: 30.7453\n",
      "Processing batch 7210/11884 - Loss: 28.1642\n",
      "Processing batch 7211/11884 - Loss: 30.3380\n",
      "Processing batch 7212/11884 - Loss: 29.3691\n",
      "Processing batch 7213/11884 - Loss: 29.5879\n",
      "Processing batch 7214/11884 - Loss: 30.0333\n",
      "Processing batch 7215/11884 - Loss: 30.0764\n",
      "Processing batch 7216/11884 - Loss: 29.8341\n",
      "Processing batch 7217/11884 - Loss: 31.7128\n",
      "Processing batch 7218/11884 - Loss: 30.9928\n",
      "Processing batch 7219/11884 - Loss: 28.0271\n",
      "Processing batch 7220/11884 - Loss: 30.0939\n",
      "Processing batch 7221/11884 - Loss: 29.3445\n",
      "Processing batch 7222/11884 - Loss: 31.1600\n",
      "Processing batch 7223/11884 - Loss: 31.3827\n",
      "Processing batch 7224/11884 - Loss: 29.3692\n",
      "Processing batch 7225/11884 - Loss: 29.9946\n",
      "Processing batch 7226/11884 - Loss: 31.0820\n",
      "Processing batch 7227/11884 - Loss: 30.2828\n",
      "Processing batch 7228/11884 - Loss: 30.7089\n",
      "Processing batch 7229/11884 - Loss: 30.2155\n",
      "Processing batch 7230/11884 - Loss: 30.5299\n",
      "Processing batch 7231/11884 - Loss: 30.4604\n",
      "Processing batch 7232/11884 - Loss: 30.4062\n",
      "Processing batch 7233/11884 - Loss: 30.9859\n",
      "Processing batch 7234/11884 - Loss: 30.0063\n",
      "Processing batch 7235/11884 - Loss: 30.2004\n",
      "Processing batch 7236/11884 - Loss: 30.1408\n",
      "Processing batch 7237/11884 - Loss: 28.8833\n",
      "Processing batch 7238/11884 - Loss: 29.6217\n",
      "Processing batch 7239/11884 - Loss: 29.8665\n",
      "Processing batch 7240/11884 - Loss: 31.6230\n",
      "Processing batch 7241/11884 - Loss: 30.2685\n",
      "Processing batch 7242/11884 - Loss: 31.2856\n",
      "Processing batch 7243/11884 - Loss: 29.9756\n",
      "Processing batch 7244/11884 - Loss: 30.0175\n",
      "Processing batch 7245/11884 - Loss: 31.1385\n",
      "Processing batch 7246/11884 - Loss: 30.2775\n",
      "Processing batch 7247/11884 - Loss: 30.3194\n",
      "Processing batch 7248/11884 - Loss: 31.5135\n",
      "Processing batch 7249/11884 - Loss: 30.0700\n",
      "Processing batch 7250/11884 - Loss: 30.0521\n",
      "Processing batch 7251/11884 - Loss: 30.3347\n",
      "Processing batch 7252/11884 - Loss: 29.6049\n",
      "Processing batch 7253/11884 - Loss: 30.0208\n",
      "Processing batch 7254/11884 - Loss: 29.0340\n",
      "Processing batch 7255/11884 - Loss: 30.3785\n",
      "Processing batch 7256/11884 - Loss: 30.5644\n",
      "Processing batch 7257/11884 - Loss: 29.6070\n",
      "Processing batch 7258/11884 - Loss: 29.1453\n",
      "Processing batch 7259/11884 - Loss: 32.2325\n",
      "Processing batch 7260/11884 - Loss: 30.4976\n",
      "Processing batch 7261/11884 - Loss: 31.0585\n",
      "Processing batch 7262/11884 - Loss: 31.2751\n",
      "Processing batch 7263/11884 - Loss: 30.4352\n",
      "Processing batch 7264/11884 - Loss: 29.7317\n",
      "Processing batch 7265/11884 - Loss: 29.2959\n",
      "Processing batch 7266/11884 - Loss: 29.8264\n",
      "Processing batch 7267/11884 - Loss: 30.5977\n",
      "Processing batch 7268/11884 - Loss: 29.3905\n",
      "Processing batch 7269/11884 - Loss: 29.1411\n",
      "Processing batch 7270/11884 - Loss: 29.5847\n",
      "Processing batch 7271/11884 - Loss: 30.7371\n",
      "Processing batch 7272/11884 - Loss: 30.2132\n",
      "Processing batch 7273/11884 - Loss: 30.2431\n",
      "Processing batch 7274/11884 - Loss: 28.9275\n",
      "Processing batch 7275/11884 - Loss: 29.4112\n",
      "Processing batch 7276/11884 - Loss: 28.8659\n",
      "Processing batch 7277/11884 - Loss: 30.7905\n",
      "Processing batch 7278/11884 - Loss: 29.2747\n",
      "Processing batch 7279/11884 - Loss: 31.0072\n",
      "Processing batch 7280/11884 - Loss: 31.1613\n",
      "Processing batch 7281/11884 - Loss: 29.8879\n",
      "Processing batch 7282/11884 - Loss: 30.9787\n",
      "Processing batch 7283/11884 - Loss: 29.1596\n",
      "Processing batch 7284/11884 - Loss: 29.9285\n",
      "Processing batch 7285/11884 - Loss: 28.6879\n",
      "Processing batch 7286/11884 - Loss: 29.0760\n",
      "Processing batch 7287/11884 - Loss: 30.1004\n",
      "Processing batch 7288/11884 - Loss: 30.0405\n",
      "Processing batch 7289/11884 - Loss: 28.8147\n",
      "Processing batch 7290/11884 - Loss: 29.4587\n",
      "Processing batch 7291/11884 - Loss: 31.9255\n",
      "Processing batch 7292/11884 - Loss: 31.0870\n",
      "Processing batch 7293/11884 - Loss: 29.6495\n",
      "Processing batch 7294/11884 - Loss: 29.3892\n",
      "Processing batch 7295/11884 - Loss: 30.5881\n",
      "Processing batch 7296/11884 - Loss: 29.2820\n",
      "Processing batch 7297/11884 - Loss: 30.1260\n",
      "Processing batch 7298/11884 - Loss: 30.4101\n",
      "Processing batch 7299/11884 - Loss: 31.0628\n",
      "Processing batch 7300/11884 - Loss: 28.6916\n",
      "Processing batch 7301/11884 - Loss: 30.1919\n",
      "Processing batch 7302/11884 - Loss: 29.9417\n",
      "Processing batch 7303/11884 - Loss: 31.5622\n",
      "Processing batch 7304/11884 - Loss: 30.3751\n",
      "Processing batch 7305/11884 - Loss: 30.0198\n",
      "Processing batch 7306/11884 - Loss: 29.4312\n",
      "Processing batch 7307/11884 - Loss: 30.8845\n",
      "Processing batch 7308/11884 - Loss: 29.5977\n",
      "Processing batch 7309/11884 - Loss: 29.0780\n",
      "Processing batch 7310/11884 - Loss: 30.1544\n",
      "Processing batch 7311/11884 - Loss: 30.4405\n",
      "Processing batch 7312/11884 - Loss: 31.8231\n",
      "Processing batch 7313/11884 - Loss: 29.9055\n",
      "Processing batch 7314/11884 - Loss: 29.7934\n",
      "Processing batch 7315/11884 - Loss: 29.8334\n",
      "Processing batch 7316/11884 - Loss: 29.5037\n",
      "Processing batch 7317/11884 - Loss: 30.0588\n",
      "Processing batch 7318/11884 - Loss: 30.6767\n",
      "Processing batch 7319/11884 - Loss: 28.7088\n",
      "Processing batch 7320/11884 - Loss: 29.7928\n",
      "Processing batch 7321/11884 - Loss: 29.2534\n",
      "Processing batch 7322/11884 - Loss: 29.1628\n",
      "Processing batch 7323/11884 - Loss: 30.3540\n",
      "Processing batch 7324/11884 - Loss: 31.7249\n",
      "Processing batch 7325/11884 - Loss: 28.6587\n",
      "Processing batch 7326/11884 - Loss: 29.4319\n",
      "Processing batch 7327/11884 - Loss: 29.5452\n",
      "Processing batch 7328/11884 - Loss: 28.2883\n",
      "Processing batch 7329/11884 - Loss: 30.1738\n",
      "Processing batch 7330/11884 - Loss: 30.6307\n",
      "Processing batch 7331/11884 - Loss: 31.3003\n",
      "Processing batch 7332/11884 - Loss: 30.2872\n",
      "Processing batch 7333/11884 - Loss: 30.7368\n",
      "Processing batch 7334/11884 - Loss: 29.8602\n",
      "Processing batch 7335/11884 - Loss: 30.7472\n",
      "Processing batch 7336/11884 - Loss: 31.7369\n",
      "Processing batch 7337/11884 - Loss: 29.2761\n",
      "Processing batch 7338/11884 - Loss: 29.6390\n",
      "Processing batch 7339/11884 - Loss: 28.9663\n",
      "Processing batch 7340/11884 - Loss: 30.4132\n",
      "Processing batch 7341/11884 - Loss: 30.8977\n",
      "Processing batch 7342/11884 - Loss: 28.5584\n",
      "Processing batch 7343/11884 - Loss: 30.3580\n",
      "Processing batch 7344/11884 - Loss: 31.4182\n",
      "Processing batch 7345/11884 - Loss: 30.6292\n",
      "Processing batch 7346/11884 - Loss: 30.6859\n",
      "Processing batch 7347/11884 - Loss: 30.2814\n",
      "Processing batch 7348/11884 - Loss: 30.0919\n",
      "Processing batch 7349/11884 - Loss: 29.3080\n",
      "Processing batch 7350/11884 - Loss: 28.6127\n",
      "Processing batch 7351/11884 - Loss: 30.8129\n",
      "Processing batch 7352/11884 - Loss: 29.7889\n",
      "Processing batch 7353/11884 - Loss: 31.1690\n",
      "Processing batch 7354/11884 - Loss: 29.8786\n",
      "Processing batch 7355/11884 - Loss: 30.0557\n",
      "Processing batch 7356/11884 - Loss: 30.5026\n",
      "Processing batch 7357/11884 - Loss: 29.5175\n",
      "Processing batch 7358/11884 - Loss: 29.5622\n",
      "Processing batch 7359/11884 - Loss: 30.1704\n",
      "Processing batch 7360/11884 - Loss: 30.2668\n",
      "Processing batch 7361/11884 - Loss: 29.8352\n",
      "Processing batch 7362/11884 - Loss: 29.5634\n",
      "Processing batch 7363/11884 - Loss: 31.2956\n",
      "Processing batch 7364/11884 - Loss: 29.2337\n",
      "Processing batch 7365/11884 - Loss: 30.1921\n",
      "Processing batch 7366/11884 - Loss: 30.2627\n",
      "Processing batch 7367/11884 - Loss: 31.0885\n",
      "Processing batch 7368/11884 - Loss: 30.7059\n",
      "Processing batch 7369/11884 - Loss: 30.5436\n",
      "Processing batch 7370/11884 - Loss: 29.7895\n",
      "Processing batch 7371/11884 - Loss: 30.8926\n",
      "Processing batch 7372/11884 - Loss: 29.8271\n",
      "Processing batch 7373/11884 - Loss: 30.7841\n",
      "Processing batch 7374/11884 - Loss: 29.7503\n",
      "Processing batch 7375/11884 - Loss: 28.5375\n",
      "Processing batch 7376/11884 - Loss: 29.0493\n",
      "Processing batch 7377/11884 - Loss: 30.1412\n",
      "Processing batch 7378/11884 - Loss: 30.8219\n",
      "Processing batch 7379/11884 - Loss: 30.9920\n",
      "Processing batch 7380/11884 - Loss: 31.3738\n",
      "Processing batch 7381/11884 - Loss: 30.3364\n",
      "Processing batch 7382/11884 - Loss: 29.3449\n",
      "Processing batch 7383/11884 - Loss: 30.5390\n",
      "Processing batch 7384/11884 - Loss: 30.8185\n",
      "Processing batch 7385/11884 - Loss: 30.1647\n",
      "Processing batch 7386/11884 - Loss: 31.4101\n",
      "Processing batch 7387/11884 - Loss: 29.8134\n",
      "Processing batch 7388/11884 - Loss: 29.7756\n",
      "Processing batch 7389/11884 - Loss: 29.5341\n",
      "Processing batch 7390/11884 - Loss: 31.0939\n",
      "Processing batch 7391/11884 - Loss: 30.6727\n",
      "Processing batch 7392/11884 - Loss: 30.9708\n",
      "Processing batch 7393/11884 - Loss: 30.9397\n",
      "Processing batch 7394/11884 - Loss: 30.2531\n",
      "Processing batch 7395/11884 - Loss: 30.4929\n",
      "Processing batch 7396/11884 - Loss: 30.0385\n",
      "Processing batch 7397/11884 - Loss: 29.5565\n",
      "Processing batch 7398/11884 - Loss: 30.0812\n",
      "Processing batch 7399/11884 - Loss: 28.7696\n",
      "Processing batch 7400/11884 - Loss: 30.4058\n",
      "Processing batch 7401/11884 - Loss: 30.0628\n",
      "Processing batch 7402/11884 - Loss: 29.5721\n",
      "Processing batch 7403/11884 - Loss: 30.6234\n",
      "Processing batch 7404/11884 - Loss: 29.3062\n",
      "Processing batch 7405/11884 - Loss: 29.7714\n",
      "Processing batch 7406/11884 - Loss: 31.1316\n",
      "Processing batch 7407/11884 - Loss: 28.6502\n",
      "Processing batch 7408/11884 - Loss: 31.0151\n",
      "Processing batch 7409/11884 - Loss: 30.9370\n",
      "Processing batch 7410/11884 - Loss: 29.9519\n",
      "Processing batch 7411/11884 - Loss: 29.2525\n",
      "Processing batch 7412/11884 - Loss: 29.2461\n",
      "Processing batch 7413/11884 - Loss: 29.4516\n",
      "Processing batch 7414/11884 - Loss: 29.1740\n",
      "Processing batch 7415/11884 - Loss: 30.5495\n",
      "Processing batch 7416/11884 - Loss: 29.7566\n",
      "Processing batch 7417/11884 - Loss: 29.7295\n",
      "Processing batch 7418/11884 - Loss: 29.9227\n",
      "Processing batch 7419/11884 - Loss: 31.1802\n",
      "Processing batch 7420/11884 - Loss: 29.9058\n",
      "Processing batch 7421/11884 - Loss: 30.2563\n",
      "Processing batch 7422/11884 - Loss: 30.7829\n",
      "Processing batch 7423/11884 - Loss: 30.3282\n",
      "Processing batch 7424/11884 - Loss: 30.5069\n",
      "Processing batch 7425/11884 - Loss: 30.1140\n",
      "Processing batch 7426/11884 - Loss: 31.2861\n",
      "Processing batch 7427/11884 - Loss: 31.2757\n",
      "Processing batch 7428/11884 - Loss: 30.5657\n",
      "Processing batch 7429/11884 - Loss: 29.6799\n",
      "Processing batch 7430/11884 - Loss: 28.9122\n",
      "Processing batch 7431/11884 - Loss: 28.6985\n",
      "Processing batch 7432/11884 - Loss: 30.6953\n",
      "Processing batch 7433/11884 - Loss: 30.1316\n",
      "Processing batch 7434/11884 - Loss: 29.3626\n",
      "Processing batch 7435/11884 - Loss: 30.3822\n",
      "Processing batch 7436/11884 - Loss: 30.5365\n",
      "Processing batch 7437/11884 - Loss: 30.1344\n",
      "Processing batch 7438/11884 - Loss: 29.7512\n",
      "Processing batch 7439/11884 - Loss: 30.0112\n",
      "Processing batch 7440/11884 - Loss: 31.8111\n",
      "Processing batch 7441/11884 - Loss: 30.2220\n",
      "Processing batch 7442/11884 - Loss: 30.6253\n",
      "Processing batch 7443/11884 - Loss: 30.4176\n",
      "Processing batch 7444/11884 - Loss: 29.1224\n",
      "Processing batch 7445/11884 - Loss: 29.8933\n",
      "Processing batch 7446/11884 - Loss: 31.4176\n",
      "Processing batch 7447/11884 - Loss: 30.0771\n",
      "Processing batch 7448/11884 - Loss: 30.5758\n",
      "Processing batch 7449/11884 - Loss: 29.6802\n",
      "Processing batch 7450/11884 - Loss: 30.4765\n",
      "Processing batch 7451/11884 - Loss: 30.6883\n",
      "Processing batch 7452/11884 - Loss: 30.5155\n",
      "Processing batch 7453/11884 - Loss: 29.9095\n",
      "Processing batch 7454/11884 - Loss: 29.0806\n",
      "Processing batch 7455/11884 - Loss: 29.5375\n",
      "Processing batch 7456/11884 - Loss: 29.9401\n",
      "Processing batch 7457/11884 - Loss: 30.3937\n",
      "Processing batch 7458/11884 - Loss: 30.3374\n",
      "Processing batch 7459/11884 - Loss: 29.8197\n",
      "Processing batch 7460/11884 - Loss: 30.2272\n",
      "Processing batch 7461/11884 - Loss: 30.0188\n",
      "Processing batch 7462/11884 - Loss: 29.6120\n",
      "Processing batch 7463/11884 - Loss: 29.6788\n",
      "Processing batch 7464/11884 - Loss: 29.3158\n",
      "Processing batch 7465/11884 - Loss: 30.5410\n",
      "Processing batch 7466/11884 - Loss: 30.9057\n",
      "Processing batch 7467/11884 - Loss: 29.8452\n",
      "Processing batch 7468/11884 - Loss: 29.8054\n",
      "Processing batch 7469/11884 - Loss: 31.5354\n",
      "Processing batch 7470/11884 - Loss: 30.1166\n",
      "Processing batch 7471/11884 - Loss: 30.2833\n",
      "Processing batch 7472/11884 - Loss: 30.5071\n",
      "Processing batch 7473/11884 - Loss: 28.6914\n",
      "Processing batch 7474/11884 - Loss: 31.2922\n",
      "Processing batch 7475/11884 - Loss: 30.3693\n",
      "Processing batch 7476/11884 - Loss: 30.1011\n",
      "Processing batch 7477/11884 - Loss: 29.2830\n",
      "Processing batch 7478/11884 - Loss: 30.4024\n",
      "Processing batch 7479/11884 - Loss: 29.8644\n",
      "Processing batch 7480/11884 - Loss: 30.1745\n",
      "Processing batch 7481/11884 - Loss: 30.2575\n",
      "Processing batch 7482/11884 - Loss: 30.5702\n",
      "Processing batch 7483/11884 - Loss: 29.8921\n",
      "Processing batch 7484/11884 - Loss: 28.4551\n",
      "Processing batch 7485/11884 - Loss: 30.0863\n",
      "Processing batch 7486/11884 - Loss: 29.6367\n",
      "Processing batch 7487/11884 - Loss: 30.1964\n",
      "Processing batch 7488/11884 - Loss: 31.0652\n",
      "Processing batch 7489/11884 - Loss: 30.9146\n",
      "Processing batch 7490/11884 - Loss: 30.4657\n",
      "Processing batch 7491/11884 - Loss: 30.0015\n",
      "Processing batch 7492/11884 - Loss: 29.3104\n",
      "Processing batch 7493/11884 - Loss: 30.4375\n",
      "Processing batch 7494/11884 - Loss: 30.2596\n",
      "Processing batch 7495/11884 - Loss: 29.6437\n",
      "Processing batch 7496/11884 - Loss: 29.9912\n",
      "Processing batch 7497/11884 - Loss: 30.0526\n",
      "Processing batch 7498/11884 - Loss: 29.7140\n",
      "Processing batch 7499/11884 - Loss: 29.8974\n",
      "Processing batch 7500/11884 - Loss: 29.4905\n",
      "Processing batch 7501/11884 - Loss: 29.4953\n",
      "Processing batch 7502/11884 - Loss: 30.5140\n",
      "Processing batch 7503/11884 - Loss: 30.4832\n",
      "Processing batch 7504/11884 - Loss: 28.9489\n",
      "Processing batch 7505/11884 - Loss: 30.5425\n",
      "Processing batch 7506/11884 - Loss: 30.6304\n",
      "Processing batch 7507/11884 - Loss: 31.1088\n",
      "Processing batch 7508/11884 - Loss: 30.6713\n",
      "Processing batch 7509/11884 - Loss: 29.6129\n",
      "Processing batch 7510/11884 - Loss: 30.5045\n",
      "Processing batch 7511/11884 - Loss: 28.1328\n",
      "Processing batch 7512/11884 - Loss: 30.7434\n",
      "Processing batch 7513/11884 - Loss: 31.7845\n",
      "Processing batch 7514/11884 - Loss: 30.1755\n",
      "Processing batch 7515/11884 - Loss: 29.7713\n",
      "Processing batch 7516/11884 - Loss: 29.5253\n",
      "Processing batch 7517/11884 - Loss: 29.3671\n",
      "Processing batch 7518/11884 - Loss: 30.0164\n",
      "Processing batch 7519/11884 - Loss: 29.6984\n",
      "Processing batch 7520/11884 - Loss: 28.7699\n",
      "Processing batch 7521/11884 - Loss: 31.1223\n",
      "Processing batch 7522/11884 - Loss: 28.4387\n",
      "Processing batch 7523/11884 - Loss: 30.7799\n",
      "Processing batch 7524/11884 - Loss: 30.2520\n",
      "Processing batch 7525/11884 - Loss: 30.9373\n",
      "Processing batch 7526/11884 - Loss: 29.0154\n",
      "Processing batch 7527/11884 - Loss: 30.3498\n",
      "Processing batch 7528/11884 - Loss: 30.5889\n",
      "Processing batch 7529/11884 - Loss: 30.8490\n",
      "Processing batch 7530/11884 - Loss: 30.5361\n",
      "Processing batch 7531/11884 - Loss: 30.4515\n",
      "Processing batch 7532/11884 - Loss: 30.1717\n",
      "Processing batch 7533/11884 - Loss: 30.5471\n",
      "Processing batch 7534/11884 - Loss: 28.6395\n",
      "Processing batch 7535/11884 - Loss: 30.8838\n",
      "Processing batch 7536/11884 - Loss: 30.6503\n",
      "Processing batch 7537/11884 - Loss: 30.7436\n",
      "Processing batch 7538/11884 - Loss: 30.2894\n",
      "Processing batch 7539/11884 - Loss: 28.9095\n",
      "Processing batch 7540/11884 - Loss: 30.1210\n",
      "Processing batch 7541/11884 - Loss: 29.4158\n",
      "Processing batch 7542/11884 - Loss: 30.1656\n",
      "Processing batch 7543/11884 - Loss: 30.6539\n",
      "Processing batch 7544/11884 - Loss: 29.4078\n",
      "Processing batch 7545/11884 - Loss: 29.2487\n",
      "Processing batch 7546/11884 - Loss: 29.8628\n",
      "Processing batch 7547/11884 - Loss: 30.6545\n",
      "Processing batch 7548/11884 - Loss: 30.0295\n",
      "Processing batch 7549/11884 - Loss: 28.8347\n",
      "Processing batch 7550/11884 - Loss: 29.7828\n",
      "Processing batch 7551/11884 - Loss: 30.0329\n",
      "Processing batch 7552/11884 - Loss: 31.4252\n",
      "Processing batch 7553/11884 - Loss: 29.8347\n",
      "Processing batch 7554/11884 - Loss: 30.7540\n",
      "Processing batch 7555/11884 - Loss: 29.6804\n",
      "Processing batch 7556/11884 - Loss: 29.1412\n",
      "Processing batch 7557/11884 - Loss: 29.7268\n",
      "Processing batch 7558/11884 - Loss: 30.1379\n",
      "Processing batch 7559/11884 - Loss: 30.3809\n",
      "Processing batch 7560/11884 - Loss: 28.5233\n",
      "Processing batch 7561/11884 - Loss: 29.0637\n",
      "Processing batch 7562/11884 - Loss: 30.5554\n",
      "Processing batch 7563/11884 - Loss: 28.4530\n",
      "Processing batch 7564/11884 - Loss: 29.5510\n",
      "Processing batch 7565/11884 - Loss: 30.7270\n",
      "Processing batch 7566/11884 - Loss: 29.0102\n",
      "Processing batch 7567/11884 - Loss: 29.2150\n",
      "Processing batch 7568/11884 - Loss: 29.6292\n",
      "Processing batch 7569/11884 - Loss: 30.6016\n",
      "Processing batch 7570/11884 - Loss: 31.4148\n",
      "Processing batch 7571/11884 - Loss: 29.7879\n",
      "Processing batch 7572/11884 - Loss: 30.0939\n",
      "Processing batch 7573/11884 - Loss: 29.5661\n",
      "Processing batch 7574/11884 - Loss: 30.1236\n",
      "Processing batch 7575/11884 - Loss: 30.2216\n",
      "Processing batch 7576/11884 - Loss: 28.7641\n",
      "Processing batch 7577/11884 - Loss: 28.8418\n",
      "Processing batch 7578/11884 - Loss: 30.2653\n",
      "Processing batch 7579/11884 - Loss: 29.4881\n",
      "Processing batch 7580/11884 - Loss: 30.3336\n",
      "Processing batch 7581/11884 - Loss: 30.0408\n",
      "Processing batch 7582/11884 - Loss: 30.4321\n",
      "Processing batch 7583/11884 - Loss: 29.6914\n",
      "Processing batch 7584/11884 - Loss: 29.7849\n",
      "Processing batch 7585/11884 - Loss: 30.8276\n",
      "Processing batch 7586/11884 - Loss: 30.5463\n",
      "Processing batch 7587/11884 - Loss: 29.3444\n",
      "Processing batch 7588/11884 - Loss: 29.7709\n",
      "Processing batch 7589/11884 - Loss: 30.6578\n",
      "Processing batch 7590/11884 - Loss: 29.9599\n",
      "Processing batch 7591/11884 - Loss: 30.4760\n",
      "Processing batch 7592/11884 - Loss: 30.4092\n",
      "Processing batch 7593/11884 - Loss: 29.7328\n",
      "Processing batch 7594/11884 - Loss: 31.2556\n",
      "Processing batch 7595/11884 - Loss: 30.2472\n",
      "Processing batch 7596/11884 - Loss: 30.0542\n",
      "Processing batch 7597/11884 - Loss: 29.7454\n",
      "Processing batch 7598/11884 - Loss: 29.1791\n",
      "Processing batch 7599/11884 - Loss: 31.1581\n",
      "Processing batch 7600/11884 - Loss: 28.4638\n",
      "Processing batch 7601/11884 - Loss: 31.0381\n",
      "Processing batch 7602/11884 - Loss: 28.9308\n",
      "Processing batch 7603/11884 - Loss: 30.5554\n",
      "Processing batch 7604/11884 - Loss: 30.5470\n",
      "Processing batch 7605/11884 - Loss: 30.7762\n",
      "Processing batch 7606/11884 - Loss: 29.5500\n",
      "Processing batch 7607/11884 - Loss: 29.1492\n",
      "Processing batch 7608/11884 - Loss: 30.3732\n",
      "Processing batch 7609/11884 - Loss: 28.8756\n",
      "Processing batch 7610/11884 - Loss: 28.6541\n",
      "Processing batch 7611/11884 - Loss: 30.0828\n",
      "Processing batch 7612/11884 - Loss: 30.0670\n",
      "Processing batch 7613/11884 - Loss: 31.3081\n",
      "Processing batch 7614/11884 - Loss: 28.9083\n",
      "Processing batch 7615/11884 - Loss: 31.0795\n",
      "Processing batch 7616/11884 - Loss: 29.0515\n",
      "Processing batch 7617/11884 - Loss: 30.1195\n",
      "Processing batch 7618/11884 - Loss: 30.6257\n",
      "Processing batch 7619/11884 - Loss: 29.9327\n",
      "Processing batch 7620/11884 - Loss: 29.8225\n",
      "Processing batch 7621/11884 - Loss: 29.9146\n",
      "Processing batch 7622/11884 - Loss: 29.6278\n",
      "Processing batch 7623/11884 - Loss: 29.4808\n",
      "Processing batch 7624/11884 - Loss: 31.4239\n",
      "Processing batch 7625/11884 - Loss: 30.5187\n",
      "Processing batch 7626/11884 - Loss: 30.0639\n",
      "Processing batch 7627/11884 - Loss: 29.9119\n",
      "Processing batch 7628/11884 - Loss: 30.5037\n",
      "Processing batch 7629/11884 - Loss: 29.4507\n",
      "Processing batch 7630/11884 - Loss: 28.7981\n",
      "Processing batch 7631/11884 - Loss: 30.3916\n",
      "Processing batch 7632/11884 - Loss: 29.8590\n",
      "Processing batch 7633/11884 - Loss: 30.1722\n",
      "Processing batch 7634/11884 - Loss: 31.0961\n",
      "Processing batch 7635/11884 - Loss: 30.9440\n",
      "Processing batch 7636/11884 - Loss: 30.0888\n",
      "Processing batch 7637/11884 - Loss: 31.0710\n",
      "Processing batch 7638/11884 - Loss: 29.4201\n",
      "Processing batch 7639/11884 - Loss: 30.4752\n",
      "Processing batch 7640/11884 - Loss: 28.7754\n",
      "Processing batch 7641/11884 - Loss: 29.8223\n",
      "Processing batch 7642/11884 - Loss: 31.4879\n",
      "Processing batch 7643/11884 - Loss: 29.6443\n",
      "Processing batch 7644/11884 - Loss: 30.1258\n",
      "Processing batch 7645/11884 - Loss: 30.1033\n",
      "Processing batch 7646/11884 - Loss: 29.0755\n",
      "Processing batch 7647/11884 - Loss: 28.9965\n",
      "Processing batch 7648/11884 - Loss: 28.0065\n",
      "Processing batch 7649/11884 - Loss: 31.4117\n",
      "Processing batch 7650/11884 - Loss: 29.6637\n",
      "Processing batch 7651/11884 - Loss: 30.0391\n",
      "Processing batch 7652/11884 - Loss: 29.4009\n",
      "Processing batch 7653/11884 - Loss: 29.8870\n",
      "Processing batch 7654/11884 - Loss: 30.3176\n",
      "Processing batch 7655/11884 - Loss: 30.1052\n",
      "Processing batch 7656/11884 - Loss: 30.5206\n",
      "Processing batch 7657/11884 - Loss: 30.7408\n",
      "Processing batch 7658/11884 - Loss: 29.8497\n",
      "Processing batch 7659/11884 - Loss: 31.0089\n",
      "Processing batch 7660/11884 - Loss: 30.6482\n",
      "Processing batch 7661/11884 - Loss: 30.0007\n",
      "Processing batch 7662/11884 - Loss: 30.4546\n",
      "Processing batch 7663/11884 - Loss: 29.2059\n",
      "Processing batch 7664/11884 - Loss: 29.2221\n",
      "Processing batch 7665/11884 - Loss: 28.5577\n",
      "Processing batch 7666/11884 - Loss: 30.5116\n",
      "Processing batch 7667/11884 - Loss: 29.8709\n",
      "Processing batch 7668/11884 - Loss: 30.3446\n",
      "Processing batch 7669/11884 - Loss: 29.4304\n",
      "Processing batch 7670/11884 - Loss: 30.6558\n",
      "Processing batch 7671/11884 - Loss: 32.0224\n",
      "Processing batch 7672/11884 - Loss: 30.8199\n",
      "Processing batch 7673/11884 - Loss: 29.6540\n",
      "Processing batch 7674/11884 - Loss: 31.0017\n",
      "Processing batch 7675/11884 - Loss: 29.5045\n",
      "Processing batch 7676/11884 - Loss: 29.4294\n",
      "Processing batch 7677/11884 - Loss: 30.2751\n",
      "Processing batch 7678/11884 - Loss: 30.4242\n",
      "Processing batch 7679/11884 - Loss: 30.5847\n",
      "Processing batch 7680/11884 - Loss: 30.2367\n",
      "Processing batch 7681/11884 - Loss: 30.6311\n",
      "Processing batch 7682/11884 - Loss: 30.6425\n",
      "Processing batch 7683/11884 - Loss: 30.0871\n",
      "Processing batch 7684/11884 - Loss: 29.4157\n",
      "Processing batch 7685/11884 - Loss: 30.5132\n",
      "Processing batch 7686/11884 - Loss: 30.0681\n",
      "Processing batch 7687/11884 - Loss: 28.5061\n",
      "Processing batch 7688/11884 - Loss: 29.4164\n",
      "Processing batch 7689/11884 - Loss: 31.8419\n",
      "Processing batch 7690/11884 - Loss: 28.4768\n",
      "Processing batch 7691/11884 - Loss: 29.9532\n",
      "Processing batch 7692/11884 - Loss: 30.9267\n",
      "Processing batch 7693/11884 - Loss: 28.6391\n",
      "Processing batch 7694/11884 - Loss: 29.6707\n",
      "Processing batch 7695/11884 - Loss: 28.3042\n",
      "Processing batch 7696/11884 - Loss: 29.6870\n",
      "Processing batch 7697/11884 - Loss: 29.7402\n",
      "Processing batch 7698/11884 - Loss: 30.5461\n",
      "Processing batch 7699/11884 - Loss: 29.7751\n",
      "Processing batch 7700/11884 - Loss: 30.2924\n",
      "Processing batch 7701/11884 - Loss: 30.0377\n",
      "Processing batch 7702/11884 - Loss: 30.3457\n",
      "Processing batch 7703/11884 - Loss: 30.0123\n",
      "Processing batch 7704/11884 - Loss: 30.3631\n",
      "Processing batch 7705/11884 - Loss: 28.5384\n",
      "Processing batch 7706/11884 - Loss: 30.2945\n",
      "Processing batch 7707/11884 - Loss: 29.9030\n",
      "Processing batch 7708/11884 - Loss: 29.9703\n",
      "Processing batch 7709/11884 - Loss: 30.6834\n",
      "Processing batch 7710/11884 - Loss: 30.1909\n",
      "Processing batch 7711/11884 - Loss: 31.7920\n",
      "Processing batch 7712/11884 - Loss: 29.5858\n",
      "Processing batch 7713/11884 - Loss: 29.3478\n",
      "Processing batch 7714/11884 - Loss: 30.1952\n",
      "Processing batch 7715/11884 - Loss: 30.7231\n",
      "Processing batch 7716/11884 - Loss: 29.1015\n",
      "Processing batch 7717/11884 - Loss: 29.8496\n",
      "Processing batch 7718/11884 - Loss: 29.6660\n",
      "Processing batch 7719/11884 - Loss: 31.7680\n",
      "Processing batch 7720/11884 - Loss: 29.3265\n",
      "Processing batch 7721/11884 - Loss: 28.5730\n",
      "Processing batch 7722/11884 - Loss: 30.4884\n",
      "Processing batch 7723/11884 - Loss: 29.2173\n",
      "Processing batch 7724/11884 - Loss: 29.7070\n",
      "Processing batch 7725/11884 - Loss: 30.2822\n",
      "Processing batch 7726/11884 - Loss: 30.1829\n",
      "Processing batch 7727/11884 - Loss: 28.8229\n",
      "Processing batch 7728/11884 - Loss: 28.7689\n",
      "Processing batch 7729/11884 - Loss: 30.1332\n",
      "Processing batch 7730/11884 - Loss: 29.5009\n",
      "Processing batch 7731/11884 - Loss: 29.3358\n",
      "Processing batch 7732/11884 - Loss: 29.5802\n",
      "Processing batch 7733/11884 - Loss: 30.6396\n",
      "Processing batch 7734/11884 - Loss: 29.8791\n",
      "Processing batch 7735/11884 - Loss: 30.1408\n",
      "Processing batch 7736/11884 - Loss: 29.7307\n",
      "Processing batch 7737/11884 - Loss: 29.5780\n",
      "Processing batch 7738/11884 - Loss: 28.3603\n",
      "Processing batch 7739/11884 - Loss: 30.1881\n",
      "Processing batch 7740/11884 - Loss: 28.7440\n",
      "Processing batch 7741/11884 - Loss: 30.8145\n",
      "Processing batch 7742/11884 - Loss: 30.2841\n",
      "Processing batch 7743/11884 - Loss: 29.6898\n",
      "Processing batch 7744/11884 - Loss: 30.9142\n",
      "Processing batch 7745/11884 - Loss: 29.9043\n",
      "Processing batch 7746/11884 - Loss: 29.1554\n",
      "Processing batch 7747/11884 - Loss: 29.8016\n",
      "Processing batch 7748/11884 - Loss: 31.3126\n",
      "Processing batch 7749/11884 - Loss: 31.0602\n",
      "Processing batch 7750/11884 - Loss: 30.5631\n",
      "Processing batch 7751/11884 - Loss: 30.6256\n",
      "Processing batch 7752/11884 - Loss: 29.0921\n",
      "Processing batch 7753/11884 - Loss: 29.6322\n",
      "Processing batch 7754/11884 - Loss: 30.4430\n",
      "Processing batch 7755/11884 - Loss: 30.3840\n",
      "Processing batch 7756/11884 - Loss: 30.7104\n",
      "Processing batch 7757/11884 - Loss: 30.2742\n",
      "Processing batch 7758/11884 - Loss: 30.2703\n",
      "Processing batch 7759/11884 - Loss: 30.2728\n",
      "Processing batch 7760/11884 - Loss: 29.5183\n",
      "Processing batch 7761/11884 - Loss: 29.4066\n",
      "Processing batch 7762/11884 - Loss: 31.3493\n",
      "Processing batch 7763/11884 - Loss: 30.9254\n",
      "Processing batch 7764/11884 - Loss: 30.4207\n",
      "Processing batch 7765/11884 - Loss: 30.8374\n",
      "Processing batch 7766/11884 - Loss: 29.1376\n",
      "Processing batch 7767/11884 - Loss: 29.3755\n",
      "Processing batch 7768/11884 - Loss: 29.9457\n",
      "Processing batch 7769/11884 - Loss: 29.3241\n",
      "Processing batch 7770/11884 - Loss: 30.8887\n",
      "Processing batch 7771/11884 - Loss: 30.0591\n",
      "Processing batch 7772/11884 - Loss: 29.1011\n",
      "Processing batch 7773/11884 - Loss: 32.0882\n",
      "Processing batch 7774/11884 - Loss: 29.9115\n",
      "Processing batch 7775/11884 - Loss: 28.8467\n",
      "Processing batch 7776/11884 - Loss: 31.7766\n",
      "Processing batch 7777/11884 - Loss: 29.4906\n",
      "Processing batch 7778/11884 - Loss: 29.2088\n",
      "Processing batch 7779/11884 - Loss: 28.7923\n",
      "Processing batch 7780/11884 - Loss: 30.2677\n",
      "Processing batch 7781/11884 - Loss: 29.1692\n",
      "Processing batch 7782/11884 - Loss: 31.8971\n",
      "Processing batch 7783/11884 - Loss: 29.9692\n",
      "Processing batch 7784/11884 - Loss: 30.1172\n",
      "Processing batch 7785/11884 - Loss: 31.1753\n",
      "Processing batch 7786/11884 - Loss: 29.2333\n",
      "Processing batch 7787/11884 - Loss: 30.6758\n",
      "Processing batch 7788/11884 - Loss: 29.5893\n",
      "Processing batch 7789/11884 - Loss: 30.2389\n",
      "Processing batch 7790/11884 - Loss: 30.5037\n",
      "Processing batch 7791/11884 - Loss: 30.2842\n",
      "Processing batch 7792/11884 - Loss: 29.3144\n",
      "Processing batch 7793/11884 - Loss: 29.3488\n",
      "Processing batch 7794/11884 - Loss: 31.2620\n",
      "Processing batch 7795/11884 - Loss: 31.2270\n",
      "Processing batch 7796/11884 - Loss: 30.6412\n",
      "Processing batch 7797/11884 - Loss: 30.0015\n",
      "Processing batch 7798/11884 - Loss: 31.4627\n",
      "Processing batch 7799/11884 - Loss: 29.6715\n",
      "Processing batch 7800/11884 - Loss: 30.4977\n",
      "Processing batch 7801/11884 - Loss: 30.3993\n",
      "Processing batch 7802/11884 - Loss: 30.3438\n",
      "Processing batch 7803/11884 - Loss: 29.6252\n",
      "Processing batch 7804/11884 - Loss: 30.1873\n",
      "Processing batch 7805/11884 - Loss: 30.0147\n",
      "Processing batch 7806/11884 - Loss: 30.6474\n",
      "Processing batch 7807/11884 - Loss: 31.1920\n",
      "Processing batch 7808/11884 - Loss: 30.2584\n",
      "Processing batch 7809/11884 - Loss: 32.4505\n",
      "Processing batch 7810/11884 - Loss: 30.2641\n",
      "Processing batch 7811/11884 - Loss: 29.1965\n",
      "Processing batch 7812/11884 - Loss: 31.0766\n",
      "Processing batch 7813/11884 - Loss: 31.0517\n",
      "Processing batch 7814/11884 - Loss: 29.0215\n",
      "Processing batch 7815/11884 - Loss: 29.1392\n",
      "Processing batch 7816/11884 - Loss: 30.4422\n",
      "Processing batch 7817/11884 - Loss: 31.8316\n",
      "Processing batch 7818/11884 - Loss: 31.2517\n",
      "Processing batch 7819/11884 - Loss: 28.9699\n",
      "Processing batch 7820/11884 - Loss: 30.6816\n",
      "Processing batch 7821/11884 - Loss: 29.5558\n",
      "Processing batch 7822/11884 - Loss: 30.1986\n",
      "Processing batch 7823/11884 - Loss: 30.7405\n",
      "Processing batch 7824/11884 - Loss: 31.2427\n",
      "Processing batch 7825/11884 - Loss: 30.2532\n",
      "Processing batch 7826/11884 - Loss: 29.1006\n",
      "Processing batch 7827/11884 - Loss: 29.2545\n",
      "Processing batch 7828/11884 - Loss: 29.2656\n",
      "Processing batch 7829/11884 - Loss: 31.3044\n",
      "Processing batch 7830/11884 - Loss: 30.2017\n",
      "Processing batch 7831/11884 - Loss: 29.4387\n",
      "Processing batch 7832/11884 - Loss: 30.0519\n",
      "Processing batch 7833/11884 - Loss: 30.7492\n",
      "Processing batch 7834/11884 - Loss: 30.7234\n",
      "Processing batch 7835/11884 - Loss: 29.8618\n",
      "Processing batch 7836/11884 - Loss: 30.0868\n",
      "Processing batch 7837/11884 - Loss: 30.3916\n",
      "Processing batch 7838/11884 - Loss: 30.5829\n",
      "Processing batch 7839/11884 - Loss: 30.3767\n",
      "Processing batch 7840/11884 - Loss: 29.4409\n",
      "Processing batch 7841/11884 - Loss: 30.3075\n",
      "Processing batch 7842/11884 - Loss: 30.7337\n",
      "Processing batch 7843/11884 - Loss: 29.4637\n",
      "Processing batch 7844/11884 - Loss: 30.6719\n",
      "Processing batch 7845/11884 - Loss: 30.4108\n",
      "Processing batch 7846/11884 - Loss: 30.6357\n",
      "Processing batch 7847/11884 - Loss: 32.1218\n",
      "Processing batch 7848/11884 - Loss: 29.9585\n",
      "Processing batch 7849/11884 - Loss: 29.4188\n",
      "Processing batch 7850/11884 - Loss: 29.8046\n",
      "Processing batch 7851/11884 - Loss: 28.6503\n",
      "Processing batch 7852/11884 - Loss: 28.0485\n",
      "Processing batch 7853/11884 - Loss: 29.5459\n",
      "Processing batch 7854/11884 - Loss: 29.2331\n",
      "Processing batch 7855/11884 - Loss: 31.0760\n",
      "Processing batch 7856/11884 - Loss: 28.2058\n",
      "Processing batch 7857/11884 - Loss: 29.1296\n",
      "Processing batch 7858/11884 - Loss: 30.2698\n",
      "Processing batch 7859/11884 - Loss: 28.9601\n",
      "Processing batch 7860/11884 - Loss: 30.4149\n",
      "Processing batch 7861/11884 - Loss: 29.4261\n",
      "Processing batch 7862/11884 - Loss: 30.8400\n",
      "Processing batch 7863/11884 - Loss: 29.4662\n",
      "Processing batch 7864/11884 - Loss: 29.4258\n",
      "Processing batch 7865/11884 - Loss: 30.8044\n",
      "Processing batch 7866/11884 - Loss: 29.7231\n",
      "Processing batch 7867/11884 - Loss: 29.7970\n",
      "Processing batch 7868/11884 - Loss: 31.0309\n",
      "Processing batch 7869/11884 - Loss: 29.7963\n",
      "Processing batch 7870/11884 - Loss: 29.6516\n",
      "Processing batch 7871/11884 - Loss: 29.3320\n",
      "Processing batch 7872/11884 - Loss: 28.1414\n",
      "Processing batch 7873/11884 - Loss: 30.4827\n",
      "Processing batch 7874/11884 - Loss: 31.1941\n",
      "Processing batch 7875/11884 - Loss: 31.7980\n",
      "Processing batch 7876/11884 - Loss: 29.0241\n",
      "Processing batch 7877/11884 - Loss: 29.9270\n",
      "Processing batch 7878/11884 - Loss: 30.1096\n",
      "Processing batch 7879/11884 - Loss: 30.4849\n",
      "Processing batch 7880/11884 - Loss: 30.5407\n",
      "Processing batch 7881/11884 - Loss: 30.4042\n",
      "Processing batch 7882/11884 - Loss: 30.3946\n",
      "Processing batch 7883/11884 - Loss: 29.8516\n",
      "Processing batch 7884/11884 - Loss: 30.0426\n",
      "Processing batch 7885/11884 - Loss: 29.7021\n",
      "Processing batch 7886/11884 - Loss: 30.6065\n",
      "Processing batch 7887/11884 - Loss: 28.6113\n",
      "Processing batch 7888/11884 - Loss: 31.2205\n",
      "Processing batch 7889/11884 - Loss: 29.5224\n",
      "Processing batch 7890/11884 - Loss: 30.5567\n",
      "Processing batch 7891/11884 - Loss: 29.2987\n",
      "Processing batch 7892/11884 - Loss: 31.5858\n",
      "Processing batch 7893/11884 - Loss: 29.8149\n",
      "Processing batch 7894/11884 - Loss: 30.5194\n",
      "Processing batch 7895/11884 - Loss: 30.7955\n",
      "Processing batch 7896/11884 - Loss: 30.5360\n",
      "Processing batch 7897/11884 - Loss: 28.7576\n",
      "Processing batch 7898/11884 - Loss: 30.6815\n",
      "Processing batch 7899/11884 - Loss: 29.5364\n",
      "Processing batch 7900/11884 - Loss: 29.2987\n",
      "Processing batch 7901/11884 - Loss: 29.9355\n",
      "Processing batch 7902/11884 - Loss: 29.6086\n",
      "Processing batch 7903/11884 - Loss: 30.6775\n",
      "Processing batch 7904/11884 - Loss: 31.2680\n",
      "Processing batch 7905/11884 - Loss: 29.5105\n",
      "Processing batch 7906/11884 - Loss: 28.7890\n",
      "Processing batch 7907/11884 - Loss: 29.8109\n",
      "Processing batch 7908/11884 - Loss: 30.9843\n",
      "Processing batch 7909/11884 - Loss: 30.3737\n",
      "Processing batch 7910/11884 - Loss: 31.0114\n",
      "Processing batch 7911/11884 - Loss: 30.1015\n",
      "Processing batch 7912/11884 - Loss: 30.5612\n",
      "Processing batch 7913/11884 - Loss: 30.9160\n",
      "Processing batch 7914/11884 - Loss: 29.8991\n",
      "Processing batch 7915/11884 - Loss: 29.9851\n",
      "Processing batch 7916/11884 - Loss: 29.5288\n",
      "Processing batch 7917/11884 - Loss: 30.2112\n",
      "Processing batch 7918/11884 - Loss: 29.7076\n",
      "Processing batch 7919/11884 - Loss: 31.1499\n",
      "Processing batch 7920/11884 - Loss: 30.1255\n",
      "Processing batch 7921/11884 - Loss: 29.1558\n",
      "Processing batch 7922/11884 - Loss: 30.5394\n",
      "Processing batch 7923/11884 - Loss: 31.1766\n",
      "Processing batch 7924/11884 - Loss: 28.3211\n",
      "Processing batch 7925/11884 - Loss: 30.6028\n",
      "Processing batch 7926/11884 - Loss: 28.3307\n",
      "Processing batch 7927/11884 - Loss: 30.4533\n",
      "Processing batch 7928/11884 - Loss: 29.5528\n",
      "Processing batch 7929/11884 - Loss: 29.6934\n",
      "Processing batch 7930/11884 - Loss: 30.4837\n",
      "Processing batch 7931/11884 - Loss: 29.0942\n",
      "Processing batch 7932/11884 - Loss: 28.5829\n",
      "Processing batch 7933/11884 - Loss: 30.4514\n",
      "Processing batch 7934/11884 - Loss: 29.3682\n",
      "Processing batch 7935/11884 - Loss: 31.0491\n",
      "Processing batch 7936/11884 - Loss: 27.7040\n",
      "Processing batch 7937/11884 - Loss: 28.8447\n",
      "Processing batch 7938/11884 - Loss: 30.4355\n",
      "Processing batch 7939/11884 - Loss: 30.3060\n",
      "Processing batch 7940/11884 - Loss: 30.5573\n",
      "Processing batch 7941/11884 - Loss: 30.9215\n",
      "Processing batch 7942/11884 - Loss: 30.7025\n",
      "Processing batch 7943/11884 - Loss: 31.1932\n",
      "Processing batch 7944/11884 - Loss: 30.5889\n",
      "Processing batch 7945/11884 - Loss: 28.5652\n",
      "Processing batch 7946/11884 - Loss: 28.9386\n",
      "Processing batch 7947/11884 - Loss: 30.0498\n",
      "Processing batch 7948/11884 - Loss: 29.4506\n",
      "Processing batch 7949/11884 - Loss: 29.7316\n",
      "Processing batch 7950/11884 - Loss: 29.8496\n",
      "Processing batch 7951/11884 - Loss: 30.1341\n",
      "Processing batch 7952/11884 - Loss: 30.5259\n",
      "Processing batch 7953/11884 - Loss: 30.2912\n",
      "Processing batch 7954/11884 - Loss: 30.2665\n",
      "Processing batch 7955/11884 - Loss: 29.9387\n",
      "Processing batch 7956/11884 - Loss: 29.9403\n",
      "Processing batch 7957/11884 - Loss: 31.6692\n",
      "Processing batch 7958/11884 - Loss: 29.9366\n",
      "Processing batch 7959/11884 - Loss: 29.4611\n",
      "Processing batch 7960/11884 - Loss: 29.9484\n",
      "Processing batch 7961/11884 - Loss: 30.1524\n",
      "Processing batch 7962/11884 - Loss: 28.9114\n",
      "Processing batch 7963/11884 - Loss: 29.5705\n",
      "Processing batch 7964/11884 - Loss: 28.3417\n",
      "Processing batch 7965/11884 - Loss: 28.9998\n",
      "Processing batch 7966/11884 - Loss: 29.7472\n",
      "Processing batch 7967/11884 - Loss: 30.3386\n",
      "Processing batch 7968/11884 - Loss: 30.7768\n",
      "Processing batch 7969/11884 - Loss: 30.4902\n",
      "Processing batch 7970/11884 - Loss: 30.6951\n",
      "Processing batch 7971/11884 - Loss: 29.0549\n",
      "Processing batch 7972/11884 - Loss: 29.7974\n",
      "Processing batch 7973/11884 - Loss: 30.3172\n",
      "Processing batch 7974/11884 - Loss: 29.5470\n",
      "Processing batch 7975/11884 - Loss: 29.7083\n",
      "Processing batch 7976/11884 - Loss: 30.8181\n",
      "Processing batch 7977/11884 - Loss: 30.1352\n",
      "Processing batch 7978/11884 - Loss: 30.4196\n",
      "Processing batch 7979/11884 - Loss: 29.3402\n",
      "Processing batch 7980/11884 - Loss: 30.9143\n",
      "Processing batch 7981/11884 - Loss: 30.4979\n",
      "Processing batch 7982/11884 - Loss: 29.8031\n",
      "Processing batch 7983/11884 - Loss: 30.7423\n",
      "Processing batch 7984/11884 - Loss: 29.6380\n",
      "Processing batch 7985/11884 - Loss: 31.5216\n",
      "Processing batch 7986/11884 - Loss: 30.0965\n",
      "Processing batch 7987/11884 - Loss: 30.0753\n",
      "Processing batch 7988/11884 - Loss: 31.0131\n",
      "Processing batch 7989/11884 - Loss: 30.3164\n",
      "Processing batch 7990/11884 - Loss: 29.8499\n",
      "Processing batch 7991/11884 - Loss: 30.5526\n",
      "Processing batch 7992/11884 - Loss: 29.4792\n",
      "Processing batch 7993/11884 - Loss: 29.4829\n",
      "Processing batch 7994/11884 - Loss: 30.6162\n",
      "Processing batch 7995/11884 - Loss: 30.5361\n",
      "Processing batch 7996/11884 - Loss: 31.3711\n",
      "Processing batch 7997/11884 - Loss: 29.7654\n",
      "Processing batch 7998/11884 - Loss: 31.1525\n",
      "Processing batch 7999/11884 - Loss: 29.9259\n",
      "Processing batch 8000/11884 - Loss: 28.6919\n",
      "Processing batch 8001/11884 - Loss: 31.1908\n",
      "Processing batch 8002/11884 - Loss: 30.8378\n",
      "Processing batch 8003/11884 - Loss: 29.7288\n",
      "Processing batch 8004/11884 - Loss: 31.2107\n",
      "Processing batch 8005/11884 - Loss: 30.3958\n",
      "Processing batch 8006/11884 - Loss: 30.4889\n",
      "Processing batch 8007/11884 - Loss: 29.3689\n",
      "Processing batch 8008/11884 - Loss: 29.3185\n",
      "Processing batch 8009/11884 - Loss: 30.4354\n",
      "Processing batch 8010/11884 - Loss: 30.8258\n",
      "Processing batch 8011/11884 - Loss: 30.2532\n",
      "Processing batch 8012/11884 - Loss: 29.7790\n",
      "Processing batch 8013/11884 - Loss: 30.4916\n",
      "Processing batch 8014/11884 - Loss: 29.3772\n",
      "Processing batch 8015/11884 - Loss: 30.2880\n",
      "Processing batch 8016/11884 - Loss: 29.9199\n",
      "Processing batch 8017/11884 - Loss: 30.2169\n",
      "Processing batch 8018/11884 - Loss: 29.8818\n",
      "Processing batch 8019/11884 - Loss: 30.0831\n",
      "Processing batch 8020/11884 - Loss: 31.0877\n",
      "Processing batch 8021/11884 - Loss: 29.0878\n",
      "Processing batch 8022/11884 - Loss: 30.3769\n",
      "Processing batch 8023/11884 - Loss: 29.7595\n",
      "Processing batch 8024/11884 - Loss: 29.9294\n",
      "Processing batch 8025/11884 - Loss: 30.2458\n",
      "Processing batch 8026/11884 - Loss: 29.7963\n",
      "Processing batch 8027/11884 - Loss: 29.4423\n",
      "Processing batch 8028/11884 - Loss: 29.7508\n",
      "Processing batch 8029/11884 - Loss: 30.9541\n",
      "Processing batch 8030/11884 - Loss: 29.7977\n",
      "Processing batch 8031/11884 - Loss: 30.2163\n",
      "Processing batch 8032/11884 - Loss: 31.0106\n",
      "Processing batch 8033/11884 - Loss: 30.2187\n",
      "Processing batch 8034/11884 - Loss: 30.4259\n",
      "Processing batch 8035/11884 - Loss: 29.2769\n",
      "Processing batch 8036/11884 - Loss: 30.7317\n",
      "Processing batch 8037/11884 - Loss: 30.5312\n",
      "Processing batch 8038/11884 - Loss: 30.9922\n",
      "Processing batch 8039/11884 - Loss: 29.5016\n",
      "Processing batch 8040/11884 - Loss: 29.0762\n",
      "Processing batch 8041/11884 - Loss: 29.7621\n",
      "Processing batch 8042/11884 - Loss: 29.6070\n",
      "Processing batch 8043/11884 - Loss: 29.6724\n",
      "Processing batch 8044/11884 - Loss: 29.8907\n",
      "Processing batch 8045/11884 - Loss: 30.3765\n",
      "Processing batch 8046/11884 - Loss: 30.2401\n",
      "Processing batch 8047/11884 - Loss: 31.4146\n",
      "Processing batch 8048/11884 - Loss: 30.4343\n",
      "Processing batch 8049/11884 - Loss: 29.8240\n",
      "Processing batch 8050/11884 - Loss: 29.8371\n",
      "Processing batch 8051/11884 - Loss: 29.8743\n",
      "Processing batch 8052/11884 - Loss: 30.5131\n",
      "Processing batch 8053/11884 - Loss: 30.7001\n",
      "Processing batch 8054/11884 - Loss: 29.9610\n",
      "Processing batch 8055/11884 - Loss: 30.0953\n",
      "Processing batch 8056/11884 - Loss: 32.0878\n",
      "Processing batch 8057/11884 - Loss: 29.6968\n",
      "Processing batch 8058/11884 - Loss: 29.8918\n",
      "Processing batch 8059/11884 - Loss: 28.9284\n",
      "Processing batch 8060/11884 - Loss: 30.1906\n",
      "Processing batch 8061/11884 - Loss: 29.1682\n",
      "Processing batch 8062/11884 - Loss: 30.3287\n",
      "Processing batch 8063/11884 - Loss: 29.0199\n",
      "Processing batch 8064/11884 - Loss: 29.5130\n",
      "Processing batch 8065/11884 - Loss: 30.3085\n",
      "Processing batch 8066/11884 - Loss: 29.1860\n",
      "Processing batch 8067/11884 - Loss: 29.5446\n",
      "Processing batch 8068/11884 - Loss: 29.0768\n",
      "Processing batch 8069/11884 - Loss: 30.2782\n",
      "Processing batch 8070/11884 - Loss: 29.9157\n",
      "Processing batch 8071/11884 - Loss: 31.3892\n",
      "Processing batch 8072/11884 - Loss: 29.6003\n",
      "Processing batch 8073/11884 - Loss: 28.7219\n",
      "Processing batch 8074/11884 - Loss: 31.0503\n",
      "Processing batch 8075/11884 - Loss: 30.7371\n",
      "Processing batch 8076/11884 - Loss: 30.1839\n",
      "Processing batch 8077/11884 - Loss: 30.5335\n",
      "Processing batch 8078/11884 - Loss: 30.3094\n",
      "Processing batch 8079/11884 - Loss: 29.8024\n",
      "Processing batch 8080/11884 - Loss: 30.4099\n",
      "Processing batch 8081/11884 - Loss: 31.1533\n",
      "Processing batch 8082/11884 - Loss: 29.6838\n",
      "Processing batch 8083/11884 - Loss: 29.7668\n",
      "Processing batch 8084/11884 - Loss: 29.2607\n",
      "Processing batch 8085/11884 - Loss: 30.4588\n",
      "Processing batch 8086/11884 - Loss: 31.1665\n",
      "Processing batch 8087/11884 - Loss: 31.4533\n",
      "Processing batch 8088/11884 - Loss: 29.3409\n",
      "Processing batch 8089/11884 - Loss: 29.9139\n",
      "Processing batch 8090/11884 - Loss: 29.9315\n",
      "Processing batch 8091/11884 - Loss: 30.1166\n",
      "Processing batch 8092/11884 - Loss: 31.0311\n",
      "Processing batch 8093/11884 - Loss: 29.9236\n",
      "Processing batch 8094/11884 - Loss: 30.0813\n",
      "Processing batch 8095/11884 - Loss: 30.5636\n",
      "Processing batch 8096/11884 - Loss: 29.3742\n",
      "Processing batch 8097/11884 - Loss: 28.9314\n",
      "Processing batch 8098/11884 - Loss: 29.5308\n",
      "Processing batch 8099/11884 - Loss: 29.8851\n",
      "Processing batch 8100/11884 - Loss: 31.4434\n",
      "Processing batch 8101/11884 - Loss: 29.7413\n",
      "Processing batch 8102/11884 - Loss: 30.8578\n",
      "Processing batch 8103/11884 - Loss: 30.1223\n",
      "Processing batch 8104/11884 - Loss: 30.0707\n",
      "Processing batch 8105/11884 - Loss: 30.3697\n",
      "Processing batch 8106/11884 - Loss: 28.3092\n",
      "Processing batch 8107/11884 - Loss: 30.4333\n",
      "Processing batch 8108/11884 - Loss: 30.3344\n",
      "Processing batch 8109/11884 - Loss: 29.3958\n",
      "Processing batch 8110/11884 - Loss: 29.8600\n",
      "Processing batch 8111/11884 - Loss: 28.9564\n",
      "Processing batch 8112/11884 - Loss: 29.7545\n",
      "Processing batch 8113/11884 - Loss: 30.1766\n",
      "Processing batch 8114/11884 - Loss: 30.9433\n",
      "Processing batch 8115/11884 - Loss: 29.3210\n",
      "Processing batch 8116/11884 - Loss: 30.5251\n",
      "Processing batch 8117/11884 - Loss: 29.6338\n",
      "Processing batch 8118/11884 - Loss: 30.0482\n",
      "Processing batch 8119/11884 - Loss: 29.3013\n",
      "Processing batch 8120/11884 - Loss: 30.8459\n",
      "Processing batch 8121/11884 - Loss: 29.9605\n",
      "Processing batch 8122/11884 - Loss: 29.8413\n",
      "Processing batch 8123/11884 - Loss: 28.8903\n",
      "Processing batch 8124/11884 - Loss: 31.0055\n",
      "Processing batch 8125/11884 - Loss: 30.7191\n",
      "Processing batch 8126/11884 - Loss: 30.6202\n",
      "Processing batch 8127/11884 - Loss: 29.0126\n",
      "Processing batch 8128/11884 - Loss: 29.2408\n",
      "Processing batch 8129/11884 - Loss: 30.7649\n",
      "Processing batch 8130/11884 - Loss: 30.7231\n",
      "Processing batch 8131/11884 - Loss: 29.6031\n",
      "Processing batch 8132/11884 - Loss: 29.3380\n",
      "Processing batch 8133/11884 - Loss: 30.9193\n",
      "Processing batch 8134/11884 - Loss: 29.8279\n",
      "Processing batch 8135/11884 - Loss: 29.1080\n",
      "Processing batch 8136/11884 - Loss: 30.0018\n",
      "Processing batch 8137/11884 - Loss: 30.6893\n",
      "Processing batch 8138/11884 - Loss: 31.0191\n",
      "Processing batch 8139/11884 - Loss: 29.6482\n",
      "Processing batch 8140/11884 - Loss: 30.7627\n",
      "Processing batch 8141/11884 - Loss: 29.4372\n",
      "Processing batch 8142/11884 - Loss: 29.8262\n",
      "Processing batch 8143/11884 - Loss: 30.4067\n",
      "Processing batch 8144/11884 - Loss: 29.5725\n",
      "Processing batch 8145/11884 - Loss: 30.0184\n",
      "Processing batch 8146/11884 - Loss: 29.6725\n",
      "Processing batch 8147/11884 - Loss: 29.5306\n",
      "Processing batch 8148/11884 - Loss: 29.9889\n",
      "Processing batch 8149/11884 - Loss: 31.4924\n",
      "Processing batch 8150/11884 - Loss: 29.8631\n",
      "Processing batch 8151/11884 - Loss: 30.0534\n",
      "Processing batch 8152/11884 - Loss: 30.5501\n",
      "Processing batch 8153/11884 - Loss: 31.1648\n",
      "Processing batch 8154/11884 - Loss: 29.8340\n",
      "Processing batch 8155/11884 - Loss: 29.6874\n",
      "Processing batch 8156/11884 - Loss: 29.4086\n",
      "Processing batch 8157/11884 - Loss: 30.9350\n",
      "Processing batch 8158/11884 - Loss: 30.3535\n",
      "Processing batch 8159/11884 - Loss: 31.6878\n",
      "Processing batch 8160/11884 - Loss: 30.4166\n",
      "Processing batch 8161/11884 - Loss: 30.9640\n",
      "Processing batch 8162/11884 - Loss: 30.1115\n",
      "Processing batch 8163/11884 - Loss: 30.4290\n",
      "Processing batch 8164/11884 - Loss: 30.4935\n",
      "Processing batch 8165/11884 - Loss: 31.5977\n",
      "Processing batch 8166/11884 - Loss: 29.7909\n",
      "Processing batch 8167/11884 - Loss: 30.1855\n",
      "Processing batch 8168/11884 - Loss: 29.6916\n",
      "Processing batch 8169/11884 - Loss: 30.9159\n",
      "Processing batch 8170/11884 - Loss: 29.7614\n",
      "Processing batch 8171/11884 - Loss: 29.6925\n",
      "Processing batch 8172/11884 - Loss: 32.0981\n",
      "Processing batch 8173/11884 - Loss: 29.2019\n",
      "Processing batch 8174/11884 - Loss: 28.5378\n",
      "Processing batch 8175/11884 - Loss: 29.8396\n",
      "Processing batch 8176/11884 - Loss: 30.8640\n",
      "Processing batch 8177/11884 - Loss: 30.1877\n",
      "Processing batch 8178/11884 - Loss: 29.5451\n",
      "Processing batch 8179/11884 - Loss: 29.4865\n",
      "Processing batch 8180/11884 - Loss: 31.0795\n",
      "Processing batch 8181/11884 - Loss: 33.0646\n",
      "Processing batch 8182/11884 - Loss: 31.3125\n",
      "Processing batch 8183/11884 - Loss: 30.1545\n",
      "Processing batch 8184/11884 - Loss: 30.4982\n",
      "Processing batch 8185/11884 - Loss: 30.5364\n",
      "Processing batch 8186/11884 - Loss: 31.0795\n",
      "Processing batch 8187/11884 - Loss: 29.6520\n",
      "Processing batch 8188/11884 - Loss: 28.6608\n",
      "Processing batch 8189/11884 - Loss: 30.0597\n",
      "Processing batch 8190/11884 - Loss: 30.1778\n",
      "Processing batch 8191/11884 - Loss: 30.6881\n",
      "Processing batch 8192/11884 - Loss: 30.0183\n",
      "Processing batch 8193/11884 - Loss: 30.4366\n",
      "Processing batch 8194/11884 - Loss: 30.5394\n",
      "Processing batch 8195/11884 - Loss: 29.1144\n",
      "Processing batch 8196/11884 - Loss: 30.8269\n",
      "Processing batch 8197/11884 - Loss: 30.1981\n",
      "Processing batch 8198/11884 - Loss: 30.5550\n",
      "Processing batch 8199/11884 - Loss: 30.4046\n",
      "Processing batch 8200/11884 - Loss: 29.7421\n",
      "Processing batch 8201/11884 - Loss: 29.0637\n",
      "Processing batch 8202/11884 - Loss: 29.9936\n",
      "Processing batch 8203/11884 - Loss: 29.1627\n",
      "Processing batch 8204/11884 - Loss: 30.0507\n",
      "Processing batch 8205/11884 - Loss: 29.5979\n",
      "Processing batch 8206/11884 - Loss: 29.6806\n",
      "Processing batch 8207/11884 - Loss: 29.4582\n",
      "Processing batch 8208/11884 - Loss: 29.3895\n",
      "Processing batch 8209/11884 - Loss: 31.2596\n",
      "Processing batch 8210/11884 - Loss: 31.0782\n",
      "Processing batch 8211/11884 - Loss: 29.5897\n",
      "Processing batch 8212/11884 - Loss: 28.8724\n",
      "Processing batch 8213/11884 - Loss: 31.4413\n",
      "Processing batch 8214/11884 - Loss: 29.1487\n",
      "Processing batch 8215/11884 - Loss: 30.4755\n",
      "Processing batch 8216/11884 - Loss: 29.9538\n",
      "Processing batch 8217/11884 - Loss: 30.1185\n",
      "Processing batch 8218/11884 - Loss: 31.2791\n",
      "Processing batch 8219/11884 - Loss: 29.8097\n",
      "Processing batch 8220/11884 - Loss: 30.5599\n",
      "Processing batch 8221/11884 - Loss: 30.0178\n",
      "Processing batch 8222/11884 - Loss: 29.4999\n",
      "Processing batch 8223/11884 - Loss: 29.3930\n",
      "Processing batch 8224/11884 - Loss: 30.1288\n",
      "Processing batch 8225/11884 - Loss: 31.4883\n",
      "Processing batch 8226/11884 - Loss: 29.9618\n",
      "Processing batch 8227/11884 - Loss: 31.0395\n",
      "Processing batch 8228/11884 - Loss: 30.3831\n",
      "Processing batch 8229/11884 - Loss: 29.8798\n",
      "Processing batch 8230/11884 - Loss: 30.6071\n",
      "Processing batch 8231/11884 - Loss: 30.4966\n",
      "Processing batch 8232/11884 - Loss: 30.4270\n",
      "Processing batch 8233/11884 - Loss: 28.1383\n",
      "Processing batch 8234/11884 - Loss: 30.3538\n",
      "Processing batch 8235/11884 - Loss: 29.5484\n",
      "Processing batch 8236/11884 - Loss: 29.7138\n",
      "Processing batch 8237/11884 - Loss: 28.5023\n",
      "Processing batch 8238/11884 - Loss: 29.1712\n",
      "Processing batch 8239/11884 - Loss: 30.6047\n",
      "Processing batch 8240/11884 - Loss: 30.0456\n",
      "Processing batch 8241/11884 - Loss: 29.9625\n",
      "Processing batch 8242/11884 - Loss: 29.9000\n",
      "Processing batch 8243/11884 - Loss: 30.7268\n",
      "Processing batch 8244/11884 - Loss: 29.7273\n",
      "Processing batch 8245/11884 - Loss: 29.7780\n",
      "Processing batch 8246/11884 - Loss: 29.8026\n",
      "Processing batch 8247/11884 - Loss: 29.8640\n",
      "Processing batch 8248/11884 - Loss: 30.7340\n",
      "Processing batch 8249/11884 - Loss: 29.9936\n",
      "Processing batch 8250/11884 - Loss: 29.2162\n",
      "Processing batch 8251/11884 - Loss: 31.4555\n",
      "Processing batch 8252/11884 - Loss: 29.6653\n",
      "Processing batch 8253/11884 - Loss: 30.5950\n",
      "Processing batch 8254/11884 - Loss: 30.1976\n",
      "Processing batch 8255/11884 - Loss: 31.9018\n",
      "Processing batch 8256/11884 - Loss: 30.1919\n",
      "Processing batch 8257/11884 - Loss: 30.4357\n",
      "Processing batch 8258/11884 - Loss: 29.4983\n",
      "Processing batch 8259/11884 - Loss: 30.5281\n",
      "Processing batch 8260/11884 - Loss: 29.9722\n",
      "Processing batch 8261/11884 - Loss: 31.0678\n",
      "Processing batch 8262/11884 - Loss: 29.8945\n",
      "Processing batch 8263/11884 - Loss: 30.5413\n",
      "Processing batch 8264/11884 - Loss: 29.3985\n",
      "Processing batch 8265/11884 - Loss: 29.0789\n",
      "Processing batch 8266/11884 - Loss: 30.9023\n",
      "Processing batch 8267/11884 - Loss: 29.7237\n",
      "Processing batch 8268/11884 - Loss: 30.9895\n",
      "Processing batch 8269/11884 - Loss: 30.7631\n",
      "Processing batch 8270/11884 - Loss: 29.5054\n",
      "Processing batch 8271/11884 - Loss: 30.4784\n",
      "Processing batch 8272/11884 - Loss: 29.4636\n",
      "Processing batch 8273/11884 - Loss: 30.8321\n",
      "Processing batch 8274/11884 - Loss: 30.0474\n",
      "Processing batch 8275/11884 - Loss: 29.1480\n",
      "Processing batch 8276/11884 - Loss: 30.2996\n",
      "Processing batch 8277/11884 - Loss: 30.8494\n",
      "Processing batch 8278/11884 - Loss: 30.2304\n",
      "Processing batch 8279/11884 - Loss: 31.2482\n",
      "Processing batch 8280/11884 - Loss: 30.7199\n",
      "Processing batch 8281/11884 - Loss: 29.3859\n",
      "Processing batch 8282/11884 - Loss: 30.7731\n",
      "Processing batch 8283/11884 - Loss: 28.6113\n",
      "Processing batch 8284/11884 - Loss: 29.3329\n",
      "Processing batch 8285/11884 - Loss: 30.4575\n",
      "Processing batch 8286/11884 - Loss: 29.9118\n",
      "Processing batch 8287/11884 - Loss: 29.8693\n",
      "Processing batch 8288/11884 - Loss: 30.9784\n",
      "Processing batch 8289/11884 - Loss: 30.3648\n",
      "Processing batch 8290/11884 - Loss: 29.4339\n",
      "Processing batch 8291/11884 - Loss: 29.3913\n",
      "Processing batch 8292/11884 - Loss: 28.9518\n",
      "Processing batch 8293/11884 - Loss: 30.2387\n",
      "Processing batch 8294/11884 - Loss: 29.6562\n",
      "Processing batch 8295/11884 - Loss: 31.1151\n",
      "Processing batch 8296/11884 - Loss: 30.3579\n",
      "Processing batch 8297/11884 - Loss: 28.7679\n",
      "Processing batch 8298/11884 - Loss: 29.9040\n",
      "Processing batch 8299/11884 - Loss: 29.3599\n",
      "Processing batch 8300/11884 - Loss: 30.7140\n",
      "Processing batch 8301/11884 - Loss: 29.6224\n",
      "Processing batch 8302/11884 - Loss: 30.3074\n",
      "Processing batch 8303/11884 - Loss: 29.9047\n",
      "Processing batch 8304/11884 - Loss: 30.5439\n",
      "Processing batch 8305/11884 - Loss: 30.3495\n",
      "Processing batch 8306/11884 - Loss: 31.1560\n",
      "Processing batch 8307/11884 - Loss: 31.5853\n",
      "Processing batch 8308/11884 - Loss: 30.0910\n",
      "Processing batch 8309/11884 - Loss: 28.9901\n",
      "Processing batch 8310/11884 - Loss: 31.5039\n",
      "Processing batch 8311/11884 - Loss: 29.7240\n",
      "Processing batch 8312/11884 - Loss: 29.9208\n",
      "Processing batch 8313/11884 - Loss: 30.0708\n",
      "Processing batch 8314/11884 - Loss: 31.7494\n",
      "Processing batch 8315/11884 - Loss: 30.2657\n",
      "Processing batch 8316/11884 - Loss: 30.3909\n",
      "Processing batch 8317/11884 - Loss: 29.8746\n",
      "Processing batch 8318/11884 - Loss: 30.7542\n",
      "Processing batch 8319/11884 - Loss: 31.0183\n",
      "Processing batch 8320/11884 - Loss: 29.7941\n",
      "Processing batch 8321/11884 - Loss: 29.3096\n",
      "Processing batch 8322/11884 - Loss: 28.7985\n",
      "Processing batch 8323/11884 - Loss: 29.8972\n",
      "Processing batch 8324/11884 - Loss: 30.5632\n",
      "Processing batch 8325/11884 - Loss: 28.2062\n",
      "Processing batch 8326/11884 - Loss: 29.1763\n",
      "Processing batch 8327/11884 - Loss: 30.0271\n",
      "Processing batch 8328/11884 - Loss: 28.8187\n",
      "Processing batch 8329/11884 - Loss: 29.8949\n",
      "Processing batch 8330/11884 - Loss: 30.1131\n",
      "Processing batch 8331/11884 - Loss: 29.3780\n",
      "Processing batch 8332/11884 - Loss: 30.3096\n",
      "Processing batch 8333/11884 - Loss: 29.0797\n",
      "Processing batch 8334/11884 - Loss: 29.5190\n",
      "Processing batch 8335/11884 - Loss: 29.9411\n",
      "Processing batch 8336/11884 - Loss: 30.4465\n",
      "Processing batch 8337/11884 - Loss: 30.5537\n",
      "Processing batch 8338/11884 - Loss: 29.9307\n",
      "Processing batch 8339/11884 - Loss: 29.9818\n",
      "Processing batch 8340/11884 - Loss: 28.2024\n",
      "Processing batch 8341/11884 - Loss: 30.1555\n",
      "Processing batch 8342/11884 - Loss: 30.4127\n",
      "Processing batch 8343/11884 - Loss: 28.7695\n",
      "Processing batch 8344/11884 - Loss: 29.2847\n",
      "Processing batch 8345/11884 - Loss: 31.0216\n",
      "Processing batch 8346/11884 - Loss: 29.8852\n",
      "Processing batch 8347/11884 - Loss: 31.3490\n",
      "Processing batch 8348/11884 - Loss: 30.0824\n",
      "Processing batch 8349/11884 - Loss: 31.3437\n",
      "Processing batch 8350/11884 - Loss: 29.9319\n",
      "Processing batch 8351/11884 - Loss: 29.8637\n",
      "Processing batch 8352/11884 - Loss: 29.3148\n",
      "Processing batch 8353/11884 - Loss: 29.2379\n",
      "Processing batch 8354/11884 - Loss: 30.9298\n",
      "Processing batch 8355/11884 - Loss: 29.6283\n",
      "Processing batch 8356/11884 - Loss: 30.5553\n",
      "Processing batch 8357/11884 - Loss: 31.0770\n",
      "Processing batch 8358/11884 - Loss: 32.0125\n",
      "Processing batch 8359/11884 - Loss: 29.5911\n",
      "Processing batch 8360/11884 - Loss: 29.0962\n",
      "Processing batch 8361/11884 - Loss: 30.0184\n",
      "Processing batch 8362/11884 - Loss: 30.0747\n",
      "Processing batch 8363/11884 - Loss: 30.1155\n",
      "Processing batch 8364/11884 - Loss: 28.9298\n",
      "Processing batch 8365/11884 - Loss: 30.2471\n",
      "Processing batch 8366/11884 - Loss: 30.2237\n",
      "Processing batch 8367/11884 - Loss: 29.9267\n",
      "Processing batch 8368/11884 - Loss: 29.7370\n",
      "Processing batch 8369/11884 - Loss: 29.6852\n",
      "Processing batch 8370/11884 - Loss: 30.4690\n",
      "Processing batch 8371/11884 - Loss: 29.8405\n",
      "Processing batch 8372/11884 - Loss: 30.0417\n",
      "Processing batch 8373/11884 - Loss: 29.7625\n",
      "Processing batch 8374/11884 - Loss: 31.7701\n",
      "Processing batch 8375/11884 - Loss: 30.2171\n",
      "Processing batch 8376/11884 - Loss: 30.2160\n",
      "Processing batch 8377/11884 - Loss: 31.9220\n",
      "Processing batch 8378/11884 - Loss: 29.5900\n",
      "Processing batch 8379/11884 - Loss: 29.5359\n",
      "Processing batch 8380/11884 - Loss: 29.3853\n",
      "Processing batch 8381/11884 - Loss: 29.5915\n",
      "Processing batch 8382/11884 - Loss: 30.7530\n",
      "Processing batch 8383/11884 - Loss: 29.9030\n",
      "Processing batch 8384/11884 - Loss: 29.3124\n",
      "Processing batch 8385/11884 - Loss: 28.5326\n",
      "Processing batch 8386/11884 - Loss: 29.0176\n",
      "Processing batch 8387/11884 - Loss: 29.5060\n",
      "Processing batch 8388/11884 - Loss: 28.0918\n",
      "Processing batch 8389/11884 - Loss: 29.0672\n",
      "Processing batch 8390/11884 - Loss: 29.6227\n",
      "Processing batch 8391/11884 - Loss: 29.8187\n",
      "Processing batch 8392/11884 - Loss: 29.7068\n",
      "Processing batch 8393/11884 - Loss: 29.5592\n",
      "Processing batch 8394/11884 - Loss: 29.2478\n",
      "Processing batch 8395/11884 - Loss: 30.5180\n",
      "Processing batch 8396/11884 - Loss: 28.9537\n",
      "Processing batch 8397/11884 - Loss: 30.4718\n",
      "Processing batch 8398/11884 - Loss: 29.6369\n",
      "Processing batch 8399/11884 - Loss: 30.2309\n",
      "Processing batch 8400/11884 - Loss: 30.1471\n",
      "Processing batch 8401/11884 - Loss: 31.2574\n",
      "Processing batch 8402/11884 - Loss: 31.6220\n",
      "Processing batch 8403/11884 - Loss: 31.4563\n",
      "Processing batch 8404/11884 - Loss: 29.8229\n",
      "Processing batch 8405/11884 - Loss: 32.2156\n",
      "Processing batch 8406/11884 - Loss: 29.4684\n",
      "Processing batch 8407/11884 - Loss: 31.0970\n",
      "Processing batch 8408/11884 - Loss: 30.2884\n",
      "Processing batch 8409/11884 - Loss: 30.2078\n",
      "Processing batch 8410/11884 - Loss: 29.7356\n",
      "Processing batch 8411/11884 - Loss: 31.0291\n",
      "Processing batch 8412/11884 - Loss: 30.1865\n",
      "Processing batch 8413/11884 - Loss: 29.1130\n",
      "Processing batch 8414/11884 - Loss: 30.8773\n",
      "Processing batch 8415/11884 - Loss: 30.3490\n",
      "Processing batch 8416/11884 - Loss: 30.8771\n",
      "Processing batch 8417/11884 - Loss: 29.7318\n",
      "Processing batch 8418/11884 - Loss: 29.9229\n",
      "Processing batch 8419/11884 - Loss: 31.0164\n",
      "Processing batch 8420/11884 - Loss: 30.8492\n",
      "Processing batch 8421/11884 - Loss: 31.3258\n",
      "Processing batch 8422/11884 - Loss: 28.6789\n",
      "Processing batch 8423/11884 - Loss: 30.3838\n",
      "Processing batch 8424/11884 - Loss: 29.1486\n",
      "Processing batch 8425/11884 - Loss: 29.5940\n",
      "Processing batch 8426/11884 - Loss: 30.6224\n",
      "Processing batch 8427/11884 - Loss: 29.3502\n",
      "Processing batch 8428/11884 - Loss: 28.8775\n",
      "Processing batch 8429/11884 - Loss: 30.4258\n",
      "Processing batch 8430/11884 - Loss: 30.6178\n",
      "Processing batch 8431/11884 - Loss: 30.1487\n",
      "Processing batch 8432/11884 - Loss: 29.2644\n",
      "Processing batch 8433/11884 - Loss: 29.2847\n",
      "Processing batch 8434/11884 - Loss: 30.0686\n",
      "Processing batch 8435/11884 - Loss: 30.8389\n",
      "Processing batch 8436/11884 - Loss: 29.9835\n",
      "Processing batch 8437/11884 - Loss: 29.3183\n",
      "Processing batch 8438/11884 - Loss: 30.0202\n",
      "Processing batch 8439/11884 - Loss: 29.2787\n",
      "Processing batch 8440/11884 - Loss: 29.9710\n",
      "Processing batch 8441/11884 - Loss: 29.4643\n",
      "Processing batch 8442/11884 - Loss: 30.6750\n",
      "Processing batch 8443/11884 - Loss: 29.9173\n",
      "Processing batch 8444/11884 - Loss: 31.0853\n",
      "Processing batch 8445/11884 - Loss: 30.2313\n",
      "Processing batch 8446/11884 - Loss: 30.6691\n",
      "Processing batch 8447/11884 - Loss: 30.6051\n",
      "Processing batch 8448/11884 - Loss: 30.2859\n",
      "Processing batch 8449/11884 - Loss: 29.6807\n",
      "Processing batch 8450/11884 - Loss: 30.4510\n",
      "Processing batch 8451/11884 - Loss: 31.0022\n",
      "Processing batch 8452/11884 - Loss: 30.0064\n",
      "Processing batch 8453/11884 - Loss: 29.8765\n",
      "Processing batch 8454/11884 - Loss: 28.8841\n",
      "Processing batch 8455/11884 - Loss: 30.8324\n",
      "Processing batch 8456/11884 - Loss: 30.6554\n",
      "Processing batch 8457/11884 - Loss: 29.4229\n",
      "Processing batch 8458/11884 - Loss: 29.2059\n",
      "Processing batch 8459/11884 - Loss: 31.0319\n",
      "Processing batch 8460/11884 - Loss: 28.8434\n",
      "Processing batch 8461/11884 - Loss: 31.3601\n",
      "Processing batch 8462/11884 - Loss: 30.9498\n",
      "Processing batch 8463/11884 - Loss: 29.6623\n",
      "Processing batch 8464/11884 - Loss: 31.3123\n",
      "Processing batch 8465/11884 - Loss: 30.6440\n",
      "Processing batch 8466/11884 - Loss: 29.6645\n",
      "Processing batch 8467/11884 - Loss: 31.2122\n",
      "Processing batch 8468/11884 - Loss: 30.5793\n",
      "Processing batch 8469/11884 - Loss: 28.3148\n",
      "Processing batch 8470/11884 - Loss: 29.7611\n",
      "Processing batch 8471/11884 - Loss: 28.8602\n",
      "Processing batch 8472/11884 - Loss: 30.0275\n",
      "Processing batch 8473/11884 - Loss: 30.1165\n",
      "Processing batch 8474/11884 - Loss: 30.1325\n",
      "Processing batch 8475/11884 - Loss: 30.2427\n",
      "Processing batch 8476/11884 - Loss: 30.0433\n",
      "Processing batch 8477/11884 - Loss: 29.7803\n",
      "Processing batch 8478/11884 - Loss: 29.6554\n",
      "Processing batch 8479/11884 - Loss: 30.5136\n",
      "Processing batch 8480/11884 - Loss: 28.9035\n",
      "Processing batch 8481/11884 - Loss: 31.7702\n",
      "Processing batch 8482/11884 - Loss: 30.6200\n",
      "Processing batch 8483/11884 - Loss: 30.7468\n",
      "Processing batch 8484/11884 - Loss: 29.1594\n",
      "Processing batch 8485/11884 - Loss: 29.8105\n",
      "Processing batch 8486/11884 - Loss: 29.6029\n",
      "Processing batch 8487/11884 - Loss: 30.4093\n",
      "Processing batch 8488/11884 - Loss: 30.6117\n",
      "Processing batch 8489/11884 - Loss: 31.5368\n",
      "Processing batch 8490/11884 - Loss: 29.8146\n",
      "Processing batch 8491/11884 - Loss: 30.4050\n",
      "Processing batch 8492/11884 - Loss: 30.2387\n",
      "Processing batch 8493/11884 - Loss: 30.7136\n",
      "Processing batch 8494/11884 - Loss: 29.4572\n",
      "Processing batch 8495/11884 - Loss: 30.1465\n",
      "Processing batch 8496/11884 - Loss: 31.2882\n",
      "Processing batch 8497/11884 - Loss: 29.9480\n",
      "Processing batch 8498/11884 - Loss: 30.7082\n",
      "Processing batch 8499/11884 - Loss: 30.4209\n",
      "Processing batch 8500/11884 - Loss: 31.0171\n",
      "Processing batch 8501/11884 - Loss: 30.4620\n",
      "Processing batch 8502/11884 - Loss: 29.7486\n",
      "Processing batch 8503/11884 - Loss: 29.9508\n",
      "Processing batch 8504/11884 - Loss: 30.0853\n",
      "Processing batch 8505/11884 - Loss: 30.1764\n",
      "Processing batch 8506/11884 - Loss: 28.5208\n",
      "Processing batch 8507/11884 - Loss: 30.7620\n",
      "Processing batch 8508/11884 - Loss: 30.7522\n",
      "Processing batch 8509/11884 - Loss: 29.7983\n",
      "Processing batch 8510/11884 - Loss: 32.1985\n",
      "Processing batch 8511/11884 - Loss: 29.3078\n",
      "Processing batch 8512/11884 - Loss: 30.1600\n",
      "Processing batch 8513/11884 - Loss: 31.3874\n",
      "Processing batch 8514/11884 - Loss: 28.4594\n",
      "Processing batch 8515/11884 - Loss: 29.2771\n",
      "Processing batch 8516/11884 - Loss: 30.4153\n",
      "Processing batch 8517/11884 - Loss: 29.4061\n",
      "Processing batch 8518/11884 - Loss: 29.4688\n",
      "Processing batch 8519/11884 - Loss: 30.8282\n",
      "Processing batch 8520/11884 - Loss: 29.6669\n",
      "Processing batch 8521/11884 - Loss: 29.8448\n",
      "Processing batch 8522/11884 - Loss: 29.9431\n",
      "Processing batch 8523/11884 - Loss: 29.9274\n",
      "Processing batch 8524/11884 - Loss: 29.7254\n",
      "Processing batch 8525/11884 - Loss: 30.7892\n",
      "Processing batch 8526/11884 - Loss: 28.7131\n",
      "Processing batch 8527/11884 - Loss: 29.8646\n",
      "Processing batch 8528/11884 - Loss: 30.8783\n",
      "Processing batch 8529/11884 - Loss: 30.5578\n",
      "Processing batch 8530/11884 - Loss: 29.5014\n",
      "Processing batch 8531/11884 - Loss: 30.0351\n",
      "Processing batch 8532/11884 - Loss: 30.8559\n",
      "Processing batch 8533/11884 - Loss: 28.7367\n",
      "Processing batch 8534/11884 - Loss: 30.3425\n",
      "Processing batch 8535/11884 - Loss: 30.6059\n",
      "Processing batch 8536/11884 - Loss: 29.4134\n",
      "Processing batch 8537/11884 - Loss: 30.8244\n",
      "Processing batch 8538/11884 - Loss: 30.2114\n",
      "Processing batch 8539/11884 - Loss: 29.4238\n",
      "Processing batch 8540/11884 - Loss: 29.6821\n",
      "Processing batch 8541/11884 - Loss: 30.5463\n",
      "Processing batch 8542/11884 - Loss: 28.7132\n",
      "Processing batch 8543/11884 - Loss: 30.6637\n",
      "Processing batch 8544/11884 - Loss: 30.6040\n",
      "Processing batch 8545/11884 - Loss: 29.9261\n",
      "Processing batch 8546/11884 - Loss: 30.0386\n",
      "Processing batch 8547/11884 - Loss: 29.3431\n",
      "Processing batch 8548/11884 - Loss: 31.6793\n",
      "Processing batch 8549/11884 - Loss: 30.4499\n",
      "Processing batch 8550/11884 - Loss: 30.1978\n",
      "Processing batch 8551/11884 - Loss: 29.8646\n",
      "Processing batch 8552/11884 - Loss: 29.1752\n",
      "Processing batch 8553/11884 - Loss: 29.5780\n",
      "Processing batch 8554/11884 - Loss: 29.2546\n",
      "Processing batch 8555/11884 - Loss: 30.5423\n",
      "Processing batch 8556/11884 - Loss: 30.2180\n",
      "Processing batch 8557/11884 - Loss: 31.8763\n",
      "Processing batch 8558/11884 - Loss: 28.9256\n",
      "Processing batch 8559/11884 - Loss: 30.0664\n",
      "Processing batch 8560/11884 - Loss: 29.9156\n",
      "Processing batch 8561/11884 - Loss: 29.8664\n",
      "Processing batch 8562/11884 - Loss: 30.6574\n",
      "Processing batch 8563/11884 - Loss: 29.7417\n",
      "Processing batch 8564/11884 - Loss: 29.4391\n",
      "Processing batch 8565/11884 - Loss: 29.5173\n",
      "Processing batch 8566/11884 - Loss: 30.7631\n",
      "Processing batch 8567/11884 - Loss: 30.5389\n",
      "Processing batch 8568/11884 - Loss: 29.9091\n",
      "Processing batch 8569/11884 - Loss: 30.0033\n",
      "Processing batch 8570/11884 - Loss: 31.5355\n",
      "Processing batch 8571/11884 - Loss: 29.7302\n",
      "Processing batch 8572/11884 - Loss: 28.1952\n",
      "Processing batch 8573/11884 - Loss: 30.6998\n",
      "Processing batch 8574/11884 - Loss: 30.4013\n",
      "Processing batch 8575/11884 - Loss: 30.6106\n",
      "Processing batch 8576/11884 - Loss: 31.0642\n",
      "Processing batch 8577/11884 - Loss: 29.1126\n",
      "Processing batch 8578/11884 - Loss: 30.5600\n",
      "Processing batch 8579/11884 - Loss: 29.6306\n",
      "Processing batch 8580/11884 - Loss: 31.3838\n",
      "Processing batch 8581/11884 - Loss: 30.1300\n",
      "Processing batch 8582/11884 - Loss: 31.0860\n",
      "Processing batch 8583/11884 - Loss: 30.3587\n",
      "Processing batch 8584/11884 - Loss: 30.8161\n",
      "Processing batch 8585/11884 - Loss: 31.3860\n",
      "Processing batch 8586/11884 - Loss: 29.4746\n",
      "Processing batch 8587/11884 - Loss: 30.0331\n",
      "Processing batch 8588/11884 - Loss: 29.0103\n",
      "Processing batch 8589/11884 - Loss: 29.9385\n",
      "Processing batch 8590/11884 - Loss: 30.0502\n",
      "Processing batch 8591/11884 - Loss: 29.9174\n",
      "Processing batch 8592/11884 - Loss: 30.0112\n",
      "Processing batch 8593/11884 - Loss: 28.9930\n",
      "Processing batch 8594/11884 - Loss: 30.2502\n",
      "Processing batch 8595/11884 - Loss: 29.2163\n",
      "Processing batch 8596/11884 - Loss: 29.9687\n",
      "Processing batch 8597/11884 - Loss: 30.9891\n",
      "Processing batch 8598/11884 - Loss: 30.4613\n",
      "Processing batch 8599/11884 - Loss: 29.9461\n",
      "Processing batch 8600/11884 - Loss: 30.8476\n",
      "Processing batch 8601/11884 - Loss: 30.6932\n",
      "Processing batch 8602/11884 - Loss: 30.0573\n",
      "Processing batch 8603/11884 - Loss: 30.0174\n",
      "Processing batch 8604/11884 - Loss: 29.6214\n",
      "Processing batch 8605/11884 - Loss: 27.9670\n",
      "Processing batch 8606/11884 - Loss: 29.9931\n",
      "Processing batch 8607/11884 - Loss: 30.5393\n",
      "Processing batch 8608/11884 - Loss: 28.8328\n",
      "Processing batch 8609/11884 - Loss: 31.2776\n",
      "Processing batch 8610/11884 - Loss: 29.1533\n",
      "Processing batch 8611/11884 - Loss: 30.3810\n",
      "Processing batch 8612/11884 - Loss: 29.7404\n",
      "Processing batch 8613/11884 - Loss: 29.8961\n",
      "Processing batch 8614/11884 - Loss: 30.5132\n",
      "Processing batch 8615/11884 - Loss: 31.3356\n",
      "Processing batch 8616/11884 - Loss: 29.0457\n",
      "Processing batch 8617/11884 - Loss: 30.0810\n",
      "Processing batch 8618/11884 - Loss: 29.4841\n",
      "Processing batch 8619/11884 - Loss: 29.4113\n",
      "Processing batch 8620/11884 - Loss: 29.0279\n",
      "Processing batch 8621/11884 - Loss: 28.6520\n",
      "Processing batch 8622/11884 - Loss: 29.8131\n",
      "Processing batch 8623/11884 - Loss: 30.1348\n",
      "Processing batch 8624/11884 - Loss: 28.9210\n",
      "Processing batch 8625/11884 - Loss: 30.1523\n",
      "Processing batch 8626/11884 - Loss: 29.8084\n",
      "Processing batch 8627/11884 - Loss: 29.8940\n",
      "Processing batch 8628/11884 - Loss: 30.8857\n",
      "Processing batch 8629/11884 - Loss: 29.8608\n",
      "Processing batch 8630/11884 - Loss: 29.7969\n",
      "Processing batch 8631/11884 - Loss: 30.8218\n",
      "Processing batch 8632/11884 - Loss: 29.1950\n",
      "Processing batch 8633/11884 - Loss: 30.0288\n",
      "Processing batch 8634/11884 - Loss: 30.0831\n",
      "Processing batch 8635/11884 - Loss: 29.6453\n",
      "Processing batch 8636/11884 - Loss: 31.0302\n",
      "Processing batch 8637/11884 - Loss: 30.1481\n",
      "Processing batch 8638/11884 - Loss: 29.8426\n",
      "Processing batch 8639/11884 - Loss: 29.7307\n",
      "Processing batch 8640/11884 - Loss: 29.7059\n",
      "Processing batch 8641/11884 - Loss: 30.9623\n",
      "Processing batch 8642/11884 - Loss: 30.1966\n",
      "Processing batch 8643/11884 - Loss: 30.7598\n",
      "Processing batch 8644/11884 - Loss: 31.4723\n",
      "Processing batch 8645/11884 - Loss: 30.0642\n",
      "Processing batch 8646/11884 - Loss: 30.0269\n",
      "Processing batch 8647/11884 - Loss: 30.2101\n",
      "Processing batch 8648/11884 - Loss: 30.5701\n",
      "Processing batch 8649/11884 - Loss: 29.0622\n",
      "Processing batch 8650/11884 - Loss: 29.4047\n",
      "Processing batch 8651/11884 - Loss: 29.3499\n",
      "Processing batch 8652/11884 - Loss: 29.7220\n",
      "Processing batch 8653/11884 - Loss: 31.5228\n",
      "Processing batch 8654/11884 - Loss: 29.6096\n",
      "Processing batch 8655/11884 - Loss: 30.8972\n",
      "Processing batch 8656/11884 - Loss: 29.5386\n",
      "Processing batch 8657/11884 - Loss: 30.8369\n",
      "Processing batch 8658/11884 - Loss: 30.1221\n",
      "Processing batch 8659/11884 - Loss: 29.4282\n",
      "Processing batch 8660/11884 - Loss: 31.0269\n",
      "Processing batch 8661/11884 - Loss: 29.7319\n",
      "Processing batch 8662/11884 - Loss: 28.5285\n",
      "Processing batch 8663/11884 - Loss: 29.1032\n",
      "Processing batch 8664/11884 - Loss: 30.3624\n",
      "Processing batch 8665/11884 - Loss: 29.8852\n",
      "Processing batch 8666/11884 - Loss: 30.2768\n",
      "Processing batch 8667/11884 - Loss: 31.6392\n",
      "Processing batch 8668/11884 - Loss: 30.5489\n",
      "Processing batch 8669/11884 - Loss: 29.9381\n",
      "Processing batch 8670/11884 - Loss: 28.8149\n",
      "Processing batch 8671/11884 - Loss: 30.4584\n",
      "Processing batch 8672/11884 - Loss: 31.0468\n",
      "Processing batch 8673/11884 - Loss: 31.2891\n",
      "Processing batch 8674/11884 - Loss: 30.7669\n",
      "Processing batch 8675/11884 - Loss: 31.3621\n",
      "Processing batch 8676/11884 - Loss: 29.5043\n",
      "Processing batch 8677/11884 - Loss: 30.8678\n",
      "Processing batch 8678/11884 - Loss: 30.5401\n",
      "Processing batch 8679/11884 - Loss: 29.5925\n",
      "Processing batch 8680/11884 - Loss: 30.0834\n",
      "Processing batch 8681/11884 - Loss: 30.7217\n",
      "Processing batch 8682/11884 - Loss: 29.9839\n",
      "Processing batch 8683/11884 - Loss: 30.4805\n",
      "Processing batch 8684/11884 - Loss: 30.5978\n",
      "Processing batch 8685/11884 - Loss: 29.2843\n",
      "Processing batch 8686/11884 - Loss: 29.3215\n",
      "Processing batch 8687/11884 - Loss: 29.5821\n",
      "Processing batch 8688/11884 - Loss: 30.6981\n",
      "Processing batch 8689/11884 - Loss: 30.5833\n",
      "Processing batch 8690/11884 - Loss: 30.9591\n",
      "Processing batch 8691/11884 - Loss: 30.3387\n",
      "Processing batch 8692/11884 - Loss: 27.8345\n",
      "Processing batch 8693/11884 - Loss: 30.0853\n",
      "Processing batch 8694/11884 - Loss: 30.9896\n",
      "Processing batch 8695/11884 - Loss: 30.8384\n",
      "Processing batch 8696/11884 - Loss: 29.8318\n",
      "Processing batch 8697/11884 - Loss: 30.1217\n",
      "Processing batch 8698/11884 - Loss: 30.4085\n",
      "Processing batch 8699/11884 - Loss: 29.7476\n",
      "Processing batch 8700/11884 - Loss: 31.2131\n",
      "Processing batch 8701/11884 - Loss: 30.5930\n",
      "Processing batch 8702/11884 - Loss: 29.0367\n",
      "Processing batch 8703/11884 - Loss: 31.2020\n",
      "Processing batch 8704/11884 - Loss: 29.8379\n",
      "Processing batch 8705/11884 - Loss: 30.8037\n",
      "Processing batch 8706/11884 - Loss: 29.5929\n",
      "Processing batch 8707/11884 - Loss: 30.0909\n",
      "Processing batch 8708/11884 - Loss: 28.4422\n",
      "Processing batch 8709/11884 - Loss: 29.1484\n",
      "Processing batch 8710/11884 - Loss: 29.9216\n",
      "Processing batch 8711/11884 - Loss: 29.6875\n",
      "Processing batch 8712/11884 - Loss: 30.6915\n",
      "Processing batch 8713/11884 - Loss: 30.8590\n",
      "Processing batch 8714/11884 - Loss: 30.3624\n",
      "Processing batch 8715/11884 - Loss: 30.9448\n",
      "Processing batch 8716/11884 - Loss: 31.0621\n",
      "Processing batch 8717/11884 - Loss: 29.8368\n",
      "Processing batch 8718/11884 - Loss: 29.6390\n",
      "Processing batch 8719/11884 - Loss: 30.3767\n",
      "Processing batch 8720/11884 - Loss: 29.4302\n",
      "Processing batch 8721/11884 - Loss: 29.5996\n",
      "Processing batch 8722/11884 - Loss: 30.3494\n",
      "Processing batch 8723/11884 - Loss: 30.0305\n",
      "Processing batch 8724/11884 - Loss: 28.0403\n",
      "Processing batch 8725/11884 - Loss: 28.1341\n",
      "Processing batch 8726/11884 - Loss: 30.0077\n",
      "Processing batch 8727/11884 - Loss: 30.0131\n",
      "Processing batch 8728/11884 - Loss: 28.7638\n",
      "Processing batch 8729/11884 - Loss: 30.2063\n",
      "Processing batch 8730/11884 - Loss: 30.9759\n",
      "Processing batch 8731/11884 - Loss: 30.0844\n",
      "Processing batch 8732/11884 - Loss: 29.6529\n",
      "Processing batch 8733/11884 - Loss: 30.4313\n",
      "Processing batch 8734/11884 - Loss: 30.5088\n",
      "Processing batch 8735/11884 - Loss: 30.0990\n",
      "Processing batch 8736/11884 - Loss: 29.5669\n",
      "Processing batch 8737/11884 - Loss: 29.4765\n",
      "Processing batch 8738/11884 - Loss: 30.1385\n",
      "Processing batch 8739/11884 - Loss: 29.8711\n",
      "Processing batch 8740/11884 - Loss: 29.5566\n",
      "Processing batch 8741/11884 - Loss: 29.6315\n",
      "Processing batch 8742/11884 - Loss: 28.9886\n",
      "Processing batch 8743/11884 - Loss: 29.6406\n",
      "Processing batch 8744/11884 - Loss: 29.7615\n",
      "Processing batch 8745/11884 - Loss: 29.9729\n",
      "Processing batch 8746/11884 - Loss: 31.2580\n",
      "Processing batch 8747/11884 - Loss: 29.9797\n",
      "Processing batch 8748/11884 - Loss: 31.4457\n",
      "Processing batch 8749/11884 - Loss: 30.8257\n",
      "Processing batch 8750/11884 - Loss: 29.7094\n",
      "Processing batch 8751/11884 - Loss: 30.2154\n",
      "Processing batch 8752/11884 - Loss: 30.6322\n",
      "Processing batch 8753/11884 - Loss: 30.5725\n",
      "Processing batch 8754/11884 - Loss: 30.1847\n",
      "Processing batch 8755/11884 - Loss: 30.1846\n",
      "Processing batch 8756/11884 - Loss: 30.4692\n",
      "Processing batch 8757/11884 - Loss: 30.0236\n",
      "Processing batch 8758/11884 - Loss: 30.9047\n",
      "Processing batch 8759/11884 - Loss: 29.4395\n",
      "Processing batch 8760/11884 - Loss: 30.5217\n",
      "Processing batch 8761/11884 - Loss: 31.3480\n",
      "Processing batch 8762/11884 - Loss: 29.8470\n",
      "Processing batch 8763/11884 - Loss: 28.9942\n",
      "Processing batch 8764/11884 - Loss: 29.4838\n",
      "Processing batch 8765/11884 - Loss: 28.6971\n",
      "Processing batch 8766/11884 - Loss: 30.6153\n",
      "Processing batch 8767/11884 - Loss: 29.3699\n",
      "Processing batch 8768/11884 - Loss: 30.0331\n",
      "Processing batch 8769/11884 - Loss: 29.7909\n",
      "Processing batch 8770/11884 - Loss: 30.0678\n",
      "Processing batch 8771/11884 - Loss: 30.7859\n",
      "Processing batch 8772/11884 - Loss: 29.8252\n",
      "Processing batch 8773/11884 - Loss: 29.1049\n",
      "Processing batch 8774/11884 - Loss: 29.8379\n",
      "Processing batch 8775/11884 - Loss: 29.5109\n",
      "Processing batch 8776/11884 - Loss: 30.0234\n",
      "Processing batch 8777/11884 - Loss: 29.4455\n",
      "Processing batch 8778/11884 - Loss: 29.3420\n",
      "Processing batch 8779/11884 - Loss: 29.3329\n",
      "Processing batch 8780/11884 - Loss: 30.6466\n",
      "Processing batch 8781/11884 - Loss: 29.9150\n",
      "Processing batch 8782/11884 - Loss: 29.1573\n",
      "Processing batch 8783/11884 - Loss: 29.1747\n",
      "Processing batch 8784/11884 - Loss: 31.1648\n",
      "Processing batch 8785/11884 - Loss: 30.4764\n",
      "Processing batch 8786/11884 - Loss: 29.4806\n",
      "Processing batch 8787/11884 - Loss: 30.9659\n",
      "Processing batch 8788/11884 - Loss: 30.0309\n",
      "Processing batch 8789/11884 - Loss: 31.1627\n",
      "Processing batch 8790/11884 - Loss: 30.8098\n",
      "Processing batch 8791/11884 - Loss: 27.5897\n",
      "Processing batch 8792/11884 - Loss: 30.3392\n",
      "Processing batch 8793/11884 - Loss: 29.5969\n",
      "Processing batch 8794/11884 - Loss: 30.3697\n",
      "Processing batch 8795/11884 - Loss: 29.3809\n",
      "Processing batch 8796/11884 - Loss: 29.7565\n",
      "Processing batch 8797/11884 - Loss: 28.9109\n",
      "Processing batch 8798/11884 - Loss: 30.0946\n",
      "Processing batch 8799/11884 - Loss: 31.6306\n",
      "Processing batch 8800/11884 - Loss: 31.0752\n",
      "Processing batch 8801/11884 - Loss: 30.6375\n",
      "Processing batch 8802/11884 - Loss: 30.2724\n",
      "Processing batch 8803/11884 - Loss: 30.5537\n",
      "Processing batch 8804/11884 - Loss: 29.3119\n",
      "Processing batch 8805/11884 - Loss: 29.9430\n",
      "Processing batch 8806/11884 - Loss: 29.3014\n",
      "Processing batch 8807/11884 - Loss: 30.8680\n",
      "Processing batch 8808/11884 - Loss: 29.6246\n",
      "Processing batch 8809/11884 - Loss: 29.9578\n",
      "Processing batch 8810/11884 - Loss: 30.3560\n",
      "Processing batch 8811/11884 - Loss: 29.9446\n",
      "Processing batch 8812/11884 - Loss: 32.6288\n",
      "Processing batch 8813/11884 - Loss: 30.5072\n",
      "Processing batch 8814/11884 - Loss: 30.3780\n",
      "Processing batch 8815/11884 - Loss: 32.5654\n",
      "Processing batch 8816/11884 - Loss: 31.1111\n",
      "Processing batch 8817/11884 - Loss: 28.9652\n",
      "Processing batch 8818/11884 - Loss: 29.6849\n",
      "Processing batch 8819/11884 - Loss: 29.5839\n",
      "Processing batch 8820/11884 - Loss: 29.4517\n",
      "Processing batch 8821/11884 - Loss: 28.9602\n",
      "Processing batch 8822/11884 - Loss: 30.2358\n",
      "Processing batch 8823/11884 - Loss: 30.7928\n",
      "Processing batch 8824/11884 - Loss: 32.0539\n",
      "Processing batch 8825/11884 - Loss: 28.7318\n",
      "Processing batch 8826/11884 - Loss: 29.9221\n",
      "Processing batch 8827/11884 - Loss: 29.2938\n",
      "Processing batch 8828/11884 - Loss: 29.7652\n",
      "Processing batch 8829/11884 - Loss: 28.1682\n",
      "Processing batch 8830/11884 - Loss: 29.0509\n",
      "Processing batch 8831/11884 - Loss: 29.5534\n",
      "Processing batch 8832/11884 - Loss: 30.3338\n",
      "Processing batch 8833/11884 - Loss: 29.5581\n",
      "Processing batch 8834/11884 - Loss: 30.6615\n",
      "Processing batch 8835/11884 - Loss: 30.3831\n",
      "Processing batch 8836/11884 - Loss: 30.0934\n",
      "Processing batch 8837/11884 - Loss: 29.4240\n",
      "Processing batch 8838/11884 - Loss: 30.2247\n",
      "Processing batch 8839/11884 - Loss: 31.1579\n",
      "Processing batch 8840/11884 - Loss: 30.0621\n",
      "Processing batch 8841/11884 - Loss: 31.1246\n",
      "Processing batch 8842/11884 - Loss: 28.9606\n",
      "Processing batch 8843/11884 - Loss: 30.2215\n",
      "Processing batch 8844/11884 - Loss: 29.5454\n",
      "Processing batch 8845/11884 - Loss: 30.6664\n",
      "Processing batch 8846/11884 - Loss: 30.0178\n",
      "Processing batch 8847/11884 - Loss: 30.9400\n",
      "Processing batch 8848/11884 - Loss: 31.0627\n",
      "Processing batch 8849/11884 - Loss: 29.4906\n",
      "Processing batch 8850/11884 - Loss: 30.6057\n",
      "Processing batch 8851/11884 - Loss: 30.9593\n",
      "Processing batch 8852/11884 - Loss: 30.6624\n",
      "Processing batch 8853/11884 - Loss: 29.7778\n",
      "Processing batch 8854/11884 - Loss: 30.7192\n",
      "Processing batch 8855/11884 - Loss: 28.6578\n",
      "Processing batch 8856/11884 - Loss: 30.2405\n",
      "Processing batch 8857/11884 - Loss: 29.8592\n",
      "Processing batch 8858/11884 - Loss: 30.0244\n",
      "Processing batch 8859/11884 - Loss: 29.0189\n",
      "Processing batch 8860/11884 - Loss: 29.8287\n",
      "Processing batch 8861/11884 - Loss: 29.9556\n",
      "Processing batch 8862/11884 - Loss: 30.5041\n",
      "Processing batch 8863/11884 - Loss: 29.5659\n",
      "Processing batch 8864/11884 - Loss: 30.8615\n",
      "Processing batch 8865/11884 - Loss: 30.2324\n",
      "Processing batch 8866/11884 - Loss: 28.4472\n",
      "Processing batch 8867/11884 - Loss: 28.8291\n",
      "Processing batch 8868/11884 - Loss: 29.9130\n",
      "Processing batch 8869/11884 - Loss: 30.5499\n",
      "Processing batch 8870/11884 - Loss: 30.7072\n",
      "Processing batch 8871/11884 - Loss: 30.3208\n",
      "Processing batch 8872/11884 - Loss: 30.0914\n",
      "Processing batch 8873/11884 - Loss: 28.6197\n",
      "Processing batch 8874/11884 - Loss: 30.0669\n",
      "Processing batch 8875/11884 - Loss: 30.3512\n",
      "Processing batch 8876/11884 - Loss: 30.2814\n",
      "Processing batch 8877/11884 - Loss: 30.1573\n",
      "Processing batch 8878/11884 - Loss: 29.0679\n",
      "Processing batch 8879/11884 - Loss: 28.8870\n",
      "Processing batch 8880/11884 - Loss: 27.6641\n",
      "Processing batch 8881/11884 - Loss: 29.6353\n",
      "Processing batch 8882/11884 - Loss: 29.5966\n",
      "Processing batch 8883/11884 - Loss: 29.9731\n",
      "Processing batch 8884/11884 - Loss: 30.2645\n",
      "Processing batch 8885/11884 - Loss: 28.0624\n",
      "Processing batch 8886/11884 - Loss: 29.2989\n",
      "Processing batch 8887/11884 - Loss: 30.1991\n",
      "Processing batch 8888/11884 - Loss: 30.1348\n",
      "Processing batch 8889/11884 - Loss: 31.2425\n",
      "Processing batch 8890/11884 - Loss: 29.7326\n",
      "Processing batch 8891/11884 - Loss: 29.5775\n",
      "Processing batch 8892/11884 - Loss: 30.6052\n",
      "Processing batch 8893/11884 - Loss: 28.9905\n",
      "Processing batch 8894/11884 - Loss: 29.1128\n",
      "Processing batch 8895/11884 - Loss: 29.5350\n",
      "Processing batch 8896/11884 - Loss: 31.7809\n",
      "Processing batch 8897/11884 - Loss: 29.8232\n",
      "Processing batch 8898/11884 - Loss: 29.6917\n",
      "Processing batch 8899/11884 - Loss: 28.2949\n",
      "Processing batch 8900/11884 - Loss: 30.0398\n",
      "Processing batch 8901/11884 - Loss: 28.8714\n",
      "Processing batch 8902/11884 - Loss: 29.9104\n",
      "Processing batch 8903/11884 - Loss: 30.7164\n",
      "Processing batch 8904/11884 - Loss: 29.9906\n",
      "Processing batch 8905/11884 - Loss: 30.5815\n",
      "Processing batch 8906/11884 - Loss: 31.5920\n",
      "Processing batch 8907/11884 - Loss: 30.3298\n",
      "Processing batch 8908/11884 - Loss: 28.3144\n",
      "Processing batch 8909/11884 - Loss: 30.1030\n",
      "Processing batch 8910/11884 - Loss: 29.6659\n",
      "Processing batch 8911/11884 - Loss: 29.3641\n",
      "Processing batch 8912/11884 - Loss: 30.8276\n",
      "Processing batch 8913/11884 - Loss: 30.2296\n",
      "Processing batch 8914/11884 - Loss: 30.3633\n",
      "Processing batch 8915/11884 - Loss: 30.3572\n",
      "Processing batch 8916/11884 - Loss: 28.9980\n",
      "Processing batch 8917/11884 - Loss: 29.4286\n",
      "Processing batch 8918/11884 - Loss: 29.5975\n",
      "Processing batch 8919/11884 - Loss: 29.4217\n",
      "Processing batch 8920/11884 - Loss: 29.4240\n",
      "Processing batch 8921/11884 - Loss: 30.3861\n",
      "Processing batch 8922/11884 - Loss: 29.4778\n",
      "Processing batch 8923/11884 - Loss: 29.3443\n",
      "Processing batch 8924/11884 - Loss: 29.9787\n",
      "Processing batch 8925/11884 - Loss: 30.2089\n",
      "Processing batch 8926/11884 - Loss: 30.3764\n",
      "Processing batch 8927/11884 - Loss: 30.9443\n",
      "Processing batch 8928/11884 - Loss: 29.7494\n",
      "Processing batch 8929/11884 - Loss: 29.1434\n",
      "Processing batch 8930/11884 - Loss: 31.1351\n",
      "Processing batch 8931/11884 - Loss: 30.2373\n",
      "Processing batch 8932/11884 - Loss: 31.7970\n",
      "Processing batch 8933/11884 - Loss: 31.5567\n",
      "Processing batch 8934/11884 - Loss: 30.3521\n",
      "Processing batch 8935/11884 - Loss: 30.9763\n",
      "Processing batch 8936/11884 - Loss: 30.3214\n",
      "Processing batch 8937/11884 - Loss: 31.2190\n",
      "Processing batch 8938/11884 - Loss: 30.5974\n",
      "Processing batch 8939/11884 - Loss: 29.7265\n",
      "Processing batch 8940/11884 - Loss: 31.1236\n",
      "Processing batch 8941/11884 - Loss: 28.2726\n",
      "Processing batch 8942/11884 - Loss: 29.5102\n",
      "Processing batch 8943/11884 - Loss: 29.9792\n",
      "Processing batch 8944/11884 - Loss: 31.5899\n",
      "Processing batch 8945/11884 - Loss: 30.5637\n",
      "Processing batch 8946/11884 - Loss: 29.4107\n",
      "Processing batch 8947/11884 - Loss: 29.3994\n",
      "Processing batch 8948/11884 - Loss: 29.4791\n",
      "Processing batch 8949/11884 - Loss: 30.4249\n",
      "Processing batch 8950/11884 - Loss: 30.2248\n",
      "Processing batch 8951/11884 - Loss: 31.3080\n",
      "Processing batch 8952/11884 - Loss: 31.3036\n",
      "Processing batch 8953/11884 - Loss: 29.8973\n",
      "Processing batch 8954/11884 - Loss: 30.2609\n",
      "Processing batch 8955/11884 - Loss: 30.2406\n",
      "Processing batch 8956/11884 - Loss: 30.3301\n",
      "Processing batch 8957/11884 - Loss: 29.6963\n",
      "Processing batch 8958/11884 - Loss: 29.0105\n",
      "Processing batch 8959/11884 - Loss: 29.0184\n",
      "Processing batch 8960/11884 - Loss: 31.7027\n",
      "Processing batch 8961/11884 - Loss: 30.7293\n",
      "Processing batch 8962/11884 - Loss: 29.9355\n",
      "Processing batch 8963/11884 - Loss: 29.2689\n",
      "Processing batch 8964/11884 - Loss: 30.5909\n",
      "Processing batch 8965/11884 - Loss: 30.8296\n",
      "Processing batch 8966/11884 - Loss: 28.5000\n",
      "Processing batch 8967/11884 - Loss: 29.0564\n",
      "Processing batch 8968/11884 - Loss: 29.3603\n",
      "Processing batch 8969/11884 - Loss: 30.2431\n",
      "Processing batch 8970/11884 - Loss: 31.4257\n",
      "Processing batch 8971/11884 - Loss: 30.2160\n",
      "Processing batch 8972/11884 - Loss: 30.5384\n",
      "Processing batch 8973/11884 - Loss: 30.3237\n",
      "Processing batch 8974/11884 - Loss: 29.1989\n",
      "Processing batch 8975/11884 - Loss: 29.5601\n",
      "Processing batch 8976/11884 - Loss: 29.8318\n",
      "Processing batch 8977/11884 - Loss: 31.0721\n",
      "Processing batch 8978/11884 - Loss: 30.6155\n",
      "Processing batch 8979/11884 - Loss: 29.5235\n",
      "Processing batch 8980/11884 - Loss: 29.6997\n",
      "Processing batch 8981/11884 - Loss: 29.1757\n",
      "Processing batch 8982/11884 - Loss: 30.2917\n",
      "Processing batch 8983/11884 - Loss: 30.6465\n",
      "Processing batch 8984/11884 - Loss: 30.8145\n",
      "Processing batch 8985/11884 - Loss: 29.8278\n",
      "Processing batch 8986/11884 - Loss: 30.5268\n",
      "Processing batch 8987/11884 - Loss: 29.0592\n",
      "Processing batch 8988/11884 - Loss: 30.3447\n",
      "Processing batch 8989/11884 - Loss: 30.2943\n",
      "Processing batch 8990/11884 - Loss: 30.2207\n",
      "Processing batch 8991/11884 - Loss: 29.4587\n",
      "Processing batch 8992/11884 - Loss: 31.6128\n",
      "Processing batch 8993/11884 - Loss: 30.2003\n",
      "Processing batch 8994/11884 - Loss: 29.9918\n",
      "Processing batch 8995/11884 - Loss: 29.6184\n",
      "Processing batch 8996/11884 - Loss: 29.9354\n",
      "Processing batch 8997/11884 - Loss: 29.6759\n",
      "Processing batch 8998/11884 - Loss: 30.1684\n",
      "Processing batch 8999/11884 - Loss: 29.0406\n",
      "Processing batch 9000/11884 - Loss: 28.7575\n",
      "Processing batch 9001/11884 - Loss: 29.8951\n",
      "Processing batch 9002/11884 - Loss: 30.0573\n",
      "Processing batch 9003/11884 - Loss: 29.0236\n",
      "Processing batch 9004/11884 - Loss: 30.1576\n",
      "Processing batch 9005/11884 - Loss: 30.0060\n",
      "Processing batch 9006/11884 - Loss: 30.6502\n",
      "Processing batch 9007/11884 - Loss: 30.7670\n",
      "Processing batch 9008/11884 - Loss: 28.2405\n",
      "Processing batch 9009/11884 - Loss: 28.6351\n",
      "Processing batch 9010/11884 - Loss: 29.8409\n",
      "Processing batch 9011/11884 - Loss: 30.2542\n",
      "Processing batch 9012/11884 - Loss: 29.8242\n",
      "Processing batch 9013/11884 - Loss: 29.1676\n",
      "Processing batch 9014/11884 - Loss: 30.5201\n",
      "Processing batch 9015/11884 - Loss: 30.0288\n",
      "Processing batch 9016/11884 - Loss: 30.8652\n",
      "Processing batch 9017/11884 - Loss: 29.9947\n",
      "Processing batch 9018/11884 - Loss: 29.2826\n",
      "Processing batch 9019/11884 - Loss: 30.2203\n",
      "Processing batch 9020/11884 - Loss: 29.5927\n",
      "Processing batch 9021/11884 - Loss: 31.2964\n",
      "Processing batch 9022/11884 - Loss: 30.3636\n",
      "Processing batch 9023/11884 - Loss: 30.2696\n",
      "Processing batch 9024/11884 - Loss: 30.0706\n",
      "Processing batch 9025/11884 - Loss: 31.1806\n",
      "Processing batch 9026/11884 - Loss: 31.6215\n",
      "Processing batch 9027/11884 - Loss: 29.6056\n",
      "Processing batch 9028/11884 - Loss: 31.0764\n",
      "Processing batch 9029/11884 - Loss: 30.2896\n",
      "Processing batch 9030/11884 - Loss: 30.0069\n",
      "Processing batch 9031/11884 - Loss: 30.0838\n",
      "Processing batch 9032/11884 - Loss: 30.1903\n",
      "Processing batch 9033/11884 - Loss: 29.7471\n",
      "Processing batch 9034/11884 - Loss: 30.1307\n",
      "Processing batch 9035/11884 - Loss: 29.7496\n",
      "Processing batch 9036/11884 - Loss: 29.7200\n",
      "Processing batch 9037/11884 - Loss: 31.3244\n",
      "Processing batch 9038/11884 - Loss: 31.7786\n",
      "Processing batch 9039/11884 - Loss: 30.6438\n",
      "Processing batch 9040/11884 - Loss: 29.2646\n",
      "Processing batch 9041/11884 - Loss: 30.0101\n",
      "Processing batch 9042/11884 - Loss: 31.4659\n",
      "Processing batch 9043/11884 - Loss: 30.4217\n",
      "Processing batch 9044/11884 - Loss: 30.4875\n",
      "Processing batch 9045/11884 - Loss: 29.8795\n",
      "Processing batch 9046/11884 - Loss: 29.9552\n",
      "Processing batch 9047/11884 - Loss: 29.8806\n",
      "Processing batch 9048/11884 - Loss: 30.4078\n",
      "Processing batch 9049/11884 - Loss: 29.5569\n",
      "Processing batch 9050/11884 - Loss: 29.4695\n",
      "Processing batch 9051/11884 - Loss: 30.9356\n",
      "Processing batch 9052/11884 - Loss: 30.1892\n",
      "Processing batch 9053/11884 - Loss: 29.6357\n",
      "Processing batch 9054/11884 - Loss: 31.0483\n",
      "Processing batch 9055/11884 - Loss: 29.4661\n",
      "Processing batch 9056/11884 - Loss: 30.1542\n",
      "Processing batch 9057/11884 - Loss: 29.7975\n",
      "Processing batch 9058/11884 - Loss: 30.8993\n",
      "Processing batch 9059/11884 - Loss: 31.0026\n",
      "Processing batch 9060/11884 - Loss: 29.8826\n",
      "Processing batch 9061/11884 - Loss: 30.4748\n",
      "Processing batch 9062/11884 - Loss: 30.1776\n",
      "Processing batch 9063/11884 - Loss: 29.4126\n",
      "Processing batch 9064/11884 - Loss: 30.2291\n",
      "Processing batch 9065/11884 - Loss: 29.6643\n",
      "Processing batch 9066/11884 - Loss: 29.1999\n",
      "Processing batch 9067/11884 - Loss: 28.9456\n",
      "Processing batch 9068/11884 - Loss: 30.0284\n",
      "Processing batch 9069/11884 - Loss: 29.9294\n",
      "Processing batch 9070/11884 - Loss: 29.5503\n",
      "Processing batch 9071/11884 - Loss: 30.8033\n",
      "Processing batch 9072/11884 - Loss: 30.6730\n",
      "Processing batch 9073/11884 - Loss: 30.1230\n",
      "Processing batch 9074/11884 - Loss: 31.6404\n",
      "Processing batch 9075/11884 - Loss: 30.5522\n",
      "Processing batch 9076/11884 - Loss: 31.3814\n",
      "Processing batch 9077/11884 - Loss: 29.5658\n",
      "Processing batch 9078/11884 - Loss: 29.1295\n",
      "Processing batch 9079/11884 - Loss: 29.4656\n",
      "Processing batch 9080/11884 - Loss: 30.2324\n",
      "Processing batch 9081/11884 - Loss: 30.0543\n",
      "Processing batch 9082/11884 - Loss: 29.2516\n",
      "Processing batch 9083/11884 - Loss: 29.3770\n",
      "Processing batch 9084/11884 - Loss: 29.3973\n",
      "Processing batch 9085/11884 - Loss: 30.3031\n",
      "Processing batch 9086/11884 - Loss: 31.2283\n",
      "Processing batch 9087/11884 - Loss: 28.4829\n",
      "Processing batch 9088/11884 - Loss: 30.0516\n",
      "Processing batch 9089/11884 - Loss: 30.8700\n",
      "Processing batch 9090/11884 - Loss: 30.1762\n",
      "Processing batch 9091/11884 - Loss: 31.1612\n",
      "Processing batch 9092/11884 - Loss: 29.2615\n",
      "Processing batch 9093/11884 - Loss: 28.7910\n",
      "Processing batch 9094/11884 - Loss: 31.5352\n",
      "Processing batch 9095/11884 - Loss: 31.0419\n",
      "Processing batch 9096/11884 - Loss: 30.5925\n",
      "Processing batch 9097/11884 - Loss: 29.6221\n",
      "Processing batch 9098/11884 - Loss: 28.9704\n",
      "Processing batch 9099/11884 - Loss: 30.7472\n",
      "Processing batch 9100/11884 - Loss: 28.6078\n",
      "Processing batch 9101/11884 - Loss: 29.5428\n",
      "Processing batch 9102/11884 - Loss: 30.7865\n",
      "Processing batch 9103/11884 - Loss: 30.0479\n",
      "Processing batch 9104/11884 - Loss: 30.8968\n",
      "Processing batch 9105/11884 - Loss: 29.2275\n",
      "Processing batch 9106/11884 - Loss: 29.8954\n",
      "Processing batch 9107/11884 - Loss: 29.2435\n",
      "Processing batch 9108/11884 - Loss: 30.1452\n",
      "Processing batch 9109/11884 - Loss: 30.1024\n",
      "Processing batch 9110/11884 - Loss: 29.8225\n",
      "Processing batch 9111/11884 - Loss: 30.4503\n",
      "Processing batch 9112/11884 - Loss: 29.7942\n",
      "Processing batch 9113/11884 - Loss: 29.1372\n",
      "Processing batch 9114/11884 - Loss: 30.2723\n",
      "Processing batch 9115/11884 - Loss: 29.8907\n",
      "Processing batch 9116/11884 - Loss: 31.7257\n",
      "Processing batch 9117/11884 - Loss: 29.8075\n",
      "Processing batch 9118/11884 - Loss: 30.4087\n",
      "Processing batch 9119/11884 - Loss: 29.9294\n",
      "Processing batch 9120/11884 - Loss: 30.4386\n",
      "Processing batch 9121/11884 - Loss: 31.1617\n",
      "Processing batch 9122/11884 - Loss: 29.5808\n",
      "Processing batch 9123/11884 - Loss: 29.8429\n",
      "Processing batch 9124/11884 - Loss: 29.5496\n",
      "Processing batch 9125/11884 - Loss: 29.8660\n",
      "Processing batch 9126/11884 - Loss: 29.7905\n",
      "Processing batch 9127/11884 - Loss: 29.1333\n",
      "Processing batch 9128/11884 - Loss: 30.8895\n",
      "Processing batch 9129/11884 - Loss: 30.4387\n",
      "Processing batch 9130/11884 - Loss: 29.8829\n",
      "Processing batch 9131/11884 - Loss: 28.9024\n",
      "Processing batch 9132/11884 - Loss: 31.1049\n",
      "Processing batch 9133/11884 - Loss: 30.5837\n",
      "Processing batch 9134/11884 - Loss: 29.4076\n",
      "Processing batch 9135/11884 - Loss: 31.8199\n",
      "Processing batch 9136/11884 - Loss: 29.3844\n",
      "Processing batch 9137/11884 - Loss: 28.9358\n",
      "Processing batch 9138/11884 - Loss: 30.2783\n",
      "Processing batch 9139/11884 - Loss: 31.8024\n",
      "Processing batch 9140/11884 - Loss: 31.0851\n",
      "Processing batch 9141/11884 - Loss: 29.8169\n",
      "Processing batch 9142/11884 - Loss: 29.4007\n",
      "Processing batch 9143/11884 - Loss: 29.6118\n",
      "Processing batch 9144/11884 - Loss: 30.3963\n",
      "Processing batch 9145/11884 - Loss: 30.0027\n",
      "Processing batch 9146/11884 - Loss: 30.3310\n",
      "Processing batch 9147/11884 - Loss: 30.3540\n",
      "Processing batch 9148/11884 - Loss: 30.4654\n",
      "Processing batch 9149/11884 - Loss: 31.4415\n",
      "Processing batch 9150/11884 - Loss: 29.4546\n",
      "Processing batch 9151/11884 - Loss: 29.8690\n",
      "Processing batch 9152/11884 - Loss: 31.2011\n",
      "Processing batch 9153/11884 - Loss: 30.7375\n",
      "Processing batch 9154/11884 - Loss: 30.7998\n",
      "Processing batch 9155/11884 - Loss: 28.9671\n",
      "Processing batch 9156/11884 - Loss: 30.3236\n",
      "Processing batch 9157/11884 - Loss: 28.2374\n",
      "Processing batch 9158/11884 - Loss: 29.5324\n",
      "Processing batch 9159/11884 - Loss: 29.9220\n",
      "Processing batch 9160/11884 - Loss: 27.5093\n",
      "Processing batch 9161/11884 - Loss: 30.4984\n",
      "Processing batch 9162/11884 - Loss: 29.7116\n",
      "Processing batch 9163/11884 - Loss: 30.9424\n",
      "Processing batch 9164/11884 - Loss: 30.1542\n",
      "Processing batch 9165/11884 - Loss: 32.3382\n",
      "Processing batch 9166/11884 - Loss: 30.0343\n",
      "Processing batch 9167/11884 - Loss: 29.3787\n",
      "Processing batch 9168/11884 - Loss: 29.9016\n",
      "Processing batch 9169/11884 - Loss: 29.9743\n",
      "Processing batch 9170/11884 - Loss: 29.7866\n",
      "Processing batch 9171/11884 - Loss: 30.8342\n",
      "Processing batch 9172/11884 - Loss: 31.0996\n",
      "Processing batch 9173/11884 - Loss: 29.8283\n",
      "Processing batch 9174/11884 - Loss: 29.9892\n",
      "Processing batch 9175/11884 - Loss: 30.8894\n",
      "Processing batch 9176/11884 - Loss: 30.8143\n",
      "Processing batch 9177/11884 - Loss: 29.7153\n",
      "Processing batch 9178/11884 - Loss: 31.7229\n",
      "Processing batch 9179/11884 - Loss: 29.6285\n",
      "Processing batch 9180/11884 - Loss: 29.7020\n",
      "Processing batch 9181/11884 - Loss: 29.7901\n",
      "Processing batch 9182/11884 - Loss: 29.7388\n",
      "Processing batch 9183/11884 - Loss: 29.9411\n",
      "Processing batch 9184/11884 - Loss: 29.5270\n",
      "Processing batch 9185/11884 - Loss: 29.0329\n",
      "Processing batch 9186/11884 - Loss: 29.6959\n",
      "Processing batch 9187/11884 - Loss: 28.2007\n",
      "Processing batch 9188/11884 - Loss: 29.9792\n",
      "Processing batch 9189/11884 - Loss: 29.7775\n",
      "Processing batch 9190/11884 - Loss: 30.4024\n",
      "Processing batch 9191/11884 - Loss: 30.0235\n",
      "Processing batch 9192/11884 - Loss: 29.6760\n",
      "Processing batch 9193/11884 - Loss: 29.0661\n",
      "Processing batch 9194/11884 - Loss: 28.9614\n",
      "Processing batch 9195/11884 - Loss: 31.0751\n",
      "Processing batch 9196/11884 - Loss: 30.1831\n",
      "Processing batch 9197/11884 - Loss: 30.9048\n",
      "Processing batch 9198/11884 - Loss: 30.3223\n",
      "Processing batch 9199/11884 - Loss: 28.4837\n",
      "Processing batch 9200/11884 - Loss: 29.0832\n",
      "Processing batch 9201/11884 - Loss: 30.9953\n",
      "Processing batch 9202/11884 - Loss: 29.2764\n",
      "Processing batch 9203/11884 - Loss: 30.2543\n",
      "Processing batch 9204/11884 - Loss: 29.0155\n",
      "Processing batch 9205/11884 - Loss: 31.0922\n",
      "Processing batch 9206/11884 - Loss: 30.5534\n",
      "Processing batch 9207/11884 - Loss: 29.6751\n",
      "Processing batch 9208/11884 - Loss: 30.3699\n",
      "Processing batch 9209/11884 - Loss: 31.1644\n",
      "Processing batch 9210/11884 - Loss: 29.7452\n",
      "Processing batch 9211/11884 - Loss: 32.0422\n",
      "Processing batch 9212/11884 - Loss: 30.9529\n",
      "Processing batch 9213/11884 - Loss: 30.0540\n",
      "Processing batch 9214/11884 - Loss: 29.6427\n",
      "Processing batch 9215/11884 - Loss: 30.6025\n",
      "Processing batch 9216/11884 - Loss: 31.2061\n",
      "Processing batch 9217/11884 - Loss: 29.9390\n",
      "Processing batch 9218/11884 - Loss: 31.1148\n",
      "Processing batch 9219/11884 - Loss: 30.4547\n",
      "Processing batch 9220/11884 - Loss: 30.5379\n",
      "Processing batch 9221/11884 - Loss: 29.7193\n",
      "Processing batch 9222/11884 - Loss: 30.5672\n",
      "Processing batch 9223/11884 - Loss: 30.6099\n",
      "Processing batch 9224/11884 - Loss: 29.4743\n",
      "Processing batch 9225/11884 - Loss: 28.5855\n",
      "Processing batch 9226/11884 - Loss: 31.2418\n",
      "Processing batch 9227/11884 - Loss: 30.3631\n",
      "Processing batch 9228/11884 - Loss: 30.2913\n",
      "Processing batch 9229/11884 - Loss: 29.7371\n",
      "Processing batch 9230/11884 - Loss: 30.4384\n",
      "Processing batch 9231/11884 - Loss: 28.8572\n",
      "Processing batch 9232/11884 - Loss: 29.3111\n",
      "Processing batch 9233/11884 - Loss: 28.8828\n",
      "Processing batch 9234/11884 - Loss: 28.3366\n",
      "Processing batch 9235/11884 - Loss: 29.0000\n",
      "Processing batch 9236/11884 - Loss: 30.1623\n",
      "Processing batch 9237/11884 - Loss: 29.1117\n",
      "Processing batch 9238/11884 - Loss: 30.7495\n",
      "Processing batch 9239/11884 - Loss: 30.8477\n",
      "Processing batch 9240/11884 - Loss: 31.0810\n",
      "Processing batch 9241/11884 - Loss: 30.4408\n",
      "Processing batch 9242/11884 - Loss: 30.0834\n",
      "Processing batch 9243/11884 - Loss: 29.8507\n",
      "Processing batch 9244/11884 - Loss: 29.7418\n",
      "Processing batch 9245/11884 - Loss: 28.4602\n",
      "Processing batch 9246/11884 - Loss: 30.5360\n",
      "Processing batch 9247/11884 - Loss: 30.7167\n",
      "Processing batch 9248/11884 - Loss: 29.8409\n",
      "Processing batch 9249/11884 - Loss: 31.1031\n",
      "Processing batch 9250/11884 - Loss: 30.1921\n",
      "Processing batch 9251/11884 - Loss: 31.0334\n",
      "Processing batch 9252/11884 - Loss: 28.9803\n",
      "Processing batch 9253/11884 - Loss: 30.0094\n",
      "Processing batch 9254/11884 - Loss: 30.7525\n",
      "Processing batch 9255/11884 - Loss: 30.5224\n",
      "Processing batch 9256/11884 - Loss: 30.6596\n",
      "Processing batch 9257/11884 - Loss: 29.2598\n",
      "Processing batch 9258/11884 - Loss: 28.8201\n",
      "Processing batch 9259/11884 - Loss: 30.1221\n",
      "Processing batch 9260/11884 - Loss: 29.3290\n",
      "Processing batch 9261/11884 - Loss: 29.4320\n",
      "Processing batch 9262/11884 - Loss: 29.6812\n",
      "Processing batch 9263/11884 - Loss: 28.5622\n",
      "Processing batch 9264/11884 - Loss: 30.8403\n",
      "Processing batch 9265/11884 - Loss: 29.7802\n",
      "Processing batch 9266/11884 - Loss: 30.3324\n",
      "Processing batch 9267/11884 - Loss: 31.4838\n",
      "Processing batch 9268/11884 - Loss: 28.9293\n",
      "Processing batch 9269/11884 - Loss: 30.2331\n",
      "Processing batch 9270/11884 - Loss: 30.6529\n",
      "Processing batch 9271/11884 - Loss: 27.9945\n",
      "Processing batch 9272/11884 - Loss: 29.6499\n",
      "Processing batch 9273/11884 - Loss: 31.9675\n",
      "Processing batch 9274/11884 - Loss: 30.0325\n",
      "Processing batch 9275/11884 - Loss: 30.6230\n",
      "Processing batch 9276/11884 - Loss: 31.0476\n",
      "Processing batch 9277/11884 - Loss: 30.3325\n",
      "Processing batch 9278/11884 - Loss: 30.4010\n",
      "Processing batch 9279/11884 - Loss: 29.9599\n",
      "Processing batch 9280/11884 - Loss: 29.5646\n",
      "Processing batch 9281/11884 - Loss: 29.9224\n",
      "Processing batch 9282/11884 - Loss: 29.1313\n",
      "Processing batch 9283/11884 - Loss: 30.8319\n",
      "Processing batch 9284/11884 - Loss: 30.1587\n",
      "Processing batch 9285/11884 - Loss: 29.3456\n",
      "Processing batch 9286/11884 - Loss: 30.6730\n",
      "Processing batch 9287/11884 - Loss: 29.0235\n",
      "Processing batch 9288/11884 - Loss: 29.8578\n",
      "Processing batch 9289/11884 - Loss: 29.4931\n",
      "Processing batch 9290/11884 - Loss: 30.8785\n",
      "Processing batch 9291/11884 - Loss: 30.8835\n",
      "Processing batch 9292/11884 - Loss: 29.5887\n",
      "Processing batch 9293/11884 - Loss: 29.5458\n",
      "Processing batch 9294/11884 - Loss: 29.4982\n",
      "Processing batch 9295/11884 - Loss: 30.2689\n",
      "Processing batch 9296/11884 - Loss: 29.4010\n",
      "Processing batch 9297/11884 - Loss: 29.1778\n",
      "Processing batch 9298/11884 - Loss: 29.3822\n",
      "Processing batch 9299/11884 - Loss: 30.3209\n",
      "Processing batch 9300/11884 - Loss: 29.2246\n",
      "Processing batch 9301/11884 - Loss: 30.0558\n",
      "Processing batch 9302/11884 - Loss: 29.5939\n",
      "Processing batch 9303/11884 - Loss: 30.6842\n",
      "Processing batch 9304/11884 - Loss: 30.1728\n",
      "Processing batch 9305/11884 - Loss: 29.4653\n",
      "Processing batch 9306/11884 - Loss: 29.9677\n",
      "Processing batch 9307/11884 - Loss: 29.7826\n",
      "Processing batch 9308/11884 - Loss: 30.7924\n",
      "Processing batch 9309/11884 - Loss: 29.6805\n",
      "Processing batch 9310/11884 - Loss: 30.6780\n",
      "Processing batch 9311/11884 - Loss: 29.9517\n",
      "Processing batch 9312/11884 - Loss: 29.7248\n",
      "Processing batch 9313/11884 - Loss: 29.2726\n",
      "Processing batch 9314/11884 - Loss: 30.5638\n",
      "Processing batch 9315/11884 - Loss: 30.7661\n",
      "Processing batch 9316/11884 - Loss: 30.7188\n",
      "Processing batch 9317/11884 - Loss: 32.3312\n",
      "Processing batch 9318/11884 - Loss: 30.5738\n",
      "Processing batch 9319/11884 - Loss: 31.4255\n",
      "Processing batch 9320/11884 - Loss: 28.7834\n",
      "Processing batch 9321/11884 - Loss: 29.7268\n",
      "Processing batch 9322/11884 - Loss: 28.5969\n",
      "Processing batch 9323/11884 - Loss: 29.1059\n",
      "Processing batch 9324/11884 - Loss: 30.8229\n",
      "Processing batch 9325/11884 - Loss: 29.0732\n",
      "Processing batch 9326/11884 - Loss: 30.2454\n",
      "Processing batch 9327/11884 - Loss: 27.8774\n",
      "Processing batch 9328/11884 - Loss: 28.7251\n",
      "Processing batch 9329/11884 - Loss: 30.4005\n",
      "Processing batch 9330/11884 - Loss: 30.1117\n",
      "Processing batch 9331/11884 - Loss: 31.1452\n",
      "Processing batch 9332/11884 - Loss: 29.1485\n",
      "Processing batch 9333/11884 - Loss: 31.0569\n",
      "Processing batch 9334/11884 - Loss: 28.3048\n",
      "Processing batch 9335/11884 - Loss: 29.9870\n",
      "Processing batch 9336/11884 - Loss: 30.7374\n",
      "Processing batch 9337/11884 - Loss: 30.6933\n",
      "Processing batch 9338/11884 - Loss: 30.2299\n",
      "Processing batch 9339/11884 - Loss: 30.2605\n",
      "Processing batch 9340/11884 - Loss: 29.7764\n",
      "Processing batch 9341/11884 - Loss: 30.9537\n",
      "Processing batch 9342/11884 - Loss: 31.3562\n",
      "Processing batch 9343/11884 - Loss: 30.6146\n",
      "Processing batch 9344/11884 - Loss: 28.0854\n",
      "Processing batch 9345/11884 - Loss: 29.7957\n",
      "Processing batch 9346/11884 - Loss: 29.3004\n",
      "Processing batch 9347/11884 - Loss: 31.9630\n",
      "Processing batch 9348/11884 - Loss: 29.3801\n",
      "Processing batch 9349/11884 - Loss: 30.4662\n",
      "Processing batch 9350/11884 - Loss: 29.9386\n",
      "Processing batch 9351/11884 - Loss: 29.7618\n",
      "Processing batch 9352/11884 - Loss: 29.6522\n",
      "Processing batch 9353/11884 - Loss: 29.4426\n",
      "Processing batch 9354/11884 - Loss: 30.0057\n",
      "Processing batch 9355/11884 - Loss: 29.7770\n",
      "Processing batch 9356/11884 - Loss: 29.4560\n",
      "Processing batch 9357/11884 - Loss: 30.6543\n",
      "Processing batch 9358/11884 - Loss: 30.0392\n",
      "Processing batch 9359/11884 - Loss: 30.8312\n",
      "Processing batch 9360/11884 - Loss: 30.0861\n",
      "Processing batch 9361/11884 - Loss: 31.1009\n",
      "Processing batch 9362/11884 - Loss: 29.8899\n",
      "Processing batch 9363/11884 - Loss: 28.6850\n",
      "Processing batch 9364/11884 - Loss: 30.2451\n",
      "Processing batch 9365/11884 - Loss: 29.3173\n",
      "Processing batch 9366/11884 - Loss: 30.2373\n",
      "Processing batch 9367/11884 - Loss: 29.0254\n",
      "Processing batch 9368/11884 - Loss: 31.0728\n",
      "Processing batch 9369/11884 - Loss: 30.4694\n",
      "Processing batch 9370/11884 - Loss: 31.3269\n",
      "Processing batch 9371/11884 - Loss: 31.5356\n",
      "Processing batch 9372/11884 - Loss: 29.0756\n",
      "Processing batch 9373/11884 - Loss: 30.5530\n",
      "Processing batch 9374/11884 - Loss: 31.3573\n",
      "Processing batch 9375/11884 - Loss: 30.5238\n",
      "Processing batch 9376/11884 - Loss: 29.8010\n",
      "Processing batch 9377/11884 - Loss: 30.1875\n",
      "Processing batch 9378/11884 - Loss: 30.5952\n",
      "Processing batch 9379/11884 - Loss: 29.6527\n",
      "Processing batch 9380/11884 - Loss: 28.6459\n",
      "Processing batch 9381/11884 - Loss: 30.6926\n",
      "Processing batch 9382/11884 - Loss: 28.9343\n",
      "Processing batch 9383/11884 - Loss: 30.6296\n",
      "Processing batch 9384/11884 - Loss: 28.0681\n",
      "Processing batch 9385/11884 - Loss: 29.3826\n",
      "Processing batch 9386/11884 - Loss: 30.3954\n",
      "Processing batch 9387/11884 - Loss: 30.0411\n",
      "Processing batch 9388/11884 - Loss: 30.2906\n",
      "Processing batch 9389/11884 - Loss: 30.1763\n",
      "Processing batch 9390/11884 - Loss: 29.8096\n",
      "Processing batch 9391/11884 - Loss: 30.5322\n",
      "Processing batch 9392/11884 - Loss: 30.1296\n",
      "Processing batch 9393/11884 - Loss: 30.2558\n",
      "Processing batch 9394/11884 - Loss: 30.5067\n",
      "Processing batch 9395/11884 - Loss: 30.1551\n",
      "Processing batch 9396/11884 - Loss: 29.6921\n",
      "Processing batch 9397/11884 - Loss: 29.8610\n",
      "Processing batch 9398/11884 - Loss: 29.6752\n",
      "Processing batch 9399/11884 - Loss: 31.4631\n",
      "Processing batch 9400/11884 - Loss: 29.5898\n",
      "Processing batch 9401/11884 - Loss: 31.7500\n",
      "Processing batch 9402/11884 - Loss: 28.6615\n",
      "Processing batch 9403/11884 - Loss: 29.0030\n",
      "Processing batch 9404/11884 - Loss: 28.6780\n",
      "Processing batch 9405/11884 - Loss: 30.1580\n",
      "Processing batch 9406/11884 - Loss: 29.6050\n",
      "Processing batch 9407/11884 - Loss: 30.5006\n",
      "Processing batch 9408/11884 - Loss: 27.8363\n",
      "Processing batch 9409/11884 - Loss: 30.0549\n",
      "Processing batch 9410/11884 - Loss: 30.9168\n",
      "Processing batch 9411/11884 - Loss: 29.6612\n",
      "Processing batch 9412/11884 - Loss: 30.0926\n",
      "Processing batch 9413/11884 - Loss: 30.7075\n",
      "Processing batch 9414/11884 - Loss: 30.5187\n",
      "Processing batch 9415/11884 - Loss: 29.0792\n",
      "Processing batch 9416/11884 - Loss: 30.2268\n",
      "Processing batch 9417/11884 - Loss: 28.7079\n",
      "Processing batch 9418/11884 - Loss: 31.0621\n",
      "Processing batch 9419/11884 - Loss: 30.3370\n",
      "Processing batch 9420/11884 - Loss: 29.6692\n",
      "Processing batch 9421/11884 - Loss: 29.9679\n",
      "Processing batch 9422/11884 - Loss: 30.7307\n",
      "Processing batch 9423/11884 - Loss: 28.7679\n",
      "Processing batch 9424/11884 - Loss: 29.0878\n",
      "Processing batch 9425/11884 - Loss: 30.0413\n",
      "Processing batch 9426/11884 - Loss: 30.7787\n",
      "Processing batch 9427/11884 - Loss: 30.4793\n",
      "Processing batch 9428/11884 - Loss: 29.1409\n",
      "Processing batch 9429/11884 - Loss: 29.2979\n",
      "Processing batch 9430/11884 - Loss: 30.2401\n",
      "Processing batch 9431/11884 - Loss: 29.8712\n",
      "Processing batch 9432/11884 - Loss: 29.7798\n",
      "Processing batch 9433/11884 - Loss: 29.3486\n",
      "Processing batch 9434/11884 - Loss: 30.6971\n",
      "Processing batch 9435/11884 - Loss: 29.8663\n",
      "Processing batch 9436/11884 - Loss: 30.8732\n",
      "Processing batch 9437/11884 - Loss: 30.4558\n",
      "Processing batch 9438/11884 - Loss: 28.3679\n",
      "Processing batch 9439/11884 - Loss: 30.6033\n",
      "Processing batch 9440/11884 - Loss: 30.0808\n",
      "Processing batch 9441/11884 - Loss: 28.2553\n",
      "Processing batch 9442/11884 - Loss: 28.9219\n",
      "Processing batch 9443/11884 - Loss: 29.7374\n",
      "Processing batch 9444/11884 - Loss: 30.5697\n",
      "Processing batch 9445/11884 - Loss: 29.1611\n",
      "Processing batch 9446/11884 - Loss: 30.1490\n",
      "Processing batch 9447/11884 - Loss: 30.0991\n",
      "Processing batch 9448/11884 - Loss: 29.9252\n",
      "Processing batch 9449/11884 - Loss: 30.4595\n",
      "Processing batch 9450/11884 - Loss: 31.6491\n",
      "Processing batch 9451/11884 - Loss: 29.5276\n",
      "Processing batch 9452/11884 - Loss: 29.5005\n",
      "Processing batch 9453/11884 - Loss: 30.1481\n",
      "Processing batch 9454/11884 - Loss: 29.6613\n",
      "Processing batch 9455/11884 - Loss: 29.2520\n",
      "Processing batch 9456/11884 - Loss: 30.0804\n",
      "Processing batch 9457/11884 - Loss: 28.8884\n",
      "Processing batch 9458/11884 - Loss: 30.7174\n",
      "Processing batch 9459/11884 - Loss: 30.2136\n",
      "Processing batch 9460/11884 - Loss: 29.1070\n",
      "Processing batch 9461/11884 - Loss: 29.3150\n",
      "Processing batch 9462/11884 - Loss: 29.1354\n",
      "Processing batch 9463/11884 - Loss: 29.4718\n",
      "Processing batch 9464/11884 - Loss: 31.6149\n",
      "Processing batch 9465/11884 - Loss: 31.4628\n",
      "Processing batch 9466/11884 - Loss: 29.7835\n",
      "Processing batch 9467/11884 - Loss: 30.9018\n",
      "Processing batch 9468/11884 - Loss: 29.1008\n",
      "Processing batch 9469/11884 - Loss: 29.2798\n",
      "Processing batch 9470/11884 - Loss: 30.2934\n",
      "Processing batch 9471/11884 - Loss: 30.9809\n",
      "Processing batch 9472/11884 - Loss: 29.5115\n",
      "Processing batch 9473/11884 - Loss: 29.4094\n",
      "Processing batch 9474/11884 - Loss: 29.7065\n",
      "Processing batch 9475/11884 - Loss: 29.3224\n",
      "Processing batch 9476/11884 - Loss: 31.5902\n",
      "Processing batch 9477/11884 - Loss: 29.1007\n",
      "Processing batch 9478/11884 - Loss: 29.0405\n",
      "Processing batch 9479/11884 - Loss: 30.2003\n",
      "Processing batch 9480/11884 - Loss: 30.7556\n",
      "Processing batch 9481/11884 - Loss: 29.2572\n",
      "Processing batch 9482/11884 - Loss: 30.5883\n",
      "Processing batch 9483/11884 - Loss: 29.7649\n",
      "Processing batch 9484/11884 - Loss: 28.6442\n",
      "Processing batch 9485/11884 - Loss: 30.2224\n",
      "Processing batch 9486/11884 - Loss: 29.0580\n",
      "Processing batch 9487/11884 - Loss: 31.2285\n",
      "Processing batch 9488/11884 - Loss: 29.1289\n",
      "Processing batch 9489/11884 - Loss: 30.3013\n",
      "Processing batch 9490/11884 - Loss: 29.5212\n",
      "Processing batch 9491/11884 - Loss: 29.5190\n",
      "Processing batch 9492/11884 - Loss: 29.7197\n",
      "Processing batch 9493/11884 - Loss: 30.2592\n",
      "Processing batch 9494/11884 - Loss: 30.8675\n",
      "Processing batch 9495/11884 - Loss: 30.5840\n",
      "Processing batch 9496/11884 - Loss: 30.9354\n",
      "Processing batch 9497/11884 - Loss: 29.6327\n",
      "Processing batch 9498/11884 - Loss: 29.0672\n",
      "Processing batch 9499/11884 - Loss: 30.5406\n",
      "Processing batch 9500/11884 - Loss: 29.6680\n",
      "Processing batch 9501/11884 - Loss: 30.0896\n",
      "Processing batch 9502/11884 - Loss: 29.3108\n",
      "Processing batch 9503/11884 - Loss: 30.8271\n",
      "Processing batch 9504/11884 - Loss: 30.3215\n",
      "Processing batch 9505/11884 - Loss: 29.6020\n",
      "Processing batch 9506/11884 - Loss: 30.2234\n",
      "Processing batch 9507/11884 - Loss: 31.8051\n",
      "Processing batch 9508/11884 - Loss: 31.2938\n",
      "Processing batch 9509/11884 - Loss: 30.1443\n",
      "Processing batch 9510/11884 - Loss: 29.1006\n",
      "Processing batch 9511/11884 - Loss: 30.7845\n",
      "Processing batch 9512/11884 - Loss: 29.5745\n",
      "Processing batch 9513/11884 - Loss: 28.8677\n",
      "Processing batch 9514/11884 - Loss: 29.6746\n",
      "Processing batch 9515/11884 - Loss: 28.8194\n",
      "Processing batch 9516/11884 - Loss: 29.6932\n",
      "Processing batch 9517/11884 - Loss: 29.4835\n",
      "Processing batch 9518/11884 - Loss: 30.2381\n",
      "Processing batch 9519/11884 - Loss: 29.8144\n",
      "Processing batch 9520/11884 - Loss: 30.6432\n",
      "Processing batch 9521/11884 - Loss: 29.0176\n",
      "Processing batch 9522/11884 - Loss: 29.0455\n",
      "Processing batch 9523/11884 - Loss: 29.3873\n",
      "Processing batch 9524/11884 - Loss: 29.4514\n",
      "Processing batch 9525/11884 - Loss: 28.8709\n",
      "Processing batch 9526/11884 - Loss: 30.3381\n",
      "Processing batch 9527/11884 - Loss: 28.4678\n",
      "Processing batch 9528/11884 - Loss: 29.8647\n",
      "Processing batch 9529/11884 - Loss: 30.1317\n",
      "Processing batch 9530/11884 - Loss: 29.1115\n",
      "Processing batch 9531/11884 - Loss: 29.7755\n",
      "Processing batch 9532/11884 - Loss: 30.4230\n",
      "Processing batch 9533/11884 - Loss: 29.9467\n",
      "Processing batch 9534/11884 - Loss: 29.4387\n",
      "Processing batch 9535/11884 - Loss: 31.2065\n",
      "Processing batch 9536/11884 - Loss: 29.5841\n",
      "Processing batch 9537/11884 - Loss: 30.1626\n",
      "Processing batch 9538/11884 - Loss: 29.9561\n",
      "Processing batch 9539/11884 - Loss: 31.2937\n",
      "Processing batch 9540/11884 - Loss: 29.4887\n",
      "Processing batch 9541/11884 - Loss: 30.1741\n",
      "Processing batch 9542/11884 - Loss: 29.1793\n",
      "Processing batch 9543/11884 - Loss: 29.5627\n",
      "Processing batch 9544/11884 - Loss: 29.4592\n",
      "Processing batch 9545/11884 - Loss: 29.5794\n",
      "Processing batch 9546/11884 - Loss: 30.0813\n",
      "Processing batch 9547/11884 - Loss: 29.8753\n",
      "Processing batch 9548/11884 - Loss: 30.2448\n",
      "Processing batch 9549/11884 - Loss: 30.3137\n",
      "Processing batch 9550/11884 - Loss: 29.3185\n",
      "Processing batch 9551/11884 - Loss: 30.9949\n",
      "Processing batch 9552/11884 - Loss: 29.7004\n",
      "Processing batch 9553/11884 - Loss: 30.1103\n",
      "Processing batch 9554/11884 - Loss: 29.1752\n",
      "Processing batch 9555/11884 - Loss: 29.8877\n",
      "Processing batch 9556/11884 - Loss: 29.9523\n",
      "Processing batch 9557/11884 - Loss: 31.2268\n",
      "Processing batch 9558/11884 - Loss: 29.5470\n",
      "Processing batch 9559/11884 - Loss: 30.6114\n",
      "Processing batch 9560/11884 - Loss: 29.6480\n",
      "Processing batch 9561/11884 - Loss: 30.0945\n",
      "Processing batch 9562/11884 - Loss: 30.1423\n",
      "Processing batch 9563/11884 - Loss: 29.4906\n",
      "Processing batch 9564/11884 - Loss: 28.7081\n",
      "Processing batch 9565/11884 - Loss: 30.0700\n",
      "Processing batch 9566/11884 - Loss: 30.4493\n",
      "Processing batch 9567/11884 - Loss: 30.0703\n",
      "Processing batch 9568/11884 - Loss: 30.5309\n",
      "Processing batch 9569/11884 - Loss: 29.4047\n",
      "Processing batch 9570/11884 - Loss: 29.9234\n",
      "Processing batch 9571/11884 - Loss: 30.7860\n",
      "Processing batch 9572/11884 - Loss: 29.7839\n",
      "Processing batch 9573/11884 - Loss: 30.5242\n",
      "Processing batch 9574/11884 - Loss: 28.9550\n",
      "Processing batch 9575/11884 - Loss: 29.9620\n",
      "Processing batch 9576/11884 - Loss: 29.8378\n",
      "Processing batch 9577/11884 - Loss: 30.1528\n",
      "Processing batch 9578/11884 - Loss: 30.2885\n",
      "Processing batch 9579/11884 - Loss: 29.9531\n",
      "Processing batch 9580/11884 - Loss: 30.0061\n",
      "Processing batch 9581/11884 - Loss: 29.0816\n",
      "Processing batch 9582/11884 - Loss: 30.1433\n",
      "Processing batch 9583/11884 - Loss: 28.6936\n",
      "Processing batch 9584/11884 - Loss: 29.9713\n",
      "Processing batch 9585/11884 - Loss: 30.6010\n",
      "Processing batch 9586/11884 - Loss: 30.1580\n",
      "Processing batch 9587/11884 - Loss: 29.6165\n",
      "Processing batch 9588/11884 - Loss: 30.2263\n",
      "Processing batch 9589/11884 - Loss: 30.0038\n",
      "Processing batch 9590/11884 - Loss: 30.8937\n",
      "Processing batch 9591/11884 - Loss: 30.3793\n",
      "Processing batch 9592/11884 - Loss: 29.9182\n",
      "Processing batch 9593/11884 - Loss: 30.1961\n",
      "Processing batch 9594/11884 - Loss: 28.7712\n",
      "Processing batch 9595/11884 - Loss: 29.1627\n",
      "Processing batch 9596/11884 - Loss: 29.2270\n",
      "Processing batch 9597/11884 - Loss: 31.0406\n",
      "Processing batch 9598/11884 - Loss: 29.4776\n",
      "Processing batch 9599/11884 - Loss: 29.9663\n",
      "Processing batch 9600/11884 - Loss: 29.8337\n",
      "Processing batch 9601/11884 - Loss: 29.6827\n",
      "Processing batch 9602/11884 - Loss: 31.7924\n",
      "Processing batch 9603/11884 - Loss: 30.4171\n",
      "Processing batch 9604/11884 - Loss: 29.6655\n",
      "Processing batch 9605/11884 - Loss: 30.8518\n",
      "Processing batch 9606/11884 - Loss: 29.2042\n",
      "Processing batch 9607/11884 - Loss: 29.2755\n",
      "Processing batch 9608/11884 - Loss: 30.9732\n",
      "Processing batch 9609/11884 - Loss: 30.3955\n",
      "Processing batch 9610/11884 - Loss: 29.2910\n",
      "Processing batch 9611/11884 - Loss: 29.6319\n",
      "Processing batch 9612/11884 - Loss: 30.4795\n",
      "Processing batch 9613/11884 - Loss: 30.2863\n",
      "Processing batch 9614/11884 - Loss: 29.9112\n",
      "Processing batch 9615/11884 - Loss: 28.9912\n",
      "Processing batch 9616/11884 - Loss: 29.8764\n",
      "Processing batch 9617/11884 - Loss: 31.2954\n",
      "Processing batch 9618/11884 - Loss: 28.6597\n",
      "Processing batch 9619/11884 - Loss: 29.0299\n",
      "Processing batch 9620/11884 - Loss: 29.1523\n",
      "Processing batch 9621/11884 - Loss: 30.3444\n",
      "Processing batch 9622/11884 - Loss: 29.7150\n",
      "Processing batch 9623/11884 - Loss: 29.6543\n",
      "Processing batch 9624/11884 - Loss: 29.5567\n",
      "Processing batch 9625/11884 - Loss: 30.2770\n",
      "Processing batch 9626/11884 - Loss: 31.2085\n",
      "Processing batch 9627/11884 - Loss: 30.8073\n",
      "Processing batch 9628/11884 - Loss: 29.5308\n",
      "Processing batch 9629/11884 - Loss: 30.3567\n",
      "Processing batch 9630/11884 - Loss: 30.2518\n",
      "Processing batch 9631/11884 - Loss: 31.0162\n",
      "Processing batch 9632/11884 - Loss: 29.8100\n",
      "Processing batch 9633/11884 - Loss: 30.0636\n",
      "Processing batch 9634/11884 - Loss: 29.9031\n",
      "Processing batch 9635/11884 - Loss: 29.9998\n",
      "Processing batch 9636/11884 - Loss: 29.2825\n",
      "Processing batch 9637/11884 - Loss: 30.7426\n",
      "Processing batch 9638/11884 - Loss: 30.5079\n",
      "Processing batch 9639/11884 - Loss: 30.2974\n",
      "Processing batch 9640/11884 - Loss: 30.1521\n",
      "Processing batch 9641/11884 - Loss: 30.4406\n",
      "Processing batch 9642/11884 - Loss: 30.9034\n",
      "Processing batch 9643/11884 - Loss: 29.1527\n",
      "Processing batch 9644/11884 - Loss: 28.8479\n",
      "Processing batch 9645/11884 - Loss: 28.9882\n",
      "Processing batch 9646/11884 - Loss: 29.2999\n",
      "Processing batch 9647/11884 - Loss: 31.2454\n",
      "Processing batch 9648/11884 - Loss: 31.9673\n",
      "Processing batch 9649/11884 - Loss: 31.4221\n",
      "Processing batch 9650/11884 - Loss: 30.3497\n",
      "Processing batch 9651/11884 - Loss: 29.6694\n",
      "Processing batch 9652/11884 - Loss: 29.4791\n",
      "Processing batch 9653/11884 - Loss: 30.1716\n",
      "Processing batch 9654/11884 - Loss: 31.5416\n",
      "Processing batch 9655/11884 - Loss: 29.9236\n",
      "Processing batch 9656/11884 - Loss: 28.6880\n",
      "Processing batch 9657/11884 - Loss: 30.4866\n",
      "Processing batch 9658/11884 - Loss: 30.8081\n",
      "Processing batch 9659/11884 - Loss: 30.6459\n",
      "Processing batch 9660/11884 - Loss: 28.6135\n",
      "Processing batch 9661/11884 - Loss: 30.5924\n",
      "Processing batch 9662/11884 - Loss: 30.0274\n",
      "Processing batch 9663/11884 - Loss: 28.9223\n",
      "Processing batch 9664/11884 - Loss: 30.3394\n",
      "Processing batch 9665/11884 - Loss: 31.4347\n",
      "Processing batch 9666/11884 - Loss: 29.2253\n",
      "Processing batch 9667/11884 - Loss: 29.6678\n",
      "Processing batch 9668/11884 - Loss: 29.3729\n",
      "Processing batch 9669/11884 - Loss: 30.1086\n",
      "Processing batch 9670/11884 - Loss: 29.7420\n",
      "Processing batch 9671/11884 - Loss: 29.7981\n",
      "Processing batch 9672/11884 - Loss: 30.2187\n",
      "Processing batch 9673/11884 - Loss: 30.1702\n",
      "Processing batch 9674/11884 - Loss: 30.9846\n",
      "Processing batch 9675/11884 - Loss: 30.1614\n",
      "Processing batch 9676/11884 - Loss: 29.2384\n",
      "Processing batch 9677/11884 - Loss: 30.8868\n",
      "Processing batch 9678/11884 - Loss: 30.6898\n",
      "Processing batch 9679/11884 - Loss: 29.5227\n",
      "Processing batch 9680/11884 - Loss: 30.6599\n",
      "Processing batch 9681/11884 - Loss: 31.1586\n",
      "Processing batch 9682/11884 - Loss: 30.2786\n",
      "Processing batch 9683/11884 - Loss: 30.5613\n",
      "Processing batch 9684/11884 - Loss: 28.8303\n",
      "Processing batch 9685/11884 - Loss: 29.6220\n",
      "Processing batch 9686/11884 - Loss: 30.8948\n",
      "Processing batch 9687/11884 - Loss: 29.9128\n",
      "Processing batch 9688/11884 - Loss: 29.1428\n",
      "Processing batch 9689/11884 - Loss: 29.3942\n",
      "Processing batch 9690/11884 - Loss: 30.4461\n",
      "Processing batch 9691/11884 - Loss: 29.7689\n",
      "Processing batch 9692/11884 - Loss: 29.3665\n",
      "Processing batch 9693/11884 - Loss: 29.7491\n",
      "Processing batch 9694/11884 - Loss: 29.5340\n",
      "Processing batch 9695/11884 - Loss: 31.4341\n",
      "Processing batch 9696/11884 - Loss: 28.3110\n",
      "Processing batch 9697/11884 - Loss: 29.9657\n",
      "Processing batch 9698/11884 - Loss: 30.0708\n",
      "Processing batch 9699/11884 - Loss: 31.0787\n",
      "Processing batch 9700/11884 - Loss: 29.2924\n",
      "Processing batch 9701/11884 - Loss: 29.9683\n",
      "Processing batch 9702/11884 - Loss: 30.0996\n",
      "Processing batch 9703/11884 - Loss: 30.2187\n",
      "Processing batch 9704/11884 - Loss: 29.6657\n",
      "Processing batch 9705/11884 - Loss: 28.3507\n",
      "Processing batch 9706/11884 - Loss: 31.0607\n",
      "Processing batch 9707/11884 - Loss: 30.3755\n",
      "Processing batch 9708/11884 - Loss: 29.6747\n",
      "Processing batch 9709/11884 - Loss: 29.6871\n",
      "Processing batch 9710/11884 - Loss: 28.7993\n",
      "Processing batch 9711/11884 - Loss: 30.4240\n",
      "Processing batch 9712/11884 - Loss: 29.0113\n",
      "Processing batch 9713/11884 - Loss: 29.1296\n",
      "Processing batch 9714/11884 - Loss: 30.1926\n",
      "Processing batch 9715/11884 - Loss: 31.0046\n",
      "Processing batch 9716/11884 - Loss: 29.8293\n",
      "Processing batch 9717/11884 - Loss: 31.0058\n",
      "Processing batch 9718/11884 - Loss: 30.0013\n",
      "Processing batch 9719/11884 - Loss: 28.9381\n",
      "Processing batch 9720/11884 - Loss: 30.2635\n",
      "Processing batch 9721/11884 - Loss: 30.6024\n",
      "Processing batch 9722/11884 - Loss: 30.2855\n",
      "Processing batch 9723/11884 - Loss: 30.1762\n",
      "Processing batch 9724/11884 - Loss: 30.1997\n",
      "Processing batch 9725/11884 - Loss: 29.4808\n",
      "Processing batch 9726/11884 - Loss: 30.9897\n",
      "Processing batch 9727/11884 - Loss: 30.1870\n",
      "Processing batch 9728/11884 - Loss: 31.6087\n",
      "Processing batch 9729/11884 - Loss: 30.7681\n",
      "Processing batch 9730/11884 - Loss: 29.7025\n",
      "Processing batch 9731/11884 - Loss: 29.6716\n",
      "Processing batch 9732/11884 - Loss: 30.2285\n",
      "Processing batch 9733/11884 - Loss: 29.9349\n",
      "Processing batch 9734/11884 - Loss: 29.9588\n",
      "Processing batch 9735/11884 - Loss: 30.0218\n",
      "Processing batch 9736/11884 - Loss: 29.6271\n",
      "Processing batch 9737/11884 - Loss: 29.2646\n",
      "Processing batch 9738/11884 - Loss: 30.1836\n",
      "Processing batch 9739/11884 - Loss: 31.0509\n",
      "Processing batch 9740/11884 - Loss: 29.1406\n",
      "Processing batch 9741/11884 - Loss: 29.0516\n",
      "Processing batch 9742/11884 - Loss: 29.9006\n",
      "Processing batch 9743/11884 - Loss: 29.4079\n",
      "Processing batch 9744/11884 - Loss: 30.7874\n",
      "Processing batch 9745/11884 - Loss: 29.6770\n",
      "Processing batch 9746/11884 - Loss: 30.5708\n",
      "Processing batch 9747/11884 - Loss: 30.4428\n",
      "Processing batch 9748/11884 - Loss: 30.5119\n",
      "Processing batch 9749/11884 - Loss: 29.8873\n",
      "Processing batch 9750/11884 - Loss: 29.9150\n",
      "Processing batch 9751/11884 - Loss: 29.2338\n",
      "Processing batch 9752/11884 - Loss: 31.1558\n",
      "Processing batch 9753/11884 - Loss: 30.1750\n",
      "Processing batch 9754/11884 - Loss: 28.9555\n",
      "Processing batch 9755/11884 - Loss: 29.4668\n",
      "Processing batch 9756/11884 - Loss: 29.0221\n",
      "Processing batch 9757/11884 - Loss: 29.9493\n",
      "Processing batch 9758/11884 - Loss: 30.5050\n",
      "Processing batch 9759/11884 - Loss: 30.4355\n",
      "Processing batch 9760/11884 - Loss: 30.2048\n",
      "Processing batch 9761/11884 - Loss: 30.0739\n",
      "Processing batch 9762/11884 - Loss: 30.1270\n",
      "Processing batch 9763/11884 - Loss: 30.5601\n",
      "Processing batch 9764/11884 - Loss: 30.3819\n",
      "Processing batch 9765/11884 - Loss: 29.7686\n",
      "Processing batch 9766/11884 - Loss: 30.1583\n",
      "Processing batch 9767/11884 - Loss: 28.5461\n",
      "Processing batch 9768/11884 - Loss: 29.3164\n",
      "Processing batch 9769/11884 - Loss: 30.3647\n",
      "Processing batch 9770/11884 - Loss: 29.5658\n",
      "Processing batch 9771/11884 - Loss: 30.3340\n",
      "Processing batch 9772/11884 - Loss: 29.9510\n",
      "Processing batch 9773/11884 - Loss: 31.5291\n",
      "Processing batch 9774/11884 - Loss: 30.8050\n",
      "Processing batch 9775/11884 - Loss: 29.4449\n",
      "Processing batch 9776/11884 - Loss: 29.3384\n",
      "Processing batch 9777/11884 - Loss: 29.9131\n",
      "Processing batch 9778/11884 - Loss: 29.9307\n",
      "Processing batch 9779/11884 - Loss: 29.6804\n",
      "Processing batch 9780/11884 - Loss: 30.5479\n",
      "Processing batch 9781/11884 - Loss: 31.3671\n",
      "Processing batch 9782/11884 - Loss: 29.5721\n",
      "Processing batch 9783/11884 - Loss: 29.3786\n",
      "Processing batch 9784/11884 - Loss: 30.5803\n",
      "Processing batch 9785/11884 - Loss: 30.2963\n",
      "Processing batch 9786/11884 - Loss: 31.0918\n",
      "Processing batch 9787/11884 - Loss: 30.5135\n",
      "Processing batch 9788/11884 - Loss: 28.9813\n",
      "Processing batch 9789/11884 - Loss: 30.1575\n",
      "Processing batch 9790/11884 - Loss: 29.9736\n",
      "Processing batch 9791/11884 - Loss: 29.7891\n",
      "Processing batch 9792/11884 - Loss: 30.0536\n",
      "Processing batch 9793/11884 - Loss: 29.8117\n",
      "Processing batch 9794/11884 - Loss: 30.2354\n",
      "Processing batch 9795/11884 - Loss: 30.5092\n",
      "Processing batch 9796/11884 - Loss: 30.7187\n",
      "Processing batch 9797/11884 - Loss: 29.3494\n",
      "Processing batch 9798/11884 - Loss: 30.4542\n",
      "Processing batch 9799/11884 - Loss: 30.3565\n",
      "Processing batch 9800/11884 - Loss: 29.1734\n",
      "Processing batch 9801/11884 - Loss: 33.0098\n",
      "Processing batch 9802/11884 - Loss: 30.6466\n",
      "Processing batch 9803/11884 - Loss: 30.5059\n",
      "Processing batch 9804/11884 - Loss: 29.7735\n",
      "Processing batch 9805/11884 - Loss: 30.3308\n",
      "Processing batch 9806/11884 - Loss: 29.3671\n",
      "Processing batch 9807/11884 - Loss: 29.2901\n",
      "Processing batch 9808/11884 - Loss: 29.5348\n",
      "Processing batch 9809/11884 - Loss: 29.5260\n",
      "Processing batch 9810/11884 - Loss: 29.5591\n",
      "Processing batch 9811/11884 - Loss: 30.9975\n",
      "Processing batch 9812/11884 - Loss: 29.2388\n",
      "Processing batch 9813/11884 - Loss: 30.1587\n",
      "Processing batch 9814/11884 - Loss: 30.5550\n",
      "Processing batch 9815/11884 - Loss: 30.3983\n",
      "Processing batch 9816/11884 - Loss: 29.0961\n",
      "Processing batch 9817/11884 - Loss: 30.0390\n",
      "Processing batch 9818/11884 - Loss: 30.6751\n",
      "Processing batch 9819/11884 - Loss: 31.3854\n",
      "Processing batch 9820/11884 - Loss: 30.8718\n",
      "Processing batch 9821/11884 - Loss: 29.6027\n",
      "Processing batch 9822/11884 - Loss: 31.1345\n",
      "Processing batch 9823/11884 - Loss: 29.8314\n",
      "Processing batch 9824/11884 - Loss: 29.2828\n",
      "Processing batch 9825/11884 - Loss: 29.7760\n",
      "Processing batch 9826/11884 - Loss: 30.0895\n",
      "Processing batch 9827/11884 - Loss: 29.1117\n",
      "Processing batch 9828/11884 - Loss: 28.9987\n",
      "Processing batch 9829/11884 - Loss: 29.7994\n",
      "Processing batch 9830/11884 - Loss: 30.6517\n",
      "Processing batch 9831/11884 - Loss: 29.8708\n",
      "Processing batch 9832/11884 - Loss: 28.4725\n",
      "Processing batch 9833/11884 - Loss: 29.7339\n",
      "Processing batch 9834/11884 - Loss: 31.6287\n",
      "Processing batch 9835/11884 - Loss: 29.0823\n",
      "Processing batch 9836/11884 - Loss: 29.1979\n",
      "Processing batch 9837/11884 - Loss: 29.9125\n",
      "Processing batch 9838/11884 - Loss: 30.8294\n",
      "Processing batch 9839/11884 - Loss: 30.9090\n",
      "Processing batch 9840/11884 - Loss: 31.3182\n",
      "Processing batch 9841/11884 - Loss: 28.4460\n",
      "Processing batch 9842/11884 - Loss: 29.6167\n",
      "Processing batch 9843/11884 - Loss: 30.5644\n",
      "Processing batch 9844/11884 - Loss: 29.6683\n",
      "Processing batch 9845/11884 - Loss: 30.4927\n",
      "Processing batch 9846/11884 - Loss: 30.1402\n",
      "Processing batch 9847/11884 - Loss: 28.6271\n",
      "Processing batch 9848/11884 - Loss: 29.8478\n",
      "Processing batch 9849/11884 - Loss: 28.6560\n",
      "Processing batch 9850/11884 - Loss: 29.6242\n",
      "Processing batch 9851/11884 - Loss: 31.5426\n",
      "Processing batch 9852/11884 - Loss: 29.4172\n",
      "Processing batch 9853/11884 - Loss: 30.5092\n",
      "Processing batch 9854/11884 - Loss: 29.5680\n",
      "Processing batch 9855/11884 - Loss: 31.0176\n",
      "Processing batch 9856/11884 - Loss: 29.7727\n",
      "Processing batch 9857/11884 - Loss: 29.9257\n",
      "Processing batch 9858/11884 - Loss: 30.4060\n",
      "Processing batch 9859/11884 - Loss: 29.5996\n",
      "Processing batch 9860/11884 - Loss: 30.4595\n",
      "Processing batch 9861/11884 - Loss: 29.2152\n",
      "Processing batch 9862/11884 - Loss: 30.8282\n",
      "Processing batch 9863/11884 - Loss: 29.0970\n",
      "Processing batch 9864/11884 - Loss: 30.7639\n",
      "Processing batch 9865/11884 - Loss: 29.2013\n",
      "Processing batch 9866/11884 - Loss: 30.7509\n",
      "Processing batch 9867/11884 - Loss: 29.4358\n",
      "Processing batch 9868/11884 - Loss: 29.7562\n",
      "Processing batch 9869/11884 - Loss: 29.4388\n",
      "Processing batch 9870/11884 - Loss: 29.6542\n",
      "Processing batch 9871/11884 - Loss: 30.3747\n",
      "Processing batch 9872/11884 - Loss: 29.3346\n",
      "Processing batch 9873/11884 - Loss: 29.0962\n",
      "Processing batch 9874/11884 - Loss: 31.2887\n",
      "Processing batch 9875/11884 - Loss: 29.7285\n",
      "Processing batch 9876/11884 - Loss: 30.1364\n",
      "Processing batch 9877/11884 - Loss: 30.4062\n",
      "Processing batch 9878/11884 - Loss: 30.3974\n",
      "Processing batch 9879/11884 - Loss: 29.7952\n",
      "Processing batch 9880/11884 - Loss: 30.1252\n",
      "Processing batch 9881/11884 - Loss: 29.7824\n",
      "Processing batch 9882/11884 - Loss: 30.2418\n",
      "Processing batch 9883/11884 - Loss: 30.0892\n",
      "Processing batch 9884/11884 - Loss: 29.1754\n",
      "Processing batch 9885/11884 - Loss: 32.1332\n",
      "Processing batch 9886/11884 - Loss: 29.4455\n",
      "Processing batch 9887/11884 - Loss: 29.3706\n",
      "Processing batch 9888/11884 - Loss: 29.8196\n",
      "Processing batch 9889/11884 - Loss: 30.1221\n",
      "Processing batch 9890/11884 - Loss: 30.7235\n",
      "Processing batch 9891/11884 - Loss: 29.6367\n",
      "Processing batch 9892/11884 - Loss: 29.5679\n",
      "Processing batch 9893/11884 - Loss: 29.7294\n",
      "Processing batch 9894/11884 - Loss: 28.3799\n",
      "Processing batch 9895/11884 - Loss: 29.9435\n",
      "Processing batch 9896/11884 - Loss: 29.6298\n",
      "Processing batch 9897/11884 - Loss: 31.4692\n",
      "Processing batch 9898/11884 - Loss: 29.5430\n",
      "Processing batch 9899/11884 - Loss: 29.3632\n",
      "Processing batch 9900/11884 - Loss: 29.7678\n",
      "Processing batch 9901/11884 - Loss: 31.6855\n",
      "Processing batch 9902/11884 - Loss: 31.3435\n",
      "Processing batch 9903/11884 - Loss: 30.4147\n",
      "Processing batch 9904/11884 - Loss: 29.7158\n",
      "Processing batch 9905/11884 - Loss: 29.4514\n",
      "Processing batch 9906/11884 - Loss: 29.5052\n",
      "Processing batch 9907/11884 - Loss: 29.5457\n",
      "Processing batch 9908/11884 - Loss: 30.3180\n",
      "Processing batch 9909/11884 - Loss: 28.9442\n",
      "Processing batch 9910/11884 - Loss: 30.7083\n",
      "Processing batch 9911/11884 - Loss: 30.7038\n",
      "Processing batch 9912/11884 - Loss: 29.5621\n",
      "Processing batch 9913/11884 - Loss: 30.0333\n",
      "Processing batch 9914/11884 - Loss: 28.4271\n",
      "Processing batch 9915/11884 - Loss: 30.1639\n",
      "Processing batch 9916/11884 - Loss: 29.5196\n",
      "Processing batch 9917/11884 - Loss: 29.8354\n",
      "Processing batch 9918/11884 - Loss: 29.8772\n",
      "Processing batch 9919/11884 - Loss: 30.4372\n",
      "Processing batch 9920/11884 - Loss: 29.6862\n",
      "Processing batch 9921/11884 - Loss: 29.7225\n",
      "Processing batch 9922/11884 - Loss: 30.1549\n",
      "Processing batch 9923/11884 - Loss: 29.5014\n",
      "Processing batch 9924/11884 - Loss: 29.5956\n",
      "Processing batch 9925/11884 - Loss: 30.2513\n",
      "Processing batch 9926/11884 - Loss: 29.1493\n",
      "Processing batch 9927/11884 - Loss: 29.7834\n",
      "Processing batch 9928/11884 - Loss: 29.4214\n",
      "Processing batch 9929/11884 - Loss: 29.3468\n",
      "Processing batch 9930/11884 - Loss: 28.7305\n",
      "Processing batch 9931/11884 - Loss: 28.7265\n",
      "Processing batch 9932/11884 - Loss: 29.9120\n",
      "Processing batch 9933/11884 - Loss: 29.9001\n",
      "Processing batch 9934/11884 - Loss: 29.9514\n",
      "Processing batch 9935/11884 - Loss: 29.7273\n",
      "Processing batch 9936/11884 - Loss: 31.1735\n",
      "Processing batch 9937/11884 - Loss: 29.2790\n",
      "Processing batch 9938/11884 - Loss: 29.6431\n",
      "Processing batch 9939/11884 - Loss: 29.7237\n",
      "Processing batch 9940/11884 - Loss: 30.5038\n",
      "Processing batch 9941/11884 - Loss: 30.4582\n",
      "Processing batch 9942/11884 - Loss: 30.2236\n",
      "Processing batch 9943/11884 - Loss: 31.6960\n",
      "Processing batch 9944/11884 - Loss: 30.9240\n",
      "Processing batch 9945/11884 - Loss: 30.5171\n",
      "Processing batch 9946/11884 - Loss: 30.8044\n",
      "Processing batch 9947/11884 - Loss: 29.1937\n",
      "Processing batch 9948/11884 - Loss: 29.5448\n",
      "Processing batch 9949/11884 - Loss: 30.2155\n",
      "Processing batch 9950/11884 - Loss: 29.3850\n",
      "Processing batch 9951/11884 - Loss: 28.1951\n",
      "Processing batch 9952/11884 - Loss: 30.1055\n",
      "Processing batch 9953/11884 - Loss: 28.7077\n",
      "Processing batch 9954/11884 - Loss: 29.9509\n",
      "Processing batch 9955/11884 - Loss: 30.3518\n",
      "Processing batch 9956/11884 - Loss: 30.1660\n",
      "Processing batch 9957/11884 - Loss: 30.0766\n",
      "Processing batch 9958/11884 - Loss: 29.1759\n",
      "Processing batch 9959/11884 - Loss: 29.5878\n",
      "Processing batch 9960/11884 - Loss: 30.4990\n",
      "Processing batch 9961/11884 - Loss: 29.0819\n",
      "Processing batch 9962/11884 - Loss: 30.0598\n",
      "Processing batch 9963/11884 - Loss: 30.6251\n",
      "Processing batch 9964/11884 - Loss: 28.6859\n",
      "Processing batch 9965/11884 - Loss: 29.0521\n",
      "Processing batch 9966/11884 - Loss: 30.3862\n",
      "Processing batch 9967/11884 - Loss: 29.8713\n",
      "Processing batch 9968/11884 - Loss: 29.1007\n",
      "Processing batch 9969/11884 - Loss: 29.7092\n",
      "Processing batch 9970/11884 - Loss: 28.4251\n",
      "Processing batch 9971/11884 - Loss: 29.2218\n",
      "Processing batch 9972/11884 - Loss: 29.9034\n",
      "Processing batch 9973/11884 - Loss: 31.8075\n",
      "Processing batch 9974/11884 - Loss: 29.3215\n",
      "Processing batch 9975/11884 - Loss: 30.1286\n",
      "Processing batch 9976/11884 - Loss: 29.9349\n",
      "Processing batch 9977/11884 - Loss: 28.8119\n",
      "Processing batch 9978/11884 - Loss: 29.5865\n",
      "Processing batch 9979/11884 - Loss: 30.3659\n",
      "Processing batch 9980/11884 - Loss: 29.8913\n",
      "Processing batch 9981/11884 - Loss: 29.8961\n",
      "Processing batch 9982/11884 - Loss: 29.3582\n",
      "Processing batch 9983/11884 - Loss: 30.8355\n",
      "Processing batch 9984/11884 - Loss: 29.7622\n",
      "Processing batch 9985/11884 - Loss: 28.7858\n",
      "Processing batch 9986/11884 - Loss: 28.7932\n",
      "Processing batch 9987/11884 - Loss: 30.3067\n",
      "Processing batch 9988/11884 - Loss: 29.4244\n",
      "Processing batch 9989/11884 - Loss: 30.6362\n",
      "Processing batch 9990/11884 - Loss: 29.5414\n",
      "Processing batch 9991/11884 - Loss: 29.6654\n",
      "Processing batch 9992/11884 - Loss: 29.8182\n",
      "Processing batch 9993/11884 - Loss: 31.7635\n",
      "Processing batch 9994/11884 - Loss: 31.6328\n",
      "Processing batch 9995/11884 - Loss: 30.0199\n",
      "Processing batch 9996/11884 - Loss: 29.7477\n",
      "Processing batch 9997/11884 - Loss: 28.2384\n",
      "Processing batch 9998/11884 - Loss: 31.1529\n",
      "Processing batch 9999/11884 - Loss: 29.7149\n",
      "Processing batch 10000/11884 - Loss: 29.5178\n",
      "Processing batch 10001/11884 - Loss: 29.4844\n",
      "Processing batch 10002/11884 - Loss: 30.0233\n",
      "Processing batch 10003/11884 - Loss: 29.7432\n",
      "Processing batch 10004/11884 - Loss: 29.6679\n",
      "Processing batch 10005/11884 - Loss: 29.1615\n",
      "Processing batch 10006/11884 - Loss: 30.2763\n",
      "Processing batch 10007/11884 - Loss: 29.4345\n",
      "Processing batch 10008/11884 - Loss: 30.5537\n",
      "Processing batch 10009/11884 - Loss: 28.5861\n",
      "Processing batch 10010/11884 - Loss: 31.0728\n",
      "Processing batch 10011/11884 - Loss: 30.2557\n",
      "Processing batch 10012/11884 - Loss: 30.6287\n",
      "Processing batch 10013/11884 - Loss: 28.8760\n",
      "Processing batch 10014/11884 - Loss: 28.0803\n",
      "Processing batch 10015/11884 - Loss: 30.6799\n",
      "Processing batch 10016/11884 - Loss: 30.6476\n",
      "Processing batch 10017/11884 - Loss: 31.4316\n",
      "Processing batch 10018/11884 - Loss: 29.7458\n",
      "Processing batch 10019/11884 - Loss: 29.9238\n",
      "Processing batch 10020/11884 - Loss: 29.0342\n",
      "Processing batch 10021/11884 - Loss: 29.1379\n",
      "Processing batch 10022/11884 - Loss: 30.2315\n",
      "Processing batch 10023/11884 - Loss: 30.2981\n",
      "Processing batch 10024/11884 - Loss: 29.3579\n",
      "Processing batch 10025/11884 - Loss: 30.7762\n",
      "Processing batch 10026/11884 - Loss: 29.9120\n",
      "Processing batch 10027/11884 - Loss: 29.1372\n",
      "Processing batch 10028/11884 - Loss: 30.3813\n",
      "Processing batch 10029/11884 - Loss: 29.0474\n",
      "Processing batch 10030/11884 - Loss: 29.8224\n",
      "Processing batch 10031/11884 - Loss: 29.8166\n",
      "Processing batch 10032/11884 - Loss: 30.2257\n",
      "Processing batch 10033/11884 - Loss: 30.5399\n",
      "Processing batch 10034/11884 - Loss: 29.8300\n",
      "Processing batch 10035/11884 - Loss: 29.9193\n",
      "Processing batch 10036/11884 - Loss: 31.1052\n",
      "Processing batch 10037/11884 - Loss: 30.0299\n",
      "Processing batch 10038/11884 - Loss: 30.5884\n",
      "Processing batch 10039/11884 - Loss: 30.3466\n",
      "Processing batch 10040/11884 - Loss: 29.5832\n",
      "Processing batch 10041/11884 - Loss: 30.8566\n",
      "Processing batch 10042/11884 - Loss: 29.7107\n",
      "Processing batch 10043/11884 - Loss: 29.4252\n",
      "Processing batch 10044/11884 - Loss: 30.4075\n",
      "Processing batch 10045/11884 - Loss: 30.1391\n",
      "Processing batch 10046/11884 - Loss: 29.6772\n",
      "Processing batch 10047/11884 - Loss: 31.6337\n",
      "Processing batch 10048/11884 - Loss: 30.7428\n",
      "Processing batch 10049/11884 - Loss: 29.9583\n",
      "Processing batch 10050/11884 - Loss: 29.7490\n",
      "Processing batch 10051/11884 - Loss: 29.7001\n",
      "Processing batch 10052/11884 - Loss: 30.4211\n",
      "Processing batch 10053/11884 - Loss: 30.2312\n",
      "Processing batch 10054/11884 - Loss: 30.6958\n",
      "Processing batch 10055/11884 - Loss: 29.6545\n",
      "Processing batch 10056/11884 - Loss: 29.0137\n",
      "Processing batch 10057/11884 - Loss: 30.5796\n",
      "Processing batch 10058/11884 - Loss: 30.4264\n",
      "Processing batch 10059/11884 - Loss: 30.0134\n",
      "Processing batch 10060/11884 - Loss: 30.9568\n",
      "Processing batch 10061/11884 - Loss: 31.3057\n",
      "Processing batch 10062/11884 - Loss: 31.0892\n",
      "Processing batch 10063/11884 - Loss: 29.0097\n",
      "Processing batch 10064/11884 - Loss: 30.4576\n",
      "Processing batch 10065/11884 - Loss: 28.8414\n",
      "Processing batch 10066/11884 - Loss: 29.7871\n",
      "Processing batch 10067/11884 - Loss: 29.7679\n",
      "Processing batch 10068/11884 - Loss: 30.2517\n",
      "Processing batch 10069/11884 - Loss: 30.7491\n",
      "Processing batch 10070/11884 - Loss: 31.5590\n",
      "Processing batch 10071/11884 - Loss: 30.3238\n",
      "Processing batch 10072/11884 - Loss: 29.4461\n",
      "Processing batch 10073/11884 - Loss: 30.6520\n",
      "Processing batch 10074/11884 - Loss: 28.4621\n",
      "Processing batch 10075/11884 - Loss: 28.6256\n",
      "Processing batch 10076/11884 - Loss: 29.1737\n",
      "Processing batch 10077/11884 - Loss: 30.7373\n",
      "Processing batch 10078/11884 - Loss: 28.5117\n",
      "Processing batch 10079/11884 - Loss: 28.6248\n",
      "Processing batch 10080/11884 - Loss: 29.1658\n",
      "Processing batch 10081/11884 - Loss: 30.8118\n",
      "Processing batch 10082/11884 - Loss: 29.4948\n",
      "Processing batch 10083/11884 - Loss: 29.7356\n",
      "Processing batch 10084/11884 - Loss: 30.5029\n",
      "Processing batch 10085/11884 - Loss: 29.1354\n",
      "Processing batch 10086/11884 - Loss: 30.8307\n",
      "Processing batch 10087/11884 - Loss: 29.9921\n",
      "Processing batch 10088/11884 - Loss: 30.2933\n",
      "Processing batch 10089/11884 - Loss: 30.4673\n",
      "Processing batch 10090/11884 - Loss: 29.9874\n",
      "Processing batch 10091/11884 - Loss: 30.0173\n",
      "Processing batch 10092/11884 - Loss: 30.9892\n",
      "Processing batch 10093/11884 - Loss: 29.8682\n",
      "Processing batch 10094/11884 - Loss: 30.2113\n",
      "Processing batch 10095/11884 - Loss: 29.7213\n",
      "Processing batch 10096/11884 - Loss: 29.2278\n",
      "Processing batch 10097/11884 - Loss: 30.1253\n",
      "Processing batch 10098/11884 - Loss: 32.1607\n",
      "Processing batch 10099/11884 - Loss: 30.5098\n",
      "Processing batch 10100/11884 - Loss: 30.5351\n",
      "Processing batch 10101/11884 - Loss: 29.5221\n",
      "Processing batch 10102/11884 - Loss: 29.7720\n",
      "Processing batch 10103/11884 - Loss: 30.1905\n",
      "Processing batch 10104/11884 - Loss: 29.1951\n",
      "Processing batch 10105/11884 - Loss: 31.2939\n",
      "Processing batch 10106/11884 - Loss: 30.8699\n",
      "Processing batch 10107/11884 - Loss: 30.4002\n",
      "Processing batch 10108/11884 - Loss: 30.1731\n",
      "Processing batch 10109/11884 - Loss: 30.5071\n",
      "Processing batch 10110/11884 - Loss: 29.5565\n",
      "Processing batch 10111/11884 - Loss: 29.5601\n",
      "Processing batch 10112/11884 - Loss: 29.1069\n",
      "Processing batch 10113/11884 - Loss: 31.6170\n",
      "Processing batch 10114/11884 - Loss: 29.0288\n",
      "Processing batch 10115/11884 - Loss: 30.0213\n",
      "Processing batch 10116/11884 - Loss: 30.3076\n",
      "Processing batch 10117/11884 - Loss: 29.8301\n",
      "Processing batch 10118/11884 - Loss: 30.3057\n",
      "Processing batch 10119/11884 - Loss: 28.6279\n",
      "Processing batch 10120/11884 - Loss: 31.2743\n",
      "Processing batch 10121/11884 - Loss: 31.0793\n",
      "Processing batch 10122/11884 - Loss: 30.2131\n",
      "Processing batch 10123/11884 - Loss: 29.2131\n",
      "Processing batch 10124/11884 - Loss: 29.8847\n",
      "Processing batch 10125/11884 - Loss: 31.5101\n",
      "Processing batch 10126/11884 - Loss: 30.6196\n",
      "Processing batch 10127/11884 - Loss: 30.3917\n",
      "Processing batch 10128/11884 - Loss: 31.3017\n",
      "Processing batch 10129/11884 - Loss: 30.4501\n",
      "Processing batch 10130/11884 - Loss: 29.7307\n",
      "Processing batch 10131/11884 - Loss: 29.7529\n",
      "Processing batch 10132/11884 - Loss: 31.3941\n",
      "Processing batch 10133/11884 - Loss: 29.1615\n",
      "Processing batch 10134/11884 - Loss: 30.8127\n",
      "Processing batch 10135/11884 - Loss: 29.6872\n",
      "Processing batch 10136/11884 - Loss: 29.1203\n",
      "Processing batch 10137/11884 - Loss: 30.0913\n",
      "Processing batch 10138/11884 - Loss: 30.4133\n",
      "Processing batch 10139/11884 - Loss: 30.3714\n",
      "Processing batch 10140/11884 - Loss: 30.6351\n",
      "Processing batch 10141/11884 - Loss: 29.2509\n",
      "Processing batch 10142/11884 - Loss: 29.7078\n",
      "Processing batch 10143/11884 - Loss: 29.0509\n",
      "Processing batch 10144/11884 - Loss: 29.7887\n",
      "Processing batch 10145/11884 - Loss: 29.7362\n",
      "Processing batch 10146/11884 - Loss: 30.1035\n",
      "Processing batch 10147/11884 - Loss: 30.7840\n",
      "Processing batch 10148/11884 - Loss: 29.7455\n",
      "Processing batch 10149/11884 - Loss: 29.3303\n",
      "Processing batch 10150/11884 - Loss: 29.9044\n",
      "Processing batch 10151/11884 - Loss: 30.2622\n",
      "Processing batch 10152/11884 - Loss: 29.4092\n",
      "Processing batch 10153/11884 - Loss: 30.8659\n",
      "Processing batch 10154/11884 - Loss: 29.7243\n",
      "Processing batch 10155/11884 - Loss: 29.1247\n",
      "Processing batch 10156/11884 - Loss: 30.2281\n",
      "Processing batch 10157/11884 - Loss: 31.1825\n",
      "Processing batch 10158/11884 - Loss: 30.6000\n",
      "Processing batch 10159/11884 - Loss: 29.2236\n",
      "Processing batch 10160/11884 - Loss: 28.7206\n",
      "Processing batch 10161/11884 - Loss: 31.0138\n",
      "Processing batch 10162/11884 - Loss: 30.0818\n",
      "Processing batch 10163/11884 - Loss: 28.8848\n",
      "Processing batch 10164/11884 - Loss: 29.5365\n",
      "Processing batch 10165/11884 - Loss: 30.0035\n",
      "Processing batch 10166/11884 - Loss: 30.0688\n",
      "Processing batch 10167/11884 - Loss: 30.9774\n",
      "Processing batch 10168/11884 - Loss: 30.4445\n",
      "Processing batch 10169/11884 - Loss: 29.0883\n",
      "Processing batch 10170/11884 - Loss: 31.0219\n",
      "Processing batch 10171/11884 - Loss: 28.7191\n",
      "Processing batch 10172/11884 - Loss: 30.9228\n",
      "Processing batch 10173/11884 - Loss: 29.4105\n",
      "Processing batch 10174/11884 - Loss: 29.9236\n",
      "Processing batch 10175/11884 - Loss: 29.9373\n",
      "Processing batch 10176/11884 - Loss: 29.9560\n",
      "Processing batch 10177/11884 - Loss: 30.3643\n",
      "Processing batch 10178/11884 - Loss: 30.7560\n",
      "Processing batch 10179/11884 - Loss: 29.3386\n",
      "Processing batch 10180/11884 - Loss: 31.0303\n",
      "Processing batch 10181/11884 - Loss: 29.9380\n",
      "Processing batch 10182/11884 - Loss: 30.4801\n",
      "Processing batch 10183/11884 - Loss: 29.8011\n",
      "Processing batch 10184/11884 - Loss: 28.9673\n",
      "Processing batch 10185/11884 - Loss: 29.7548\n",
      "Processing batch 10186/11884 - Loss: 30.4498\n",
      "Processing batch 10187/11884 - Loss: 31.0672\n",
      "Processing batch 10188/11884 - Loss: 30.0794\n",
      "Processing batch 10189/11884 - Loss: 29.0396\n",
      "Processing batch 10190/11884 - Loss: 29.8241\n",
      "Processing batch 10191/11884 - Loss: 29.6691\n",
      "Processing batch 10192/11884 - Loss: 29.5439\n",
      "Processing batch 10193/11884 - Loss: 28.8015\n",
      "Processing batch 10194/11884 - Loss: 29.0391\n",
      "Processing batch 10195/11884 - Loss: 30.3532\n",
      "Processing batch 10196/11884 - Loss: 29.5100\n",
      "Processing batch 10197/11884 - Loss: 29.8473\n",
      "Processing batch 10198/11884 - Loss: 29.8864\n",
      "Processing batch 10199/11884 - Loss: 30.2781\n",
      "Processing batch 10200/11884 - Loss: 29.7674\n",
      "Processing batch 10201/11884 - Loss: 30.0552\n",
      "Processing batch 10202/11884 - Loss: 29.3218\n",
      "Processing batch 10203/11884 - Loss: 30.4002\n",
      "Processing batch 10204/11884 - Loss: 30.8968\n",
      "Processing batch 10205/11884 - Loss: 28.7768\n",
      "Processing batch 10206/11884 - Loss: 30.4160\n",
      "Processing batch 10207/11884 - Loss: 30.3057\n",
      "Processing batch 10208/11884 - Loss: 30.1364\n",
      "Processing batch 10209/11884 - Loss: 28.9246\n",
      "Processing batch 10210/11884 - Loss: 30.7320\n",
      "Processing batch 10211/11884 - Loss: 28.8854\n",
      "Processing batch 10212/11884 - Loss: 30.1834\n",
      "Processing batch 10213/11884 - Loss: 29.7794\n",
      "Processing batch 10214/11884 - Loss: 29.6403\n",
      "Processing batch 10215/11884 - Loss: 29.7107\n",
      "Processing batch 10216/11884 - Loss: 30.8806\n",
      "Processing batch 10217/11884 - Loss: 30.8887\n",
      "Processing batch 10218/11884 - Loss: 28.9600\n",
      "Processing batch 10219/11884 - Loss: 30.0097\n",
      "Processing batch 10220/11884 - Loss: 29.6365\n",
      "Processing batch 10221/11884 - Loss: 30.7007\n",
      "Processing batch 10222/11884 - Loss: 28.9826\n",
      "Processing batch 10223/11884 - Loss: 29.6208\n",
      "Processing batch 10224/11884 - Loss: 29.7088\n",
      "Processing batch 10225/11884 - Loss: 30.4012\n",
      "Processing batch 10226/11884 - Loss: 30.1363\n",
      "Processing batch 10227/11884 - Loss: 29.4531\n",
      "Processing batch 10228/11884 - Loss: 28.9004\n",
      "Processing batch 10229/11884 - Loss: 29.8809\n",
      "Processing batch 10230/11884 - Loss: 30.7076\n",
      "Processing batch 10231/11884 - Loss: 30.3671\n",
      "Processing batch 10232/11884 - Loss: 29.1087\n",
      "Processing batch 10233/11884 - Loss: 30.9629\n",
      "Processing batch 10234/11884 - Loss: 30.0227\n",
      "Processing batch 10235/11884 - Loss: 30.0634\n",
      "Processing batch 10236/11884 - Loss: 29.5357\n",
      "Processing batch 10237/11884 - Loss: 30.7268\n",
      "Processing batch 10238/11884 - Loss: 29.5577\n",
      "Processing batch 10239/11884 - Loss: 29.9838\n",
      "Processing batch 10240/11884 - Loss: 28.8479\n",
      "Processing batch 10241/11884 - Loss: 29.7550\n",
      "Processing batch 10242/11884 - Loss: 30.6336\n",
      "Processing batch 10243/11884 - Loss: 29.3691\n",
      "Processing batch 10244/11884 - Loss: 28.9938\n",
      "Processing batch 10245/11884 - Loss: 30.0156\n",
      "Processing batch 10246/11884 - Loss: 30.8434\n",
      "Processing batch 10247/11884 - Loss: 30.8638\n",
      "Processing batch 10248/11884 - Loss: 30.5152\n",
      "Processing batch 10249/11884 - Loss: 31.0162\n",
      "Processing batch 10250/11884 - Loss: 30.7681\n",
      "Processing batch 10251/11884 - Loss: 29.6139\n",
      "Processing batch 10252/11884 - Loss: 29.5371\n",
      "Processing batch 10253/11884 - Loss: 29.5617\n",
      "Processing batch 10254/11884 - Loss: 29.6135\n",
      "Processing batch 10255/11884 - Loss: 29.5146\n",
      "Processing batch 10256/11884 - Loss: 29.8612\n",
      "Processing batch 10257/11884 - Loss: 30.9469\n",
      "Processing batch 10258/11884 - Loss: 30.0727\n",
      "Processing batch 10259/11884 - Loss: 30.2577\n",
      "Processing batch 10260/11884 - Loss: 31.8145\n",
      "Processing batch 10261/11884 - Loss: 30.8185\n",
      "Processing batch 10262/11884 - Loss: 30.0069\n",
      "Processing batch 10263/11884 - Loss: 29.5418\n",
      "Processing batch 10264/11884 - Loss: 30.9806\n",
      "Processing batch 10265/11884 - Loss: 30.1955\n",
      "Processing batch 10266/11884 - Loss: 30.1400\n",
      "Processing batch 10267/11884 - Loss: 30.5789\n",
      "Processing batch 10268/11884 - Loss: 28.4360\n",
      "Processing batch 10269/11884 - Loss: 30.3978\n",
      "Processing batch 10270/11884 - Loss: 29.5088\n",
      "Processing batch 10271/11884 - Loss: 29.8623\n",
      "Processing batch 10272/11884 - Loss: 30.2133\n",
      "Processing batch 10273/11884 - Loss: 29.0432\n",
      "Processing batch 10274/11884 - Loss: 29.8817\n",
      "Processing batch 10275/11884 - Loss: 30.5037\n",
      "Processing batch 10276/11884 - Loss: 29.7162\n",
      "Processing batch 10277/11884 - Loss: 28.9714\n",
      "Processing batch 10278/11884 - Loss: 31.4040\n",
      "Processing batch 10279/11884 - Loss: 30.7385\n",
      "Processing batch 10280/11884 - Loss: 28.9438\n",
      "Processing batch 10281/11884 - Loss: 29.1323\n",
      "Processing batch 10282/11884 - Loss: 28.9241\n",
      "Processing batch 10283/11884 - Loss: 29.4603\n",
      "Processing batch 10284/11884 - Loss: 30.4459\n",
      "Processing batch 10285/11884 - Loss: 29.1840\n",
      "Processing batch 10286/11884 - Loss: 31.5413\n",
      "Processing batch 10287/11884 - Loss: 29.2784\n",
      "Processing batch 10288/11884 - Loss: 29.0664\n",
      "Processing batch 10289/11884 - Loss: 30.0195\n",
      "Processing batch 10290/11884 - Loss: 31.1406\n",
      "Processing batch 10291/11884 - Loss: 30.2067\n",
      "Processing batch 10292/11884 - Loss: 29.8686\n",
      "Processing batch 10293/11884 - Loss: 29.8491\n",
      "Processing batch 10294/11884 - Loss: 31.3046\n",
      "Processing batch 10295/11884 - Loss: 29.8790\n",
      "Processing batch 10296/11884 - Loss: 30.7128\n",
      "Processing batch 10297/11884 - Loss: 29.5222\n",
      "Processing batch 10298/11884 - Loss: 30.3477\n",
      "Processing batch 10299/11884 - Loss: 29.9615\n",
      "Processing batch 10300/11884 - Loss: 29.9888\n",
      "Processing batch 10301/11884 - Loss: 30.3077\n",
      "Processing batch 10302/11884 - Loss: 30.7010\n",
      "Processing batch 10303/11884 - Loss: 29.7663\n",
      "Processing batch 10304/11884 - Loss: 31.2855\n",
      "Processing batch 10305/11884 - Loss: 29.0679\n",
      "Processing batch 10306/11884 - Loss: 29.9490\n",
      "Processing batch 10307/11884 - Loss: 29.4824\n",
      "Processing batch 10308/11884 - Loss: 30.9610\n",
      "Processing batch 10309/11884 - Loss: 29.4967\n",
      "Processing batch 10310/11884 - Loss: 30.7182\n",
      "Processing batch 10311/11884 - Loss: 31.1014\n",
      "Processing batch 10312/11884 - Loss: 29.1295\n",
      "Processing batch 10313/11884 - Loss: 31.0591\n",
      "Processing batch 10314/11884 - Loss: 30.6602\n",
      "Processing batch 10315/11884 - Loss: 29.6130\n",
      "Processing batch 10316/11884 - Loss: 28.3923\n",
      "Processing batch 10317/11884 - Loss: 29.7932\n",
      "Processing batch 10318/11884 - Loss: 30.4326\n",
      "Processing batch 10319/11884 - Loss: 31.1173\n",
      "Processing batch 10320/11884 - Loss: 30.1016\n",
      "Processing batch 10321/11884 - Loss: 29.4694\n",
      "Processing batch 10322/11884 - Loss: 30.8165\n",
      "Processing batch 10323/11884 - Loss: 31.1208\n",
      "Processing batch 10324/11884 - Loss: 30.5321\n",
      "Processing batch 10325/11884 - Loss: 29.5807\n",
      "Processing batch 10326/11884 - Loss: 29.1611\n",
      "Processing batch 10327/11884 - Loss: 28.7847\n",
      "Processing batch 10328/11884 - Loss: 30.7684\n",
      "Processing batch 10329/11884 - Loss: 30.4754\n",
      "Processing batch 10330/11884 - Loss: 29.9599\n",
      "Processing batch 10331/11884 - Loss: 30.3394\n",
      "Processing batch 10332/11884 - Loss: 29.7510\n",
      "Processing batch 10333/11884 - Loss: 29.2474\n",
      "Processing batch 10334/11884 - Loss: 29.7335\n",
      "Processing batch 10335/11884 - Loss: 29.3687\n",
      "Processing batch 10336/11884 - Loss: 28.8554\n",
      "Processing batch 10337/11884 - Loss: 30.5693\n",
      "Processing batch 10338/11884 - Loss: 30.1283\n",
      "Processing batch 10339/11884 - Loss: 30.3078\n",
      "Processing batch 10340/11884 - Loss: 30.5518\n",
      "Processing batch 10341/11884 - Loss: 31.2828\n",
      "Processing batch 10342/11884 - Loss: 30.2792\n",
      "Processing batch 10343/11884 - Loss: 29.3043\n",
      "Processing batch 10344/11884 - Loss: 30.5311\n",
      "Processing batch 10345/11884 - Loss: 31.7099\n",
      "Processing batch 10346/11884 - Loss: 29.7745\n",
      "Processing batch 10347/11884 - Loss: 30.4537\n",
      "Processing batch 10348/11884 - Loss: 31.2344\n",
      "Processing batch 10349/11884 - Loss: 30.0260\n",
      "Processing batch 10350/11884 - Loss: 31.3915\n",
      "Processing batch 10351/11884 - Loss: 31.6326\n",
      "Processing batch 10352/11884 - Loss: 30.3904\n",
      "Processing batch 10353/11884 - Loss: 29.1743\n",
      "Processing batch 10354/11884 - Loss: 29.6801\n",
      "Processing batch 10355/11884 - Loss: 29.9392\n",
      "Processing batch 10356/11884 - Loss: 30.4017\n",
      "Processing batch 10357/11884 - Loss: 29.0465\n",
      "Processing batch 10358/11884 - Loss: 29.5420\n",
      "Processing batch 10359/11884 - Loss: 30.7051\n",
      "Processing batch 10360/11884 - Loss: 30.9452\n",
      "Processing batch 10361/11884 - Loss: 28.3417\n",
      "Processing batch 10362/11884 - Loss: 30.7109\n",
      "Processing batch 10363/11884 - Loss: 30.7216\n",
      "Processing batch 10364/11884 - Loss: 28.9098\n",
      "Processing batch 10365/11884 - Loss: 29.4390\n",
      "Processing batch 10366/11884 - Loss: 30.6186\n",
      "Processing batch 10367/11884 - Loss: 31.8208\n",
      "Processing batch 10368/11884 - Loss: 29.4992\n",
      "Processing batch 10369/11884 - Loss: 30.8309\n",
      "Processing batch 10370/11884 - Loss: 29.3231\n",
      "Processing batch 10371/11884 - Loss: 29.9045\n",
      "Processing batch 10372/11884 - Loss: 29.5003\n",
      "Processing batch 10373/11884 - Loss: 30.6594\n",
      "Processing batch 10374/11884 - Loss: 28.9224\n",
      "Processing batch 10375/11884 - Loss: 29.8023\n",
      "Processing batch 10376/11884 - Loss: 30.6835\n",
      "Processing batch 10377/11884 - Loss: 29.8902\n",
      "Processing batch 10378/11884 - Loss: 29.6057\n",
      "Processing batch 10379/11884 - Loss: 30.4136\n",
      "Processing batch 10380/11884 - Loss: 29.4577\n",
      "Processing batch 10381/11884 - Loss: 29.4442\n",
      "Processing batch 10382/11884 - Loss: 29.1596\n",
      "Processing batch 10383/11884 - Loss: 30.2395\n",
      "Processing batch 10384/11884 - Loss: 31.4137\n",
      "Processing batch 10385/11884 - Loss: 29.9608\n",
      "Processing batch 10386/11884 - Loss: 29.4949\n",
      "Processing batch 10387/11884 - Loss: 29.3154\n",
      "Processing batch 10388/11884 - Loss: 30.0218\n",
      "Processing batch 10389/11884 - Loss: 29.4739\n",
      "Processing batch 10390/11884 - Loss: 30.3372\n",
      "Processing batch 10391/11884 - Loss: 29.9242\n",
      "Processing batch 10392/11884 - Loss: 30.9336\n",
      "Processing batch 10393/11884 - Loss: 29.5991\n",
      "Processing batch 10394/11884 - Loss: 29.6377\n",
      "Processing batch 10395/11884 - Loss: 29.8678\n",
      "Processing batch 10396/11884 - Loss: 30.2354\n",
      "Processing batch 10397/11884 - Loss: 29.1520\n",
      "Processing batch 10398/11884 - Loss: 30.9816\n",
      "Processing batch 10399/11884 - Loss: 29.2261\n",
      "Processing batch 10400/11884 - Loss: 30.2684\n",
      "Processing batch 10401/11884 - Loss: 30.4967\n",
      "Processing batch 10402/11884 - Loss: 28.6404\n",
      "Processing batch 10403/11884 - Loss: 30.6465\n",
      "Processing batch 10404/11884 - Loss: 30.4200\n",
      "Processing batch 10405/11884 - Loss: 29.2771\n",
      "Processing batch 10406/11884 - Loss: 29.7498\n",
      "Processing batch 10407/11884 - Loss: 30.9197\n",
      "Processing batch 10408/11884 - Loss: 29.6981\n",
      "Processing batch 10409/11884 - Loss: 30.1939\n",
      "Processing batch 10410/11884 - Loss: 29.6268\n",
      "Processing batch 10411/11884 - Loss: 29.8695\n",
      "Processing batch 10412/11884 - Loss: 28.6623\n",
      "Processing batch 10413/11884 - Loss: 30.3367\n",
      "Processing batch 10414/11884 - Loss: 29.5568\n",
      "Processing batch 10415/11884 - Loss: 29.7414\n",
      "Processing batch 10416/11884 - Loss: 29.7850\n",
      "Processing batch 10417/11884 - Loss: 31.3126\n",
      "Processing batch 10418/11884 - Loss: 29.6498\n",
      "Processing batch 10419/11884 - Loss: 30.0843\n",
      "Processing batch 10420/11884 - Loss: 30.6633\n",
      "Processing batch 10421/11884 - Loss: 30.0961\n",
      "Processing batch 10422/11884 - Loss: 30.5410\n",
      "Processing batch 10423/11884 - Loss: 29.8104\n",
      "Processing batch 10424/11884 - Loss: 28.9454\n",
      "Processing batch 10425/11884 - Loss: 30.9130\n",
      "Processing batch 10426/11884 - Loss: 30.7829\n",
      "Processing batch 10427/11884 - Loss: 30.2882\n",
      "Processing batch 10428/11884 - Loss: 31.8509\n",
      "Processing batch 10429/11884 - Loss: 29.8190\n",
      "Processing batch 10430/11884 - Loss: 30.4492\n",
      "Processing batch 10431/11884 - Loss: 30.5530\n",
      "Processing batch 10432/11884 - Loss: 30.0274\n",
      "Processing batch 10433/11884 - Loss: 29.1750\n",
      "Processing batch 10434/11884 - Loss: 29.8585\n",
      "Processing batch 10435/11884 - Loss: 29.5971\n",
      "Processing batch 10436/11884 - Loss: 30.2351\n",
      "Processing batch 10437/11884 - Loss: 29.3031\n",
      "Processing batch 10438/11884 - Loss: 29.9464\n",
      "Processing batch 10439/11884 - Loss: 28.9154\n",
      "Processing batch 10440/11884 - Loss: 29.3813\n",
      "Processing batch 10441/11884 - Loss: 31.6923\n",
      "Processing batch 10442/11884 - Loss: 31.0412\n",
      "Processing batch 10443/11884 - Loss: 30.2574\n",
      "Processing batch 10444/11884 - Loss: 30.5311\n",
      "Processing batch 10445/11884 - Loss: 31.0266\n",
      "Processing batch 10446/11884 - Loss: 31.4234\n",
      "Processing batch 10447/11884 - Loss: 29.6316\n",
      "Processing batch 10448/11884 - Loss: 28.5036\n",
      "Processing batch 10449/11884 - Loss: 29.9165\n",
      "Processing batch 10450/11884 - Loss: 30.8205\n",
      "Processing batch 10451/11884 - Loss: 30.2854\n",
      "Processing batch 10452/11884 - Loss: 28.1827\n",
      "Processing batch 10453/11884 - Loss: 29.5871\n",
      "Processing batch 10454/11884 - Loss: 29.8323\n",
      "Processing batch 10455/11884 - Loss: 29.8853\n",
      "Processing batch 10456/11884 - Loss: 29.5662\n",
      "Processing batch 10457/11884 - Loss: 30.1552\n",
      "Processing batch 10458/11884 - Loss: 30.1574\n",
      "Processing batch 10459/11884 - Loss: 29.9356\n",
      "Processing batch 10460/11884 - Loss: 29.5679\n",
      "Processing batch 10461/11884 - Loss: 30.6727\n",
      "Processing batch 10462/11884 - Loss: 30.4018\n",
      "Processing batch 10463/11884 - Loss: 30.1232\n",
      "Processing batch 10464/11884 - Loss: 30.6051\n",
      "Processing batch 10465/11884 - Loss: 29.8582\n",
      "Processing batch 10466/11884 - Loss: 30.2509\n",
      "Processing batch 10467/11884 - Loss: 29.8873\n",
      "Processing batch 10468/11884 - Loss: 31.6811\n",
      "Processing batch 10469/11884 - Loss: 29.4549\n",
      "Processing batch 10470/11884 - Loss: 30.2929\n",
      "Processing batch 10471/11884 - Loss: 28.0534\n",
      "Processing batch 10472/11884 - Loss: 29.9268\n",
      "Processing batch 10473/11884 - Loss: 29.7597\n",
      "Processing batch 10474/11884 - Loss: 28.0636\n",
      "Processing batch 10475/11884 - Loss: 30.4201\n",
      "Processing batch 10476/11884 - Loss: 28.0292\n",
      "Processing batch 10477/11884 - Loss: 29.2664\n",
      "Processing batch 10478/11884 - Loss: 30.4114\n",
      "Processing batch 10479/11884 - Loss: 29.9145\n",
      "Processing batch 10480/11884 - Loss: 30.4727\n",
      "Processing batch 10481/11884 - Loss: 30.0183\n",
      "Processing batch 10482/11884 - Loss: 30.3660\n",
      "Processing batch 10483/11884 - Loss: 28.9871\n",
      "Processing batch 10484/11884 - Loss: 30.3930\n",
      "Processing batch 10485/11884 - Loss: 28.7871\n",
      "Processing batch 10486/11884 - Loss: 29.8274\n",
      "Processing batch 10487/11884 - Loss: 30.7722\n",
      "Processing batch 10488/11884 - Loss: 29.7330\n",
      "Processing batch 10489/11884 - Loss: 30.2314\n",
      "Processing batch 10490/11884 - Loss: 31.3438\n",
      "Processing batch 10491/11884 - Loss: 30.0956\n",
      "Processing batch 10492/11884 - Loss: 29.9691\n",
      "Processing batch 10493/11884 - Loss: 30.5400\n",
      "Processing batch 10494/11884 - Loss: 28.5165\n",
      "Processing batch 10495/11884 - Loss: 28.6651\n",
      "Processing batch 10496/11884 - Loss: 29.8294\n",
      "Processing batch 10497/11884 - Loss: 30.4596\n",
      "Processing batch 10498/11884 - Loss: 30.2938\n",
      "Processing batch 10499/11884 - Loss: 30.1933\n",
      "Processing batch 10500/11884 - Loss: 29.5240\n",
      "Processing batch 10501/11884 - Loss: 29.8506\n",
      "Processing batch 10502/11884 - Loss: 30.4130\n",
      "Processing batch 10503/11884 - Loss: 29.7095\n",
      "Processing batch 10504/11884 - Loss: 30.5359\n",
      "Processing batch 10505/11884 - Loss: 28.7356\n",
      "Processing batch 10506/11884 - Loss: 30.1335\n",
      "Processing batch 10507/11884 - Loss: 30.2620\n",
      "Processing batch 10508/11884 - Loss: 31.4257\n",
      "Processing batch 10509/11884 - Loss: 30.1520\n",
      "Processing batch 10510/11884 - Loss: 30.2177\n",
      "Processing batch 10511/11884 - Loss: 29.6963\n",
      "Processing batch 10512/11884 - Loss: 30.3308\n",
      "Processing batch 10513/11884 - Loss: 30.0033\n",
      "Processing batch 10514/11884 - Loss: 29.9390\n",
      "Processing batch 10515/11884 - Loss: 30.6165\n",
      "Processing batch 10516/11884 - Loss: 29.5960\n",
      "Processing batch 10517/11884 - Loss: 30.0892\n",
      "Processing batch 10518/11884 - Loss: 30.5172\n",
      "Processing batch 10519/11884 - Loss: 29.4643\n",
      "Processing batch 10520/11884 - Loss: 30.2525\n",
      "Processing batch 10521/11884 - Loss: 29.5751\n",
      "Processing batch 10522/11884 - Loss: 29.4088\n",
      "Processing batch 10523/11884 - Loss: 29.6520\n",
      "Processing batch 10524/11884 - Loss: 28.1337\n",
      "Processing batch 10525/11884 - Loss: 29.1611\n",
      "Processing batch 10526/11884 - Loss: 30.3839\n",
      "Processing batch 10527/11884 - Loss: 29.3235\n",
      "Processing batch 10528/11884 - Loss: 30.0474\n",
      "Processing batch 10529/11884 - Loss: 30.1648\n",
      "Processing batch 10530/11884 - Loss: 30.2680\n",
      "Processing batch 10531/11884 - Loss: 29.3984\n",
      "Processing batch 10532/11884 - Loss: 30.0358\n",
      "Processing batch 10533/11884 - Loss: 29.7544\n",
      "Processing batch 10534/11884 - Loss: 30.5828\n",
      "Processing batch 10535/11884 - Loss: 31.4732\n",
      "Processing batch 10536/11884 - Loss: 30.6233\n",
      "Processing batch 10537/11884 - Loss: 31.8581\n",
      "Processing batch 10538/11884 - Loss: 29.2528\n",
      "Processing batch 10539/11884 - Loss: 30.8361\n",
      "Processing batch 10540/11884 - Loss: 29.1004\n",
      "Processing batch 10541/11884 - Loss: 32.0614\n",
      "Processing batch 10542/11884 - Loss: 30.9456\n",
      "Processing batch 10543/11884 - Loss: 30.4494\n",
      "Processing batch 10544/11884 - Loss: 30.0870\n",
      "Processing batch 10545/11884 - Loss: 30.2659\n",
      "Processing batch 10546/11884 - Loss: 28.8842\n",
      "Processing batch 10547/11884 - Loss: 30.8861\n",
      "Processing batch 10548/11884 - Loss: 29.6972\n",
      "Processing batch 10549/11884 - Loss: 29.6465\n",
      "Processing batch 10550/11884 - Loss: 29.5159\n",
      "Processing batch 10551/11884 - Loss: 30.1492\n",
      "Processing batch 10552/11884 - Loss: 30.6068\n",
      "Processing batch 10553/11884 - Loss: 30.6166\n",
      "Processing batch 10554/11884 - Loss: 29.8855\n",
      "Processing batch 10555/11884 - Loss: 30.3026\n",
      "Processing batch 10556/11884 - Loss: 28.1005\n",
      "Processing batch 10557/11884 - Loss: 30.2042\n",
      "Processing batch 10558/11884 - Loss: 31.5883\n",
      "Processing batch 10559/11884 - Loss: 29.4296\n",
      "Processing batch 10560/11884 - Loss: 29.9117\n",
      "Processing batch 10561/11884 - Loss: 31.3946\n",
      "Processing batch 10562/11884 - Loss: 30.5645\n",
      "Processing batch 10563/11884 - Loss: 29.2521\n",
      "Processing batch 10564/11884 - Loss: 29.4896\n",
      "Processing batch 10565/11884 - Loss: 29.1882\n",
      "Processing batch 10566/11884 - Loss: 31.2678\n",
      "Processing batch 10567/11884 - Loss: 30.2362\n",
      "Processing batch 10568/11884 - Loss: 30.5728\n",
      "Processing batch 10569/11884 - Loss: 29.1360\n",
      "Processing batch 10570/11884 - Loss: 30.3271\n",
      "Processing batch 10571/11884 - Loss: 30.4749\n",
      "Processing batch 10572/11884 - Loss: 29.3539\n",
      "Processing batch 10573/11884 - Loss: 30.4758\n",
      "Processing batch 10574/11884 - Loss: 31.1104\n",
      "Processing batch 10575/11884 - Loss: 30.3630\n",
      "Processing batch 10576/11884 - Loss: 29.7607\n",
      "Processing batch 10577/11884 - Loss: 29.3338\n",
      "Processing batch 10578/11884 - Loss: 29.8618\n",
      "Processing batch 10579/11884 - Loss: 30.4309\n",
      "Processing batch 10580/11884 - Loss: 29.6139\n",
      "Processing batch 10581/11884 - Loss: 30.7695\n",
      "Processing batch 10582/11884 - Loss: 30.9824\n",
      "Processing batch 10583/11884 - Loss: 29.7398\n",
      "Processing batch 10584/11884 - Loss: 29.5994\n",
      "Processing batch 10585/11884 - Loss: 29.3166\n",
      "Processing batch 10586/11884 - Loss: 29.3536\n",
      "Processing batch 10587/11884 - Loss: 29.0298\n",
      "Processing batch 10588/11884 - Loss: 32.2662\n",
      "Processing batch 10589/11884 - Loss: 30.2405\n",
      "Processing batch 10590/11884 - Loss: 30.6166\n",
      "Processing batch 10591/11884 - Loss: 28.7776\n",
      "Processing batch 10592/11884 - Loss: 29.1066\n",
      "Processing batch 10593/11884 - Loss: 30.2394\n",
      "Processing batch 10594/11884 - Loss: 30.2601\n",
      "Processing batch 10595/11884 - Loss: 30.3793\n",
      "Processing batch 10596/11884 - Loss: 29.3206\n",
      "Processing batch 10597/11884 - Loss: 30.3571\n",
      "Processing batch 10598/11884 - Loss: 29.4066\n",
      "Processing batch 10599/11884 - Loss: 29.1511\n",
      "Processing batch 10600/11884 - Loss: 29.3868\n",
      "Processing batch 10601/11884 - Loss: 31.1860\n",
      "Processing batch 10602/11884 - Loss: 30.3448\n",
      "Processing batch 10603/11884 - Loss: 29.8900\n",
      "Processing batch 10604/11884 - Loss: 29.5848\n",
      "Processing batch 10605/11884 - Loss: 30.7141\n",
      "Processing batch 10606/11884 - Loss: 30.6645\n",
      "Processing batch 10607/11884 - Loss: 29.5503\n",
      "Processing batch 10608/11884 - Loss: 29.8003\n",
      "Processing batch 10609/11884 - Loss: 30.0561\n",
      "Processing batch 10610/11884 - Loss: 30.6121\n",
      "Processing batch 10611/11884 - Loss: 29.5315\n",
      "Processing batch 10612/11884 - Loss: 30.0029\n",
      "Processing batch 10613/11884 - Loss: 30.1554\n",
      "Processing batch 10614/11884 - Loss: 30.3752\n",
      "Processing batch 10615/11884 - Loss: 29.3155\n",
      "Processing batch 10616/11884 - Loss: 29.3016\n",
      "Processing batch 10617/11884 - Loss: 28.8284\n",
      "Processing batch 10618/11884 - Loss: 30.6916\n",
      "Processing batch 10619/11884 - Loss: 30.7466\n",
      "Processing batch 10620/11884 - Loss: 30.6017\n",
      "Processing batch 10621/11884 - Loss: 29.6162\n",
      "Processing batch 10622/11884 - Loss: 29.9455\n",
      "Processing batch 10623/11884 - Loss: 29.6752\n",
      "Processing batch 10624/11884 - Loss: 28.3055\n",
      "Processing batch 10625/11884 - Loss: 30.0232\n",
      "Processing batch 10626/11884 - Loss: 30.7266\n",
      "Processing batch 10627/11884 - Loss: 30.5856\n",
      "Processing batch 10628/11884 - Loss: 29.6392\n",
      "Processing batch 10629/11884 - Loss: 28.3699\n",
      "Processing batch 10630/11884 - Loss: 29.8288\n",
      "Processing batch 10631/11884 - Loss: 28.7519\n",
      "Processing batch 10632/11884 - Loss: 30.8400\n",
      "Processing batch 10633/11884 - Loss: 30.6946\n",
      "Processing batch 10634/11884 - Loss: 28.7610\n",
      "Processing batch 10635/11884 - Loss: 30.5267\n",
      "Processing batch 10636/11884 - Loss: 30.4113\n",
      "Processing batch 10637/11884 - Loss: 29.8956\n",
      "Processing batch 10638/11884 - Loss: 29.9637\n",
      "Processing batch 10639/11884 - Loss: 30.2352\n",
      "Processing batch 10640/11884 - Loss: 29.0487\n",
      "Processing batch 10641/11884 - Loss: 29.7996\n",
      "Processing batch 10642/11884 - Loss: 29.8358\n",
      "Processing batch 10643/11884 - Loss: 29.9557\n",
      "Processing batch 10644/11884 - Loss: 30.2589\n",
      "Processing batch 10645/11884 - Loss: 30.2603\n",
      "Processing batch 10646/11884 - Loss: 29.7062\n",
      "Processing batch 10647/11884 - Loss: 29.2396\n",
      "Processing batch 10648/11884 - Loss: 28.9648\n",
      "Processing batch 10649/11884 - Loss: 30.0371\n",
      "Processing batch 10650/11884 - Loss: 29.4865\n",
      "Processing batch 10651/11884 - Loss: 31.3269\n",
      "Processing batch 10652/11884 - Loss: 30.5736\n",
      "Processing batch 10653/11884 - Loss: 29.6402\n",
      "Processing batch 10654/11884 - Loss: 30.6933\n",
      "Processing batch 10655/11884 - Loss: 29.9642\n",
      "Processing batch 10656/11884 - Loss: 29.7453\n",
      "Processing batch 10657/11884 - Loss: 30.2777\n",
      "Processing batch 10658/11884 - Loss: 31.5447\n",
      "Processing batch 10659/11884 - Loss: 29.9038\n",
      "Processing batch 10660/11884 - Loss: 30.4255\n",
      "Processing batch 10661/11884 - Loss: 29.7875\n",
      "Processing batch 10662/11884 - Loss: 29.1542\n",
      "Processing batch 10663/11884 - Loss: 30.1224\n",
      "Processing batch 10664/11884 - Loss: 28.5833\n",
      "Processing batch 10665/11884 - Loss: 30.7403\n",
      "Processing batch 10666/11884 - Loss: 29.7318\n",
      "Processing batch 10667/11884 - Loss: 29.9695\n",
      "Processing batch 10668/11884 - Loss: 29.9785\n",
      "Processing batch 10669/11884 - Loss: 29.9098\n",
      "Processing batch 10670/11884 - Loss: 29.4004\n",
      "Processing batch 10671/11884 - Loss: 28.0777\n",
      "Processing batch 10672/11884 - Loss: 30.2003\n",
      "Processing batch 10673/11884 - Loss: 28.9072\n",
      "Processing batch 10674/11884 - Loss: 29.1315\n",
      "Processing batch 10675/11884 - Loss: 29.0267\n",
      "Processing batch 10676/11884 - Loss: 29.6422\n",
      "Processing batch 10677/11884 - Loss: 30.2038\n",
      "Processing batch 10678/11884 - Loss: 30.1054\n",
      "Processing batch 10679/11884 - Loss: 29.3231\n",
      "Processing batch 10680/11884 - Loss: 30.0117\n",
      "Processing batch 10681/11884 - Loss: 30.0148\n",
      "Processing batch 10682/11884 - Loss: 29.7405\n",
      "Processing batch 10683/11884 - Loss: 30.1901\n",
      "Processing batch 10684/11884 - Loss: 31.2978\n",
      "Processing batch 10685/11884 - Loss: 30.4866\n",
      "Processing batch 10686/11884 - Loss: 29.7652\n",
      "Processing batch 10687/11884 - Loss: 30.5016\n",
      "Processing batch 10688/11884 - Loss: 29.6356\n",
      "Processing batch 10689/11884 - Loss: 29.1224\n",
      "Processing batch 10690/11884 - Loss: 30.1555\n",
      "Processing batch 10691/11884 - Loss: 28.9100\n",
      "Processing batch 10692/11884 - Loss: 29.6163\n",
      "Processing batch 10693/11884 - Loss: 29.5464\n",
      "Processing batch 10694/11884 - Loss: 29.8630\n",
      "Processing batch 10695/11884 - Loss: 32.1639\n",
      "Processing batch 10696/11884 - Loss: 29.8608\n",
      "Processing batch 10697/11884 - Loss: 28.7166\n",
      "Processing batch 10698/11884 - Loss: 28.7481\n",
      "Processing batch 10699/11884 - Loss: 30.4970\n",
      "Processing batch 10700/11884 - Loss: 31.6785\n",
      "Processing batch 10701/11884 - Loss: 31.3663\n",
      "Processing batch 10702/11884 - Loss: 30.8491\n",
      "Processing batch 10703/11884 - Loss: 31.3488\n",
      "Processing batch 10704/11884 - Loss: 30.8926\n",
      "Processing batch 10705/11884 - Loss: 30.0560\n",
      "Processing batch 10706/11884 - Loss: 29.0231\n",
      "Processing batch 10707/11884 - Loss: 28.4077\n",
      "Processing batch 10708/11884 - Loss: 29.6647\n",
      "Processing batch 10709/11884 - Loss: 29.5145\n",
      "Processing batch 10710/11884 - Loss: 30.3167\n",
      "Processing batch 10711/11884 - Loss: 29.1973\n",
      "Processing batch 10712/11884 - Loss: 27.8286\n",
      "Processing batch 10713/11884 - Loss: 29.2766\n",
      "Processing batch 10714/11884 - Loss: 29.4913\n",
      "Processing batch 10715/11884 - Loss: 30.1425\n",
      "Processing batch 10716/11884 - Loss: 30.6162\n",
      "Processing batch 10717/11884 - Loss: 30.3792\n",
      "Processing batch 10718/11884 - Loss: 29.7262\n",
      "Processing batch 10719/11884 - Loss: 30.5744\n",
      "Processing batch 10720/11884 - Loss: 29.1152\n",
      "Processing batch 10721/11884 - Loss: 28.5624\n",
      "Processing batch 10722/11884 - Loss: 30.4643\n",
      "Processing batch 10723/11884 - Loss: 29.7725\n",
      "Processing batch 10724/11884 - Loss: 31.0527\n",
      "Processing batch 10725/11884 - Loss: 29.2244\n",
      "Processing batch 10726/11884 - Loss: 29.3646\n",
      "Processing batch 10727/11884 - Loss: 31.0936\n",
      "Processing batch 10728/11884 - Loss: 31.7492\n",
      "Processing batch 10729/11884 - Loss: 28.6455\n",
      "Processing batch 10730/11884 - Loss: 30.2028\n",
      "Processing batch 10731/11884 - Loss: 29.6883\n",
      "Processing batch 10732/11884 - Loss: 29.7187\n",
      "Processing batch 10733/11884 - Loss: 30.6606\n",
      "Processing batch 10734/11884 - Loss: 30.2514\n",
      "Processing batch 10735/11884 - Loss: 29.9644\n",
      "Processing batch 10736/11884 - Loss: 29.9667\n",
      "Processing batch 10737/11884 - Loss: 29.9707\n",
      "Processing batch 10738/11884 - Loss: 30.2538\n",
      "Processing batch 10739/11884 - Loss: 30.6342\n",
      "Processing batch 10740/11884 - Loss: 31.3154\n",
      "Processing batch 10741/11884 - Loss: 29.6736\n",
      "Processing batch 10742/11884 - Loss: 29.6508\n",
      "Processing batch 10743/11884 - Loss: 29.7092\n",
      "Processing batch 10744/11884 - Loss: 30.2918\n",
      "Processing batch 10745/11884 - Loss: 30.3496\n",
      "Processing batch 10746/11884 - Loss: 30.6539\n",
      "Processing batch 10747/11884 - Loss: 29.1173\n",
      "Processing batch 10748/11884 - Loss: 29.7793\n",
      "Processing batch 10749/11884 - Loss: 29.3891\n",
      "Processing batch 10750/11884 - Loss: 30.9260\n",
      "Processing batch 10751/11884 - Loss: 28.6708\n",
      "Processing batch 10752/11884 - Loss: 28.1331\n",
      "Processing batch 10753/11884 - Loss: 31.2232\n",
      "Processing batch 10754/11884 - Loss: 28.9891\n",
      "Processing batch 10755/11884 - Loss: 28.1639\n",
      "Processing batch 10756/11884 - Loss: 30.1006\n",
      "Processing batch 10757/11884 - Loss: 29.5485\n",
      "Processing batch 10758/11884 - Loss: 28.5722\n",
      "Processing batch 10759/11884 - Loss: 28.5641\n",
      "Processing batch 10760/11884 - Loss: 30.3714\n",
      "Processing batch 10761/11884 - Loss: 30.9683\n",
      "Processing batch 10762/11884 - Loss: 29.1322\n",
      "Processing batch 10763/11884 - Loss: 29.1621\n",
      "Processing batch 10764/11884 - Loss: 29.9137\n",
      "Processing batch 10765/11884 - Loss: 28.8841\n",
      "Processing batch 10766/11884 - Loss: 31.0914\n",
      "Processing batch 10767/11884 - Loss: 28.2839\n",
      "Processing batch 10768/11884 - Loss: 29.9392\n",
      "Processing batch 10769/11884 - Loss: 29.9726\n",
      "Processing batch 10770/11884 - Loss: 28.8790\n",
      "Processing batch 10771/11884 - Loss: 29.0386\n",
      "Processing batch 10772/11884 - Loss: 30.8075\n",
      "Processing batch 10773/11884 - Loss: 29.0636\n",
      "Processing batch 10774/11884 - Loss: 29.9313\n",
      "Processing batch 10775/11884 - Loss: 28.3449\n",
      "Processing batch 10776/11884 - Loss: 31.2046\n",
      "Processing batch 10777/11884 - Loss: 29.9643\n",
      "Processing batch 10778/11884 - Loss: 31.7645\n",
      "Processing batch 10779/11884 - Loss: 31.0598\n",
      "Processing batch 10780/11884 - Loss: 30.8266\n",
      "Processing batch 10781/11884 - Loss: 30.2759\n",
      "Processing batch 10782/11884 - Loss: 30.5486\n",
      "Processing batch 10783/11884 - Loss: 29.8133\n",
      "Processing batch 10784/11884 - Loss: 29.7218\n",
      "Processing batch 10785/11884 - Loss: 29.4690\n",
      "Processing batch 10786/11884 - Loss: 31.0801\n",
      "Processing batch 10787/11884 - Loss: 29.3813\n",
      "Processing batch 10788/11884 - Loss: 30.9232\n",
      "Processing batch 10789/11884 - Loss: 30.7957\n",
      "Processing batch 10790/11884 - Loss: 29.4540\n",
      "Processing batch 10791/11884 - Loss: 29.3183\n",
      "Processing batch 10792/11884 - Loss: 29.4138\n",
      "Processing batch 10793/11884 - Loss: 28.5963\n",
      "Processing batch 10794/11884 - Loss: 29.6364\n",
      "Processing batch 10795/11884 - Loss: 29.5677\n",
      "Processing batch 10796/11884 - Loss: 30.7341\n",
      "Processing batch 10797/11884 - Loss: 29.9383\n",
      "Processing batch 10798/11884 - Loss: 30.8646\n",
      "Processing batch 10799/11884 - Loss: 30.4054\n",
      "Processing batch 10800/11884 - Loss: 31.2298\n",
      "Processing batch 10801/11884 - Loss: 30.0839\n",
      "Processing batch 10802/11884 - Loss: 29.3074\n",
      "Processing batch 10803/11884 - Loss: 30.6598\n",
      "Processing batch 10804/11884 - Loss: 29.1606\n",
      "Processing batch 10805/11884 - Loss: 28.7781\n",
      "Processing batch 10806/11884 - Loss: 30.1980\n",
      "Processing batch 10807/11884 - Loss: 29.0171\n",
      "Processing batch 10808/11884 - Loss: 30.4653\n",
      "Processing batch 10809/11884 - Loss: 29.1959\n",
      "Processing batch 10810/11884 - Loss: 30.4247\n",
      "Processing batch 10811/11884 - Loss: 31.3487\n",
      "Processing batch 10812/11884 - Loss: 30.3683\n",
      "Processing batch 10813/11884 - Loss: 31.3050\n",
      "Processing batch 10814/11884 - Loss: 29.1354\n",
      "Processing batch 10815/11884 - Loss: 29.4926\n",
      "Processing batch 10816/11884 - Loss: 30.0842\n",
      "Processing batch 10817/11884 - Loss: 28.9403\n",
      "Processing batch 10818/11884 - Loss: 30.1798\n",
      "Processing batch 10819/11884 - Loss: 30.0766\n",
      "Processing batch 10820/11884 - Loss: 31.1149\n",
      "Processing batch 10821/11884 - Loss: 30.1951\n",
      "Processing batch 10822/11884 - Loss: 29.3360\n",
      "Processing batch 10823/11884 - Loss: 29.2269\n",
      "Processing batch 10824/11884 - Loss: 30.1742\n",
      "Processing batch 10825/11884 - Loss: 29.3931\n",
      "Processing batch 10826/11884 - Loss: 29.8790\n",
      "Processing batch 10827/11884 - Loss: 29.4661\n",
      "Processing batch 10828/11884 - Loss: 30.6095\n",
      "Processing batch 10829/11884 - Loss: 30.0133\n",
      "Processing batch 10830/11884 - Loss: 30.6124\n",
      "Processing batch 10831/11884 - Loss: 31.1839\n",
      "Processing batch 10832/11884 - Loss: 29.6603\n",
      "Processing batch 10833/11884 - Loss: 28.1403\n",
      "Processing batch 10834/11884 - Loss: 30.7997\n",
      "Processing batch 10835/11884 - Loss: 30.3989\n",
      "Processing batch 10836/11884 - Loss: 30.7720\n",
      "Processing batch 10837/11884 - Loss: 29.7014\n",
      "Processing batch 10838/11884 - Loss: 30.0756\n",
      "Processing batch 10839/11884 - Loss: 31.3037\n",
      "Processing batch 10840/11884 - Loss: 29.0950\n",
      "Processing batch 10841/11884 - Loss: 29.4989\n",
      "Processing batch 10842/11884 - Loss: 30.0600\n",
      "Processing batch 10843/11884 - Loss: 30.7067\n",
      "Processing batch 10844/11884 - Loss: 28.9951\n",
      "Processing batch 10845/11884 - Loss: 30.1331\n",
      "Processing batch 10846/11884 - Loss: 29.5392\n",
      "Processing batch 10847/11884 - Loss: 30.6087\n",
      "Processing batch 10848/11884 - Loss: 31.4457\n",
      "Processing batch 10849/11884 - Loss: 29.2313\n",
      "Processing batch 10850/11884 - Loss: 29.8835\n",
      "Processing batch 10851/11884 - Loss: 28.1255\n",
      "Processing batch 10852/11884 - Loss: 29.4539\n",
      "Processing batch 10853/11884 - Loss: 29.7282\n",
      "Processing batch 10854/11884 - Loss: 29.5828\n",
      "Processing batch 10855/11884 - Loss: 30.6888\n",
      "Processing batch 10856/11884 - Loss: 29.5559\n",
      "Processing batch 10857/11884 - Loss: 29.2783\n",
      "Processing batch 10858/11884 - Loss: 30.4969\n",
      "Processing batch 10859/11884 - Loss: 29.2149\n",
      "Processing batch 10860/11884 - Loss: 27.8370\n",
      "Processing batch 10861/11884 - Loss: 28.8626\n",
      "Processing batch 10862/11884 - Loss: 29.2787\n",
      "Processing batch 10863/11884 - Loss: 29.4554\n",
      "Processing batch 10864/11884 - Loss: 30.4217\n",
      "Processing batch 10865/11884 - Loss: 30.2149\n",
      "Processing batch 10866/11884 - Loss: 30.5829\n",
      "Processing batch 10867/11884 - Loss: 30.1641\n",
      "Processing batch 10868/11884 - Loss: 30.7912\n",
      "Processing batch 10869/11884 - Loss: 29.9312\n",
      "Processing batch 10870/11884 - Loss: 28.4649\n",
      "Processing batch 10871/11884 - Loss: 30.3047\n",
      "Processing batch 10872/11884 - Loss: 30.0317\n",
      "Processing batch 10873/11884 - Loss: 30.1321\n",
      "Processing batch 10874/11884 - Loss: 29.1581\n",
      "Processing batch 10875/11884 - Loss: 30.6921\n",
      "Processing batch 10876/11884 - Loss: 29.4677\n",
      "Processing batch 10877/11884 - Loss: 30.1514\n",
      "Processing batch 10878/11884 - Loss: 30.0887\n",
      "Processing batch 10879/11884 - Loss: 29.8966\n",
      "Processing batch 10880/11884 - Loss: 30.8566\n",
      "Processing batch 10881/11884 - Loss: 30.8382\n",
      "Processing batch 10882/11884 - Loss: 30.6827\n",
      "Processing batch 10883/11884 - Loss: 29.9766\n",
      "Processing batch 10884/11884 - Loss: 28.8114\n",
      "Processing batch 10885/11884 - Loss: 28.8933\n",
      "Processing batch 10886/11884 - Loss: 30.5288\n",
      "Processing batch 10887/11884 - Loss: 31.5495\n",
      "Processing batch 10888/11884 - Loss: 30.7269\n",
      "Processing batch 10889/11884 - Loss: 30.7107\n",
      "Processing batch 10890/11884 - Loss: 29.6743\n",
      "Processing batch 10891/11884 - Loss: 30.3653\n",
      "Processing batch 10892/11884 - Loss: 30.4374\n",
      "Processing batch 10893/11884 - Loss: 30.6624\n",
      "Processing batch 10894/11884 - Loss: 31.1998\n",
      "Processing batch 10895/11884 - Loss: 29.7716\n",
      "Processing batch 10896/11884 - Loss: 29.1820\n",
      "Processing batch 10897/11884 - Loss: 28.3349\n",
      "Processing batch 10898/11884 - Loss: 30.6584\n",
      "Processing batch 10899/11884 - Loss: 30.4389\n",
      "Processing batch 10900/11884 - Loss: 29.8899\n",
      "Processing batch 10901/11884 - Loss: 31.0193\n",
      "Processing batch 10902/11884 - Loss: 29.8072\n",
      "Processing batch 10903/11884 - Loss: 29.8982\n",
      "Processing batch 10904/11884 - Loss: 30.0103\n",
      "Processing batch 10905/11884 - Loss: 31.2688\n",
      "Processing batch 10906/11884 - Loss: 30.7659\n",
      "Processing batch 10907/11884 - Loss: 28.8967\n",
      "Processing batch 10908/11884 - Loss: 28.5624\n",
      "Processing batch 10909/11884 - Loss: 29.4814\n",
      "Processing batch 10910/11884 - Loss: 30.0934\n",
      "Processing batch 10911/11884 - Loss: 28.9757\n",
      "Processing batch 10912/11884 - Loss: 29.4649\n",
      "Processing batch 10913/11884 - Loss: 28.2872\n",
      "Processing batch 10914/11884 - Loss: 27.4377\n",
      "Processing batch 10915/11884 - Loss: 31.1023\n",
      "Processing batch 10916/11884 - Loss: 28.2515\n",
      "Processing batch 10917/11884 - Loss: 29.4025\n",
      "Processing batch 10918/11884 - Loss: 30.5677\n",
      "Processing batch 10919/11884 - Loss: 29.5209\n",
      "Processing batch 10920/11884 - Loss: 30.5462\n",
      "Processing batch 10921/11884 - Loss: 30.2690\n",
      "Processing batch 10922/11884 - Loss: 30.2048\n",
      "Processing batch 10923/11884 - Loss: 29.5497\n",
      "Processing batch 10924/11884 - Loss: 30.4628\n",
      "Processing batch 10925/11884 - Loss: 29.6777\n",
      "Processing batch 10926/11884 - Loss: 28.9516\n",
      "Processing batch 10927/11884 - Loss: 30.2608\n",
      "Processing batch 10928/11884 - Loss: 28.8128\n",
      "Processing batch 10929/11884 - Loss: 30.3763\n",
      "Processing batch 10930/11884 - Loss: 28.2115\n",
      "Processing batch 10931/11884 - Loss: 31.1811\n",
      "Processing batch 10932/11884 - Loss: 31.1240\n",
      "Processing batch 10933/11884 - Loss: 29.4932\n",
      "Processing batch 10934/11884 - Loss: 29.4555\n",
      "Processing batch 10935/11884 - Loss: 29.0843\n",
      "Processing batch 10936/11884 - Loss: 29.8908\n",
      "Processing batch 10937/11884 - Loss: 29.5887\n",
      "Processing batch 10938/11884 - Loss: 31.2813\n",
      "Processing batch 10939/11884 - Loss: 30.3143\n",
      "Processing batch 10940/11884 - Loss: 30.8971\n",
      "Processing batch 10941/11884 - Loss: 29.2669\n",
      "Processing batch 10942/11884 - Loss: 29.5805\n",
      "Processing batch 10943/11884 - Loss: 29.9982\n",
      "Processing batch 10944/11884 - Loss: 30.6300\n",
      "Processing batch 10945/11884 - Loss: 30.6430\n",
      "Processing batch 10946/11884 - Loss: 31.0767\n",
      "Processing batch 10947/11884 - Loss: 30.0785\n",
      "Processing batch 10948/11884 - Loss: 29.8722\n",
      "Processing batch 10949/11884 - Loss: 30.3047\n",
      "Processing batch 10950/11884 - Loss: 29.7566\n",
      "Processing batch 10951/11884 - Loss: 30.6054\n",
      "Processing batch 10952/11884 - Loss: 28.8337\n",
      "Processing batch 10953/11884 - Loss: 29.5712\n",
      "Processing batch 10954/11884 - Loss: 29.8785\n",
      "Processing batch 10955/11884 - Loss: 31.2413\n",
      "Processing batch 10956/11884 - Loss: 30.1377\n",
      "Processing batch 10957/11884 - Loss: 30.6625\n",
      "Processing batch 10958/11884 - Loss: 31.0407\n",
      "Processing batch 10959/11884 - Loss: 30.5221\n",
      "Processing batch 10960/11884 - Loss: 29.7378\n",
      "Processing batch 10961/11884 - Loss: 30.4752\n",
      "Processing batch 10962/11884 - Loss: 29.5139\n",
      "Processing batch 10963/11884 - Loss: 29.5277\n",
      "Processing batch 10964/11884 - Loss: 31.3835\n",
      "Processing batch 10965/11884 - Loss: 30.0975\n",
      "Processing batch 10966/11884 - Loss: 30.8179\n",
      "Processing batch 10967/11884 - Loss: 30.2197\n",
      "Processing batch 10968/11884 - Loss: 29.4212\n",
      "Processing batch 10969/11884 - Loss: 30.3312\n",
      "Processing batch 10970/11884 - Loss: 31.0860\n",
      "Processing batch 10971/11884 - Loss: 29.7332\n",
      "Processing batch 10972/11884 - Loss: 29.1640\n",
      "Processing batch 10973/11884 - Loss: 28.8026\n",
      "Processing batch 10974/11884 - Loss: 29.4808\n",
      "Processing batch 10975/11884 - Loss: 29.7433\n",
      "Processing batch 10976/11884 - Loss: 28.6585\n",
      "Processing batch 10977/11884 - Loss: 29.7070\n",
      "Processing batch 10978/11884 - Loss: 30.5201\n",
      "Processing batch 10979/11884 - Loss: 29.1619\n",
      "Processing batch 10980/11884 - Loss: 29.2911\n",
      "Processing batch 10981/11884 - Loss: 29.3933\n",
      "Processing batch 10982/11884 - Loss: 29.3302\n",
      "Processing batch 10983/11884 - Loss: 29.5539\n",
      "Processing batch 10984/11884 - Loss: 30.1925\n",
      "Processing batch 10985/11884 - Loss: 30.4883\n",
      "Processing batch 10986/11884 - Loss: 30.6877\n",
      "Processing batch 10987/11884 - Loss: 31.2417\n",
      "Processing batch 10988/11884 - Loss: 30.2046\n",
      "Processing batch 10989/11884 - Loss: 30.4853\n",
      "Processing batch 10990/11884 - Loss: 31.1849\n",
      "Processing batch 10991/11884 - Loss: 30.4621\n",
      "Processing batch 10992/11884 - Loss: 30.7250\n",
      "Processing batch 10993/11884 - Loss: 29.2445\n",
      "Processing batch 10994/11884 - Loss: 29.8140\n",
      "Processing batch 10995/11884 - Loss: 29.7664\n",
      "Processing batch 10996/11884 - Loss: 30.0489\n",
      "Processing batch 10997/11884 - Loss: 28.7650\n",
      "Processing batch 10998/11884 - Loss: 29.4385\n",
      "Processing batch 10999/11884 - Loss: 29.8662\n",
      "Processing batch 11000/11884 - Loss: 30.1735\n",
      "Processing batch 11001/11884 - Loss: 30.5536\n",
      "Processing batch 11002/11884 - Loss: 30.4354\n",
      "Processing batch 11003/11884 - Loss: 29.7744\n",
      "Processing batch 11004/11884 - Loss: 31.5746\n",
      "Processing batch 11005/11884 - Loss: 29.7914\n",
      "Processing batch 11006/11884 - Loss: 28.5854\n",
      "Processing batch 11007/11884 - Loss: 30.3532\n",
      "Processing batch 11008/11884 - Loss: 30.2905\n",
      "Processing batch 11009/11884 - Loss: 28.9056\n",
      "Processing batch 11010/11884 - Loss: 29.6781\n",
      "Processing batch 11011/11884 - Loss: 30.1290\n",
      "Processing batch 11012/11884 - Loss: 31.0335\n",
      "Processing batch 11013/11884 - Loss: 29.9524\n",
      "Processing batch 11014/11884 - Loss: 31.4254\n",
      "Processing batch 11015/11884 - Loss: 29.8625\n",
      "Processing batch 11016/11884 - Loss: 30.9814\n",
      "Processing batch 11017/11884 - Loss: 29.9169\n",
      "Processing batch 11018/11884 - Loss: 30.7406\n",
      "Processing batch 11019/11884 - Loss: 31.8361\n",
      "Processing batch 11020/11884 - Loss: 30.3456\n",
      "Processing batch 11021/11884 - Loss: 29.8556\n",
      "Processing batch 11022/11884 - Loss: 28.8000\n",
      "Processing batch 11023/11884 - Loss: 29.9296\n",
      "Processing batch 11024/11884 - Loss: 30.8651\n",
      "Processing batch 11025/11884 - Loss: 30.1656\n",
      "Processing batch 11026/11884 - Loss: 30.5224\n",
      "Processing batch 11027/11884 - Loss: 30.8829\n",
      "Processing batch 11028/11884 - Loss: 29.8093\n",
      "Processing batch 11029/11884 - Loss: 30.6011\n",
      "Processing batch 11030/11884 - Loss: 29.1690\n",
      "Processing batch 11031/11884 - Loss: 31.1616\n",
      "Processing batch 11032/11884 - Loss: 29.1758\n",
      "Processing batch 11033/11884 - Loss: 29.2269\n",
      "Processing batch 11034/11884 - Loss: 30.0643\n",
      "Processing batch 11035/11884 - Loss: 29.7094\n",
      "Processing batch 11036/11884 - Loss: 29.8400\n",
      "Processing batch 11037/11884 - Loss: 30.0683\n",
      "Processing batch 11038/11884 - Loss: 29.5405\n",
      "Processing batch 11039/11884 - Loss: 29.7185\n",
      "Processing batch 11040/11884 - Loss: 29.1543\n",
      "Processing batch 11041/11884 - Loss: 29.1824\n",
      "Processing batch 11042/11884 - Loss: 29.5038\n",
      "Processing batch 11043/11884 - Loss: 29.3980\n",
      "Processing batch 11044/11884 - Loss: 28.7737\n",
      "Processing batch 11045/11884 - Loss: 29.8248\n",
      "Processing batch 11046/11884 - Loss: 29.7055\n",
      "Processing batch 11047/11884 - Loss: 30.0760\n",
      "Processing batch 11048/11884 - Loss: 30.1005\n",
      "Processing batch 11049/11884 - Loss: 28.9098\n",
      "Processing batch 11050/11884 - Loss: 29.9051\n",
      "Processing batch 11051/11884 - Loss: 30.3052\n",
      "Processing batch 11052/11884 - Loss: 29.2730\n",
      "Processing batch 11053/11884 - Loss: 30.3065\n",
      "Processing batch 11054/11884 - Loss: 29.9440\n",
      "Processing batch 11055/11884 - Loss: 30.5903\n",
      "Processing batch 11056/11884 - Loss: 29.5385\n",
      "Processing batch 11057/11884 - Loss: 30.1533\n",
      "Processing batch 11058/11884 - Loss: 30.6306\n",
      "Processing batch 11059/11884 - Loss: 29.1029\n",
      "Processing batch 11060/11884 - Loss: 28.2361\n",
      "Processing batch 11061/11884 - Loss: 30.2949\n",
      "Processing batch 11062/11884 - Loss: 30.8148\n",
      "Processing batch 11063/11884 - Loss: 30.3078\n",
      "Processing batch 11064/11884 - Loss: 31.2353\n",
      "Processing batch 11065/11884 - Loss: 28.6440\n",
      "Processing batch 11066/11884 - Loss: 30.3652\n",
      "Processing batch 11067/11884 - Loss: 30.4026\n",
      "Processing batch 11068/11884 - Loss: 30.5776\n",
      "Processing batch 11069/11884 - Loss: 28.5137\n",
      "Processing batch 11070/11884 - Loss: 30.1760\n",
      "Processing batch 11071/11884 - Loss: 30.6615\n",
      "Processing batch 11072/11884 - Loss: 30.9076\n",
      "Processing batch 11073/11884 - Loss: 31.1054\n",
      "Processing batch 11074/11884 - Loss: 30.5240\n",
      "Processing batch 11075/11884 - Loss: 28.8019\n",
      "Processing batch 11076/11884 - Loss: 30.6654\n",
      "Processing batch 11077/11884 - Loss: 30.2436\n",
      "Processing batch 11078/11884 - Loss: 29.8355\n",
      "Processing batch 11079/11884 - Loss: 30.2582\n",
      "Processing batch 11080/11884 - Loss: 29.1346\n",
      "Processing batch 11081/11884 - Loss: 29.8909\n",
      "Processing batch 11082/11884 - Loss: 29.5869\n",
      "Processing batch 11083/11884 - Loss: 29.9674\n",
      "Processing batch 11084/11884 - Loss: 30.1987\n",
      "Processing batch 11085/11884 - Loss: 28.8643\n",
      "Processing batch 11086/11884 - Loss: 29.9131\n",
      "Processing batch 11087/11884 - Loss: 29.0035\n",
      "Processing batch 11088/11884 - Loss: 29.9371\n",
      "Processing batch 11089/11884 - Loss: 28.9461\n",
      "Processing batch 11090/11884 - Loss: 29.4304\n",
      "Processing batch 11091/11884 - Loss: 31.5780\n",
      "Processing batch 11092/11884 - Loss: 30.7805\n",
      "Processing batch 11093/11884 - Loss: 29.1421\n",
      "Processing batch 11094/11884 - Loss: 29.4079\n",
      "Processing batch 11095/11884 - Loss: 29.9677\n",
      "Processing batch 11096/11884 - Loss: 29.8096\n",
      "Processing batch 11097/11884 - Loss: 30.2342\n",
      "Processing batch 11098/11884 - Loss: 28.3124\n",
      "Processing batch 11099/11884 - Loss: 29.8357\n",
      "Processing batch 11100/11884 - Loss: 30.0258\n",
      "Processing batch 11101/11884 - Loss: 29.5198\n",
      "Processing batch 11102/11884 - Loss: 30.0477\n",
      "Processing batch 11103/11884 - Loss: 31.2743\n",
      "Processing batch 11104/11884 - Loss: 30.2060\n",
      "Processing batch 11105/11884 - Loss: 29.7907\n",
      "Processing batch 11106/11884 - Loss: 28.1628\n",
      "Processing batch 11107/11884 - Loss: 30.3246\n",
      "Processing batch 11108/11884 - Loss: 30.7917\n",
      "Processing batch 11109/11884 - Loss: 29.4180\n",
      "Processing batch 11110/11884 - Loss: 30.5610\n",
      "Processing batch 11111/11884 - Loss: 28.8450\n",
      "Processing batch 11112/11884 - Loss: 29.2132\n",
      "Processing batch 11113/11884 - Loss: 28.3997\n",
      "Processing batch 11114/11884 - Loss: 30.2413\n",
      "Processing batch 11115/11884 - Loss: 29.8337\n",
      "Processing batch 11116/11884 - Loss: 30.5383\n",
      "Processing batch 11117/11884 - Loss: 29.3163\n",
      "Processing batch 11118/11884 - Loss: 29.8605\n",
      "Processing batch 11119/11884 - Loss: 29.2445\n",
      "Processing batch 11120/11884 - Loss: 29.3555\n",
      "Processing batch 11121/11884 - Loss: 28.9355\n",
      "Processing batch 11122/11884 - Loss: 31.5660\n",
      "Processing batch 11123/11884 - Loss: 30.1932\n",
      "Processing batch 11124/11884 - Loss: 30.3800\n",
      "Processing batch 11125/11884 - Loss: 30.2738\n",
      "Processing batch 11126/11884 - Loss: 30.1483\n",
      "Processing batch 11127/11884 - Loss: 29.9758\n",
      "Processing batch 11128/11884 - Loss: 30.4850\n",
      "Processing batch 11129/11884 - Loss: 29.2318\n",
      "Processing batch 11130/11884 - Loss: 30.5404\n",
      "Processing batch 11131/11884 - Loss: 29.5488\n",
      "Processing batch 11132/11884 - Loss: 29.1371\n",
      "Processing batch 11133/11884 - Loss: 29.6851\n",
      "Processing batch 11134/11884 - Loss: 29.9110\n",
      "Processing batch 11135/11884 - Loss: 31.8381\n",
      "Processing batch 11136/11884 - Loss: 32.2852\n",
      "Processing batch 11137/11884 - Loss: 30.7424\n",
      "Processing batch 11138/11884 - Loss: 29.3318\n",
      "Processing batch 11139/11884 - Loss: 29.5150\n",
      "Processing batch 11140/11884 - Loss: 30.8217\n",
      "Processing batch 11141/11884 - Loss: 30.4570\n",
      "Processing batch 11142/11884 - Loss: 30.2221\n",
      "Processing batch 11143/11884 - Loss: 29.3655\n",
      "Processing batch 11144/11884 - Loss: 31.0301\n",
      "Processing batch 11145/11884 - Loss: 30.3885\n",
      "Processing batch 11146/11884 - Loss: 28.6956\n",
      "Processing batch 11147/11884 - Loss: 29.7600\n",
      "Processing batch 11148/11884 - Loss: 29.9582\n",
      "Processing batch 11149/11884 - Loss: 29.3216\n",
      "Processing batch 11150/11884 - Loss: 29.3425\n",
      "Processing batch 11151/11884 - Loss: 28.2226\n",
      "Processing batch 11152/11884 - Loss: 29.7377\n",
      "Processing batch 11153/11884 - Loss: 30.0900\n",
      "Processing batch 11154/11884 - Loss: 31.2007\n",
      "Processing batch 11155/11884 - Loss: 30.5915\n",
      "Processing batch 11156/11884 - Loss: 29.7899\n",
      "Processing batch 11157/11884 - Loss: 28.8818\n",
      "Processing batch 11158/11884 - Loss: 30.2799\n",
      "Processing batch 11159/11884 - Loss: 30.2253\n",
      "Processing batch 11160/11884 - Loss: 30.1615\n",
      "Processing batch 11161/11884 - Loss: 29.4844\n",
      "Processing batch 11162/11884 - Loss: 30.8779\n",
      "Processing batch 11163/11884 - Loss: 30.3843\n",
      "Processing batch 11164/11884 - Loss: 29.5794\n",
      "Processing batch 11165/11884 - Loss: 28.9446\n",
      "Processing batch 11166/11884 - Loss: 30.4290\n",
      "Processing batch 11167/11884 - Loss: 28.4783\n",
      "Processing batch 11168/11884 - Loss: 31.1293\n",
      "Processing batch 11169/11884 - Loss: 29.4036\n",
      "Processing batch 11170/11884 - Loss: 30.8463\n",
      "Processing batch 11171/11884 - Loss: 28.6621\n",
      "Processing batch 11172/11884 - Loss: 31.4089\n",
      "Processing batch 11173/11884 - Loss: 29.9589\n",
      "Processing batch 11174/11884 - Loss: 29.7932\n",
      "Processing batch 11175/11884 - Loss: 30.9673\n",
      "Processing batch 11176/11884 - Loss: 29.4093\n",
      "Processing batch 11177/11884 - Loss: 29.2166\n",
      "Processing batch 11178/11884 - Loss: 29.4579\n",
      "Processing batch 11179/11884 - Loss: 30.2370\n",
      "Processing batch 11180/11884 - Loss: 30.4695\n",
      "Processing batch 11181/11884 - Loss: 30.3753\n",
      "Processing batch 11182/11884 - Loss: 30.2860\n",
      "Processing batch 11183/11884 - Loss: 30.7412\n",
      "Processing batch 11184/11884 - Loss: 30.1517\n",
      "Processing batch 11185/11884 - Loss: 30.8287\n",
      "Processing batch 11186/11884 - Loss: 29.6387\n",
      "Processing batch 11187/11884 - Loss: 28.9668\n",
      "Processing batch 11188/11884 - Loss: 30.2083\n",
      "Processing batch 11189/11884 - Loss: 29.6836\n",
      "Processing batch 11190/11884 - Loss: 29.0385\n",
      "Processing batch 11191/11884 - Loss: 30.7195\n",
      "Processing batch 11192/11884 - Loss: 29.4767\n",
      "Processing batch 11193/11884 - Loss: 29.9109\n",
      "Processing batch 11194/11884 - Loss: 30.3960\n",
      "Processing batch 11195/11884 - Loss: 29.6501\n",
      "Processing batch 11196/11884 - Loss: 30.3246\n",
      "Processing batch 11197/11884 - Loss: 29.8576\n",
      "Processing batch 11198/11884 - Loss: 29.6714\n",
      "Processing batch 11199/11884 - Loss: 29.5905\n",
      "Processing batch 11200/11884 - Loss: 29.6730\n",
      "Processing batch 11201/11884 - Loss: 28.8139\n",
      "Processing batch 11202/11884 - Loss: 29.0395\n",
      "Processing batch 11203/11884 - Loss: 30.3988\n",
      "Processing batch 11204/11884 - Loss: 28.8985\n",
      "Processing batch 11205/11884 - Loss: 29.2885\n",
      "Processing batch 11206/11884 - Loss: 29.3788\n",
      "Processing batch 11207/11884 - Loss: 29.4162\n",
      "Processing batch 11208/11884 - Loss: 31.1253\n",
      "Processing batch 11209/11884 - Loss: 30.1909\n",
      "Processing batch 11210/11884 - Loss: 29.3461\n",
      "Processing batch 11211/11884 - Loss: 30.6343\n",
      "Processing batch 11212/11884 - Loss: 29.2588\n",
      "Processing batch 11213/11884 - Loss: 31.1178\n",
      "Processing batch 11214/11884 - Loss: 29.3685\n",
      "Processing batch 11215/11884 - Loss: 30.8015\n",
      "Processing batch 11216/11884 - Loss: 29.6341\n",
      "Processing batch 11217/11884 - Loss: 30.7699\n",
      "Processing batch 11218/11884 - Loss: 30.3968\n",
      "Processing batch 11219/11884 - Loss: 30.2758\n",
      "Processing batch 11220/11884 - Loss: 29.7266\n",
      "Processing batch 11221/11884 - Loss: 30.0506\n",
      "Processing batch 11222/11884 - Loss: 29.0356\n",
      "Processing batch 11223/11884 - Loss: 29.1213\n",
      "Processing batch 11224/11884 - Loss: 30.4924\n",
      "Processing batch 11225/11884 - Loss: 30.3746\n",
      "Processing batch 11226/11884 - Loss: 29.9479\n",
      "Processing batch 11227/11884 - Loss: 30.1717\n",
      "Processing batch 11228/11884 - Loss: 29.5544\n",
      "Processing batch 11229/11884 - Loss: 30.3484\n",
      "Processing batch 11230/11884 - Loss: 30.7258\n",
      "Processing batch 11231/11884 - Loss: 30.9136\n",
      "Processing batch 11232/11884 - Loss: 29.5062\n",
      "Processing batch 11233/11884 - Loss: 29.4648\n",
      "Processing batch 11234/11884 - Loss: 30.4917\n",
      "Processing batch 11235/11884 - Loss: 29.9440\n",
      "Processing batch 11236/11884 - Loss: 29.8573\n",
      "Processing batch 11237/11884 - Loss: 30.3867\n",
      "Processing batch 11238/11884 - Loss: 29.7010\n",
      "Processing batch 11239/11884 - Loss: 29.8537\n",
      "Processing batch 11240/11884 - Loss: 29.5514\n",
      "Processing batch 11241/11884 - Loss: 29.7889\n",
      "Processing batch 11242/11884 - Loss: 30.9929\n",
      "Processing batch 11243/11884 - Loss: 29.3141\n",
      "Processing batch 11244/11884 - Loss: 30.4158\n",
      "Processing batch 11245/11884 - Loss: 29.4715\n",
      "Processing batch 11246/11884 - Loss: 30.1984\n",
      "Processing batch 11247/11884 - Loss: 30.1326\n",
      "Processing batch 11248/11884 - Loss: 29.8836\n",
      "Processing batch 11249/11884 - Loss: 30.2596\n",
      "Processing batch 11250/11884 - Loss: 29.4198\n",
      "Processing batch 11251/11884 - Loss: 30.3387\n",
      "Processing batch 11252/11884 - Loss: 30.3846\n",
      "Processing batch 11253/11884 - Loss: 30.1317\n",
      "Processing batch 11254/11884 - Loss: 30.3816\n",
      "Processing batch 11255/11884 - Loss: 30.7185\n",
      "Processing batch 11256/11884 - Loss: 31.8532\n",
      "Processing batch 11257/11884 - Loss: 28.7172\n",
      "Processing batch 11258/11884 - Loss: 29.6709\n",
      "Processing batch 11259/11884 - Loss: 30.7406\n",
      "Processing batch 11260/11884 - Loss: 30.1677\n",
      "Processing batch 11261/11884 - Loss: 28.9986\n",
      "Processing batch 11262/11884 - Loss: 29.4908\n",
      "Processing batch 11263/11884 - Loss: 29.9423\n",
      "Processing batch 11264/11884 - Loss: 30.2249\n",
      "Processing batch 11265/11884 - Loss: 29.8111\n",
      "Processing batch 11266/11884 - Loss: 30.4205\n",
      "Processing batch 11267/11884 - Loss: 29.3236\n",
      "Processing batch 11268/11884 - Loss: 30.8113\n",
      "Processing batch 11269/11884 - Loss: 29.7963\n",
      "Processing batch 11270/11884 - Loss: 30.6708\n",
      "Processing batch 11271/11884 - Loss: 29.1900\n",
      "Processing batch 11272/11884 - Loss: 29.9643\n",
      "Processing batch 11273/11884 - Loss: 29.0865\n",
      "Processing batch 11274/11884 - Loss: 31.7646\n",
      "Processing batch 11275/11884 - Loss: 30.9045\n",
      "Processing batch 11276/11884 - Loss: 29.8618\n",
      "Processing batch 11277/11884 - Loss: 31.3740\n",
      "Processing batch 11278/11884 - Loss: 29.8008\n",
      "Processing batch 11279/11884 - Loss: 29.9470\n",
      "Processing batch 11280/11884 - Loss: 29.3759\n",
      "Processing batch 11281/11884 - Loss: 30.0249\n",
      "Processing batch 11282/11884 - Loss: 29.1526\n",
      "Processing batch 11283/11884 - Loss: 29.9849\n",
      "Processing batch 11284/11884 - Loss: 30.8660\n",
      "Processing batch 11285/11884 - Loss: 30.3398\n",
      "Processing batch 11286/11884 - Loss: 30.1688\n",
      "Processing batch 11287/11884 - Loss: 29.6355\n",
      "Processing batch 11288/11884 - Loss: 29.5708\n",
      "Processing batch 11289/11884 - Loss: 29.4790\n",
      "Processing batch 11290/11884 - Loss: 28.2122\n",
      "Processing batch 11291/11884 - Loss: 30.4292\n",
      "Processing batch 11292/11884 - Loss: 29.8514\n",
      "Processing batch 11293/11884 - Loss: 30.9363\n",
      "Processing batch 11294/11884 - Loss: 29.8844\n",
      "Processing batch 11295/11884 - Loss: 30.2371\n",
      "Processing batch 11296/11884 - Loss: 28.8308\n",
      "Processing batch 11297/11884 - Loss: 30.3453\n",
      "Processing batch 11298/11884 - Loss: 30.1609\n",
      "Processing batch 11299/11884 - Loss: 30.7577\n",
      "Processing batch 11300/11884 - Loss: 30.8091\n",
      "Processing batch 11301/11884 - Loss: 28.7342\n",
      "Processing batch 11302/11884 - Loss: 29.6952\n",
      "Processing batch 11303/11884 - Loss: 30.1669\n",
      "Processing batch 11304/11884 - Loss: 28.8098\n",
      "Processing batch 11305/11884 - Loss: 30.1817\n",
      "Processing batch 11306/11884 - Loss: 29.4348\n",
      "Processing batch 11307/11884 - Loss: 29.8600\n",
      "Processing batch 11308/11884 - Loss: 30.7246\n",
      "Processing batch 11309/11884 - Loss: 29.6805\n",
      "Processing batch 11310/11884 - Loss: 30.2785\n",
      "Processing batch 11311/11884 - Loss: 29.5390\n",
      "Processing batch 11312/11884 - Loss: 29.7227\n",
      "Processing batch 11313/11884 - Loss: 30.0233\n",
      "Processing batch 11314/11884 - Loss: 30.5632\n",
      "Processing batch 11315/11884 - Loss: 30.2509\n",
      "Processing batch 11316/11884 - Loss: 31.3070\n",
      "Processing batch 11317/11884 - Loss: 30.5964\n",
      "Processing batch 11318/11884 - Loss: 30.4587\n",
      "Processing batch 11319/11884 - Loss: 29.8961\n",
      "Processing batch 11320/11884 - Loss: 31.0415\n",
      "Processing batch 11321/11884 - Loss: 29.6387\n",
      "Processing batch 11322/11884 - Loss: 29.8612\n",
      "Processing batch 11323/11884 - Loss: 29.5852\n",
      "Processing batch 11324/11884 - Loss: 28.8469\n",
      "Processing batch 11325/11884 - Loss: 29.7589\n",
      "Processing batch 11326/11884 - Loss: 29.0851\n",
      "Processing batch 11327/11884 - Loss: 29.8224\n",
      "Processing batch 11328/11884 - Loss: 29.8999\n",
      "Processing batch 11329/11884 - Loss: 31.5047\n",
      "Processing batch 11330/11884 - Loss: 28.2638\n",
      "Processing batch 11331/11884 - Loss: 30.4709\n",
      "Processing batch 11332/11884 - Loss: 30.1326\n",
      "Processing batch 11333/11884 - Loss: 29.7441\n",
      "Processing batch 11334/11884 - Loss: 30.0501\n",
      "Processing batch 11335/11884 - Loss: 29.5794\n",
      "Processing batch 11336/11884 - Loss: 30.2936\n",
      "Processing batch 11337/11884 - Loss: 29.4557\n",
      "Processing batch 11338/11884 - Loss: 29.9041\n",
      "Processing batch 11339/11884 - Loss: 30.4055\n",
      "Processing batch 11340/11884 - Loss: 29.5654\n",
      "Processing batch 11341/11884 - Loss: 30.8379\n",
      "Processing batch 11342/11884 - Loss: 29.7721\n",
      "Processing batch 11343/11884 - Loss: 30.9342\n",
      "Processing batch 11344/11884 - Loss: 29.3101\n",
      "Processing batch 11345/11884 - Loss: 30.6705\n",
      "Processing batch 11346/11884 - Loss: 30.3189\n",
      "Processing batch 11347/11884 - Loss: 29.3826\n",
      "Processing batch 11348/11884 - Loss: 29.2090\n",
      "Processing batch 11349/11884 - Loss: 29.6138\n",
      "Processing batch 11350/11884 - Loss: 30.2476\n",
      "Processing batch 11351/11884 - Loss: 29.7509\n",
      "Processing batch 11352/11884 - Loss: 31.3391\n",
      "Processing batch 11353/11884 - Loss: 30.3134\n",
      "Processing batch 11354/11884 - Loss: 31.0539\n",
      "Processing batch 11355/11884 - Loss: 30.4825\n",
      "Processing batch 11356/11884 - Loss: 30.0974\n",
      "Processing batch 11357/11884 - Loss: 29.7238\n",
      "Processing batch 11358/11884 - Loss: 29.1656\n",
      "Processing batch 11359/11884 - Loss: 30.2271\n",
      "Processing batch 11360/11884 - Loss: 29.5072\n",
      "Processing batch 11361/11884 - Loss: 29.0960\n",
      "Processing batch 11362/11884 - Loss: 30.8088\n",
      "Processing batch 11363/11884 - Loss: 30.1565\n",
      "Processing batch 11364/11884 - Loss: 29.1963\n",
      "Processing batch 11365/11884 - Loss: 29.6174\n",
      "Processing batch 11366/11884 - Loss: 29.5255\n",
      "Processing batch 11367/11884 - Loss: 29.1897\n",
      "Processing batch 11368/11884 - Loss: 30.4372\n",
      "Processing batch 11369/11884 - Loss: 29.1299\n",
      "Processing batch 11370/11884 - Loss: 30.4931\n",
      "Processing batch 11371/11884 - Loss: 30.3863\n",
      "Processing batch 11372/11884 - Loss: 28.6891\n",
      "Processing batch 11373/11884 - Loss: 29.5178\n",
      "Processing batch 11374/11884 - Loss: 30.6444\n",
      "Processing batch 11375/11884 - Loss: 30.3518\n",
      "Processing batch 11376/11884 - Loss: 29.7961\n",
      "Processing batch 11377/11884 - Loss: 31.1229\n",
      "Processing batch 11378/11884 - Loss: 29.6907\n",
      "Processing batch 11379/11884 - Loss: 30.1139\n",
      "Processing batch 11380/11884 - Loss: 29.8645\n",
      "Processing batch 11381/11884 - Loss: 29.9961\n",
      "Processing batch 11382/11884 - Loss: 29.5028\n",
      "Processing batch 11383/11884 - Loss: 30.2765\n",
      "Processing batch 11384/11884 - Loss: 29.0991\n",
      "Processing batch 11385/11884 - Loss: 29.5181\n",
      "Processing batch 11386/11884 - Loss: 28.5678\n",
      "Processing batch 11387/11884 - Loss: 29.6386\n",
      "Processing batch 11388/11884 - Loss: 30.4529\n",
      "Processing batch 11389/11884 - Loss: 30.4277\n",
      "Processing batch 11390/11884 - Loss: 30.3471\n",
      "Processing batch 11391/11884 - Loss: 28.5643\n",
      "Processing batch 11392/11884 - Loss: 30.2613\n",
      "Processing batch 11393/11884 - Loss: 29.4198\n",
      "Processing batch 11394/11884 - Loss: 29.0714\n",
      "Processing batch 11395/11884 - Loss: 30.1272\n",
      "Processing batch 11396/11884 - Loss: 29.4882\n",
      "Processing batch 11397/11884 - Loss: 30.5032\n",
      "Processing batch 11398/11884 - Loss: 30.0307\n",
      "Processing batch 11399/11884 - Loss: 28.9048\n",
      "Processing batch 11400/11884 - Loss: 29.8956\n",
      "Processing batch 11401/11884 - Loss: 29.6900\n",
      "Processing batch 11402/11884 - Loss: 29.7704\n",
      "Processing batch 11403/11884 - Loss: 29.8219\n",
      "Processing batch 11404/11884 - Loss: 30.0203\n",
      "Processing batch 11405/11884 - Loss: 29.4386\n",
      "Processing batch 11406/11884 - Loss: 29.8693\n",
      "Processing batch 11407/11884 - Loss: 30.5740\n",
      "Processing batch 11408/11884 - Loss: 30.5950\n",
      "Processing batch 11409/11884 - Loss: 32.0662\n",
      "Processing batch 11410/11884 - Loss: 28.8090\n",
      "Processing batch 11411/11884 - Loss: 31.5316\n",
      "Processing batch 11412/11884 - Loss: 29.1063\n",
      "Processing batch 11413/11884 - Loss: 29.4810\n",
      "Processing batch 11414/11884 - Loss: 30.1269\n",
      "Processing batch 11415/11884 - Loss: 29.4988\n",
      "Processing batch 11416/11884 - Loss: 29.1731\n",
      "Processing batch 11417/11884 - Loss: 30.1777\n",
      "Processing batch 11418/11884 - Loss: 29.4747\n",
      "Processing batch 11419/11884 - Loss: 30.8160\n",
      "Processing batch 11420/11884 - Loss: 29.9057\n",
      "Processing batch 11421/11884 - Loss: 30.3545\n",
      "Processing batch 11422/11884 - Loss: 30.5199\n",
      "Processing batch 11423/11884 - Loss: 29.1145\n",
      "Processing batch 11424/11884 - Loss: 30.0542\n",
      "Processing batch 11425/11884 - Loss: 30.0158\n",
      "Processing batch 11426/11884 - Loss: 28.3209\n",
      "Processing batch 11427/11884 - Loss: 29.5776\n",
      "Processing batch 11428/11884 - Loss: 29.5890\n",
      "Processing batch 11429/11884 - Loss: 30.3612\n",
      "Processing batch 11430/11884 - Loss: 30.6289\n",
      "Processing batch 11431/11884 - Loss: 31.0253\n",
      "Processing batch 11432/11884 - Loss: 30.6316\n",
      "Processing batch 11433/11884 - Loss: 31.1143\n",
      "Processing batch 11434/11884 - Loss: 29.9450\n",
      "Processing batch 11435/11884 - Loss: 30.9783\n",
      "Processing batch 11436/11884 - Loss: 28.9412\n",
      "Processing batch 11437/11884 - Loss: 29.0036\n",
      "Processing batch 11438/11884 - Loss: 30.9489\n",
      "Processing batch 11439/11884 - Loss: 30.7218\n",
      "Processing batch 11440/11884 - Loss: 29.4301\n",
      "Processing batch 11441/11884 - Loss: 30.4187\n",
      "Processing batch 11442/11884 - Loss: 29.1307\n",
      "Processing batch 11443/11884 - Loss: 29.3479\n",
      "Processing batch 11444/11884 - Loss: 30.2681\n",
      "Processing batch 11445/11884 - Loss: 30.0725\n",
      "Processing batch 11446/11884 - Loss: 30.8779\n",
      "Processing batch 11447/11884 - Loss: 29.9431\n",
      "Processing batch 11448/11884 - Loss: 29.9203\n",
      "Processing batch 11449/11884 - Loss: 31.2640\n",
      "Processing batch 11450/11884 - Loss: 29.1229\n",
      "Processing batch 11451/11884 - Loss: 30.4944\n",
      "Processing batch 11452/11884 - Loss: 30.2675\n",
      "Processing batch 11453/11884 - Loss: 29.8674\n",
      "Processing batch 11454/11884 - Loss: 30.0430\n",
      "Processing batch 11455/11884 - Loss: 30.6913\n",
      "Processing batch 11456/11884 - Loss: 29.5895\n",
      "Processing batch 11457/11884 - Loss: 29.6479\n",
      "Processing batch 11458/11884 - Loss: 28.7486\n",
      "Processing batch 11459/11884 - Loss: 30.7427\n",
      "Processing batch 11460/11884 - Loss: 31.3685\n",
      "Processing batch 11461/11884 - Loss: 29.4878\n",
      "Processing batch 11462/11884 - Loss: 29.4311\n",
      "Processing batch 11463/11884 - Loss: 29.7734\n",
      "Processing batch 11464/11884 - Loss: 30.0786\n",
      "Processing batch 11465/11884 - Loss: 30.6683\n",
      "Processing batch 11466/11884 - Loss: 31.1362\n",
      "Processing batch 11467/11884 - Loss: 29.9839\n",
      "Processing batch 11468/11884 - Loss: 29.1194\n",
      "Processing batch 11469/11884 - Loss: 29.0636\n",
      "Processing batch 11470/11884 - Loss: 30.7910\n",
      "Processing batch 11471/11884 - Loss: 30.1355\n",
      "Processing batch 11472/11884 - Loss: 29.4674\n",
      "Processing batch 11473/11884 - Loss: 29.8916\n",
      "Processing batch 11474/11884 - Loss: 29.4629\n",
      "Processing batch 11475/11884 - Loss: 29.6680\n",
      "Processing batch 11476/11884 - Loss: 29.7747\n",
      "Processing batch 11477/11884 - Loss: 29.7292\n",
      "Processing batch 11478/11884 - Loss: 30.7123\n",
      "Processing batch 11479/11884 - Loss: 29.4300\n",
      "Processing batch 11480/11884 - Loss: 30.3910\n",
      "Processing batch 11481/11884 - Loss: 30.5272\n",
      "Processing batch 11482/11884 - Loss: 28.9631\n",
      "Processing batch 11483/11884 - Loss: 29.3301\n",
      "Processing batch 11484/11884 - Loss: 30.1041\n",
      "Processing batch 11485/11884 - Loss: 30.5531\n",
      "Processing batch 11486/11884 - Loss: 29.5687\n",
      "Processing batch 11487/11884 - Loss: 30.4254\n",
      "Processing batch 11488/11884 - Loss: 30.2749\n",
      "Processing batch 11489/11884 - Loss: 28.7434\n",
      "Processing batch 11490/11884 - Loss: 29.6138\n",
      "Processing batch 11491/11884 - Loss: 30.2521\n",
      "Processing batch 11492/11884 - Loss: 29.0125\n",
      "Processing batch 11493/11884 - Loss: 30.1896\n",
      "Processing batch 11494/11884 - Loss: 29.1639\n",
      "Processing batch 11495/11884 - Loss: 30.4743\n",
      "Processing batch 11496/11884 - Loss: 30.8420\n",
      "Processing batch 11497/11884 - Loss: 31.3340\n",
      "Processing batch 11498/11884 - Loss: 31.0984\n",
      "Processing batch 11499/11884 - Loss: 29.2658\n",
      "Processing batch 11500/11884 - Loss: 28.8827\n",
      "Processing batch 11501/11884 - Loss: 29.4454\n",
      "Processing batch 11502/11884 - Loss: 31.2018\n",
      "Processing batch 11503/11884 - Loss: 30.8149\n",
      "Processing batch 11504/11884 - Loss: 30.6442\n",
      "Processing batch 11505/11884 - Loss: 29.3647\n",
      "Processing batch 11506/11884 - Loss: 29.0815\n",
      "Processing batch 11507/11884 - Loss: 30.2750\n",
      "Processing batch 11508/11884 - Loss: 30.9996\n",
      "Processing batch 11509/11884 - Loss: 29.7610\n",
      "Processing batch 11510/11884 - Loss: 29.9248\n",
      "Processing batch 11511/11884 - Loss: 29.5854\n",
      "Processing batch 11512/11884 - Loss: 30.6527\n",
      "Processing batch 11513/11884 - Loss: 29.8065\n",
      "Processing batch 11514/11884 - Loss: 30.7843\n",
      "Processing batch 11515/11884 - Loss: 29.8590\n",
      "Processing batch 11516/11884 - Loss: 29.6265\n",
      "Processing batch 11517/11884 - Loss: 30.3295\n",
      "Processing batch 11518/11884 - Loss: 30.0169\n",
      "Processing batch 11519/11884 - Loss: 30.1873\n",
      "Processing batch 11520/11884 - Loss: 30.0185\n",
      "Processing batch 11521/11884 - Loss: 29.5496\n",
      "Processing batch 11522/11884 - Loss: 29.8728\n",
      "Processing batch 11523/11884 - Loss: 29.8385\n",
      "Processing batch 11524/11884 - Loss: 29.2797\n",
      "Processing batch 11525/11884 - Loss: 28.7138\n",
      "Processing batch 11526/11884 - Loss: 28.9315\n",
      "Processing batch 11527/11884 - Loss: 30.7319\n",
      "Processing batch 11528/11884 - Loss: 29.1124\n",
      "Processing batch 11529/11884 - Loss: 28.6631\n",
      "Processing batch 11530/11884 - Loss: 30.9795\n",
      "Processing batch 11531/11884 - Loss: 29.9600\n",
      "Processing batch 11532/11884 - Loss: 30.3836\n",
      "Processing batch 11533/11884 - Loss: 30.0567\n",
      "Processing batch 11534/11884 - Loss: 31.1057\n",
      "Processing batch 11535/11884 - Loss: 31.5873\n",
      "Processing batch 11536/11884 - Loss: 31.5043\n",
      "Processing batch 11537/11884 - Loss: 30.8566\n",
      "Processing batch 11538/11884 - Loss: 30.6998\n",
      "Processing batch 11539/11884 - Loss: 29.7650\n",
      "Processing batch 11540/11884 - Loss: 28.3056\n",
      "Processing batch 11541/11884 - Loss: 30.0890\n",
      "Processing batch 11542/11884 - Loss: 29.2821\n",
      "Processing batch 11543/11884 - Loss: 31.6001\n",
      "Processing batch 11544/11884 - Loss: 30.2946\n",
      "Processing batch 11545/11884 - Loss: 29.7214\n",
      "Processing batch 11546/11884 - Loss: 28.8519\n",
      "Processing batch 11547/11884 - Loss: 29.4423\n",
      "Processing batch 11548/11884 - Loss: 30.1951\n",
      "Processing batch 11549/11884 - Loss: 30.3020\n",
      "Processing batch 11550/11884 - Loss: 29.0952\n",
      "Processing batch 11551/11884 - Loss: 30.3876\n",
      "Processing batch 11552/11884 - Loss: 30.6028\n",
      "Processing batch 11553/11884 - Loss: 29.8425\n",
      "Processing batch 11554/11884 - Loss: 29.4535\n",
      "Processing batch 11555/11884 - Loss: 30.0376\n",
      "Processing batch 11556/11884 - Loss: 30.8622\n",
      "Processing batch 11557/11884 - Loss: 30.3443\n",
      "Processing batch 11558/11884 - Loss: 30.5124\n",
      "Processing batch 11559/11884 - Loss: 28.8077\n",
      "Processing batch 11560/11884 - Loss: 29.3870\n",
      "Processing batch 11561/11884 - Loss: 30.3518\n",
      "Processing batch 11562/11884 - Loss: 31.5481\n",
      "Processing batch 11563/11884 - Loss: 29.1384\n",
      "Processing batch 11564/11884 - Loss: 30.2057\n",
      "Processing batch 11565/11884 - Loss: 31.1063\n",
      "Processing batch 11566/11884 - Loss: 30.2456\n",
      "Processing batch 11567/11884 - Loss: 29.1475\n",
      "Processing batch 11568/11884 - Loss: 29.7661\n",
      "Processing batch 11569/11884 - Loss: 29.3862\n",
      "Processing batch 11570/11884 - Loss: 30.4578\n",
      "Processing batch 11571/11884 - Loss: 30.4167\n",
      "Processing batch 11572/11884 - Loss: 29.4989\n",
      "Processing batch 11573/11884 - Loss: 30.1349\n",
      "Processing batch 11574/11884 - Loss: 29.5603\n",
      "Processing batch 11575/11884 - Loss: 28.8247\n",
      "Processing batch 11576/11884 - Loss: 29.7335\n",
      "Processing batch 11577/11884 - Loss: 30.1958\n",
      "Processing batch 11578/11884 - Loss: 30.4549\n",
      "Processing batch 11579/11884 - Loss: 31.0188\n",
      "Processing batch 11580/11884 - Loss: 30.4848\n",
      "Processing batch 11581/11884 - Loss: 29.0068\n",
      "Processing batch 11582/11884 - Loss: 29.6222\n",
      "Processing batch 11583/11884 - Loss: 31.1154\n",
      "Processing batch 11584/11884 - Loss: 30.2401\n",
      "Processing batch 11585/11884 - Loss: 28.8436\n",
      "Processing batch 11586/11884 - Loss: 30.1360\n",
      "Processing batch 11587/11884 - Loss: 30.1287\n",
      "Processing batch 11588/11884 - Loss: 30.2330\n",
      "Processing batch 11589/11884 - Loss: 30.2509\n",
      "Processing batch 11590/11884 - Loss: 29.5972\n",
      "Processing batch 11591/11884 - Loss: 28.8504\n",
      "Processing batch 11592/11884 - Loss: 30.5229\n",
      "Processing batch 11593/11884 - Loss: 28.5024\n",
      "Processing batch 11594/11884 - Loss: 29.4813\n",
      "Processing batch 11595/11884 - Loss: 29.8958\n",
      "Processing batch 11596/11884 - Loss: 29.4249\n",
      "Processing batch 11597/11884 - Loss: 30.5495\n",
      "Processing batch 11598/11884 - Loss: 29.6289\n",
      "Processing batch 11599/11884 - Loss: 29.7010\n",
      "Processing batch 11600/11884 - Loss: 29.6007\n",
      "Processing batch 11601/11884 - Loss: 29.6301\n",
      "Processing batch 11602/11884 - Loss: 29.2521\n",
      "Processing batch 11603/11884 - Loss: 30.0634\n",
      "Processing batch 11604/11884 - Loss: 31.1658\n",
      "Processing batch 11605/11884 - Loss: 28.9737\n",
      "Processing batch 11606/11884 - Loss: 30.5154\n",
      "Processing batch 11607/11884 - Loss: 29.7193\n",
      "Processing batch 11608/11884 - Loss: 30.5386\n",
      "Processing batch 11609/11884 - Loss: 30.2554\n",
      "Processing batch 11610/11884 - Loss: 29.2632\n",
      "Processing batch 11611/11884 - Loss: 29.8178\n",
      "Processing batch 11612/11884 - Loss: 30.6774\n",
      "Processing batch 11613/11884 - Loss: 29.4559\n",
      "Processing batch 11614/11884 - Loss: 30.1893\n",
      "Processing batch 11615/11884 - Loss: 31.2371\n",
      "Processing batch 11616/11884 - Loss: 30.1683\n",
      "Processing batch 11617/11884 - Loss: 31.4174\n",
      "Processing batch 11618/11884 - Loss: 29.9123\n",
      "Processing batch 11619/11884 - Loss: 28.9730\n",
      "Processing batch 11620/11884 - Loss: 29.7276\n",
      "Processing batch 11621/11884 - Loss: 30.1654\n",
      "Processing batch 11622/11884 - Loss: 29.7722\n",
      "Processing batch 11623/11884 - Loss: 29.2686\n",
      "Processing batch 11624/11884 - Loss: 30.1973\n",
      "Processing batch 11625/11884 - Loss: 30.4680\n",
      "Processing batch 11626/11884 - Loss: 31.0902\n",
      "Processing batch 11627/11884 - Loss: 31.7414\n",
      "Processing batch 11628/11884 - Loss: 29.6979\n",
      "Processing batch 11629/11884 - Loss: 29.6233\n",
      "Processing batch 11630/11884 - Loss: 30.9098\n",
      "Processing batch 11631/11884 - Loss: 29.8865\n",
      "Processing batch 11632/11884 - Loss: 29.1741\n",
      "Processing batch 11633/11884 - Loss: 30.1418\n",
      "Processing batch 11634/11884 - Loss: 29.3364\n",
      "Processing batch 11635/11884 - Loss: 30.8021\n",
      "Processing batch 11636/11884 - Loss: 28.6246\n",
      "Processing batch 11637/11884 - Loss: 29.8270\n",
      "Processing batch 11638/11884 - Loss: 29.7119\n",
      "Processing batch 11639/11884 - Loss: 29.1105\n",
      "Processing batch 11640/11884 - Loss: 29.9143\n",
      "Processing batch 11641/11884 - Loss: 29.9969\n",
      "Processing batch 11642/11884 - Loss: 30.2508\n",
      "Processing batch 11643/11884 - Loss: 29.0335\n",
      "Processing batch 11644/11884 - Loss: 30.1777\n",
      "Processing batch 11645/11884 - Loss: 30.1750\n",
      "Processing batch 11646/11884 - Loss: 30.1278\n",
      "Processing batch 11647/11884 - Loss: 30.0991\n",
      "Processing batch 11648/11884 - Loss: 29.5350\n",
      "Processing batch 11649/11884 - Loss: 31.4537\n",
      "Processing batch 11650/11884 - Loss: 30.2072\n",
      "Processing batch 11651/11884 - Loss: 31.3650\n",
      "Processing batch 11652/11884 - Loss: 29.9337\n",
      "Processing batch 11653/11884 - Loss: 29.8399\n",
      "Processing batch 11654/11884 - Loss: 27.7235\n",
      "Processing batch 11655/11884 - Loss: 30.0980\n",
      "Processing batch 11656/11884 - Loss: 28.8372\n",
      "Processing batch 11657/11884 - Loss: 31.3665\n",
      "Processing batch 11658/11884 - Loss: 29.7750\n",
      "Processing batch 11659/11884 - Loss: 28.9528\n",
      "Processing batch 11660/11884 - Loss: 30.3008\n",
      "Processing batch 11661/11884 - Loss: 29.9553\n",
      "Processing batch 11662/11884 - Loss: 30.6100\n",
      "Processing batch 11663/11884 - Loss: 28.2147\n",
      "Processing batch 11664/11884 - Loss: 29.2514\n",
      "Processing batch 11665/11884 - Loss: 29.6012\n",
      "Processing batch 11666/11884 - Loss: 30.1215\n",
      "Processing batch 11667/11884 - Loss: 30.6595\n",
      "Processing batch 11668/11884 - Loss: 29.6650\n",
      "Processing batch 11669/11884 - Loss: 29.0793\n",
      "Processing batch 11670/11884 - Loss: 30.6723\n",
      "Processing batch 11671/11884 - Loss: 30.4516\n",
      "Processing batch 11672/11884 - Loss: 29.5827\n",
      "Processing batch 11673/11884 - Loss: 30.2301\n",
      "Processing batch 11674/11884 - Loss: 29.7596\n",
      "Processing batch 11675/11884 - Loss: 29.2730\n",
      "Processing batch 11676/11884 - Loss: 30.5450\n",
      "Processing batch 11677/11884 - Loss: 30.4119\n",
      "Processing batch 11678/11884 - Loss: 30.8445\n",
      "Processing batch 11679/11884 - Loss: 30.5014\n",
      "Processing batch 11680/11884 - Loss: 29.2272\n",
      "Processing batch 11681/11884 - Loss: 30.6651\n",
      "Processing batch 11682/11884 - Loss: 29.4060\n",
      "Processing batch 11683/11884 - Loss: 29.0309\n",
      "Processing batch 11684/11884 - Loss: 29.8935\n",
      "Processing batch 11685/11884 - Loss: 30.2592\n",
      "Processing batch 11686/11884 - Loss: 29.1371\n",
      "Processing batch 11687/11884 - Loss: 29.5237\n",
      "Processing batch 11688/11884 - Loss: 30.2542\n",
      "Processing batch 11689/11884 - Loss: 29.8700\n",
      "Processing batch 11690/11884 - Loss: 29.6260\n",
      "Processing batch 11691/11884 - Loss: 30.2616\n",
      "Processing batch 11692/11884 - Loss: 30.3055\n",
      "Processing batch 11693/11884 - Loss: 29.3212\n",
      "Processing batch 11694/11884 - Loss: 29.7286\n",
      "Processing batch 11695/11884 - Loss: 29.1325\n",
      "Processing batch 11696/11884 - Loss: 29.5695\n",
      "Processing batch 11697/11884 - Loss: 29.2743\n",
      "Processing batch 11698/11884 - Loss: 28.9568\n",
      "Processing batch 11699/11884 - Loss: 29.5367\n",
      "Processing batch 11700/11884 - Loss: 30.8220\n",
      "Processing batch 11701/11884 - Loss: 29.5243\n",
      "Processing batch 11702/11884 - Loss: 29.6285\n",
      "Processing batch 11703/11884 - Loss: 29.9925\n",
      "Processing batch 11704/11884 - Loss: 29.8134\n",
      "Processing batch 11705/11884 - Loss: 30.2321\n",
      "Processing batch 11706/11884 - Loss: 28.1218\n",
      "Processing batch 11707/11884 - Loss: 29.5069\n",
      "Processing batch 11708/11884 - Loss: 30.5109\n",
      "Processing batch 11709/11884 - Loss: 30.1141\n",
      "Processing batch 11710/11884 - Loss: 31.0862\n",
      "Processing batch 11711/11884 - Loss: 29.1599\n",
      "Processing batch 11712/11884 - Loss: 29.9160\n",
      "Processing batch 11713/11884 - Loss: 29.7654\n",
      "Processing batch 11714/11884 - Loss: 29.6198\n",
      "Processing batch 11715/11884 - Loss: 29.5071\n",
      "Processing batch 11716/11884 - Loss: 29.2435\n",
      "Processing batch 11717/11884 - Loss: 30.7080\n",
      "Processing batch 11718/11884 - Loss: 29.2423\n",
      "Processing batch 11719/11884 - Loss: 30.3228\n",
      "Processing batch 11720/11884 - Loss: 29.3689\n",
      "Processing batch 11721/11884 - Loss: 30.6465\n",
      "Processing batch 11722/11884 - Loss: 30.9148\n",
      "Processing batch 11723/11884 - Loss: 29.4487\n",
      "Processing batch 11724/11884 - Loss: 28.6166\n",
      "Processing batch 11725/11884 - Loss: 30.2646\n",
      "Processing batch 11726/11884 - Loss: 30.1844\n",
      "Processing batch 11727/11884 - Loss: 29.0186\n",
      "Processing batch 11728/11884 - Loss: 30.0671\n",
      "Processing batch 11729/11884 - Loss: 29.3742\n",
      "Processing batch 11730/11884 - Loss: 30.4533\n",
      "Processing batch 11731/11884 - Loss: 29.2265\n",
      "Processing batch 11732/11884 - Loss: 29.8179\n",
      "Processing batch 11733/11884 - Loss: 29.5817\n",
      "Processing batch 11734/11884 - Loss: 29.9360\n",
      "Processing batch 11735/11884 - Loss: 29.4802\n",
      "Processing batch 11736/11884 - Loss: 30.2129\n",
      "Processing batch 11737/11884 - Loss: 31.0522\n",
      "Processing batch 11738/11884 - Loss: 30.9926\n",
      "Processing batch 11739/11884 - Loss: 29.4102\n",
      "Processing batch 11740/11884 - Loss: 29.2716\n",
      "Processing batch 11741/11884 - Loss: 29.8100\n",
      "Processing batch 11742/11884 - Loss: 30.2635\n",
      "Processing batch 11743/11884 - Loss: 29.4259\n",
      "Processing batch 11744/11884 - Loss: 29.9693\n",
      "Processing batch 11745/11884 - Loss: 30.7867\n",
      "Processing batch 11746/11884 - Loss: 31.6552\n",
      "Processing batch 11747/11884 - Loss: 31.8930\n",
      "Processing batch 11748/11884 - Loss: 29.1897\n",
      "Processing batch 11749/11884 - Loss: 29.5697\n",
      "Processing batch 11750/11884 - Loss: 28.7212\n",
      "Processing batch 11751/11884 - Loss: 29.6111\n",
      "Processing batch 11752/11884 - Loss: 31.1122\n",
      "Processing batch 11753/11884 - Loss: 29.8604\n",
      "Processing batch 11754/11884 - Loss: 30.0803\n",
      "Processing batch 11755/11884 - Loss: 30.5059\n",
      "Processing batch 11756/11884 - Loss: 29.7641\n",
      "Processing batch 11757/11884 - Loss: 29.1889\n",
      "Processing batch 11758/11884 - Loss: 28.2589\n",
      "Processing batch 11759/11884 - Loss: 30.2762\n",
      "Processing batch 11760/11884 - Loss: 29.7411\n",
      "Processing batch 11761/11884 - Loss: 29.8615\n",
      "Processing batch 11762/11884 - Loss: 30.1829\n",
      "Processing batch 11763/11884 - Loss: 31.2436\n",
      "Processing batch 11764/11884 - Loss: 28.8710\n",
      "Processing batch 11765/11884 - Loss: 30.1577\n",
      "Processing batch 11766/11884 - Loss: 29.6047\n",
      "Processing batch 11767/11884 - Loss: 29.3652\n",
      "Processing batch 11768/11884 - Loss: 30.8344\n",
      "Processing batch 11769/11884 - Loss: 28.9073\n",
      "Processing batch 11770/11884 - Loss: 30.5887\n",
      "Processing batch 11771/11884 - Loss: 29.5713\n",
      "Processing batch 11772/11884 - Loss: 28.6652\n",
      "Processing batch 11773/11884 - Loss: 30.0150\n",
      "Processing batch 11774/11884 - Loss: 29.5344\n",
      "Processing batch 11775/11884 - Loss: 30.8729\n",
      "Processing batch 11776/11884 - Loss: 29.4666\n",
      "Processing batch 11777/11884 - Loss: 30.2859\n",
      "Processing batch 11778/11884 - Loss: 31.1395\n",
      "Processing batch 11779/11884 - Loss: 29.6788\n",
      "Processing batch 11780/11884 - Loss: 29.3354\n",
      "Processing batch 11781/11884 - Loss: 29.5956\n",
      "Processing batch 11782/11884 - Loss: 28.8947\n",
      "Processing batch 11783/11884 - Loss: 30.4933\n",
      "Processing batch 11784/11884 - Loss: 30.3349\n",
      "Processing batch 11785/11884 - Loss: 29.2051\n",
      "Processing batch 11786/11884 - Loss: 30.4876\n",
      "Processing batch 11787/11884 - Loss: 29.1774\n",
      "Processing batch 11788/11884 - Loss: 30.6302\n",
      "Processing batch 11789/11884 - Loss: 29.5004\n",
      "Processing batch 11790/11884 - Loss: 29.8099\n",
      "Processing batch 11791/11884 - Loss: 29.1167\n",
      "Processing batch 11792/11884 - Loss: 30.1634\n",
      "Processing batch 11793/11884 - Loss: 28.9087\n",
      "Processing batch 11794/11884 - Loss: 31.1361\n",
      "Processing batch 11795/11884 - Loss: 30.7441\n",
      "Processing batch 11796/11884 - Loss: 31.4829\n",
      "Processing batch 11797/11884 - Loss: 30.6437\n",
      "Processing batch 11798/11884 - Loss: 29.8492\n",
      "Processing batch 11799/11884 - Loss: 30.1985\n",
      "Processing batch 11800/11884 - Loss: 29.8904\n",
      "Processing batch 11801/11884 - Loss: 29.3177\n",
      "Processing batch 11802/11884 - Loss: 30.2137\n",
      "Processing batch 11803/11884 - Loss: 28.3726\n",
      "Processing batch 11804/11884 - Loss: 29.1877\n",
      "Processing batch 11805/11884 - Loss: 30.1516\n",
      "Processing batch 11806/11884 - Loss: 31.5836\n",
      "Processing batch 11807/11884 - Loss: 29.7954\n",
      "Processing batch 11808/11884 - Loss: 28.7863\n",
      "Processing batch 11809/11884 - Loss: 30.2189\n",
      "Processing batch 11810/11884 - Loss: 28.6890\n",
      "Processing batch 11811/11884 - Loss: 31.4324\n",
      "Processing batch 11812/11884 - Loss: 29.8001\n",
      "Processing batch 11813/11884 - Loss: 30.8811\n",
      "Processing batch 11814/11884 - Loss: 30.2340\n",
      "Processing batch 11815/11884 - Loss: 31.2656\n",
      "Processing batch 11816/11884 - Loss: 29.0572\n",
      "Processing batch 11817/11884 - Loss: 29.8572\n",
      "Processing batch 11818/11884 - Loss: 29.3907\n",
      "Processing batch 11819/11884 - Loss: 29.4125\n",
      "Processing batch 11820/11884 - Loss: 28.9123\n",
      "Processing batch 11821/11884 - Loss: 28.1771\n",
      "Processing batch 11822/11884 - Loss: 31.2973\n",
      "Processing batch 11823/11884 - Loss: 29.9720\n",
      "Processing batch 11824/11884 - Loss: 29.3524\n",
      "Processing batch 11825/11884 - Loss: 30.0412\n",
      "Processing batch 11826/11884 - Loss: 30.0363\n",
      "Processing batch 11827/11884 - Loss: 30.4264\n",
      "Processing batch 11828/11884 - Loss: 29.8976\n",
      "Processing batch 11829/11884 - Loss: 29.8237\n",
      "Processing batch 11830/11884 - Loss: 29.0704\n",
      "Processing batch 11831/11884 - Loss: 31.4002\n",
      "Processing batch 11832/11884 - Loss: 30.3075\n",
      "Processing batch 11833/11884 - Loss: 30.4409\n",
      "Processing batch 11834/11884 - Loss: 29.8111\n",
      "Processing batch 11835/11884 - Loss: 30.5722\n",
      "Processing batch 11836/11884 - Loss: 29.1238\n",
      "Processing batch 11837/11884 - Loss: 28.7521\n",
      "Processing batch 11838/11884 - Loss: 30.5133\n",
      "Processing batch 11839/11884 - Loss: 30.9002\n",
      "Processing batch 11840/11884 - Loss: 29.1296\n",
      "Processing batch 11841/11884 - Loss: 29.1824\n",
      "Processing batch 11842/11884 - Loss: 29.2015\n",
      "Processing batch 11843/11884 - Loss: 30.0305\n",
      "Processing batch 11844/11884 - Loss: 29.8795\n",
      "Processing batch 11845/11884 - Loss: 30.5030\n",
      "Processing batch 11846/11884 - Loss: 29.8207\n",
      "Processing batch 11847/11884 - Loss: 30.1738\n",
      "Processing batch 11848/11884 - Loss: 29.7210\n",
      "Processing batch 11849/11884 - Loss: 30.6993\n",
      "Processing batch 11850/11884 - Loss: 29.3181\n",
      "Processing batch 11851/11884 - Loss: 28.5527\n",
      "Processing batch 11852/11884 - Loss: 29.6773\n",
      "Processing batch 11853/11884 - Loss: 30.1914\n",
      "Processing batch 11854/11884 - Loss: 30.7562\n",
      "Processing batch 11855/11884 - Loss: 29.7542\n",
      "Processing batch 11856/11884 - Loss: 30.9121\n",
      "Processing batch 11857/11884 - Loss: 28.3211\n",
      "Processing batch 11858/11884 - Loss: 30.6851\n",
      "Processing batch 11859/11884 - Loss: 29.9616\n",
      "Processing batch 11860/11884 - Loss: 28.7681\n",
      "Processing batch 11861/11884 - Loss: 29.7879\n",
      "Processing batch 11862/11884 - Loss: 30.4584\n",
      "Processing batch 11863/11884 - Loss: 30.1653\n",
      "Processing batch 11864/11884 - Loss: 30.8782\n",
      "Processing batch 11865/11884 - Loss: 29.4038\n",
      "Processing batch 11866/11884 - Loss: 31.1018\n",
      "Processing batch 11867/11884 - Loss: 31.4280\n",
      "Processing batch 11868/11884 - Loss: 30.7829\n",
      "Processing batch 11869/11884 - Loss: 29.6264\n",
      "Processing batch 11870/11884 - Loss: 29.4665\n",
      "Processing batch 11871/11884 - Loss: 30.7178\n",
      "Processing batch 11872/11884 - Loss: 29.9752\n",
      "Processing batch 11873/11884 - Loss: 29.1747\n",
      "Processing batch 11874/11884 - Loss: 29.9475\n",
      "Processing batch 11875/11884 - Loss: 29.4968\n",
      "Processing batch 11876/11884 - Loss: 31.1023\n",
      "Processing batch 11877/11884 - Loss: 29.7704\n",
      "Processing batch 11878/11884 - Loss: 29.8802\n",
      "Processing batch 11879/11884 - Loss: 29.8448\n",
      "Processing batch 11880/11884 - Loss: 30.1297\n",
      "Processing batch 11881/11884 - Loss: 30.3748\n",
      "Processing batch 11882/11884 - Loss: 30.7172\n",
      "Processing batch 11883/11884 - Loss: 29.7443\n",
      "Processing batch 11884/11884 - Loss: 31.1456\n",
      "Processing batch 1/11884 - Val_Loss: 28.6094\n",
      "Processing batch 2/11884 - Val_Loss: 24.1191\n",
      "Processing batch 3/11884 - Val_Loss: 29.0824\n",
      "Processing batch 4/11884 - Val_Loss: 24.4461\n",
      "Processing batch 5/11884 - Val_Loss: 24.4142\n",
      "Processing batch 6/11884 - Val_Loss: 28.2055\n",
      "Processing batch 7/11884 - Val_Loss: 25.6285\n",
      "Processing batch 8/11884 - Val_Loss: 25.8144\n",
      "Processing batch 9/11884 - Val_Loss: 27.8797\n",
      "Processing batch 10/11884 - Val_Loss: 26.5341\n",
      "Processing batch 11/11884 - Val_Loss: 26.1069\n",
      "Processing batch 12/11884 - Val_Loss: 28.6129\n",
      "Processing batch 13/11884 - Val_Loss: 25.1351\n",
      "Processing batch 14/11884 - Val_Loss: 24.1173\n",
      "Processing batch 15/11884 - Val_Loss: 24.2263\n",
      "Processing batch 16/11884 - Val_Loss: 24.9163\n",
      "Processing batch 17/11884 - Val_Loss: 24.9541\n",
      "Processing batch 18/11884 - Val_Loss: 22.8107\n",
      "Processing batch 19/11884 - Val_Loss: 24.6675\n",
      "Processing batch 20/11884 - Val_Loss: 25.2287\n",
      "Processing batch 21/11884 - Val_Loss: 22.9400\n",
      "Processing batch 22/11884 - Val_Loss: 24.3475\n",
      "Processing batch 23/11884 - Val_Loss: 26.2017\n",
      "Processing batch 24/11884 - Val_Loss: 25.7564\n",
      "Processing batch 25/11884 - Val_Loss: 25.3070\n",
      "Processing batch 26/11884 - Val_Loss: 24.0416\n",
      "Processing batch 27/11884 - Val_Loss: 25.8886\n",
      "Processing batch 28/11884 - Val_Loss: 25.7331\n",
      "Processing batch 29/11884 - Val_Loss: 25.9907\n",
      "Processing batch 30/11884 - Val_Loss: 24.5519\n",
      "Processing batch 31/11884 - Val_Loss: 25.9497\n",
      "Processing batch 32/11884 - Val_Loss: 24.2876\n",
      "Processing batch 33/11884 - Val_Loss: 24.4633\n",
      "Processing batch 34/11884 - Val_Loss: 25.5693\n",
      "Processing batch 35/11884 - Val_Loss: 24.7745\n",
      "Processing batch 36/11884 - Val_Loss: 26.0389\n",
      "Processing batch 37/11884 - Val_Loss: 24.8067\n",
      "Processing batch 38/11884 - Val_Loss: 25.0519\n",
      "Processing batch 39/11884 - Val_Loss: 24.2959\n",
      "Processing batch 40/11884 - Val_Loss: 23.9350\n",
      "Processing batch 41/11884 - Val_Loss: 23.7542\n",
      "Processing batch 42/11884 - Val_Loss: 27.0786\n",
      "Processing batch 43/11884 - Val_Loss: 22.3884\n",
      "Processing batch 44/11884 - Val_Loss: 26.1236\n",
      "Processing batch 45/11884 - Val_Loss: 27.2647\n",
      "Processing batch 46/11884 - Val_Loss: 26.4981\n",
      "Processing batch 47/11884 - Val_Loss: 25.2192\n",
      "Processing batch 48/11884 - Val_Loss: 25.0772\n",
      "Processing batch 49/11884 - Val_Loss: 22.8227\n",
      "Processing batch 50/11884 - Val_Loss: 26.3525\n",
      "Processing batch 51/11884 - Val_Loss: 26.4214\n",
      "Processing batch 52/11884 - Val_Loss: 24.4562\n",
      "Processing batch 53/11884 - Val_Loss: 23.8759\n",
      "Processing batch 54/11884 - Val_Loss: 26.0483\n",
      "Processing batch 55/11884 - Val_Loss: 24.3354\n",
      "Processing batch 56/11884 - Val_Loss: 25.0209\n",
      "Processing batch 57/11884 - Val_Loss: 26.5729\n",
      "Processing batch 58/11884 - Val_Loss: 26.7069\n",
      "Processing batch 59/11884 - Val_Loss: 24.8222\n",
      "Processing batch 60/11884 - Val_Loss: 23.5990\n",
      "Processing batch 61/11884 - Val_Loss: 25.2813\n",
      "Processing batch 62/11884 - Val_Loss: 23.9309\n",
      "Processing batch 63/11884 - Val_Loss: 26.4336\n",
      "Processing batch 64/11884 - Val_Loss: 26.0002\n",
      "Processing batch 65/11884 - Val_Loss: 23.8724\n",
      "Processing batch 66/11884 - Val_Loss: 26.4688\n",
      "Processing batch 67/11884 - Val_Loss: 22.6516\n",
      "Processing batch 68/11884 - Val_Loss: 28.3561\n",
      "Processing batch 69/11884 - Val_Loss: 29.3406\n",
      "Processing batch 70/11884 - Val_Loss: 27.7825\n",
      "Processing batch 71/11884 - Val_Loss: 25.1457\n",
      "Processing batch 72/11884 - Val_Loss: 24.1116\n",
      "Processing batch 73/11884 - Val_Loss: 24.7012\n",
      "Processing batch 74/11884 - Val_Loss: 25.3113\n",
      "Processing batch 75/11884 - Val_Loss: 24.9740\n",
      "Processing batch 76/11884 - Val_Loss: 24.5851\n",
      "Processing batch 77/11884 - Val_Loss: 28.2503\n",
      "Processing batch 78/11884 - Val_Loss: 26.0551\n",
      "Processing batch 79/11884 - Val_Loss: 24.4528\n",
      "Processing batch 80/11884 - Val_Loss: 24.7818\n",
      "Processing batch 81/11884 - Val_Loss: 26.6520\n",
      "Processing batch 82/11884 - Val_Loss: 25.0319\n",
      "Processing batch 83/11884 - Val_Loss: 25.4833\n",
      "Processing batch 84/11884 - Val_Loss: 25.7589\n",
      "Processing batch 85/11884 - Val_Loss: 25.5845\n",
      "Processing batch 86/11884 - Val_Loss: 24.6499\n",
      "Processing batch 87/11884 - Val_Loss: 24.8133\n",
      "Processing batch 88/11884 - Val_Loss: 27.5599\n",
      "Processing batch 89/11884 - Val_Loss: 27.6379\n",
      "Processing batch 90/11884 - Val_Loss: 26.3633\n",
      "Processing batch 91/11884 - Val_Loss: 24.2881\n",
      "Processing batch 92/11884 - Val_Loss: 26.5893\n",
      "Processing batch 93/11884 - Val_Loss: 23.0575\n",
      "Processing batch 94/11884 - Val_Loss: 23.9830\n",
      "Processing batch 95/11884 - Val_Loss: 24.2692\n",
      "Processing batch 96/11884 - Val_Loss: 25.3846\n",
      "Processing batch 97/11884 - Val_Loss: 24.6695\n",
      "Processing batch 98/11884 - Val_Loss: 28.0874\n",
      "Processing batch 99/11884 - Val_Loss: 25.6444\n",
      "Processing batch 100/11884 - Val_Loss: 26.5644\n",
      "Processing batch 101/11884 - Val_Loss: 26.4130\n",
      "Processing batch 102/11884 - Val_Loss: 24.5449\n",
      "Processing batch 103/11884 - Val_Loss: 25.0045\n",
      "Processing batch 104/11884 - Val_Loss: 26.3359\n",
      "Processing batch 105/11884 - Val_Loss: 25.9037\n",
      "Processing batch 106/11884 - Val_Loss: 25.5463\n",
      "Processing batch 107/11884 - Val_Loss: 26.9448\n",
      "Processing batch 108/11884 - Val_Loss: 21.2962\n",
      "Processing batch 109/11884 - Val_Loss: 24.0488\n",
      "Processing batch 110/11884 - Val_Loss: 23.5620\n",
      "Processing batch 111/11884 - Val_Loss: 26.6757\n",
      "Processing batch 112/11884 - Val_Loss: 24.8788\n",
      "Processing batch 113/11884 - Val_Loss: 27.2140\n",
      "Processing batch 114/11884 - Val_Loss: 22.6055\n",
      "Processing batch 115/11884 - Val_Loss: 23.5456\n",
      "Processing batch 116/11884 - Val_Loss: 24.6402\n",
      "Processing batch 117/11884 - Val_Loss: 26.0380\n",
      "Processing batch 118/11884 - Val_Loss: 27.0215\n",
      "Processing batch 119/11884 - Val_Loss: 22.5098\n",
      "Processing batch 120/11884 - Val_Loss: 25.6824\n",
      "Processing batch 121/11884 - Val_Loss: 26.7115\n",
      "Processing batch 122/11884 - Val_Loss: 29.0287\n",
      "Processing batch 123/11884 - Val_Loss: 22.8692\n",
      "Processing batch 124/11884 - Val_Loss: 24.4499\n",
      "Processing batch 125/11884 - Val_Loss: 23.0939\n",
      "Processing batch 126/11884 - Val_Loss: 27.6232\n",
      "Processing batch 127/11884 - Val_Loss: 25.5969\n",
      "Processing batch 128/11884 - Val_Loss: 23.6974\n",
      "Processing batch 129/11884 - Val_Loss: 23.4276\n",
      "Processing batch 130/11884 - Val_Loss: 25.3091\n",
      "Processing batch 131/11884 - Val_Loss: 28.2656\n",
      "Processing batch 132/11884 - Val_Loss: 25.7944\n",
      "Processing batch 133/11884 - Val_Loss: 27.7336\n",
      "Processing batch 134/11884 - Val_Loss: 24.4118\n",
      "Processing batch 135/11884 - Val_Loss: 28.4953\n",
      "Processing batch 136/11884 - Val_Loss: 25.4619\n",
      "Processing batch 137/11884 - Val_Loss: 23.4830\n",
      "Processing batch 138/11884 - Val_Loss: 26.3914\n",
      "Processing batch 139/11884 - Val_Loss: 23.3743\n",
      "Processing batch 140/11884 - Val_Loss: 25.1441\n",
      "Processing batch 141/11884 - Val_Loss: 23.8965\n",
      "Processing batch 142/11884 - Val_Loss: 25.7554\n",
      "Processing batch 143/11884 - Val_Loss: 26.8396\n",
      "Processing batch 144/11884 - Val_Loss: 27.9938\n",
      "Processing batch 145/11884 - Val_Loss: 23.8049\n",
      "Processing batch 146/11884 - Val_Loss: 26.6167\n",
      "Processing batch 147/11884 - Val_Loss: 25.7518\n",
      "Processing batch 148/11884 - Val_Loss: 23.0216\n",
      "Processing batch 149/11884 - Val_Loss: 27.9110\n",
      "Processing batch 150/11884 - Val_Loss: 30.4772\n",
      "Processing batch 151/11884 - Val_Loss: 25.7469\n",
      "Processing batch 152/11884 - Val_Loss: 25.3728\n",
      "Processing batch 153/11884 - Val_Loss: 26.4501\n",
      "Processing batch 154/11884 - Val_Loss: 27.7850\n",
      "Processing batch 155/11884 - Val_Loss: 27.1065\n",
      "Processing batch 156/11884 - Val_Loss: 26.6804\n",
      "Processing batch 157/11884 - Val_Loss: 25.8287\n",
      "Processing batch 158/11884 - Val_Loss: 25.1772\n",
      "Processing batch 159/11884 - Val_Loss: 24.5838\n",
      "Processing batch 160/11884 - Val_Loss: 24.8420\n",
      "Processing batch 161/11884 - Val_Loss: 25.1274\n",
      "Processing batch 162/11884 - Val_Loss: 27.7711\n",
      "Processing batch 163/11884 - Val_Loss: 26.2152\n",
      "Processing batch 164/11884 - Val_Loss: 21.8337\n",
      "Processing batch 165/11884 - Val_Loss: 24.9179\n",
      "Processing batch 166/11884 - Val_Loss: 23.9607\n",
      "Processing batch 167/11884 - Val_Loss: 22.8239\n",
      "Processing batch 168/11884 - Val_Loss: 24.6248\n",
      "Processing batch 169/11884 - Val_Loss: 25.7992\n",
      "Processing batch 170/11884 - Val_Loss: 27.0521\n",
      "Processing batch 171/11884 - Val_Loss: 25.9428\n",
      "Processing batch 172/11884 - Val_Loss: 27.9853\n",
      "Processing batch 173/11884 - Val_Loss: 26.9117\n",
      "Processing batch 174/11884 - Val_Loss: 25.8062\n",
      "Processing batch 175/11884 - Val_Loss: 25.5259\n",
      "Processing batch 176/11884 - Val_Loss: 27.3567\n",
      "Processing batch 177/11884 - Val_Loss: 24.9826\n",
      "Processing batch 178/11884 - Val_Loss: 26.2593\n",
      "Processing batch 179/11884 - Val_Loss: 24.3855\n",
      "Processing batch 180/11884 - Val_Loss: 24.2175\n",
      "Processing batch 181/11884 - Val_Loss: 26.2051\n",
      "Processing batch 182/11884 - Val_Loss: 25.9936\n",
      "Processing batch 183/11884 - Val_Loss: 28.2391\n",
      "Processing batch 184/11884 - Val_Loss: 24.3741\n",
      "Processing batch 185/11884 - Val_Loss: 24.6359\n",
      "Processing batch 186/11884 - Val_Loss: 27.6055\n",
      "Processing batch 187/11884 - Val_Loss: 25.3776\n",
      "Processing batch 188/11884 - Val_Loss: 21.3216\n",
      "Processing batch 189/11884 - Val_Loss: 25.0229\n",
      "Processing batch 190/11884 - Val_Loss: 26.0438\n",
      "Processing batch 191/11884 - Val_Loss: 23.1264\n",
      "Processing batch 192/11884 - Val_Loss: 23.3362\n",
      "Processing batch 193/11884 - Val_Loss: 21.4037\n",
      "Processing batch 194/11884 - Val_Loss: 26.1813\n",
      "Processing batch 195/11884 - Val_Loss: 22.7997\n",
      "Processing batch 196/11884 - Val_Loss: 26.9505\n",
      "Processing batch 197/11884 - Val_Loss: 27.9415\n",
      "Processing batch 198/11884 - Val_Loss: 23.0119\n",
      "Processing batch 199/11884 - Val_Loss: 25.1866\n",
      "Processing batch 200/11884 - Val_Loss: 24.5416\n",
      "Processing batch 201/11884 - Val_Loss: 24.6731\n",
      "Processing batch 202/11884 - Val_Loss: 28.6053\n",
      "Processing batch 203/11884 - Val_Loss: 25.4150\n",
      "Processing batch 204/11884 - Val_Loss: 26.4156\n",
      "Processing batch 205/11884 - Val_Loss: 20.8206\n",
      "Processing batch 206/11884 - Val_Loss: 24.8962\n",
      "Processing batch 207/11884 - Val_Loss: 25.3093\n",
      "Processing batch 208/11884 - Val_Loss: 24.5248\n",
      "Processing batch 209/11884 - Val_Loss: 24.1541\n",
      "Processing batch 210/11884 - Val_Loss: 27.6143\n",
      "Processing batch 211/11884 - Val_Loss: 23.5671\n",
      "Processing batch 212/11884 - Val_Loss: 24.0128\n",
      "Processing batch 213/11884 - Val_Loss: 22.7840\n",
      "Processing batch 214/11884 - Val_Loss: 25.8207\n",
      "Processing batch 215/11884 - Val_Loss: 24.5136\n",
      "Processing batch 216/11884 - Val_Loss: 24.4438\n",
      "Processing batch 217/11884 - Val_Loss: 29.1597\n",
      "Processing batch 218/11884 - Val_Loss: 28.9675\n",
      "Processing batch 219/11884 - Val_Loss: 22.1157\n",
      "Processing batch 220/11884 - Val_Loss: 26.9864\n",
      "Processing batch 221/11884 - Val_Loss: 24.8544\n",
      "Processing batch 222/11884 - Val_Loss: 24.9701\n",
      "Processing batch 223/11884 - Val_Loss: 26.3312\n",
      "Processing batch 224/11884 - Val_Loss: 24.0181\n",
      "Processing batch 225/11884 - Val_Loss: 24.6868\n",
      "Processing batch 226/11884 - Val_Loss: 24.6632\n",
      "Processing batch 227/11884 - Val_Loss: 25.5161\n",
      "Processing batch 228/11884 - Val_Loss: 29.0406\n",
      "Processing batch 229/11884 - Val_Loss: 23.2245\n",
      "Processing batch 230/11884 - Val_Loss: 23.5055\n",
      "Processing batch 231/11884 - Val_Loss: 25.3897\n",
      "Processing batch 232/11884 - Val_Loss: 27.9884\n",
      "Processing batch 233/11884 - Val_Loss: 25.1274\n",
      "Processing batch 234/11884 - Val_Loss: 23.3017\n",
      "Processing batch 235/11884 - Val_Loss: 23.9011\n",
      "Processing batch 236/11884 - Val_Loss: 24.6760\n",
      "Processing batch 237/11884 - Val_Loss: 25.0652\n",
      "Processing batch 238/11884 - Val_Loss: 26.9301\n",
      "Processing batch 239/11884 - Val_Loss: 24.9043\n",
      "Processing batch 240/11884 - Val_Loss: 26.7472\n",
      "Processing batch 241/11884 - Val_Loss: 27.4782\n",
      "Processing batch 242/11884 - Val_Loss: 24.6938\n",
      "Processing batch 243/11884 - Val_Loss: 23.7569\n",
      "Processing batch 244/11884 - Val_Loss: 26.4241\n",
      "Processing batch 245/11884 - Val_Loss: 25.3093\n",
      "Processing batch 246/11884 - Val_Loss: 22.5297\n",
      "Processing batch 247/11884 - Val_Loss: 22.0522\n",
      "Processing batch 248/11884 - Val_Loss: 26.6444\n",
      "Processing batch 249/11884 - Val_Loss: 26.4097\n",
      "Processing batch 250/11884 - Val_Loss: 25.3749\n",
      "Processing batch 251/11884 - Val_Loss: 23.3049\n",
      "Processing batch 252/11884 - Val_Loss: 22.1374\n",
      "Processing batch 253/11884 - Val_Loss: 26.7969\n",
      "Processing batch 254/11884 - Val_Loss: 24.6482\n",
      "Processing batch 255/11884 - Val_Loss: 25.4537\n",
      "Processing batch 256/11884 - Val_Loss: 23.1971\n",
      "Processing batch 257/11884 - Val_Loss: 27.9453\n",
      "Processing batch 258/11884 - Val_Loss: 24.5342\n",
      "Processing batch 259/11884 - Val_Loss: 26.5175\n",
      "Processing batch 260/11884 - Val_Loss: 26.5291\n",
      "Processing batch 261/11884 - Val_Loss: 26.8423\n",
      "Processing batch 262/11884 - Val_Loss: 25.5369\n",
      "Processing batch 263/11884 - Val_Loss: 23.3069\n",
      "Processing batch 264/11884 - Val_Loss: 25.9307\n",
      "Processing batch 265/11884 - Val_Loss: 22.4394\n",
      "Processing batch 266/11884 - Val_Loss: 24.8825\n",
      "Processing batch 267/11884 - Val_Loss: 25.9837\n",
      "Processing batch 268/11884 - Val_Loss: 25.1974\n",
      "Processing batch 269/11884 - Val_Loss: 21.5580\n",
      "Processing batch 270/11884 - Val_Loss: 24.6101\n",
      "Processing batch 271/11884 - Val_Loss: 24.8264\n",
      "Processing batch 272/11884 - Val_Loss: 24.7541\n",
      "Processing batch 273/11884 - Val_Loss: 26.0085\n",
      "Processing batch 274/11884 - Val_Loss: 25.4126\n",
      "Processing batch 275/11884 - Val_Loss: 26.8298\n",
      "Processing batch 276/11884 - Val_Loss: 24.4849\n",
      "Processing batch 277/11884 - Val_Loss: 23.2196\n",
      "Processing batch 278/11884 - Val_Loss: 24.5277\n",
      "Processing batch 279/11884 - Val_Loss: 27.0727\n",
      "Processing batch 280/11884 - Val_Loss: 25.2644\n",
      "Processing batch 281/11884 - Val_Loss: 26.5852\n",
      "Processing batch 282/11884 - Val_Loss: 24.2312\n",
      "Processing batch 283/11884 - Val_Loss: 24.9534\n",
      "Processing batch 284/11884 - Val_Loss: 23.6688\n",
      "Processing batch 285/11884 - Val_Loss: 26.3283\n",
      "Processing batch 286/11884 - Val_Loss: 24.7677\n",
      "Processing batch 287/11884 - Val_Loss: 26.2804\n",
      "Processing batch 288/11884 - Val_Loss: 26.1531\n",
      "Processing batch 289/11884 - Val_Loss: 27.1566\n",
      "Processing batch 290/11884 - Val_Loss: 23.8975\n",
      "Processing batch 291/11884 - Val_Loss: 26.1494\n",
      "Processing batch 292/11884 - Val_Loss: 26.5455\n",
      "Processing batch 293/11884 - Val_Loss: 26.7091\n",
      "Processing batch 294/11884 - Val_Loss: 25.2761\n",
      "Processing batch 295/11884 - Val_Loss: 25.7955\n",
      "Processing batch 296/11884 - Val_Loss: 25.7725\n",
      "Processing batch 297/11884 - Val_Loss: 24.5583\n",
      "Processing batch 298/11884 - Val_Loss: 25.7982\n",
      "Processing batch 299/11884 - Val_Loss: 24.4383\n",
      "Processing batch 300/11884 - Val_Loss: 24.3803\n",
      "Processing batch 301/11884 - Val_Loss: 27.1160\n",
      "Processing batch 302/11884 - Val_Loss: 26.6380\n",
      "Processing batch 303/11884 - Val_Loss: 24.5322\n",
      "Processing batch 304/11884 - Val_Loss: 29.4260\n",
      "Processing batch 305/11884 - Val_Loss: 26.8096\n",
      "Processing batch 306/11884 - Val_Loss: 25.1229\n",
      "Processing batch 307/11884 - Val_Loss: 25.1563\n",
      "Processing batch 308/11884 - Val_Loss: 22.1803\n",
      "Processing batch 309/11884 - Val_Loss: 23.8422\n",
      "Processing batch 310/11884 - Val_Loss: 28.5603\n",
      "Processing batch 311/11884 - Val_Loss: 23.6646\n",
      "Processing batch 312/11884 - Val_Loss: 22.0238\n",
      "Processing batch 313/11884 - Val_Loss: 22.6493\n",
      "Processing batch 314/11884 - Val_Loss: 25.5154\n",
      "Processing batch 315/11884 - Val_Loss: 25.9375\n",
      "Processing batch 316/11884 - Val_Loss: 24.8374\n",
      "Processing batch 317/11884 - Val_Loss: 27.5125\n",
      "Processing batch 318/11884 - Val_Loss: 24.8956\n",
      "Processing batch 319/11884 - Val_Loss: 23.2772\n",
      "Processing batch 320/11884 - Val_Loss: 24.0860\n",
      "Processing batch 321/11884 - Val_Loss: 25.8767\n",
      "Processing batch 322/11884 - Val_Loss: 26.5838\n",
      "Processing batch 323/11884 - Val_Loss: 23.4903\n",
      "Processing batch 324/11884 - Val_Loss: 25.6438\n",
      "Processing batch 325/11884 - Val_Loss: 25.0727\n",
      "Processing batch 326/11884 - Val_Loss: 27.1329\n",
      "Processing batch 327/11884 - Val_Loss: 22.5507\n",
      "Processing batch 328/11884 - Val_Loss: 23.8646\n",
      "Processing batch 329/11884 - Val_Loss: 25.2153\n",
      "Processing batch 330/11884 - Val_Loss: 23.8784\n",
      "Processing batch 331/11884 - Val_Loss: 24.9379\n",
      "Processing batch 332/11884 - Val_Loss: 28.9447\n",
      "Processing batch 333/11884 - Val_Loss: 25.2447\n",
      "Processing batch 334/11884 - Val_Loss: 23.5392\n",
      "Processing batch 335/11884 - Val_Loss: 26.2780\n",
      "Processing batch 336/11884 - Val_Loss: 24.5361\n",
      "Processing batch 337/11884 - Val_Loss: 26.9245\n",
      "Processing batch 338/11884 - Val_Loss: 25.1848\n",
      "Processing batch 339/11884 - Val_Loss: 25.3384\n",
      "Processing batch 340/11884 - Val_Loss: 26.5568\n",
      "Processing batch 341/11884 - Val_Loss: 27.1622\n",
      "Processing batch 342/11884 - Val_Loss: 25.5900\n",
      "Processing batch 343/11884 - Val_Loss: 26.2166\n",
      "Processing batch 344/11884 - Val_Loss: 24.8932\n",
      "Processing batch 345/11884 - Val_Loss: 25.6461\n",
      "Processing batch 346/11884 - Val_Loss: 28.5170\n",
      "Processing batch 347/11884 - Val_Loss: 25.8335\n",
      "Processing batch 348/11884 - Val_Loss: 28.5661\n",
      "Processing batch 349/11884 - Val_Loss: 29.8216\n",
      "Processing batch 350/11884 - Val_Loss: 25.9772\n",
      "Processing batch 351/11884 - Val_Loss: 25.8441\n",
      "Processing batch 352/11884 - Val_Loss: 26.4681\n",
      "Processing batch 353/11884 - Val_Loss: 23.4357\n",
      "Processing batch 354/11884 - Val_Loss: 22.1787\n",
      "Processing batch 355/11884 - Val_Loss: 25.9982\n",
      "Processing batch 356/11884 - Val_Loss: 24.5831\n",
      "Processing batch 357/11884 - Val_Loss: 23.6943\n",
      "Processing batch 358/11884 - Val_Loss: 25.7482\n",
      "Processing batch 359/11884 - Val_Loss: 25.7429\n",
      "Processing batch 360/11884 - Val_Loss: 23.0191\n",
      "Processing batch 361/11884 - Val_Loss: 22.4588\n",
      "Processing batch 362/11884 - Val_Loss: 24.4203\n",
      "Processing batch 363/11884 - Val_Loss: 25.8776\n",
      "Processing batch 364/11884 - Val_Loss: 25.1618\n",
      "Processing batch 365/11884 - Val_Loss: 23.9904\n",
      "Processing batch 366/11884 - Val_Loss: 22.4388\n",
      "Processing batch 367/11884 - Val_Loss: 24.6158\n",
      "Processing batch 368/11884 - Val_Loss: 26.6063\n",
      "Processing batch 369/11884 - Val_Loss: 26.7296\n",
      "Processing batch 370/11884 - Val_Loss: 25.3451\n",
      "Processing batch 371/11884 - Val_Loss: 26.2177\n",
      "Processing batch 372/11884 - Val_Loss: 28.0426\n",
      "Processing batch 373/11884 - Val_Loss: 25.6894\n",
      "Processing batch 374/11884 - Val_Loss: 24.0148\n",
      "Processing batch 375/11884 - Val_Loss: 23.8303\n",
      "Processing batch 376/11884 - Val_Loss: 26.5988\n",
      "Processing batch 377/11884 - Val_Loss: 23.4332\n",
      "Processing batch 378/11884 - Val_Loss: 25.4356\n",
      "Processing batch 379/11884 - Val_Loss: 23.4383\n",
      "Processing batch 380/11884 - Val_Loss: 23.9991\n",
      "Processing batch 381/11884 - Val_Loss: 25.5017\n",
      "Processing batch 382/11884 - Val_Loss: 25.8169\n",
      "Processing batch 383/11884 - Val_Loss: 26.6594\n",
      "Processing batch 384/11884 - Val_Loss: 26.7749\n",
      "Processing batch 385/11884 - Val_Loss: 25.4156\n",
      "Processing batch 386/11884 - Val_Loss: 24.6702\n",
      "Processing batch 387/11884 - Val_Loss: 22.8025\n",
      "Processing batch 388/11884 - Val_Loss: 25.5470\n",
      "Processing batch 389/11884 - Val_Loss: 23.6078\n",
      "Processing batch 390/11884 - Val_Loss: 27.2025\n",
      "Processing batch 391/11884 - Val_Loss: 25.4259\n",
      "Processing batch 392/11884 - Val_Loss: 24.1026\n",
      "Processing batch 393/11884 - Val_Loss: 28.0831\n",
      "Processing batch 394/11884 - Val_Loss: 23.8484\n",
      "Processing batch 395/11884 - Val_Loss: 24.7823\n",
      "Processing batch 396/11884 - Val_Loss: 22.8518\n",
      "Processing batch 397/11884 - Val_Loss: 24.5458\n",
      "Processing batch 398/11884 - Val_Loss: 25.1840\n",
      "Processing batch 399/11884 - Val_Loss: 26.5067\n",
      "Processing batch 400/11884 - Val_Loss: 24.8596\n",
      "Processing batch 401/11884 - Val_Loss: 24.2439\n",
      "Processing batch 402/11884 - Val_Loss: 25.9618\n",
      "Processing batch 403/11884 - Val_Loss: 23.3434\n",
      "Processing batch 404/11884 - Val_Loss: 26.4597\n",
      "Processing batch 405/11884 - Val_Loss: 27.9006\n",
      "Processing batch 406/11884 - Val_Loss: 24.7792\n",
      "Processing batch 407/11884 - Val_Loss: 27.5395\n",
      "Processing batch 408/11884 - Val_Loss: 28.5312\n",
      "Processing batch 409/11884 - Val_Loss: 29.2690\n",
      "Processing batch 410/11884 - Val_Loss: 24.1678\n",
      "Processing batch 411/11884 - Val_Loss: 25.7590\n",
      "Processing batch 412/11884 - Val_Loss: 28.5219\n",
      "Processing batch 413/11884 - Val_Loss: 27.1851\n",
      "Processing batch 414/11884 - Val_Loss: 26.4842\n",
      "Processing batch 415/11884 - Val_Loss: 28.5115\n",
      "Processing batch 416/11884 - Val_Loss: 25.8994\n",
      "Processing batch 417/11884 - Val_Loss: 29.5662\n",
      "Processing batch 418/11884 - Val_Loss: 23.6745\n",
      "Processing batch 419/11884 - Val_Loss: 26.9924\n",
      "Processing batch 420/11884 - Val_Loss: 23.6861\n",
      "Processing batch 421/11884 - Val_Loss: 25.9382\n",
      "Processing batch 422/11884 - Val_Loss: 24.9472\n",
      "Processing batch 423/11884 - Val_Loss: 21.8189\n",
      "Processing batch 424/11884 - Val_Loss: 24.5157\n",
      "Processing batch 425/11884 - Val_Loss: 24.2109\n",
      "Processing batch 426/11884 - Val_Loss: 26.6981\n",
      "Processing batch 427/11884 - Val_Loss: 27.2084\n",
      "Processing batch 428/11884 - Val_Loss: 24.9441\n",
      "Processing batch 429/11884 - Val_Loss: 25.3630\n",
      "Processing batch 430/11884 - Val_Loss: 26.3249\n",
      "Processing batch 431/11884 - Val_Loss: 26.2808\n",
      "Processing batch 432/11884 - Val_Loss: 24.2347\n",
      "Processing batch 433/11884 - Val_Loss: 25.7817\n",
      "Processing batch 434/11884 - Val_Loss: 24.8188\n",
      "Processing batch 435/11884 - Val_Loss: 28.4802\n",
      "Processing batch 436/11884 - Val_Loss: 24.9682\n",
      "Processing batch 437/11884 - Val_Loss: 26.2887\n",
      "Processing batch 438/11884 - Val_Loss: 25.5594\n",
      "Processing batch 439/11884 - Val_Loss: 25.1057\n",
      "Processing batch 440/11884 - Val_Loss: 26.6264\n",
      "Processing batch 441/11884 - Val_Loss: 27.4766\n",
      "Processing batch 442/11884 - Val_Loss: 25.7690\n",
      "Processing batch 443/11884 - Val_Loss: 25.8428\n",
      "Processing batch 444/11884 - Val_Loss: 24.6058\n",
      "Processing batch 445/11884 - Val_Loss: 26.5375\n",
      "Processing batch 446/11884 - Val_Loss: 27.5193\n",
      "Processing batch 447/11884 - Val_Loss: 25.2020\n",
      "Processing batch 448/11884 - Val_Loss: 25.3380\n",
      "Processing batch 449/11884 - Val_Loss: 25.7263\n",
      "Processing batch 450/11884 - Val_Loss: 24.2760\n",
      "Processing batch 451/11884 - Val_Loss: 25.6620\n",
      "Processing batch 452/11884 - Val_Loss: 24.6125\n",
      "Processing batch 453/11884 - Val_Loss: 22.2477\n",
      "Processing batch 454/11884 - Val_Loss: 27.4105\n",
      "Processing batch 455/11884 - Val_Loss: 26.7322\n",
      "Processing batch 456/11884 - Val_Loss: 25.9340\n",
      "Processing batch 457/11884 - Val_Loss: 25.5726\n",
      "Processing batch 458/11884 - Val_Loss: 24.8245\n",
      "Processing batch 459/11884 - Val_Loss: 25.9800\n",
      "Processing batch 460/11884 - Val_Loss: 27.3116\n",
      "Processing batch 461/11884 - Val_Loss: 23.2579\n",
      "Processing batch 462/11884 - Val_Loss: 26.4022\n",
      "Processing batch 463/11884 - Val_Loss: 24.5967\n",
      "Processing batch 464/11884 - Val_Loss: 25.1048\n",
      "Processing batch 465/11884 - Val_Loss: 24.0235\n",
      "Processing batch 466/11884 - Val_Loss: 24.2930\n",
      "Processing batch 467/11884 - Val_Loss: 24.6596\n",
      "Processing batch 468/11884 - Val_Loss: 26.6860\n",
      "Processing batch 469/11884 - Val_Loss: 24.8569\n",
      "Processing batch 470/11884 - Val_Loss: 24.3996\n",
      "Processing batch 471/11884 - Val_Loss: 23.7121\n",
      "Processing batch 472/11884 - Val_Loss: 26.0766\n",
      "Processing batch 473/11884 - Val_Loss: 25.7225\n",
      "Processing batch 474/11884 - Val_Loss: 25.7912\n",
      "Processing batch 475/11884 - Val_Loss: 23.1994\n",
      "Processing batch 476/11884 - Val_Loss: 26.9557\n",
      "Processing batch 477/11884 - Val_Loss: 27.2971\n",
      "Processing batch 478/11884 - Val_Loss: 27.9246\n",
      "Processing batch 479/11884 - Val_Loss: 27.5080\n",
      "Processing batch 480/11884 - Val_Loss: 23.5872\n",
      "Processing batch 481/11884 - Val_Loss: 25.7152\n",
      "Processing batch 482/11884 - Val_Loss: 24.1124\n",
      "Processing batch 483/11884 - Val_Loss: 26.1123\n",
      "Processing batch 484/11884 - Val_Loss: 24.9768\n",
      "Processing batch 485/11884 - Val_Loss: 25.4464\n",
      "Processing batch 486/11884 - Val_Loss: 23.5728\n",
      "Processing batch 487/11884 - Val_Loss: 23.3117\n",
      "Processing batch 488/11884 - Val_Loss: 26.2125\n",
      "Processing batch 489/11884 - Val_Loss: 26.8948\n",
      "Processing batch 490/11884 - Val_Loss: 25.1856\n",
      "Processing batch 491/11884 - Val_Loss: 24.4580\n",
      "Processing batch 492/11884 - Val_Loss: 25.7410\n",
      "Processing batch 493/11884 - Val_Loss: 25.6670\n",
      "Processing batch 494/11884 - Val_Loss: 24.9783\n",
      "Processing batch 495/11884 - Val_Loss: 27.2344\n",
      "Processing batch 496/11884 - Val_Loss: 24.2069\n",
      "Processing batch 497/11884 - Val_Loss: 27.0559\n",
      "Processing batch 498/11884 - Val_Loss: 28.0915\n",
      "Processing batch 499/11884 - Val_Loss: 26.5925\n",
      "Processing batch 500/11884 - Val_Loss: 24.7791\n",
      "Processing batch 501/11884 - Val_Loss: 26.9620\n",
      "Processing batch 502/11884 - Val_Loss: 22.5239\n",
      "Processing batch 503/11884 - Val_Loss: 24.8718\n",
      "Processing batch 504/11884 - Val_Loss: 24.8685\n",
      "Processing batch 505/11884 - Val_Loss: 21.5596\n",
      "Processing batch 506/11884 - Val_Loss: 23.6458\n",
      "Processing batch 507/11884 - Val_Loss: 23.8295\n",
      "Processing batch 508/11884 - Val_Loss: 24.7305\n",
      "Processing batch 509/11884 - Val_Loss: 25.3392\n",
      "Processing batch 510/11884 - Val_Loss: 24.3121\n",
      "Processing batch 511/11884 - Val_Loss: 27.1084\n",
      "Processing batch 512/11884 - Val_Loss: 25.2359\n",
      "Processing batch 513/11884 - Val_Loss: 25.3248\n",
      "Processing batch 514/11884 - Val_Loss: 25.5456\n",
      "Processing batch 515/11884 - Val_Loss: 28.0194\n",
      "Processing batch 516/11884 - Val_Loss: 25.7114\n",
      "Processing batch 517/11884 - Val_Loss: 23.4696\n",
      "Processing batch 518/11884 - Val_Loss: 25.8036\n",
      "Processing batch 519/11884 - Val_Loss: 24.3955\n",
      "Processing batch 520/11884 - Val_Loss: 23.8614\n",
      "Processing batch 521/11884 - Val_Loss: 24.7843\n",
      "Processing batch 522/11884 - Val_Loss: 26.3931\n",
      "Processing batch 523/11884 - Val_Loss: 25.3686\n",
      "Processing batch 524/11884 - Val_Loss: 23.6810\n",
      "Processing batch 525/11884 - Val_Loss: 23.4727\n",
      "Processing batch 526/11884 - Val_Loss: 25.9904\n",
      "Processing batch 527/11884 - Val_Loss: 27.7259\n",
      "Processing batch 528/11884 - Val_Loss: 25.4156\n",
      "Processing batch 529/11884 - Val_Loss: 28.4748\n",
      "Processing batch 530/11884 - Val_Loss: 25.5039\n",
      "Processing batch 531/11884 - Val_Loss: 25.4712\n",
      "Processing batch 532/11884 - Val_Loss: 25.0018\n",
      "Processing batch 533/11884 - Val_Loss: 24.7161\n",
      "Processing batch 534/11884 - Val_Loss: 24.7191\n",
      "Processing batch 535/11884 - Val_Loss: 24.6098\n",
      "Processing batch 536/11884 - Val_Loss: 24.6457\n",
      "Processing batch 537/11884 - Val_Loss: 27.9419\n",
      "Processing batch 538/11884 - Val_Loss: 24.3470\n",
      "Processing batch 539/11884 - Val_Loss: 22.4600\n",
      "Processing batch 540/11884 - Val_Loss: 27.6920\n",
      "Processing batch 541/11884 - Val_Loss: 25.8856\n",
      "Processing batch 542/11884 - Val_Loss: 34.3746\n",
      "Processing batch 543/11884 - Val_Loss: 21.9130\n",
      "Processing batch 544/11884 - Val_Loss: 19.6599\n",
      "Processing batch 545/11884 - Val_Loss: 25.6625\n",
      "Processing batch 546/11884 - Val_Loss: 23.4224\n",
      "Processing batch 547/11884 - Val_Loss: 25.6113\n",
      "Processing batch 548/11884 - Val_Loss: 25.7830\n",
      "Processing batch 549/11884 - Val_Loss: 25.0902\n",
      "Processing batch 550/11884 - Val_Loss: 23.6569\n",
      "Processing batch 551/11884 - Val_Loss: 23.9519\n",
      "Processing batch 552/11884 - Val_Loss: 25.3359\n",
      "Processing batch 553/11884 - Val_Loss: 24.3234\n",
      "Processing batch 554/11884 - Val_Loss: 25.9880\n",
      "Processing batch 555/11884 - Val_Loss: 26.0109\n",
      "Processing batch 556/11884 - Val_Loss: 26.9105\n",
      "Processing batch 557/11884 - Val_Loss: 25.6210\n",
      "Processing batch 558/11884 - Val_Loss: 21.6736\n",
      "Processing batch 559/11884 - Val_Loss: 26.0962\n",
      "Processing batch 560/11884 - Val_Loss: 24.1189\n",
      "Processing batch 561/11884 - Val_Loss: 21.1234\n",
      "Processing batch 562/11884 - Val_Loss: 26.0690\n",
      "Processing batch 563/11884 - Val_Loss: 25.3367\n",
      "Processing batch 564/11884 - Val_Loss: 26.5308\n",
      "Processing batch 565/11884 - Val_Loss: 24.5162\n",
      "Processing batch 566/11884 - Val_Loss: 24.6776\n",
      "Processing batch 567/11884 - Val_Loss: 24.2192\n",
      "Processing batch 568/11884 - Val_Loss: 25.9968\n",
      "Processing batch 569/11884 - Val_Loss: 23.8461\n",
      "Processing batch 570/11884 - Val_Loss: 25.1766\n",
      "Processing batch 571/11884 - Val_Loss: 25.1687\n",
      "Processing batch 572/11884 - Val_Loss: 24.7345\n",
      "Processing batch 573/11884 - Val_Loss: 27.2071\n",
      "Processing batch 574/11884 - Val_Loss: 27.6472\n",
      "Processing batch 575/11884 - Val_Loss: 24.8156\n",
      "Processing batch 576/11884 - Val_Loss: 21.2158\n",
      "Processing batch 577/11884 - Val_Loss: 22.1845\n",
      "Processing batch 578/11884 - Val_Loss: 25.7218\n",
      "Processing batch 579/11884 - Val_Loss: 24.0394\n",
      "Processing batch 580/11884 - Val_Loss: 27.7632\n",
      "Processing batch 581/11884 - Val_Loss: 26.4739\n",
      "Processing batch 582/11884 - Val_Loss: 25.3438\n",
      "Processing batch 583/11884 - Val_Loss: 25.0987\n",
      "Processing batch 584/11884 - Val_Loss: 23.9837\n",
      "Processing batch 585/11884 - Val_Loss: 26.4718\n",
      "Processing batch 586/11884 - Val_Loss: 23.9285\n",
      "Processing batch 587/11884 - Val_Loss: 26.9685\n",
      "Processing batch 588/11884 - Val_Loss: 24.0743\n",
      "Processing batch 589/11884 - Val_Loss: 24.0860\n",
      "Processing batch 590/11884 - Val_Loss: 23.5925\n",
      "Processing batch 591/11884 - Val_Loss: 23.8744\n",
      "Processing batch 592/11884 - Val_Loss: 26.6445\n",
      "Processing batch 593/11884 - Val_Loss: 25.4033\n",
      "Processing batch 594/11884 - Val_Loss: 25.1799\n",
      "Processing batch 595/11884 - Val_Loss: 26.5933\n",
      "Processing batch 596/11884 - Val_Loss: 25.5736\n",
      "Processing batch 597/11884 - Val_Loss: 26.0959\n",
      "Processing batch 598/11884 - Val_Loss: 25.2643\n",
      "Processing batch 599/11884 - Val_Loss: 25.5815\n",
      "Processing batch 600/11884 - Val_Loss: 24.8557\n",
      "Processing batch 601/11884 - Val_Loss: 22.2218\n",
      "Processing batch 602/11884 - Val_Loss: 25.3256\n",
      "Processing batch 603/11884 - Val_Loss: 24.1752\n",
      "Processing batch 604/11884 - Val_Loss: 23.6619\n",
      "Processing batch 605/11884 - Val_Loss: 25.8580\n",
      "Processing batch 606/11884 - Val_Loss: 25.9753\n",
      "Processing batch 607/11884 - Val_Loss: 30.4261\n",
      "Processing batch 608/11884 - Val_Loss: 25.4991\n",
      "Processing batch 609/11884 - Val_Loss: 24.7569\n",
      "Processing batch 610/11884 - Val_Loss: 24.5566\n",
      "Processing batch 611/11884 - Val_Loss: 26.1355\n",
      "Processing batch 612/11884 - Val_Loss: 24.1001\n",
      "Processing batch 613/11884 - Val_Loss: 26.0564\n",
      "Processing batch 614/11884 - Val_Loss: 24.5180\n",
      "Processing batch 615/11884 - Val_Loss: 26.3797\n",
      "Processing batch 616/11884 - Val_Loss: 26.4603\n",
      "Processing batch 617/11884 - Val_Loss: 26.3349\n",
      "Processing batch 618/11884 - Val_Loss: 22.4337\n",
      "Processing batch 619/11884 - Val_Loss: 28.6392\n",
      "Processing batch 620/11884 - Val_Loss: 26.7591\n",
      "Processing batch 621/11884 - Val_Loss: 26.0234\n",
      "Processing batch 622/11884 - Val_Loss: 26.2893\n",
      "Processing batch 623/11884 - Val_Loss: 26.6496\n",
      "Processing batch 624/11884 - Val_Loss: 27.5662\n",
      "Processing batch 625/11884 - Val_Loss: 24.7713\n",
      "Processing batch 626/11884 - Val_Loss: 25.2400\n",
      "Processing batch 627/11884 - Val_Loss: 21.2944\n",
      "Processing batch 628/11884 - Val_Loss: 24.4682\n",
      "Processing batch 629/11884 - Val_Loss: 24.3515\n",
      "Processing batch 630/11884 - Val_Loss: 23.9568\n",
      "Processing batch 631/11884 - Val_Loss: 24.2676\n",
      "Processing batch 632/11884 - Val_Loss: 25.8028\n",
      "Processing batch 633/11884 - Val_Loss: 25.6653\n",
      "Processing batch 634/11884 - Val_Loss: 23.6032\n",
      "Processing batch 635/11884 - Val_Loss: 24.0092\n",
      "Processing batch 636/11884 - Val_Loss: 24.6153\n",
      "Processing batch 637/11884 - Val_Loss: 27.5575\n",
      "Processing batch 638/11884 - Val_Loss: 26.0348\n",
      "Processing batch 639/11884 - Val_Loss: 22.9125\n",
      "Processing batch 640/11884 - Val_Loss: 27.3113\n",
      "Processing batch 641/11884 - Val_Loss: 30.9810\n",
      "Processing batch 642/11884 - Val_Loss: 25.5867\n",
      "Processing batch 643/11884 - Val_Loss: 25.2810\n",
      "Processing batch 644/11884 - Val_Loss: 24.2167\n",
      "Processing batch 645/11884 - Val_Loss: 24.1015\n",
      "Processing batch 646/11884 - Val_Loss: 24.7112\n",
      "Processing batch 647/11884 - Val_Loss: 24.6084\n",
      "Processing batch 648/11884 - Val_Loss: 23.5217\n",
      "Processing batch 649/11884 - Val_Loss: 22.6134\n",
      "Processing batch 650/11884 - Val_Loss: 26.6863\n",
      "Processing batch 651/11884 - Val_Loss: 23.8783\n",
      "Processing batch 652/11884 - Val_Loss: 28.4199\n",
      "Processing batch 653/11884 - Val_Loss: 25.7181\n",
      "Processing batch 654/11884 - Val_Loss: 22.6427\n",
      "Processing batch 655/11884 - Val_Loss: 27.0094\n",
      "Processing batch 656/11884 - Val_Loss: 25.0650\n",
      "Processing batch 657/11884 - Val_Loss: 24.0123\n",
      "Processing batch 658/11884 - Val_Loss: 25.4615\n",
      "Processing batch 659/11884 - Val_Loss: 25.5060\n",
      "Processing batch 660/11884 - Val_Loss: 27.0000\n",
      "Processing batch 661/11884 - Val_Loss: 28.7656\n",
      "Processing batch 662/11884 - Val_Loss: 22.8596\n",
      "Processing batch 663/11884 - Val_Loss: 24.6724\n",
      "Processing batch 664/11884 - Val_Loss: 25.1005\n",
      "Processing batch 665/11884 - Val_Loss: 25.5194\n",
      "Processing batch 666/11884 - Val_Loss: 23.6158\n",
      "Processing batch 667/11884 - Val_Loss: 26.8574\n",
      "Processing batch 668/11884 - Val_Loss: 19.4861\n",
      "Processing batch 669/11884 - Val_Loss: 28.4490\n",
      "Processing batch 670/11884 - Val_Loss: 26.0616\n",
      "Processing batch 671/11884 - Val_Loss: 25.7844\n",
      "Processing batch 672/11884 - Val_Loss: 27.0281\n",
      "Processing batch 673/11884 - Val_Loss: 26.6557\n",
      "Processing batch 674/11884 - Val_Loss: 30.7262\n",
      "Processing batch 675/11884 - Val_Loss: 26.9452\n",
      "Processing batch 676/11884 - Val_Loss: 22.6866\n",
      "Processing batch 677/11884 - Val_Loss: 27.4180\n",
      "Processing batch 678/11884 - Val_Loss: 27.3003\n",
      "Processing batch 679/11884 - Val_Loss: 25.0771\n",
      "Processing batch 680/11884 - Val_Loss: 27.4049\n",
      "Processing batch 681/11884 - Val_Loss: 26.7159\n",
      "Processing batch 682/11884 - Val_Loss: 26.3525\n",
      "Processing batch 683/11884 - Val_Loss: 23.8431\n",
      "Processing batch 684/11884 - Val_Loss: 27.5118\n",
      "Processing batch 685/11884 - Val_Loss: 26.7562\n",
      "Processing batch 686/11884 - Val_Loss: 26.6013\n",
      "Processing batch 687/11884 - Val_Loss: 25.5601\n",
      "Processing batch 688/11884 - Val_Loss: 25.9770\n",
      "Processing batch 689/11884 - Val_Loss: 26.8038\n",
      "Processing batch 690/11884 - Val_Loss: 26.9110\n",
      "Processing batch 691/11884 - Val_Loss: 26.7319\n",
      "Processing batch 692/11884 - Val_Loss: 28.3811\n",
      "Processing batch 693/11884 - Val_Loss: 25.9057\n",
      "Processing batch 694/11884 - Val_Loss: 26.5862\n",
      "Processing batch 695/11884 - Val_Loss: 26.5957\n",
      "Processing batch 696/11884 - Val_Loss: 23.5891\n",
      "Processing batch 697/11884 - Val_Loss: 25.4838\n",
      "Processing batch 698/11884 - Val_Loss: 25.1831\n",
      "Processing batch 699/11884 - Val_Loss: 24.4504\n",
      "Processing batch 700/11884 - Val_Loss: 23.3426\n",
      "Processing batch 701/11884 - Val_Loss: 25.5080\n",
      "Processing batch 702/11884 - Val_Loss: 23.3636\n",
      "Processing batch 703/11884 - Val_Loss: 22.9739\n",
      "Processing batch 704/11884 - Val_Loss: 26.9963\n",
      "Processing batch 705/11884 - Val_Loss: 25.7661\n",
      "Processing batch 706/11884 - Val_Loss: 23.4051\n",
      "Processing batch 707/11884 - Val_Loss: 25.0134\n",
      "Processing batch 708/11884 - Val_Loss: 27.1720\n",
      "Processing batch 709/11884 - Val_Loss: 25.0560\n",
      "Processing batch 710/11884 - Val_Loss: 25.8494\n",
      "Processing batch 711/11884 - Val_Loss: 24.4889\n",
      "Processing batch 712/11884 - Val_Loss: 23.3572\n",
      "Processing batch 713/11884 - Val_Loss: 24.5979\n",
      "Processing batch 714/11884 - Val_Loss: 23.7183\n",
      "Processing batch 715/11884 - Val_Loss: 26.1730\n",
      "Processing batch 716/11884 - Val_Loss: 23.3389\n",
      "Processing batch 717/11884 - Val_Loss: 28.2845\n",
      "Processing batch 718/11884 - Val_Loss: 24.1410\n",
      "Processing batch 719/11884 - Val_Loss: 28.5139\n",
      "Processing batch 720/11884 - Val_Loss: 27.5595\n",
      "Processing batch 721/11884 - Val_Loss: 27.4127\n",
      "Processing batch 722/11884 - Val_Loss: 26.4798\n",
      "Processing batch 723/11884 - Val_Loss: 29.1482\n",
      "Processing batch 724/11884 - Val_Loss: 26.4414\n",
      "Processing batch 725/11884 - Val_Loss: 24.5604\n",
      "Processing batch 726/11884 - Val_Loss: 27.3791\n",
      "Processing batch 727/11884 - Val_Loss: 26.7014\n",
      "Processing batch 728/11884 - Val_Loss: 23.6594\n",
      "Processing batch 729/11884 - Val_Loss: 26.2111\n",
      "Processing batch 730/11884 - Val_Loss: 24.2491\n",
      "Processing batch 731/11884 - Val_Loss: 25.8973\n",
      "Processing batch 732/11884 - Val_Loss: 25.2566\n",
      "Processing batch 733/11884 - Val_Loss: 25.9066\n",
      "Processing batch 734/11884 - Val_Loss: 27.6565\n",
      "Processing batch 735/11884 - Val_Loss: 28.5547\n",
      "Processing batch 736/11884 - Val_Loss: 24.2394\n",
      "Processing batch 737/11884 - Val_Loss: 25.9115\n",
      "Processing batch 738/11884 - Val_Loss: 23.4182\n",
      "Processing batch 739/11884 - Val_Loss: 25.8075\n",
      "Processing batch 740/11884 - Val_Loss: 27.6741\n",
      "Processing batch 741/11884 - Val_Loss: 25.2975\n",
      "Processing batch 742/11884 - Val_Loss: 24.4092\n",
      "Processing batch 743/11884 - Val_Loss: 24.6600\n",
      "Processing batch 744/11884 - Val_Loss: 27.0551\n",
      "Processing batch 745/11884 - Val_Loss: 25.0432\n",
      "Processing batch 746/11884 - Val_Loss: 26.7578\n",
      "Processing batch 747/11884 - Val_Loss: 26.6208\n",
      "Processing batch 748/11884 - Val_Loss: 26.6941\n",
      "Processing batch 749/11884 - Val_Loss: 24.6671\n",
      "Processing batch 750/11884 - Val_Loss: 25.9840\n",
      "Processing batch 751/11884 - Val_Loss: 25.9593\n",
      "Processing batch 752/11884 - Val_Loss: 27.1856\n",
      "Processing batch 753/11884 - Val_Loss: 23.6587\n",
      "Processing batch 754/11884 - Val_Loss: 27.5160\n",
      "Processing batch 755/11884 - Val_Loss: 24.0665\n",
      "Processing batch 756/11884 - Val_Loss: 26.1339\n",
      "Processing batch 757/11884 - Val_Loss: 22.0839\n",
      "Processing batch 758/11884 - Val_Loss: 25.9473\n",
      "Processing batch 759/11884 - Val_Loss: 28.5571\n",
      "Processing batch 760/11884 - Val_Loss: 24.6007\n",
      "Processing batch 761/11884 - Val_Loss: 25.4753\n",
      "Processing batch 762/11884 - Val_Loss: 24.3380\n",
      "Processing batch 763/11884 - Val_Loss: 29.5758\n",
      "Processing batch 764/11884 - Val_Loss: 24.3215\n",
      "Processing batch 765/11884 - Val_Loss: 25.6102\n",
      "Processing batch 766/11884 - Val_Loss: 23.5517\n",
      "Processing batch 767/11884 - Val_Loss: 25.6946\n",
      "Processing batch 768/11884 - Val_Loss: 28.0986\n",
      "Processing batch 769/11884 - Val_Loss: 27.0345\n",
      "Processing batch 770/11884 - Val_Loss: 23.6574\n",
      "Processing batch 771/11884 - Val_Loss: 23.7717\n",
      "Processing batch 772/11884 - Val_Loss: 25.1368\n",
      "Processing batch 773/11884 - Val_Loss: 27.1609\n",
      "Processing batch 774/11884 - Val_Loss: 25.2248\n",
      "Processing batch 775/11884 - Val_Loss: 24.7447\n",
      "Processing batch 776/11884 - Val_Loss: 26.6806\n",
      "Processing batch 777/11884 - Val_Loss: 25.4560\n",
      "Processing batch 778/11884 - Val_Loss: 29.5028\n",
      "Processing batch 779/11884 - Val_Loss: 25.6501\n",
      "Processing batch 780/11884 - Val_Loss: 28.5553\n",
      "Processing batch 781/11884 - Val_Loss: 24.9773\n",
      "Processing batch 782/11884 - Val_Loss: 26.8153\n",
      "Processing batch 783/11884 - Val_Loss: 26.0295\n",
      "Processing batch 784/11884 - Val_Loss: 26.8840\n",
      "Processing batch 785/11884 - Val_Loss: 24.5028\n",
      "Processing batch 786/11884 - Val_Loss: 25.4245\n",
      "Processing batch 787/11884 - Val_Loss: 25.0196\n",
      "Processing batch 788/11884 - Val_Loss: 23.9734\n",
      "Processing batch 789/11884 - Val_Loss: 25.5690\n",
      "Processing batch 790/11884 - Val_Loss: 22.5699\n",
      "Processing batch 791/11884 - Val_Loss: 23.4281\n",
      "Processing batch 792/11884 - Val_Loss: 26.8791\n",
      "Processing batch 793/11884 - Val_Loss: 24.2357\n",
      "Processing batch 794/11884 - Val_Loss: 26.6808\n",
      "Processing batch 795/11884 - Val_Loss: 22.9406\n",
      "Processing batch 796/11884 - Val_Loss: 24.7012\n",
      "Processing batch 797/11884 - Val_Loss: 25.2081\n",
      "Processing batch 798/11884 - Val_Loss: 27.6934\n",
      "Processing batch 799/11884 - Val_Loss: 26.1406\n",
      "Processing batch 800/11884 - Val_Loss: 29.3045\n",
      "Processing batch 801/11884 - Val_Loss: 24.2849\n",
      "Processing batch 802/11884 - Val_Loss: 25.2175\n",
      "Processing batch 803/11884 - Val_Loss: 23.6339\n",
      "Processing batch 804/11884 - Val_Loss: 24.0748\n",
      "Processing batch 805/11884 - Val_Loss: 25.1297\n",
      "Processing batch 806/11884 - Val_Loss: 25.0971\n",
      "Processing batch 807/11884 - Val_Loss: 25.0141\n",
      "Processing batch 808/11884 - Val_Loss: 23.8217\n",
      "Processing batch 809/11884 - Val_Loss: 25.4072\n",
      "Processing batch 810/11884 - Val_Loss: 25.1562\n",
      "Processing batch 811/11884 - Val_Loss: 27.7051\n",
      "Processing batch 812/11884 - Val_Loss: 27.3765\n",
      "Processing batch 813/11884 - Val_Loss: 23.6709\n",
      "Processing batch 814/11884 - Val_Loss: 24.6731\n",
      "Processing batch 815/11884 - Val_Loss: 25.8949\n",
      "Processing batch 816/11884 - Val_Loss: 26.9502\n",
      "Processing batch 817/11884 - Val_Loss: 23.9071\n",
      "Processing batch 818/11884 - Val_Loss: 23.9858\n",
      "Processing batch 819/11884 - Val_Loss: 24.3861\n",
      "Processing batch 820/11884 - Val_Loss: 23.7737\n",
      "Processing batch 821/11884 - Val_Loss: 24.3357\n",
      "Processing batch 822/11884 - Val_Loss: 24.5014\n",
      "Processing batch 823/11884 - Val_Loss: 28.6898\n",
      "Processing batch 824/11884 - Val_Loss: 24.5545\n",
      "Processing batch 825/11884 - Val_Loss: 25.2885\n",
      "Processing batch 826/11884 - Val_Loss: 25.7460\n",
      "Processing batch 827/11884 - Val_Loss: 26.2153\n",
      "Processing batch 828/11884 - Val_Loss: 25.4340\n",
      "Processing batch 829/11884 - Val_Loss: 21.7714\n",
      "Processing batch 830/11884 - Val_Loss: 25.8543\n",
      "Processing batch 831/11884 - Val_Loss: 25.3918\n",
      "Processing batch 832/11884 - Val_Loss: 24.9362\n",
      "Processing batch 833/11884 - Val_Loss: 26.5289\n",
      "Processing batch 834/11884 - Val_Loss: 29.1021\n",
      "Processing batch 835/11884 - Val_Loss: 25.9543\n",
      "Processing batch 836/11884 - Val_Loss: 24.5491\n",
      "Processing batch 837/11884 - Val_Loss: 26.5441\n",
      "Processing batch 838/11884 - Val_Loss: 26.6274\n",
      "Processing batch 839/11884 - Val_Loss: 27.4851\n",
      "Processing batch 840/11884 - Val_Loss: 21.9514\n",
      "Processing batch 841/11884 - Val_Loss: 21.4405\n",
      "Processing batch 842/11884 - Val_Loss: 26.8899\n",
      "Processing batch 843/11884 - Val_Loss: 27.5876\n",
      "Processing batch 844/11884 - Val_Loss: 24.0563\n",
      "Processing batch 845/11884 - Val_Loss: 24.5275\n",
      "Processing batch 846/11884 - Val_Loss: 28.2635\n",
      "Processing batch 847/11884 - Val_Loss: 24.1328\n",
      "Processing batch 848/11884 - Val_Loss: 25.6650\n",
      "Processing batch 849/11884 - Val_Loss: 25.9419\n",
      "Processing batch 850/11884 - Val_Loss: 23.8012\n",
      "Processing batch 851/11884 - Val_Loss: 24.0241\n",
      "Processing batch 852/11884 - Val_Loss: 24.1627\n",
      "Processing batch 853/11884 - Val_Loss: 27.5432\n",
      "Processing batch 854/11884 - Val_Loss: 26.7671\n",
      "Processing batch 855/11884 - Val_Loss: 26.3469\n",
      "Processing batch 856/11884 - Val_Loss: 26.3525\n",
      "Processing batch 857/11884 - Val_Loss: 27.2934\n",
      "Processing batch 858/11884 - Val_Loss: 24.6220\n",
      "Processing batch 859/11884 - Val_Loss: 25.5525\n",
      "Processing batch 860/11884 - Val_Loss: 25.9747\n",
      "Processing batch 861/11884 - Val_Loss: 24.5972\n",
      "Processing batch 862/11884 - Val_Loss: 24.3019\n",
      "Processing batch 863/11884 - Val_Loss: 24.7297\n",
      "Processing batch 864/11884 - Val_Loss: 25.8545\n",
      "Processing batch 865/11884 - Val_Loss: 21.7383\n",
      "Processing batch 866/11884 - Val_Loss: 23.0988\n",
      "Processing batch 867/11884 - Val_Loss: 23.1670\n",
      "Processing batch 868/11884 - Val_Loss: 27.2468\n",
      "Processing batch 869/11884 - Val_Loss: 27.4503\n",
      "Processing batch 870/11884 - Val_Loss: 24.5912\n",
      "Processing batch 871/11884 - Val_Loss: 27.2073\n",
      "Processing batch 872/11884 - Val_Loss: 25.6893\n",
      "Processing batch 873/11884 - Val_Loss: 21.4892\n",
      "Processing batch 874/11884 - Val_Loss: 25.3193\n",
      "Processing batch 875/11884 - Val_Loss: 26.5287\n",
      "Processing batch 876/11884 - Val_Loss: 27.3488\n",
      "Processing batch 877/11884 - Val_Loss: 27.5754\n",
      "Processing batch 878/11884 - Val_Loss: 24.4905\n",
      "Processing batch 879/11884 - Val_Loss: 26.4841\n",
      "Processing batch 880/11884 - Val_Loss: 25.0018\n",
      "Processing batch 881/11884 - Val_Loss: 28.5670\n",
      "Processing batch 882/11884 - Val_Loss: 25.1538\n",
      "Processing batch 883/11884 - Val_Loss: 27.2719\n",
      "Processing batch 884/11884 - Val_Loss: 23.2041\n",
      "Processing batch 885/11884 - Val_Loss: 27.1235\n",
      "Processing batch 886/11884 - Val_Loss: 27.0747\n",
      "Processing batch 887/11884 - Val_Loss: 24.9538\n",
      "Processing batch 888/11884 - Val_Loss: 24.8977\n",
      "Processing batch 889/11884 - Val_Loss: 24.6710\n",
      "Processing batch 890/11884 - Val_Loss: 23.3706\n",
      "Processing batch 891/11884 - Val_Loss: 28.6841\n",
      "Processing batch 892/11884 - Val_Loss: 25.7781\n",
      "Processing batch 893/11884 - Val_Loss: 24.4474\n",
      "Processing batch 894/11884 - Val_Loss: 24.0636\n",
      "Processing batch 895/11884 - Val_Loss: 26.1125\n",
      "Processing batch 896/11884 - Val_Loss: 26.9781\n",
      "Processing batch 897/11884 - Val_Loss: 24.6295\n",
      "Processing batch 898/11884 - Val_Loss: 28.2145\n",
      "Processing batch 899/11884 - Val_Loss: 27.4397\n",
      "Processing batch 900/11884 - Val_Loss: 27.1306\n",
      "Processing batch 901/11884 - Val_Loss: 24.9196\n",
      "Processing batch 902/11884 - Val_Loss: 27.0921\n",
      "Processing batch 903/11884 - Val_Loss: 26.4967\n",
      "Processing batch 904/11884 - Val_Loss: 24.1048\n",
      "Processing batch 905/11884 - Val_Loss: 26.1332\n",
      "Processing batch 906/11884 - Val_Loss: 27.5483\n",
      "Processing batch 907/11884 - Val_Loss: 27.0537\n",
      "Processing batch 908/11884 - Val_Loss: 25.0559\n",
      "Processing batch 909/11884 - Val_Loss: 29.4755\n",
      "Processing batch 910/11884 - Val_Loss: 26.6597\n",
      "Processing batch 911/11884 - Val_Loss: 26.9489\n",
      "Processing batch 912/11884 - Val_Loss: 25.9887\n",
      "Processing batch 913/11884 - Val_Loss: 26.6463\n",
      "Processing batch 914/11884 - Val_Loss: 26.0144\n",
      "Processing batch 915/11884 - Val_Loss: 25.6293\n",
      "Processing batch 916/11884 - Val_Loss: 24.4407\n",
      "Processing batch 917/11884 - Val_Loss: 25.3140\n",
      "Processing batch 918/11884 - Val_Loss: 24.4728\n",
      "Processing batch 919/11884 - Val_Loss: 24.4976\n",
      "Processing batch 920/11884 - Val_Loss: 23.2012\n",
      "Processing batch 921/11884 - Val_Loss: 26.8806\n",
      "Processing batch 922/11884 - Val_Loss: 26.2666\n",
      "Processing batch 923/11884 - Val_Loss: 25.1704\n",
      "Processing batch 924/11884 - Val_Loss: 25.4369\n",
      "Processing batch 925/11884 - Val_Loss: 26.3582\n",
      "Processing batch 926/11884 - Val_Loss: 25.8464\n",
      "Processing batch 927/11884 - Val_Loss: 27.2889\n",
      "Processing batch 928/11884 - Val_Loss: 25.5160\n",
      "Processing batch 929/11884 - Val_Loss: 26.3380\n",
      "Processing batch 930/11884 - Val_Loss: 25.1931\n",
      "Processing batch 931/11884 - Val_Loss: 26.1985\n",
      "Processing batch 932/11884 - Val_Loss: 25.0733\n",
      "Processing batch 933/11884 - Val_Loss: 26.4254\n",
      "Processing batch 934/11884 - Val_Loss: 24.1445\n",
      "Processing batch 935/11884 - Val_Loss: 25.9119\n",
      "Processing batch 936/11884 - Val_Loss: 25.7707\n",
      "Processing batch 937/11884 - Val_Loss: 27.2150\n",
      "Processing batch 938/11884 - Val_Loss: 28.2037\n",
      "Processing batch 939/11884 - Val_Loss: 22.8225\n",
      "Processing batch 940/11884 - Val_Loss: 25.2383\n",
      "Processing batch 941/11884 - Val_Loss: 24.3690\n",
      "Processing batch 942/11884 - Val_Loss: 28.3077\n",
      "Processing batch 943/11884 - Val_Loss: 26.9628\n",
      "Processing batch 944/11884 - Val_Loss: 24.3668\n",
      "Processing batch 945/11884 - Val_Loss: 25.5459\n",
      "Processing batch 946/11884 - Val_Loss: 28.8157\n",
      "Processing batch 947/11884 - Val_Loss: 23.3856\n",
      "Processing batch 948/11884 - Val_Loss: 25.0099\n",
      "Processing batch 949/11884 - Val_Loss: 25.4811\n",
      "Processing batch 950/11884 - Val_Loss: 24.1178\n",
      "Processing batch 951/11884 - Val_Loss: 24.4357\n",
      "Processing batch 952/11884 - Val_Loss: 25.2519\n",
      "Processing batch 953/11884 - Val_Loss: 23.7019\n",
      "Processing batch 954/11884 - Val_Loss: 24.0371\n",
      "Processing batch 955/11884 - Val_Loss: 25.1623\n",
      "Processing batch 956/11884 - Val_Loss: 24.2010\n",
      "Processing batch 957/11884 - Val_Loss: 23.2182\n",
      "Processing batch 958/11884 - Val_Loss: 26.7362\n",
      "Processing batch 959/11884 - Val_Loss: 25.7004\n",
      "Processing batch 960/11884 - Val_Loss: 24.3193\n",
      "Processing batch 961/11884 - Val_Loss: 25.7425\n",
      "Processing batch 962/11884 - Val_Loss: 25.8846\n",
      "Processing batch 963/11884 - Val_Loss: 25.7496\n",
      "Processing batch 964/11884 - Val_Loss: 27.0023\n",
      "Processing batch 965/11884 - Val_Loss: 24.3307\n",
      "Processing batch 966/11884 - Val_Loss: 27.2704\n",
      "Processing batch 967/11884 - Val_Loss: 27.3597\n",
      "Processing batch 968/11884 - Val_Loss: 24.1457\n",
      "Processing batch 969/11884 - Val_Loss: 26.9818\n",
      "Processing batch 970/11884 - Val_Loss: 26.3795\n",
      "Processing batch 971/11884 - Val_Loss: 22.5424\n",
      "Processing batch 972/11884 - Val_Loss: 26.2989\n",
      "Processing batch 973/11884 - Val_Loss: 23.1749\n",
      "Processing batch 974/11884 - Val_Loss: 27.0825\n",
      "Processing batch 975/11884 - Val_Loss: 26.5236\n",
      "Processing batch 976/11884 - Val_Loss: 27.5347\n",
      "Processing batch 977/11884 - Val_Loss: 26.8503\n",
      "Processing batch 978/11884 - Val_Loss: 26.8359\n",
      "Processing batch 979/11884 - Val_Loss: 22.0690\n",
      "Processing batch 980/11884 - Val_Loss: 23.7897\n",
      "Processing batch 981/11884 - Val_Loss: 23.2955\n",
      "Processing batch 982/11884 - Val_Loss: 23.5881\n",
      "Processing batch 983/11884 - Val_Loss: 22.1998\n",
      "Processing batch 984/11884 - Val_Loss: 24.0414\n",
      "Processing batch 985/11884 - Val_Loss: 27.5393\n",
      "Processing batch 986/11884 - Val_Loss: 24.1628\n",
      "Processing batch 987/11884 - Val_Loss: 24.7709\n",
      "Processing batch 988/11884 - Val_Loss: 25.5261\n",
      "Processing batch 989/11884 - Val_Loss: 22.2324\n",
      "Processing batch 990/11884 - Val_Loss: 26.6725\n",
      "Processing batch 991/11884 - Val_Loss: 26.2347\n",
      "Processing batch 992/11884 - Val_Loss: 25.3265\n",
      "Processing batch 993/11884 - Val_Loss: 23.4581\n",
      "Processing batch 994/11884 - Val_Loss: 22.1164\n",
      "Processing batch 995/11884 - Val_Loss: 24.1382\n",
      "Processing batch 996/11884 - Val_Loss: 24.9125\n",
      "Processing batch 997/11884 - Val_Loss: 24.4110\n",
      "Processing batch 998/11884 - Val_Loss: 25.5748\n",
      "Processing batch 999/11884 - Val_Loss: 25.6300\n",
      "Processing batch 1000/11884 - Val_Loss: 25.5379\n",
      "Processing batch 1001/11884 - Val_Loss: 26.0317\n",
      "Processing batch 1002/11884 - Val_Loss: 25.7727\n",
      "Processing batch 1003/11884 - Val_Loss: 24.5453\n",
      "Processing batch 1004/11884 - Val_Loss: 26.0310\n",
      "Processing batch 1005/11884 - Val_Loss: 28.2828\n",
      "Processing batch 1006/11884 - Val_Loss: 25.4059\n",
      "Processing batch 1007/11884 - Val_Loss: 24.7841\n",
      "Processing batch 1008/11884 - Val_Loss: 24.7295\n",
      "Processing batch 1009/11884 - Val_Loss: 23.3178\n",
      "Processing batch 1010/11884 - Val_Loss: 26.0338\n",
      "Processing batch 1011/11884 - Val_Loss: 23.9109\n",
      "Processing batch 1012/11884 - Val_Loss: 26.1409\n",
      "Processing batch 1013/11884 - Val_Loss: 25.3605\n",
      "Processing batch 1014/11884 - Val_Loss: 24.8855\n",
      "Processing batch 1015/11884 - Val_Loss: 23.7843\n",
      "Processing batch 1016/11884 - Val_Loss: 27.3367\n",
      "Processing batch 1017/11884 - Val_Loss: 26.5689\n",
      "Processing batch 1018/11884 - Val_Loss: 25.2969\n",
      "Processing batch 1019/11884 - Val_Loss: 25.6268\n",
      "Processing batch 1020/11884 - Val_Loss: 24.6324\n",
      "Processing batch 1021/11884 - Val_Loss: 24.8357\n",
      "Processing batch 1022/11884 - Val_Loss: 26.5624\n",
      "Processing batch 1023/11884 - Val_Loss: 28.5429\n",
      "Processing batch 1024/11884 - Val_Loss: 24.4268\n",
      "Processing batch 1025/11884 - Val_Loss: 25.1043\n",
      "Processing batch 1026/11884 - Val_Loss: 24.5599\n",
      "Processing batch 1027/11884 - Val_Loss: 23.8871\n",
      "Processing batch 1028/11884 - Val_Loss: 25.2733\n",
      "Processing batch 1029/11884 - Val_Loss: 27.2170\n",
      "Processing batch 1030/11884 - Val_Loss: 27.7485\n",
      "Processing batch 1031/11884 - Val_Loss: 25.6367\n",
      "Processing batch 1032/11884 - Val_Loss: 26.4257\n",
      "Processing batch 1033/11884 - Val_Loss: 26.0649\n",
      "Processing batch 1034/11884 - Val_Loss: 24.6520\n",
      "Processing batch 1035/11884 - Val_Loss: 24.2668\n",
      "Processing batch 1036/11884 - Val_Loss: 25.8209\n",
      "Processing batch 1037/11884 - Val_Loss: 26.4098\n",
      "Processing batch 1038/11884 - Val_Loss: 26.5241\n",
      "Processing batch 1039/11884 - Val_Loss: 24.3491\n",
      "Processing batch 1040/11884 - Val_Loss: 25.2754\n",
      "Processing batch 1041/11884 - Val_Loss: 23.7059\n",
      "Processing batch 1042/11884 - Val_Loss: 26.7749\n",
      "Processing batch 1043/11884 - Val_Loss: 25.8024\n",
      "Processing batch 1044/11884 - Val_Loss: 24.9390\n",
      "Processing batch 1045/11884 - Val_Loss: 27.5845\n",
      "Processing batch 1046/11884 - Val_Loss: 26.7217\n",
      "Processing batch 1047/11884 - Val_Loss: 25.5675\n",
      "Processing batch 1048/11884 - Val_Loss: 25.6543\n",
      "Processing batch 1049/11884 - Val_Loss: 28.3986\n",
      "Processing batch 1050/11884 - Val_Loss: 24.8299\n",
      "Processing batch 1051/11884 - Val_Loss: 27.3474\n",
      "Processing batch 1052/11884 - Val_Loss: 26.3928\n",
      "Processing batch 1053/11884 - Val_Loss: 25.5898\n",
      "Processing batch 1054/11884 - Val_Loss: 25.5339\n",
      "Processing batch 1055/11884 - Val_Loss: 24.3184\n",
      "Processing batch 1056/11884 - Val_Loss: 26.9034\n",
      "Processing batch 1057/11884 - Val_Loss: 24.4198\n",
      "Processing batch 1058/11884 - Val_Loss: 24.3644\n",
      "Processing batch 1059/11884 - Val_Loss: 27.2742\n",
      "Processing batch 1060/11884 - Val_Loss: 23.3829\n",
      "Processing batch 1061/11884 - Val_Loss: 24.7450\n",
      "Processing batch 1062/11884 - Val_Loss: 28.6890\n",
      "Processing batch 1063/11884 - Val_Loss: 23.5895\n",
      "Processing batch 1064/11884 - Val_Loss: 25.8709\n",
      "Processing batch 1065/11884 - Val_Loss: 24.3597\n",
      "Processing batch 1066/11884 - Val_Loss: 28.3512\n",
      "Processing batch 1067/11884 - Val_Loss: 23.9672\n",
      "Processing batch 1068/11884 - Val_Loss: 26.8373\n",
      "Processing batch 1069/11884 - Val_Loss: 24.1141\n",
      "Processing batch 1070/11884 - Val_Loss: 25.5581\n",
      "Processing batch 1071/11884 - Val_Loss: 22.9156\n",
      "Processing batch 1072/11884 - Val_Loss: 26.5195\n",
      "Processing batch 1073/11884 - Val_Loss: 22.0948\n",
      "Processing batch 1074/11884 - Val_Loss: 24.9401\n",
      "Processing batch 1075/11884 - Val_Loss: 24.4598\n",
      "Processing batch 1076/11884 - Val_Loss: 25.9705\n",
      "Processing batch 1077/11884 - Val_Loss: 25.3835\n",
      "Processing batch 1078/11884 - Val_Loss: 28.5904\n",
      "Processing batch 1079/11884 - Val_Loss: 25.7101\n",
      "Processing batch 1080/11884 - Val_Loss: 26.7455\n",
      "Processing batch 1081/11884 - Val_Loss: 26.4182\n",
      "Processing batch 1082/11884 - Val_Loss: 24.1081\n",
      "Processing batch 1083/11884 - Val_Loss: 24.3608\n",
      "Processing batch 1084/11884 - Val_Loss: 29.0086\n",
      "Processing batch 1085/11884 - Val_Loss: 22.7042\n",
      "Processing batch 1086/11884 - Val_Loss: 21.2517\n",
      "Processing batch 1087/11884 - Val_Loss: 23.5422\n",
      "Processing batch 1088/11884 - Val_Loss: 24.4926\n",
      "Processing batch 1089/11884 - Val_Loss: 26.0017\n",
      "Processing batch 1090/11884 - Val_Loss: 26.8933\n",
      "Processing batch 1091/11884 - Val_Loss: 28.7745\n",
      "Processing batch 1092/11884 - Val_Loss: 30.0769\n",
      "Processing batch 1093/11884 - Val_Loss: 23.9218\n",
      "Processing batch 1094/11884 - Val_Loss: 25.6032\n",
      "Processing batch 1095/11884 - Val_Loss: 27.4492\n",
      "Processing batch 1096/11884 - Val_Loss: 24.4300\n",
      "Processing batch 1097/11884 - Val_Loss: 26.9623\n",
      "Processing batch 1098/11884 - Val_Loss: 28.1210\n",
      "Processing batch 1099/11884 - Val_Loss: 24.9679\n",
      "Processing batch 1100/11884 - Val_Loss: 21.9401\n",
      "Processing batch 1101/11884 - Val_Loss: 25.9307\n",
      "Processing batch 1102/11884 - Val_Loss: 28.1319\n",
      "Processing batch 1103/11884 - Val_Loss: 25.2031\n",
      "Processing batch 1104/11884 - Val_Loss: 25.4453\n",
      "Processing batch 1105/11884 - Val_Loss: 30.5331\n",
      "Processing batch 1106/11884 - Val_Loss: 26.0205\n",
      "Processing batch 1107/11884 - Val_Loss: 25.5070\n",
      "Processing batch 1108/11884 - Val_Loss: 25.0876\n",
      "Processing batch 1109/11884 - Val_Loss: 25.3653\n",
      "Processing batch 1110/11884 - Val_Loss: 25.8568\n",
      "Processing batch 1111/11884 - Val_Loss: 25.4419\n",
      "Processing batch 1112/11884 - Val_Loss: 26.4907\n",
      "Processing batch 1113/11884 - Val_Loss: 23.4223\n",
      "Processing batch 1114/11884 - Val_Loss: 27.1604\n",
      "Processing batch 1115/11884 - Val_Loss: 25.8632\n",
      "Processing batch 1116/11884 - Val_Loss: 25.6381\n",
      "Processing batch 1117/11884 - Val_Loss: 25.0459\n",
      "Processing batch 1118/11884 - Val_Loss: 26.3622\n",
      "Processing batch 1119/11884 - Val_Loss: 23.3765\n",
      "Processing batch 1120/11884 - Val_Loss: 25.8012\n",
      "Processing batch 1121/11884 - Val_Loss: 22.6664\n",
      "Processing batch 1122/11884 - Val_Loss: 25.2874\n",
      "Processing batch 1123/11884 - Val_Loss: 26.0677\n",
      "Processing batch 1124/11884 - Val_Loss: 27.7883\n",
      "Processing batch 1125/11884 - Val_Loss: 26.3802\n",
      "Processing batch 1126/11884 - Val_Loss: 24.9796\n",
      "Processing batch 1127/11884 - Val_Loss: 25.3067\n",
      "Processing batch 1128/11884 - Val_Loss: 25.7451\n",
      "Processing batch 1129/11884 - Val_Loss: 24.9012\n",
      "Processing batch 1130/11884 - Val_Loss: 24.4791\n",
      "Processing batch 1131/11884 - Val_Loss: 26.6985\n",
      "Processing batch 1132/11884 - Val_Loss: 25.1507\n",
      "Processing batch 1133/11884 - Val_Loss: 26.0268\n",
      "Processing batch 1134/11884 - Val_Loss: 25.3923\n",
      "Processing batch 1135/11884 - Val_Loss: 21.7465\n",
      "Processing batch 1136/11884 - Val_Loss: 24.5522\n",
      "Processing batch 1137/11884 - Val_Loss: 24.8228\n",
      "Processing batch 1138/11884 - Val_Loss: 23.4909\n",
      "Processing batch 1139/11884 - Val_Loss: 25.5104\n",
      "Processing batch 1140/11884 - Val_Loss: 26.0142\n",
      "Processing batch 1141/11884 - Val_Loss: 27.6477\n",
      "Processing batch 1142/11884 - Val_Loss: 23.8479\n",
      "Processing batch 1143/11884 - Val_Loss: 24.7564\n",
      "Processing batch 1144/11884 - Val_Loss: 23.7826\n",
      "Processing batch 1145/11884 - Val_Loss: 25.5360\n",
      "Processing batch 1146/11884 - Val_Loss: 27.8431\n",
      "Processing batch 1147/11884 - Val_Loss: 31.1846\n",
      "Processing batch 1148/11884 - Val_Loss: 28.2427\n",
      "Processing batch 1149/11884 - Val_Loss: 27.1247\n",
      "Processing batch 1150/11884 - Val_Loss: 24.5170\n",
      "Processing batch 1151/11884 - Val_Loss: 24.2709\n",
      "Processing batch 1152/11884 - Val_Loss: 26.1970\n",
      "Processing batch 1153/11884 - Val_Loss: 25.7333\n",
      "Processing batch 1154/11884 - Val_Loss: 29.2115\n",
      "Processing batch 1155/11884 - Val_Loss: 27.9522\n",
      "Processing batch 1156/11884 - Val_Loss: 24.8636\n",
      "Processing batch 1157/11884 - Val_Loss: 24.5395\n",
      "Processing batch 1158/11884 - Val_Loss: 28.4480\n",
      "Processing batch 1159/11884 - Val_Loss: 23.0676\n",
      "Processing batch 1160/11884 - Val_Loss: 24.8972\n",
      "Processing batch 1161/11884 - Val_Loss: 26.7652\n",
      "Processing batch 1162/11884 - Val_Loss: 27.7299\n",
      "Processing batch 1163/11884 - Val_Loss: 24.2461\n",
      "Processing batch 1164/11884 - Val_Loss: 28.2172\n",
      "Processing batch 1165/11884 - Val_Loss: 26.5491\n",
      "Processing batch 1166/11884 - Val_Loss: 28.3262\n",
      "Processing batch 1167/11884 - Val_Loss: 23.7590\n",
      "Processing batch 1168/11884 - Val_Loss: 26.7639\n",
      "Processing batch 1169/11884 - Val_Loss: 25.1848\n",
      "Processing batch 1170/11884 - Val_Loss: 23.1016\n",
      "Processing batch 1171/11884 - Val_Loss: 24.5034\n",
      "Processing batch 1172/11884 - Val_Loss: 25.1744\n",
      "Processing batch 1173/11884 - Val_Loss: 25.3525\n",
      "Processing batch 1174/11884 - Val_Loss: 27.3314\n",
      "Processing batch 1175/11884 - Val_Loss: 24.1317\n",
      "Processing batch 1176/11884 - Val_Loss: 24.6132\n",
      "Processing batch 1177/11884 - Val_Loss: 24.0546\n",
      "Processing batch 1178/11884 - Val_Loss: 25.2387\n",
      "Processing batch 1179/11884 - Val_Loss: 26.2342\n",
      "Processing batch 1180/11884 - Val_Loss: 28.7517\n",
      "Processing batch 1181/11884 - Val_Loss: 26.5522\n",
      "Processing batch 1182/11884 - Val_Loss: 25.0702\n",
      "Processing batch 1183/11884 - Val_Loss: 23.9788\n",
      "Processing batch 1184/11884 - Val_Loss: 23.3913\n",
      "Processing batch 1185/11884 - Val_Loss: 26.4201\n",
      "Processing batch 1186/11884 - Val_Loss: 24.8805\n",
      "Processing batch 1187/11884 - Val_Loss: 28.5596\n",
      "Processing batch 1188/11884 - Val_Loss: 23.1962\n",
      "Processing batch 1189/11884 - Val_Loss: 24.1553\n",
      "Processing batch 1190/11884 - Val_Loss: 24.9794\n",
      "Processing batch 1191/11884 - Val_Loss: 24.8476\n",
      "Processing batch 1192/11884 - Val_Loss: 29.4080\n",
      "Processing batch 1193/11884 - Val_Loss: 27.0494\n",
      "Processing batch 1194/11884 - Val_Loss: 24.5319\n",
      "Processing batch 1195/11884 - Val_Loss: 23.1275\n",
      "Processing batch 1196/11884 - Val_Loss: 29.1572\n",
      "Processing batch 1197/11884 - Val_Loss: 29.0839\n",
      "Processing batch 1198/11884 - Val_Loss: 24.8576\n",
      "Processing batch 1199/11884 - Val_Loss: 24.2386\n",
      "Processing batch 1200/11884 - Val_Loss: 24.7998\n",
      "Processing batch 1201/11884 - Val_Loss: 24.8901\n",
      "Processing batch 1202/11884 - Val_Loss: 23.2212\n",
      "Processing batch 1203/11884 - Val_Loss: 26.9655\n",
      "Processing batch 1204/11884 - Val_Loss: 23.2824\n",
      "Processing batch 1205/11884 - Val_Loss: 25.3873\n",
      "Processing batch 1206/11884 - Val_Loss: 25.8683\n",
      "Processing batch 1207/11884 - Val_Loss: 24.1441\n",
      "Processing batch 1208/11884 - Val_Loss: 27.0471\n",
      "Processing batch 1209/11884 - Val_Loss: 26.1917\n",
      "Processing batch 1210/11884 - Val_Loss: 27.0673\n",
      "Processing batch 1211/11884 - Val_Loss: 22.8679\n",
      "Processing batch 1212/11884 - Val_Loss: 28.8363\n",
      "Processing batch 1213/11884 - Val_Loss: 28.4051\n",
      "Processing batch 1214/11884 - Val_Loss: 27.7152\n",
      "Processing batch 1215/11884 - Val_Loss: 25.8978\n",
      "Processing batch 1216/11884 - Val_Loss: 29.7051\n",
      "Processing batch 1217/11884 - Val_Loss: 26.3491\n",
      "Processing batch 1218/11884 - Val_Loss: 25.0682\n",
      "Processing batch 1219/11884 - Val_Loss: 25.6830\n",
      "Processing batch 1220/11884 - Val_Loss: 31.3142\n",
      "Processing batch 1221/11884 - Val_Loss: 23.2526\n",
      "Processing batch 1222/11884 - Val_Loss: 26.3099\n",
      "Processing batch 1223/11884 - Val_Loss: 24.0164\n",
      "Processing batch 1224/11884 - Val_Loss: 28.9814\n",
      "Processing batch 1225/11884 - Val_Loss: 23.5160\n",
      "Processing batch 1226/11884 - Val_Loss: 25.4336\n",
      "Processing batch 1227/11884 - Val_Loss: 26.6211\n",
      "Processing batch 1228/11884 - Val_Loss: 21.5294\n",
      "Processing batch 1229/11884 - Val_Loss: 24.7554\n",
      "Processing batch 1230/11884 - Val_Loss: 23.4850\n",
      "Processing batch 1231/11884 - Val_Loss: 22.5290\n",
      "Processing batch 1232/11884 - Val_Loss: 24.4826\n",
      "Processing batch 1233/11884 - Val_Loss: 25.0527\n",
      "Processing batch 1234/11884 - Val_Loss: 26.0017\n",
      "Processing batch 1235/11884 - Val_Loss: 27.8563\n",
      "Processing batch 1236/11884 - Val_Loss: 23.7858\n",
      "Processing batch 1237/11884 - Val_Loss: 26.1650\n",
      "Processing batch 1238/11884 - Val_Loss: 25.6945\n",
      "Processing batch 1239/11884 - Val_Loss: 26.0597\n",
      "Processing batch 1240/11884 - Val_Loss: 23.8926\n",
      "Processing batch 1241/11884 - Val_Loss: 26.7578\n",
      "Processing batch 1242/11884 - Val_Loss: 23.6415\n",
      "Processing batch 1243/11884 - Val_Loss: 27.5220\n",
      "Processing batch 1244/11884 - Val_Loss: 24.7462\n",
      "Processing batch 1245/11884 - Val_Loss: 24.5765\n",
      "Processing batch 1246/11884 - Val_Loss: 22.6833\n",
      "Processing batch 1247/11884 - Val_Loss: 26.6966\n",
      "Processing batch 1248/11884 - Val_Loss: 25.4302\n",
      "Processing batch 1249/11884 - Val_Loss: 25.2301\n",
      "Processing batch 1250/11884 - Val_Loss: 23.9440\n",
      "Processing batch 1251/11884 - Val_Loss: 24.6161\n",
      "Processing batch 1252/11884 - Val_Loss: 23.8322\n",
      "Processing batch 1253/11884 - Val_Loss: 24.6453\n",
      "Processing batch 1254/11884 - Val_Loss: 24.8410\n",
      "Processing batch 1255/11884 - Val_Loss: 24.1629\n",
      "Processing batch 1256/11884 - Val_Loss: 23.1859\n",
      "Processing batch 1257/11884 - Val_Loss: 26.3593\n",
      "Processing batch 1258/11884 - Val_Loss: 30.2130\n",
      "Processing batch 1259/11884 - Val_Loss: 24.4735\n",
      "Processing batch 1260/11884 - Val_Loss: 29.1751\n",
      "Processing batch 1261/11884 - Val_Loss: 26.3987\n",
      "Processing batch 1262/11884 - Val_Loss: 25.2628\n",
      "Processing batch 1263/11884 - Val_Loss: 25.2629\n",
      "Processing batch 1264/11884 - Val_Loss: 28.0758\n",
      "Processing batch 1265/11884 - Val_Loss: 25.0302\n",
      "Processing batch 1266/11884 - Val_Loss: 24.7537\n",
      "Processing batch 1267/11884 - Val_Loss: 28.1699\n",
      "Processing batch 1268/11884 - Val_Loss: 26.0911\n",
      "Processing batch 1269/11884 - Val_Loss: 22.9215\n",
      "Processing batch 1270/11884 - Val_Loss: 25.2860\n",
      "Processing batch 1271/11884 - Val_Loss: 23.2197\n",
      "Processing batch 1272/11884 - Val_Loss: 25.1563\n",
      "Processing batch 1273/11884 - Val_Loss: 26.6825\n",
      "Processing batch 1274/11884 - Val_Loss: 29.7300\n",
      "Processing batch 1275/11884 - Val_Loss: 22.6314\n",
      "Processing batch 1276/11884 - Val_Loss: 26.7630\n",
      "Processing batch 1277/11884 - Val_Loss: 25.7944\n",
      "Processing batch 1278/11884 - Val_Loss: 27.8914\n",
      "Processing batch 1279/11884 - Val_Loss: 24.9785\n",
      "Processing batch 1280/11884 - Val_Loss: 23.0218\n",
      "Processing batch 1281/11884 - Val_Loss: 26.3835\n",
      "Processing batch 1282/11884 - Val_Loss: 24.4235\n",
      "Processing batch 1283/11884 - Val_Loss: 26.6635\n",
      "Processing batch 1284/11884 - Val_Loss: 25.9922\n",
      "Processing batch 1285/11884 - Val_Loss: 26.3275\n",
      "Processing batch 1286/11884 - Val_Loss: 26.4959\n",
      "Processing batch 1287/11884 - Val_Loss: 26.9172\n",
      "Processing batch 1288/11884 - Val_Loss: 28.6396\n",
      "Processing batch 1289/11884 - Val_Loss: 24.1463\n",
      "Processing batch 1290/11884 - Val_Loss: 25.6949\n",
      "Processing batch 1291/11884 - Val_Loss: 24.2952\n",
      "Processing batch 1292/11884 - Val_Loss: 25.4458\n",
      "Processing batch 1293/11884 - Val_Loss: 23.4334\n",
      "Processing batch 1294/11884 - Val_Loss: 22.3578\n",
      "Processing batch 1295/11884 - Val_Loss: 22.8945\n",
      "Processing batch 1296/11884 - Val_Loss: 27.9006\n",
      "Processing batch 1297/11884 - Val_Loss: 22.7432\n",
      "Processing batch 1298/11884 - Val_Loss: 25.6843\n",
      "Processing batch 1299/11884 - Val_Loss: 23.9622\n",
      "Processing batch 1300/11884 - Val_Loss: 23.6998\n",
      "Processing batch 1301/11884 - Val_Loss: 23.2192\n",
      "Processing batch 1302/11884 - Val_Loss: 23.1413\n",
      "Processing batch 1303/11884 - Val_Loss: 25.3188\n",
      "Processing batch 1304/11884 - Val_Loss: 26.7569\n",
      "Processing batch 1305/11884 - Val_Loss: 24.6502\n",
      "Processing batch 1306/11884 - Val_Loss: 28.0820\n",
      "Processing batch 1307/11884 - Val_Loss: 24.1596\n",
      "Processing batch 1308/11884 - Val_Loss: 26.2203\n",
      "Processing batch 1309/11884 - Val_Loss: 23.9112\n",
      "Processing batch 1310/11884 - Val_Loss: 22.2764\n",
      "Processing batch 1311/11884 - Val_Loss: 26.5658\n",
      "Processing batch 1312/11884 - Val_Loss: 26.7166\n",
      "Processing batch 1313/11884 - Val_Loss: 26.8439\n",
      "Processing batch 1314/11884 - Val_Loss: 26.6033\n",
      "Processing batch 1315/11884 - Val_Loss: 24.0770\n",
      "Processing batch 1316/11884 - Val_Loss: 31.5558\n",
      "Processing batch 1317/11884 - Val_Loss: 26.0265\n",
      "Processing batch 1318/11884 - Val_Loss: 22.7768\n",
      "Processing batch 1319/11884 - Val_Loss: 27.7601\n",
      "Processing batch 1320/11884 - Val_Loss: 25.7702\n",
      "Processing batch 1321/11884 - Val_Loss: 29.5308\n",
      "Processing batch 1322/11884 - Val_Loss: 25.2873\n",
      "Processing batch 1323/11884 - Val_Loss: 24.6873\n",
      "Processing batch 1324/11884 - Val_Loss: 25.1023\n",
      "Processing batch 1325/11884 - Val_Loss: 27.1237\n",
      "Processing batch 1326/11884 - Val_Loss: 25.5534\n",
      "Processing batch 1327/11884 - Val_Loss: 31.5985\n",
      "Processing batch 1328/11884 - Val_Loss: 27.1509\n",
      "Processing batch 1329/11884 - Val_Loss: 25.5757\n",
      "Processing batch 1330/11884 - Val_Loss: 22.1987\n",
      "Processing batch 1331/11884 - Val_Loss: 22.4537\n",
      "Processing batch 1332/11884 - Val_Loss: 25.4374\n",
      "Processing batch 1333/11884 - Val_Loss: 24.3487\n",
      "Processing batch 1334/11884 - Val_Loss: 25.3638\n",
      "Processing batch 1335/11884 - Val_Loss: 23.7490\n",
      "Processing batch 1336/11884 - Val_Loss: 25.4134\n",
      "Processing batch 1337/11884 - Val_Loss: 24.0690\n",
      "Processing batch 1338/11884 - Val_Loss: 25.5840\n",
      "Processing batch 1339/11884 - Val_Loss: 26.4505\n",
      "Processing batch 1340/11884 - Val_Loss: 28.0093\n",
      "Processing batch 1341/11884 - Val_Loss: 23.4981\n",
      "Processing batch 1342/11884 - Val_Loss: 22.0253\n",
      "Processing batch 1343/11884 - Val_Loss: 23.2349\n",
      "Processing batch 1344/11884 - Val_Loss: 30.5713\n",
      "Processing batch 1345/11884 - Val_Loss: 26.7537\n",
      "Processing batch 1346/11884 - Val_Loss: 26.4361\n",
      "Processing batch 1347/11884 - Val_Loss: 25.3666\n",
      "Processing batch 1348/11884 - Val_Loss: 25.4530\n",
      "Processing batch 1349/11884 - Val_Loss: 25.6942\n",
      "Processing batch 1350/11884 - Val_Loss: 24.5691\n",
      "Processing batch 1351/11884 - Val_Loss: 25.4501\n",
      "Processing batch 1352/11884 - Val_Loss: 24.6527\n",
      "Processing batch 1353/11884 - Val_Loss: 25.1697\n",
      "Processing batch 1354/11884 - Val_Loss: 26.5663\n",
      "Processing batch 1355/11884 - Val_Loss: 29.1797\n",
      "Processing batch 1356/11884 - Val_Loss: 26.2228\n",
      "Processing batch 1357/11884 - Val_Loss: 24.3397\n",
      "Processing batch 1358/11884 - Val_Loss: 22.4712\n",
      "Processing batch 1359/11884 - Val_Loss: 27.3010\n",
      "Processing batch 1360/11884 - Val_Loss: 26.1497\n",
      "Processing batch 1361/11884 - Val_Loss: 24.5695\n",
      "Processing batch 1362/11884 - Val_Loss: 25.2460\n",
      "Processing batch 1363/11884 - Val_Loss: 24.1853\n",
      "Processing batch 1364/11884 - Val_Loss: 24.2977\n",
      "Processing batch 1365/11884 - Val_Loss: 26.0564\n",
      "Processing batch 1366/11884 - Val_Loss: 27.2730\n",
      "Processing batch 1367/11884 - Val_Loss: 24.4232\n",
      "Processing batch 1368/11884 - Val_Loss: 25.7829\n",
      "Processing batch 1369/11884 - Val_Loss: 22.8004\n",
      "Processing batch 1370/11884 - Val_Loss: 24.4128\n",
      "Processing batch 1371/11884 - Val_Loss: 27.2015\n",
      "Processing batch 1372/11884 - Val_Loss: 28.7519\n",
      "Processing batch 1373/11884 - Val_Loss: 24.5894\n",
      "Processing batch 1374/11884 - Val_Loss: 27.8573\n",
      "Processing batch 1375/11884 - Val_Loss: 27.8478\n",
      "Processing batch 1376/11884 - Val_Loss: 26.6110\n",
      "Processing batch 1377/11884 - Val_Loss: 26.5310\n",
      "Processing batch 1378/11884 - Val_Loss: 24.2758\n",
      "Processing batch 1379/11884 - Val_Loss: 23.6110\n",
      "Processing batch 1380/11884 - Val_Loss: 26.3886\n",
      "Processing batch 1381/11884 - Val_Loss: 24.8040\n",
      "Processing batch 1382/11884 - Val_Loss: 28.9283\n",
      "Processing batch 1383/11884 - Val_Loss: 23.1443\n",
      "Processing batch 1384/11884 - Val_Loss: 25.5394\n",
      "Processing batch 1385/11884 - Val_Loss: 21.7474\n",
      "Processing batch 1386/11884 - Val_Loss: 25.2468\n",
      "Processing batch 1387/11884 - Val_Loss: 28.6119\n",
      "Processing batch 1388/11884 - Val_Loss: 27.9938\n",
      "Processing batch 1389/11884 - Val_Loss: 27.6035\n",
      "Processing batch 1390/11884 - Val_Loss: 24.8702\n",
      "Processing batch 1391/11884 - Val_Loss: 25.6098\n",
      "Processing batch 1392/11884 - Val_Loss: 25.3902\n",
      "Processing batch 1393/11884 - Val_Loss: 25.1603\n",
      "Processing batch 1394/11884 - Val_Loss: 24.1285\n",
      "Processing batch 1395/11884 - Val_Loss: 26.9508\n",
      "Processing batch 1396/11884 - Val_Loss: 26.3628\n",
      "Processing batch 1397/11884 - Val_Loss: 26.9528\n",
      "Processing batch 1398/11884 - Val_Loss: 26.7009\n",
      "Processing batch 1399/11884 - Val_Loss: 27.0317\n",
      "Processing batch 1400/11884 - Val_Loss: 23.3411\n",
      "Processing batch 1401/11884 - Val_Loss: 28.2402\n",
      "Processing batch 1402/11884 - Val_Loss: 23.0726\n",
      "Processing batch 1403/11884 - Val_Loss: 26.1954\n",
      "Processing batch 1404/11884 - Val_Loss: 23.0175\n",
      "Processing batch 1405/11884 - Val_Loss: 29.0031\n",
      "Processing batch 1406/11884 - Val_Loss: 25.6644\n",
      "Processing batch 1407/11884 - Val_Loss: 24.3018\n",
      "Processing batch 1408/11884 - Val_Loss: 26.0662\n",
      "Processing batch 1409/11884 - Val_Loss: 24.2178\n",
      "Processing batch 1410/11884 - Val_Loss: 25.7925\n",
      "Processing batch 1411/11884 - Val_Loss: 27.2658\n",
      "Processing batch 1412/11884 - Val_Loss: 24.1162\n",
      "Processing batch 1413/11884 - Val_Loss: 26.3657\n",
      "Processing batch 1414/11884 - Val_Loss: 25.7746\n",
      "Processing batch 1415/11884 - Val_Loss: 24.1362\n",
      "Processing batch 1416/11884 - Val_Loss: 25.6841\n",
      "Processing batch 1417/11884 - Val_Loss: 23.4232\n",
      "Processing batch 1418/11884 - Val_Loss: 25.1292\n",
      "Processing batch 1419/11884 - Val_Loss: 27.3739\n",
      "Processing batch 1420/11884 - Val_Loss: 24.2254\n",
      "Processing batch 1421/11884 - Val_Loss: 26.4967\n",
      "Processing batch 1422/11884 - Val_Loss: 22.8592\n",
      "Processing batch 1423/11884 - Val_Loss: 24.7696\n",
      "Processing batch 1424/11884 - Val_Loss: 27.4880\n",
      "Processing batch 1425/11884 - Val_Loss: 28.2646\n",
      "Processing batch 1426/11884 - Val_Loss: 28.5875\n",
      "Processing batch 1427/11884 - Val_Loss: 24.6911\n",
      "Processing batch 1428/11884 - Val_Loss: 25.3292\n",
      "Processing batch 1429/11884 - Val_Loss: 25.2726\n",
      "Processing batch 1430/11884 - Val_Loss: 24.8571\n",
      "Processing batch 1431/11884 - Val_Loss: 27.6757\n",
      "Processing batch 1432/11884 - Val_Loss: 28.0966\n",
      "Processing batch 1433/11884 - Val_Loss: 26.1280\n",
      "Processing batch 1434/11884 - Val_Loss: 23.7106\n",
      "Processing batch 1435/11884 - Val_Loss: 23.9060\n",
      "Processing batch 1436/11884 - Val_Loss: 24.0764\n",
      "Processing batch 1437/11884 - Val_Loss: 25.5075\n",
      "Processing batch 1438/11884 - Val_Loss: 24.0753\n",
      "Processing batch 1439/11884 - Val_Loss: 28.7296\n",
      "Processing batch 1440/11884 - Val_Loss: 27.0808\n",
      "Processing batch 1441/11884 - Val_Loss: 25.8356\n",
      "Processing batch 1442/11884 - Val_Loss: 26.0920\n",
      "Processing batch 1443/11884 - Val_Loss: 24.8327\n",
      "Processing batch 1444/11884 - Val_Loss: 27.6651\n",
      "Processing batch 1445/11884 - Val_Loss: 27.9616\n",
      "Processing batch 1446/11884 - Val_Loss: 27.4049\n",
      "Processing batch 1447/11884 - Val_Loss: 27.2933\n",
      "Processing batch 1448/11884 - Val_Loss: 24.5547\n",
      "Processing batch 1449/11884 - Val_Loss: 28.2193\n",
      "Processing batch 1450/11884 - Val_Loss: 25.1719\n",
      "Processing batch 1451/11884 - Val_Loss: 24.0137\n",
      "Processing batch 1452/11884 - Val_Loss: 23.8922\n",
      "Processing batch 1453/11884 - Val_Loss: 25.5178\n",
      "Processing batch 1454/11884 - Val_Loss: 26.6629\n",
      "Processing batch 1455/11884 - Val_Loss: 24.5254\n",
      "Processing batch 1456/11884 - Val_Loss: 23.7168\n",
      "Processing batch 1457/11884 - Val_Loss: 25.0547\n",
      "Processing batch 1458/11884 - Val_Loss: 28.6417\n",
      "Processing batch 1459/11884 - Val_Loss: 25.1930\n",
      "Processing batch 1460/11884 - Val_Loss: 26.7060\n",
      "Processing batch 1461/11884 - Val_Loss: 25.8875\n",
      "Processing batch 1462/11884 - Val_Loss: 25.2508\n",
      "Processing batch 1463/11884 - Val_Loss: 23.9339\n",
      "Processing batch 1464/11884 - Val_Loss: 25.9316\n",
      "Processing batch 1465/11884 - Val_Loss: 27.5273\n",
      "Processing batch 1466/11884 - Val_Loss: 25.0717\n",
      "Processing batch 1467/11884 - Val_Loss: 24.6964\n",
      "Processing batch 1468/11884 - Val_Loss: 26.8587\n",
      "Processing batch 1469/11884 - Val_Loss: 27.8685\n",
      "Processing batch 1470/11884 - Val_Loss: 25.7441\n",
      "Processing batch 1471/11884 - Val_Loss: 24.7827\n",
      "Processing batch 1472/11884 - Val_Loss: 25.8815\n",
      "Processing batch 1473/11884 - Val_Loss: 24.7298\n",
      "Processing batch 1474/11884 - Val_Loss: 25.2533\n",
      "Processing batch 1475/11884 - Val_Loss: 25.7411\n",
      "Processing batch 1476/11884 - Val_Loss: 22.7965\n",
      "Processing batch 1477/11884 - Val_Loss: 24.0639\n",
      "Processing batch 1478/11884 - Val_Loss: 24.3823\n",
      "Processing batch 1479/11884 - Val_Loss: 25.6533\n",
      "Processing batch 1480/11884 - Val_Loss: 24.2652\n",
      "Processing batch 1481/11884 - Val_Loss: 24.1282\n",
      "Processing batch 1482/11884 - Val_Loss: 20.3408\n",
      "Processing batch 1483/11884 - Val_Loss: 26.7724\n",
      "Processing batch 1484/11884 - Val_Loss: 25.9489\n",
      "Processing batch 1485/11884 - Val_Loss: 23.6919\n",
      "Processing batch 1486/11884 - Val_Loss: 22.5927\n",
      "Processing batch 1487/11884 - Val_Loss: 24.9072\n",
      "Processing batch 1488/11884 - Val_Loss: 25.3068\n",
      "Processing batch 1489/11884 - Val_Loss: 25.5041\n",
      "Processing batch 1490/11884 - Val_Loss: 28.1330\n",
      "Processing batch 1491/11884 - Val_Loss: 26.6513\n",
      "Processing batch 1492/11884 - Val_Loss: 26.1694\n",
      "Processing batch 1493/11884 - Val_Loss: 27.2759\n",
      "Processing batch 1494/11884 - Val_Loss: 27.5174\n",
      "Processing batch 1495/11884 - Val_Loss: 25.1753\n",
      "Processing batch 1496/11884 - Val_Loss: 24.8654\n",
      "Processing batch 1497/11884 - Val_Loss: 25.3945\n",
      "Processing batch 1498/11884 - Val_Loss: 27.2359\n",
      "Processing batch 1499/11884 - Val_Loss: 26.6593\n",
      "Processing batch 1500/11884 - Val_Loss: 27.9271\n",
      "Processing batch 1501/11884 - Val_Loss: 23.6972\n",
      "Processing batch 1502/11884 - Val_Loss: 25.3273\n",
      "Processing batch 1503/11884 - Val_Loss: 23.6356\n",
      "Processing batch 1504/11884 - Val_Loss: 25.8698\n",
      "Processing batch 1505/11884 - Val_Loss: 25.1130\n",
      "Processing batch 1506/11884 - Val_Loss: 25.8439\n",
      "Processing batch 1507/11884 - Val_Loss: 25.5679\n",
      "Processing batch 1508/11884 - Val_Loss: 25.2111\n",
      "Processing batch 1509/11884 - Val_Loss: 26.6649\n",
      "Processing batch 1510/11884 - Val_Loss: 23.4351\n",
      "Processing batch 1511/11884 - Val_Loss: 30.5016\n",
      "Processing batch 1512/11884 - Val_Loss: 25.8668\n",
      "Processing batch 1513/11884 - Val_Loss: 26.8077\n",
      "Processing batch 1514/11884 - Val_Loss: 26.3895\n",
      "Processing batch 1515/11884 - Val_Loss: 24.1918\n",
      "Processing batch 1516/11884 - Val_Loss: 23.7833\n",
      "Processing batch 1517/11884 - Val_Loss: 25.8397\n",
      "Processing batch 1518/11884 - Val_Loss: 24.3412\n",
      "Processing batch 1519/11884 - Val_Loss: 24.2455\n",
      "Processing batch 1520/11884 - Val_Loss: 26.6725\n",
      "Processing batch 1521/11884 - Val_Loss: 24.3739\n",
      "Processing batch 1522/11884 - Val_Loss: 24.8426\n",
      "Processing batch 1523/11884 - Val_Loss: 23.6234\n",
      "Processing batch 1524/11884 - Val_Loss: 25.2001\n",
      "Processing batch 1525/11884 - Val_Loss: 25.2984\n",
      "Processing batch 1526/11884 - Val_Loss: 25.8889\n",
      "Processing batch 1527/11884 - Val_Loss: 25.8132\n",
      "Processing batch 1528/11884 - Val_Loss: 26.3445\n",
      "Processing batch 1529/11884 - Val_Loss: 26.9141\n",
      "Processing batch 1530/11884 - Val_Loss: 25.0828\n",
      "Processing batch 1531/11884 - Val_Loss: 24.3548\n",
      "Processing batch 1532/11884 - Val_Loss: 25.2283\n",
      "Processing batch 1533/11884 - Val_Loss: 24.9552\n",
      "Processing batch 1534/11884 - Val_Loss: 26.1344\n",
      "Processing batch 1535/11884 - Val_Loss: 24.7315\n",
      "Processing batch 1536/11884 - Val_Loss: 26.6492\n",
      "Processing batch 1537/11884 - Val_Loss: 24.0408\n",
      "Processing batch 1538/11884 - Val_Loss: 27.8727\n",
      "Processing batch 1539/11884 - Val_Loss: 26.1784\n",
      "Processing batch 1540/11884 - Val_Loss: 26.9784\n",
      "Processing batch 1541/11884 - Val_Loss: 23.9937\n",
      "Processing batch 1542/11884 - Val_Loss: 23.5383\n",
      "Processing batch 1543/11884 - Val_Loss: 24.3846\n",
      "Processing batch 1544/11884 - Val_Loss: 26.3601\n",
      "Processing batch 1545/11884 - Val_Loss: 26.8924\n",
      "Processing batch 1546/11884 - Val_Loss: 23.8526\n",
      "Processing batch 1547/11884 - Val_Loss: 28.3905\n",
      "Processing batch 1548/11884 - Val_Loss: 22.2261\n",
      "Processing batch 1549/11884 - Val_Loss: 26.0003\n",
      "Processing batch 1550/11884 - Val_Loss: 25.2817\n",
      "Processing batch 1551/11884 - Val_Loss: 26.5298\n",
      "Processing batch 1552/11884 - Val_Loss: 23.9823\n",
      "Processing batch 1553/11884 - Val_Loss: 24.1381\n",
      "Processing batch 1554/11884 - Val_Loss: 25.2013\n",
      "Processing batch 1555/11884 - Val_Loss: 25.7017\n",
      "Processing batch 1556/11884 - Val_Loss: 24.4907\n",
      "Processing batch 1557/11884 - Val_Loss: 27.1199\n",
      "Processing batch 1558/11884 - Val_Loss: 25.6949\n",
      "Processing batch 1559/11884 - Val_Loss: 23.8510\n",
      "Processing batch 1560/11884 - Val_Loss: 26.3645\n",
      "Processing batch 1561/11884 - Val_Loss: 22.6171\n",
      "Processing batch 1562/11884 - Val_Loss: 23.2403\n",
      "Processing batch 1563/11884 - Val_Loss: 27.5473\n",
      "Processing batch 1564/11884 - Val_Loss: 24.4661\n",
      "Processing batch 1565/11884 - Val_Loss: 24.9379\n",
      "Processing batch 1566/11884 - Val_Loss: 26.5264\n",
      "Processing batch 1567/11884 - Val_Loss: 24.5983\n",
      "Processing batch 1568/11884 - Val_Loss: 26.8269\n",
      "Processing batch 1569/11884 - Val_Loss: 24.6534\n",
      "Processing batch 1570/11884 - Val_Loss: 27.1736\n",
      "Processing batch 1571/11884 - Val_Loss: 25.2265\n",
      "Processing batch 1572/11884 - Val_Loss: 28.7534\n",
      "Processing batch 1573/11884 - Val_Loss: 26.8195\n",
      "Processing batch 1574/11884 - Val_Loss: 25.3685\n",
      "Processing batch 1575/11884 - Val_Loss: 26.1294\n",
      "Processing batch 1576/11884 - Val_Loss: 26.7270\n",
      "Processing batch 1577/11884 - Val_Loss: 24.6122\n",
      "Processing batch 1578/11884 - Val_Loss: 25.3417\n",
      "Processing batch 1579/11884 - Val_Loss: 23.9529\n",
      "Processing batch 1580/11884 - Val_Loss: 25.7955\n",
      "Processing batch 1581/11884 - Val_Loss: 24.6959\n",
      "Processing batch 1582/11884 - Val_Loss: 26.5973\n",
      "Processing batch 1583/11884 - Val_Loss: 25.0289\n",
      "Processing batch 1584/11884 - Val_Loss: 28.6414\n",
      "Processing batch 1585/11884 - Val_Loss: 26.3380\n",
      "Processing batch 1586/11884 - Val_Loss: 26.1136\n",
      "Processing batch 1587/11884 - Val_Loss: 25.3657\n",
      "Processing batch 1588/11884 - Val_Loss: 26.0717\n",
      "Processing batch 1589/11884 - Val_Loss: 24.4871\n",
      "Processing batch 1590/11884 - Val_Loss: 23.3158\n",
      "Processing batch 1591/11884 - Val_Loss: 24.8916\n",
      "Processing batch 1592/11884 - Val_Loss: 24.9200\n",
      "Processing batch 1593/11884 - Val_Loss: 24.9021\n",
      "Processing batch 1594/11884 - Val_Loss: 25.0698\n",
      "Processing batch 1595/11884 - Val_Loss: 24.7158\n",
      "Processing batch 1596/11884 - Val_Loss: 25.4234\n",
      "Processing batch 1597/11884 - Val_Loss: 25.9546\n",
      "Processing batch 1598/11884 - Val_Loss: 25.9163\n",
      "Processing batch 1599/11884 - Val_Loss: 26.1452\n",
      "Processing batch 1600/11884 - Val_Loss: 25.9029\n",
      "Processing batch 1601/11884 - Val_Loss: 26.4363\n",
      "Processing batch 1602/11884 - Val_Loss: 26.1270\n",
      "Processing batch 1603/11884 - Val_Loss: 25.0357\n",
      "Processing batch 1604/11884 - Val_Loss: 23.8609\n",
      "Processing batch 1605/11884 - Val_Loss: 24.2848\n",
      "Processing batch 1606/11884 - Val_Loss: 24.1775\n",
      "Processing batch 1607/11884 - Val_Loss: 26.8951\n",
      "Processing batch 1608/11884 - Val_Loss: 26.0865\n",
      "Processing batch 1609/11884 - Val_Loss: 28.2131\n",
      "Processing batch 1610/11884 - Val_Loss: 24.0415\n",
      "Processing batch 1611/11884 - Val_Loss: 27.6197\n",
      "Processing batch 1612/11884 - Val_Loss: 25.3459\n",
      "Processing batch 1613/11884 - Val_Loss: 23.6292\n",
      "Processing batch 1614/11884 - Val_Loss: 28.4349\n",
      "Processing batch 1615/11884 - Val_Loss: 25.8366\n",
      "Processing batch 1616/11884 - Val_Loss: 26.2100\n",
      "Processing batch 1617/11884 - Val_Loss: 24.5958\n",
      "Processing batch 1618/11884 - Val_Loss: 25.2620\n",
      "Processing batch 1619/11884 - Val_Loss: 25.5302\n",
      "Processing batch 1620/11884 - Val_Loss: 26.0831\n",
      "Processing batch 1621/11884 - Val_Loss: 28.7531\n",
      "Processing batch 1622/11884 - Val_Loss: 22.8433\n",
      "Processing batch 1623/11884 - Val_Loss: 27.5089\n",
      "Processing batch 1624/11884 - Val_Loss: 24.2330\n",
      "Processing batch 1625/11884 - Val_Loss: 25.4634\n",
      "Processing batch 1626/11884 - Val_Loss: 28.6636\n",
      "Processing batch 1627/11884 - Val_Loss: 27.3207\n",
      "Processing batch 1628/11884 - Val_Loss: 24.4306\n",
      "Processing batch 1629/11884 - Val_Loss: 25.9245\n",
      "Processing batch 1630/11884 - Val_Loss: 25.6271\n",
      "Processing batch 1631/11884 - Val_Loss: 25.6274\n",
      "Processing batch 1632/11884 - Val_Loss: 29.9149\n",
      "Processing batch 1633/11884 - Val_Loss: 22.8803\n",
      "Processing batch 1634/11884 - Val_Loss: 24.7163\n",
      "Processing batch 1635/11884 - Val_Loss: 26.2503\n",
      "Processing batch 1636/11884 - Val_Loss: 26.1542\n",
      "Processing batch 1637/11884 - Val_Loss: 25.0325\n",
      "Processing batch 1638/11884 - Val_Loss: 31.1714\n",
      "Processing batch 1639/11884 - Val_Loss: 23.6960\n",
      "Processing batch 1640/11884 - Val_Loss: 24.6772\n",
      "Processing batch 1641/11884 - Val_Loss: 24.8500\n",
      "Processing batch 1642/11884 - Val_Loss: 24.3817\n",
      "Processing batch 1643/11884 - Val_Loss: 24.3026\n",
      "Processing batch 1644/11884 - Val_Loss: 23.5989\n",
      "Processing batch 1645/11884 - Val_Loss: 25.7763\n",
      "Processing batch 1646/11884 - Val_Loss: 26.5977\n",
      "Processing batch 1647/11884 - Val_Loss: 23.3827\n",
      "Processing batch 1648/11884 - Val_Loss: 21.8722\n",
      "Processing batch 1649/11884 - Val_Loss: 25.0599\n",
      "Processing batch 1650/11884 - Val_Loss: 25.0938\n",
      "Processing batch 1651/11884 - Val_Loss: 27.6355\n",
      "Processing batch 1652/11884 - Val_Loss: 25.2257\n",
      "Processing batch 1653/11884 - Val_Loss: 24.9583\n",
      "Processing batch 1654/11884 - Val_Loss: 27.9090\n",
      "Processing batch 1655/11884 - Val_Loss: 24.9386\n",
      "Processing batch 1656/11884 - Val_Loss: 21.6292\n",
      "Processing batch 1657/11884 - Val_Loss: 26.4471\n",
      "Processing batch 1658/11884 - Val_Loss: 23.4141\n",
      "Processing batch 1659/11884 - Val_Loss: 25.6556\n",
      "Processing batch 1660/11884 - Val_Loss: 25.3863\n",
      "Processing batch 1661/11884 - Val_Loss: 28.5045\n",
      "Processing batch 1662/11884 - Val_Loss: 22.6501\n",
      "Processing batch 1663/11884 - Val_Loss: 22.3899\n",
      "Processing batch 1664/11884 - Val_Loss: 27.2449\n",
      "Processing batch 1665/11884 - Val_Loss: 27.3981\n",
      "Processing batch 1666/11884 - Val_Loss: 29.8529\n",
      "Processing batch 1667/11884 - Val_Loss: 25.1061\n",
      "Processing batch 1668/11884 - Val_Loss: 26.0346\n",
      "Processing batch 1669/11884 - Val_Loss: 26.2146\n",
      "Processing batch 1670/11884 - Val_Loss: 25.5753\n",
      "Processing batch 1671/11884 - Val_Loss: 24.1262\n",
      "Processing batch 1672/11884 - Val_Loss: 23.4407\n",
      "Processing batch 1673/11884 - Val_Loss: 24.7589\n",
      "Processing batch 1674/11884 - Val_Loss: 26.4382\n",
      "Processing batch 1675/11884 - Val_Loss: 23.3031\n",
      "Processing batch 1676/11884 - Val_Loss: 24.1373\n",
      "Processing batch 1677/11884 - Val_Loss: 25.4861\n",
      "Processing batch 1678/11884 - Val_Loss: 25.4908\n",
      "Processing batch 1679/11884 - Val_Loss: 27.0249\n",
      "Processing batch 1680/11884 - Val_Loss: 25.2987\n",
      "Processing batch 1681/11884 - Val_Loss: 24.6855\n",
      "Processing batch 1682/11884 - Val_Loss: 27.3072\n",
      "Processing batch 1683/11884 - Val_Loss: 25.4926\n",
      "Processing batch 1684/11884 - Val_Loss: 26.2276\n",
      "Processing batch 1685/11884 - Val_Loss: 24.8059\n",
      "Processing batch 1686/11884 - Val_Loss: 24.3964\n",
      "Processing batch 1687/11884 - Val_Loss: 26.1242\n",
      "Processing batch 1688/11884 - Val_Loss: 24.8593\n",
      "Processing batch 1689/11884 - Val_Loss: 27.5836\n",
      "Processing batch 1690/11884 - Val_Loss: 27.1807\n",
      "Processing batch 1691/11884 - Val_Loss: 24.8783\n",
      "Processing batch 1692/11884 - Val_Loss: 23.4927\n",
      "Processing batch 1693/11884 - Val_Loss: 24.4100\n",
      "Processing batch 1694/11884 - Val_Loss: 24.9583\n",
      "Processing batch 1695/11884 - Val_Loss: 25.9069\n",
      "Processing batch 1696/11884 - Val_Loss: 25.2434\n",
      "Processing batch 1697/11884 - Val_Loss: 26.6640\n",
      "Processing batch 1698/11884 - Val_Loss: 25.8308\n",
      "Processing batch 1699/11884 - Val_Loss: 26.6907\n",
      "Processing batch 1700/11884 - Val_Loss: 25.2101\n",
      "Processing batch 1701/11884 - Val_Loss: 24.7044\n",
      "Processing batch 1702/11884 - Val_Loss: 25.5889\n",
      "Processing batch 1703/11884 - Val_Loss: 25.3286\n",
      "Processing batch 1704/11884 - Val_Loss: 25.8850\n",
      "Processing batch 1705/11884 - Val_Loss: 23.3431\n",
      "Processing batch 1706/11884 - Val_Loss: 24.6403\n",
      "Processing batch 1707/11884 - Val_Loss: 21.9861\n",
      "Processing batch 1708/11884 - Val_Loss: 27.9727\n",
      "Processing batch 1709/11884 - Val_Loss: 25.6367\n",
      "Processing batch 1710/11884 - Val_Loss: 23.7765\n",
      "Processing batch 1711/11884 - Val_Loss: 27.0473\n",
      "Processing batch 1712/11884 - Val_Loss: 24.7312\n",
      "Processing batch 1713/11884 - Val_Loss: 27.3217\n",
      "Processing batch 1714/11884 - Val_Loss: 23.7956\n",
      "Processing batch 1715/11884 - Val_Loss: 25.1542\n",
      "Processing batch 1716/11884 - Val_Loss: 24.6686\n",
      "Processing batch 1717/11884 - Val_Loss: 24.4394\n",
      "Processing batch 1718/11884 - Val_Loss: 27.5055\n",
      "Processing batch 1719/11884 - Val_Loss: 27.2460\n",
      "Processing batch 1720/11884 - Val_Loss: 25.2875\n",
      "Processing batch 1721/11884 - Val_Loss: 27.0579\n",
      "Processing batch 1722/11884 - Val_Loss: 23.7046\n",
      "Processing batch 1723/11884 - Val_Loss: 22.6572\n",
      "Processing batch 1724/11884 - Val_Loss: 25.3736\n",
      "Processing batch 1725/11884 - Val_Loss: 25.5050\n",
      "Processing batch 1726/11884 - Val_Loss: 28.1260\n",
      "Processing batch 1727/11884 - Val_Loss: 24.8542\n",
      "Processing batch 1728/11884 - Val_Loss: 26.9049\n",
      "Processing batch 1729/11884 - Val_Loss: 25.3622\n",
      "Processing batch 1730/11884 - Val_Loss: 26.2397\n",
      "Processing batch 1731/11884 - Val_Loss: 26.9095\n",
      "Processing batch 1732/11884 - Val_Loss: 25.6024\n",
      "Processing batch 1733/11884 - Val_Loss: 24.6101\n",
      "Processing batch 1734/11884 - Val_Loss: 26.7675\n",
      "Processing batch 1735/11884 - Val_Loss: 28.2358\n",
      "Processing batch 1736/11884 - Val_Loss: 25.8932\n",
      "Processing batch 1737/11884 - Val_Loss: 26.0053\n",
      "Processing batch 1738/11884 - Val_Loss: 28.0828\n",
      "Processing batch 1739/11884 - Val_Loss: 24.3937\n",
      "Processing batch 1740/11884 - Val_Loss: 25.1167\n",
      "Processing batch 1741/11884 - Val_Loss: 28.7353\n",
      "Processing batch 1742/11884 - Val_Loss: 26.6522\n",
      "Processing batch 1743/11884 - Val_Loss: 24.4047\n",
      "Processing batch 1744/11884 - Val_Loss: 26.1563\n",
      "Processing batch 1745/11884 - Val_Loss: 24.1462\n",
      "Processing batch 1746/11884 - Val_Loss: 25.7457\n",
      "Processing batch 1747/11884 - Val_Loss: 22.9229\n",
      "Processing batch 1748/11884 - Val_Loss: 28.7846\n",
      "Processing batch 1749/11884 - Val_Loss: 26.7084\n",
      "Processing batch 1750/11884 - Val_Loss: 23.7887\n",
      "Processing batch 1751/11884 - Val_Loss: 26.5629\n",
      "Processing batch 1752/11884 - Val_Loss: 23.1271\n",
      "Processing batch 1753/11884 - Val_Loss: 24.3448\n",
      "Processing batch 1754/11884 - Val_Loss: 23.6960\n",
      "Processing batch 1755/11884 - Val_Loss: 23.1725\n",
      "Processing batch 1756/11884 - Val_Loss: 24.5408\n",
      "Processing batch 1757/11884 - Val_Loss: 25.3521\n",
      "Processing batch 1758/11884 - Val_Loss: 25.3796\n",
      "Processing batch 1759/11884 - Val_Loss: 25.8452\n",
      "Processing batch 1760/11884 - Val_Loss: 26.0324\n",
      "Processing batch 1761/11884 - Val_Loss: 24.4776\n",
      "Processing batch 1762/11884 - Val_Loss: 24.2290\n",
      "Processing batch 1763/11884 - Val_Loss: 24.9400\n",
      "Processing batch 1764/11884 - Val_Loss: 25.6451\n",
      "Processing batch 1765/11884 - Val_Loss: 23.9152\n",
      "Processing batch 1766/11884 - Val_Loss: 25.2177\n",
      "Processing batch 1767/11884 - Val_Loss: 24.6983\n",
      "Processing batch 1768/11884 - Val_Loss: 29.4398\n",
      "Processing batch 1769/11884 - Val_Loss: 25.3712\n",
      "Processing batch 1770/11884 - Val_Loss: 27.2135\n",
      "Processing batch 1771/11884 - Val_Loss: 26.3956\n",
      "Processing batch 1772/11884 - Val_Loss: 26.5395\n",
      "Processing batch 1773/11884 - Val_Loss: 26.7140\n",
      "Processing batch 1774/11884 - Val_Loss: 27.3660\n",
      "Processing batch 1775/11884 - Val_Loss: 27.2739\n",
      "Processing batch 1776/11884 - Val_Loss: 24.7345\n",
      "Processing batch 1777/11884 - Val_Loss: 24.7374\n",
      "Processing batch 1778/11884 - Val_Loss: 25.3491\n",
      "Processing batch 1779/11884 - Val_Loss: 27.1505\n",
      "Processing batch 1780/11884 - Val_Loss: 28.2410\n",
      "Processing batch 1781/11884 - Val_Loss: 27.5275\n",
      "Processing batch 1782/11884 - Val_Loss: 24.8321\n",
      "Processing batch 1783/11884 - Val_Loss: 23.6380\n",
      "Processing batch 1784/11884 - Val_Loss: 26.3462\n",
      "Processing batch 1785/11884 - Val_Loss: 26.1437\n",
      "Processing batch 1786/11884 - Val_Loss: 24.1230\n",
      "Processing batch 1787/11884 - Val_Loss: 27.6991\n",
      "Processing batch 1788/11884 - Val_Loss: 24.5368\n",
      "Processing batch 1789/11884 - Val_Loss: 25.7395\n",
      "Processing batch 1790/11884 - Val_Loss: 26.4096\n",
      "Processing batch 1791/11884 - Val_Loss: 24.0043\n",
      "Processing batch 1792/11884 - Val_Loss: 25.9311\n",
      "Processing batch 1793/11884 - Val_Loss: 27.6569\n",
      "Processing batch 1794/11884 - Val_Loss: 25.2813\n",
      "Processing batch 1795/11884 - Val_Loss: 24.5771\n",
      "Processing batch 1796/11884 - Val_Loss: 24.9376\n",
      "Processing batch 1797/11884 - Val_Loss: 25.4846\n",
      "Processing batch 1798/11884 - Val_Loss: 23.2865\n",
      "Processing batch 1799/11884 - Val_Loss: 24.8899\n",
      "Processing batch 1800/11884 - Val_Loss: 26.3204\n",
      "Processing batch 1801/11884 - Val_Loss: 23.7179\n",
      "Processing batch 1802/11884 - Val_Loss: 23.7941\n",
      "Processing batch 1803/11884 - Val_Loss: 25.4812\n",
      "Processing batch 1804/11884 - Val_Loss: 23.3829\n",
      "Processing batch 1805/11884 - Val_Loss: 23.4477\n",
      "Processing batch 1806/11884 - Val_Loss: 24.3225\n",
      "Processing batch 1807/11884 - Val_Loss: 23.9701\n",
      "Processing batch 1808/11884 - Val_Loss: 25.0518\n",
      "Processing batch 1809/11884 - Val_Loss: 25.5271\n",
      "Processing batch 1810/11884 - Val_Loss: 26.4951\n",
      "Processing batch 1811/11884 - Val_Loss: 26.6286\n",
      "Processing batch 1812/11884 - Val_Loss: 24.7472\n",
      "Processing batch 1813/11884 - Val_Loss: 24.4840\n",
      "Processing batch 1814/11884 - Val_Loss: 25.4133\n",
      "Processing batch 1815/11884 - Val_Loss: 26.9580\n",
      "Processing batch 1816/11884 - Val_Loss: 24.2563\n",
      "Processing batch 1817/11884 - Val_Loss: 24.8740\n",
      "Processing batch 1818/11884 - Val_Loss: 25.4469\n",
      "Processing batch 1819/11884 - Val_Loss: 22.8823\n",
      "Processing batch 1820/11884 - Val_Loss: 28.6511\n",
      "Processing batch 1821/11884 - Val_Loss: 24.9679\n",
      "Processing batch 1822/11884 - Val_Loss: 29.6664\n",
      "Processing batch 1823/11884 - Val_Loss: 26.2172\n",
      "Processing batch 1824/11884 - Val_Loss: 27.1092\n",
      "Processing batch 1825/11884 - Val_Loss: 25.3479\n",
      "Processing batch 1826/11884 - Val_Loss: 26.2066\n",
      "Processing batch 1827/11884 - Val_Loss: 26.9966\n",
      "Processing batch 1828/11884 - Val_Loss: 25.8222\n",
      "Processing batch 1829/11884 - Val_Loss: 24.5105\n",
      "Processing batch 1830/11884 - Val_Loss: 24.2768\n",
      "Processing batch 1831/11884 - Val_Loss: 23.9688\n",
      "Processing batch 1832/11884 - Val_Loss: 26.6514\n",
      "Processing batch 1833/11884 - Val_Loss: 26.7210\n",
      "Processing batch 1834/11884 - Val_Loss: 26.6789\n",
      "Processing batch 1835/11884 - Val_Loss: 24.1232\n",
      "Processing batch 1836/11884 - Val_Loss: 26.2388\n",
      "Processing batch 1837/11884 - Val_Loss: 25.9738\n",
      "Processing batch 1838/11884 - Val_Loss: 24.7559\n",
      "Processing batch 1839/11884 - Val_Loss: 23.3528\n",
      "Processing batch 1840/11884 - Val_Loss: 23.8463\n",
      "Processing batch 1841/11884 - Val_Loss: 26.5029\n",
      "Processing batch 1842/11884 - Val_Loss: 26.4771\n",
      "Processing batch 1843/11884 - Val_Loss: 28.8506\n",
      "Processing batch 1844/11884 - Val_Loss: 26.2940\n",
      "Processing batch 1845/11884 - Val_Loss: 27.6881\n",
      "Processing batch 1846/11884 - Val_Loss: 25.5807\n",
      "Processing batch 1847/11884 - Val_Loss: 24.8878\n",
      "Processing batch 1848/11884 - Val_Loss: 22.0464\n",
      "Processing batch 1849/11884 - Val_Loss: 24.9818\n",
      "Processing batch 1850/11884 - Val_Loss: 25.6266\n",
      "Processing batch 1851/11884 - Val_Loss: 27.4561\n",
      "Processing batch 1852/11884 - Val_Loss: 23.0551\n",
      "Processing batch 1853/11884 - Val_Loss: 24.7498\n",
      "Processing batch 1854/11884 - Val_Loss: 24.2005\n",
      "Processing batch 1855/11884 - Val_Loss: 25.0427\n",
      "Processing batch 1856/11884 - Val_Loss: 25.8301\n",
      "Processing batch 1857/11884 - Val_Loss: 24.4691\n",
      "Processing batch 1858/11884 - Val_Loss: 25.6493\n",
      "Processing batch 1859/11884 - Val_Loss: 23.5458\n",
      "Processing batch 1860/11884 - Val_Loss: 27.1118\n",
      "Processing batch 1861/11884 - Val_Loss: 25.5944\n",
      "Processing batch 1862/11884 - Val_Loss: 23.2567\n",
      "Processing batch 1863/11884 - Val_Loss: 23.9943\n",
      "Processing batch 1864/11884 - Val_Loss: 28.0172\n",
      "Processing batch 1865/11884 - Val_Loss: 25.9957\n",
      "Processing batch 1866/11884 - Val_Loss: 20.9611\n",
      "Processing batch 1867/11884 - Val_Loss: 27.2263\n",
      "Processing batch 1868/11884 - Val_Loss: 25.5977\n",
      "Processing batch 1869/11884 - Val_Loss: 26.5407\n",
      "Processing batch 1870/11884 - Val_Loss: 25.9483\n",
      "Processing batch 1871/11884 - Val_Loss: 26.6846\n",
      "Processing batch 1872/11884 - Val_Loss: 25.1421\n",
      "Processing batch 1873/11884 - Val_Loss: 26.3065\n",
      "Processing batch 1874/11884 - Val_Loss: 29.2484\n",
      "Processing batch 1875/11884 - Val_Loss: 24.1305\n",
      "Processing batch 1876/11884 - Val_Loss: 27.2934\n",
      "Processing batch 1877/11884 - Val_Loss: 26.6560\n",
      "Processing batch 1878/11884 - Val_Loss: 26.0754\n",
      "Processing batch 1879/11884 - Val_Loss: 25.4526\n",
      "Processing batch 1880/11884 - Val_Loss: 22.0178\n",
      "Processing batch 1881/11884 - Val_Loss: 25.5839\n",
      "Processing batch 1882/11884 - Val_Loss: 25.5627\n",
      "Processing batch 1883/11884 - Val_Loss: 22.9727\n",
      "Processing batch 1884/11884 - Val_Loss: 26.1034\n",
      "Processing batch 1885/11884 - Val_Loss: 24.1488\n",
      "Processing batch 1886/11884 - Val_Loss: 26.1328\n",
      "Processing batch 1887/11884 - Val_Loss: 23.4314\n",
      "Processing batch 1888/11884 - Val_Loss: 25.6124\n",
      "Processing batch 1889/11884 - Val_Loss: 25.3660\n",
      "Processing batch 1890/11884 - Val_Loss: 25.8074\n",
      "Processing batch 1891/11884 - Val_Loss: 27.4757\n",
      "Processing batch 1892/11884 - Val_Loss: 27.1846\n",
      "Processing batch 1893/11884 - Val_Loss: 23.0839\n",
      "Processing batch 1894/11884 - Val_Loss: 26.1308\n",
      "Processing batch 1895/11884 - Val_Loss: 22.0739\n",
      "Processing batch 1896/11884 - Val_Loss: 23.7663\n",
      "Processing batch 1897/11884 - Val_Loss: 26.5387\n",
      "Processing batch 1898/11884 - Val_Loss: 24.4131\n",
      "Processing batch 1899/11884 - Val_Loss: 24.5732\n",
      "Processing batch 1900/11884 - Val_Loss: 25.8590\n",
      "Processing batch 1901/11884 - Val_Loss: 25.9760\n",
      "Processing batch 1902/11884 - Val_Loss: 23.8788\n",
      "Processing batch 1903/11884 - Val_Loss: 24.2416\n",
      "Processing batch 1904/11884 - Val_Loss: 29.0495\n",
      "Processing batch 1905/11884 - Val_Loss: 26.0172\n",
      "Processing batch 1906/11884 - Val_Loss: 24.8139\n",
      "Processing batch 1907/11884 - Val_Loss: 24.5415\n",
      "Processing batch 1908/11884 - Val_Loss: 26.5102\n",
      "Processing batch 1909/11884 - Val_Loss: 26.9574\n",
      "Processing batch 1910/11884 - Val_Loss: 26.0776\n",
      "Processing batch 1911/11884 - Val_Loss: 25.9381\n",
      "Processing batch 1912/11884 - Val_Loss: 24.7747\n",
      "Processing batch 1913/11884 - Val_Loss: 28.7097\n",
      "Processing batch 1914/11884 - Val_Loss: 24.9321\n",
      "Processing batch 1915/11884 - Val_Loss: 22.9828\n",
      "Processing batch 1916/11884 - Val_Loss: 27.5127\n",
      "Processing batch 1917/11884 - Val_Loss: 25.5813\n",
      "Processing batch 1918/11884 - Val_Loss: 28.5497\n",
      "Processing batch 1919/11884 - Val_Loss: 25.0229\n",
      "Processing batch 1920/11884 - Val_Loss: 24.2776\n",
      "Processing batch 1921/11884 - Val_Loss: 27.9574\n",
      "Processing batch 1922/11884 - Val_Loss: 27.5219\n",
      "Processing batch 1923/11884 - Val_Loss: 25.9476\n",
      "Processing batch 1924/11884 - Val_Loss: 28.0081\n",
      "Processing batch 1925/11884 - Val_Loss: 25.1215\n",
      "Processing batch 1926/11884 - Val_Loss: 26.6052\n",
      "Processing batch 1927/11884 - Val_Loss: 24.9511\n",
      "Processing batch 1928/11884 - Val_Loss: 28.5980\n",
      "Processing batch 1929/11884 - Val_Loss: 24.9882\n",
      "Processing batch 1930/11884 - Val_Loss: 28.3155\n",
      "Processing batch 1931/11884 - Val_Loss: 27.6894\n",
      "Processing batch 1932/11884 - Val_Loss: 26.8559\n",
      "Processing batch 1933/11884 - Val_Loss: 27.1645\n",
      "Processing batch 1934/11884 - Val_Loss: 25.2589\n",
      "Processing batch 1935/11884 - Val_Loss: 27.0182\n",
      "Processing batch 1936/11884 - Val_Loss: 22.3937\n",
      "Processing batch 1937/11884 - Val_Loss: 24.9717\n",
      "Processing batch 1938/11884 - Val_Loss: 24.7761\n",
      "Processing batch 1939/11884 - Val_Loss: 26.1338\n",
      "Processing batch 1940/11884 - Val_Loss: 26.1759\n",
      "Processing batch 1941/11884 - Val_Loss: 26.8712\n",
      "Processing batch 1942/11884 - Val_Loss: 26.0362\n",
      "Processing batch 1943/11884 - Val_Loss: 27.7066\n",
      "Processing batch 1944/11884 - Val_Loss: 23.4877\n",
      "Processing batch 1945/11884 - Val_Loss: 23.1393\n",
      "Processing batch 1946/11884 - Val_Loss: 25.2921\n",
      "Processing batch 1947/11884 - Val_Loss: 27.7386\n",
      "Processing batch 1948/11884 - Val_Loss: 24.6815\n",
      "Processing batch 1949/11884 - Val_Loss: 26.0693\n",
      "Processing batch 1950/11884 - Val_Loss: 27.3167\n",
      "Processing batch 1951/11884 - Val_Loss: 26.4375\n",
      "Processing batch 1952/11884 - Val_Loss: 26.7196\n",
      "Processing batch 1953/11884 - Val_Loss: 26.4109\n",
      "Processing batch 1954/11884 - Val_Loss: 25.5012\n",
      "Processing batch 1955/11884 - Val_Loss: 27.2357\n",
      "Processing batch 1956/11884 - Val_Loss: 26.3057\n",
      "Processing batch 1957/11884 - Val_Loss: 26.1388\n",
      "Processing batch 1958/11884 - Val_Loss: 23.1381\n",
      "Processing batch 1959/11884 - Val_Loss: 27.7307\n",
      "Processing batch 1960/11884 - Val_Loss: 25.2840\n",
      "Processing batch 1961/11884 - Val_Loss: 25.4649\n",
      "Processing batch 1962/11884 - Val_Loss: 24.9391\n",
      "Processing batch 1963/11884 - Val_Loss: 26.2646\n",
      "Processing batch 1964/11884 - Val_Loss: 24.9616\n",
      "Processing batch 1965/11884 - Val_Loss: 23.5366\n",
      "Processing batch 1966/11884 - Val_Loss: 25.1238\n",
      "Processing batch 1967/11884 - Val_Loss: 27.1999\n",
      "Processing batch 1968/11884 - Val_Loss: 24.9229\n",
      "Processing batch 1969/11884 - Val_Loss: 26.9169\n",
      "Processing batch 1970/11884 - Val_Loss: 21.5386\n",
      "Processing batch 1971/11884 - Val_Loss: 24.7851\n",
      "Processing batch 1972/11884 - Val_Loss: 20.2677\n",
      "Processing batch 1973/11884 - Val_Loss: 22.7506\n",
      "Processing batch 1974/11884 - Val_Loss: 26.9857\n",
      "Processing batch 1975/11884 - Val_Loss: 26.0651\n",
      "Processing batch 1976/11884 - Val_Loss: 24.8225\n",
      "Processing batch 1977/11884 - Val_Loss: 26.7094\n",
      "Processing batch 1978/11884 - Val_Loss: 24.8194\n",
      "Processing batch 1979/11884 - Val_Loss: 24.8483\n",
      "Processing batch 1980/11884 - Val_Loss: 23.5033\n",
      "Processing batch 1981/11884 - Val_Loss: 23.9107\n",
      "Processing batch 1982/11884 - Val_Loss: 23.9564\n",
      "Processing batch 1983/11884 - Val_Loss: 26.6153\n",
      "Processing batch 1984/11884 - Val_Loss: 26.4680\n",
      "Processing batch 1985/11884 - Val_Loss: 25.6688\n",
      "Processing batch 1986/11884 - Val_Loss: 24.7368\n",
      "Processing batch 1987/11884 - Val_Loss: 24.8922\n",
      "Processing batch 1988/11884 - Val_Loss: 24.8564\n",
      "Processing batch 1989/11884 - Val_Loss: 28.0871\n",
      "Processing batch 1990/11884 - Val_Loss: 26.5125\n",
      "Processing batch 1991/11884 - Val_Loss: 26.4422\n",
      "Processing batch 1992/11884 - Val_Loss: 23.4793\n",
      "Processing batch 1993/11884 - Val_Loss: 26.0268\n",
      "Processing batch 1994/11884 - Val_Loss: 26.5877\n",
      "Processing batch 1995/11884 - Val_Loss: 26.7237\n",
      "Processing batch 1996/11884 - Val_Loss: 23.8062\n",
      "Processing batch 1997/11884 - Val_Loss: 26.7727\n",
      "Processing batch 1998/11884 - Val_Loss: 25.7769\n",
      "Processing batch 1999/11884 - Val_Loss: 24.8758\n",
      "Processing batch 2000/11884 - Val_Loss: 26.1188\n",
      "Processing batch 2001/11884 - Val_Loss: 24.7871\n",
      "Processing batch 2002/11884 - Val_Loss: 24.3019\n",
      "Processing batch 2003/11884 - Val_Loss: 26.6639\n",
      "Processing batch 2004/11884 - Val_Loss: 25.7863\n",
      "Processing batch 2005/11884 - Val_Loss: 24.8473\n",
      "Processing batch 2006/11884 - Val_Loss: 26.1320\n",
      "Processing batch 2007/11884 - Val_Loss: 28.3355\n",
      "Processing batch 2008/11884 - Val_Loss: 26.7200\n",
      "Processing batch 2009/11884 - Val_Loss: 26.6166\n",
      "Processing batch 2010/11884 - Val_Loss: 26.2868\n",
      "Processing batch 2011/11884 - Val_Loss: 21.3671\n",
      "Processing batch 2012/11884 - Val_Loss: 28.0611\n",
      "Processing batch 2013/11884 - Val_Loss: 25.5774\n",
      "Processing batch 2014/11884 - Val_Loss: 27.7600\n",
      "Processing batch 2015/11884 - Val_Loss: 23.8877\n",
      "Processing batch 2016/11884 - Val_Loss: 26.3377\n",
      "Processing batch 2017/11884 - Val_Loss: 25.9490\n",
      "Processing batch 2018/11884 - Val_Loss: 25.7579\n",
      "Processing batch 2019/11884 - Val_Loss: 22.8330\n",
      "Processing batch 2020/11884 - Val_Loss: 25.8364\n",
      "Processing batch 2021/11884 - Val_Loss: 24.4154\n",
      "Processing batch 2022/11884 - Val_Loss: 24.0603\n",
      "Processing batch 2023/11884 - Val_Loss: 26.7667\n",
      "Processing batch 2024/11884 - Val_Loss: 27.9985\n",
      "Processing batch 2025/11884 - Val_Loss: 27.7012\n",
      "Processing batch 2026/11884 - Val_Loss: 25.1544\n",
      "Processing batch 2027/11884 - Val_Loss: 23.9749\n",
      "Processing batch 2028/11884 - Val_Loss: 24.9293\n",
      "Processing batch 2029/11884 - Val_Loss: 25.2965\n",
      "Processing batch 2030/11884 - Val_Loss: 25.6468\n",
      "Processing batch 2031/11884 - Val_Loss: 24.7686\n",
      "Processing batch 2032/11884 - Val_Loss: 27.6729\n",
      "Processing batch 2033/11884 - Val_Loss: 23.9454\n",
      "Processing batch 2034/11884 - Val_Loss: 26.4512\n",
      "Processing batch 2035/11884 - Val_Loss: 26.0403\n",
      "Processing batch 2036/11884 - Val_Loss: 24.9787\n",
      "Processing batch 2037/11884 - Val_Loss: 26.1558\n",
      "Processing batch 2038/11884 - Val_Loss: 23.3360\n",
      "Processing batch 2039/11884 - Val_Loss: 25.3712\n",
      "Processing batch 2040/11884 - Val_Loss: 27.2525\n",
      "Processing batch 2041/11884 - Val_Loss: 26.3655\n",
      "Processing batch 2042/11884 - Val_Loss: 27.0095\n",
      "Processing batch 2043/11884 - Val_Loss: 26.1024\n",
      "Processing batch 2044/11884 - Val_Loss: 26.7915\n",
      "Processing batch 2045/11884 - Val_Loss: 29.1347\n",
      "Processing batch 2046/11884 - Val_Loss: 22.8557\n",
      "Processing batch 2047/11884 - Val_Loss: 25.8588\n",
      "Processing batch 2048/11884 - Val_Loss: 22.8094\n",
      "Processing batch 2049/11884 - Val_Loss: 25.3136\n",
      "Processing batch 2050/11884 - Val_Loss: 27.5013\n",
      "Processing batch 2051/11884 - Val_Loss: 22.1395\n",
      "Processing batch 2052/11884 - Val_Loss: 26.5511\n",
      "Processing batch 2053/11884 - Val_Loss: 23.2164\n",
      "Processing batch 2054/11884 - Val_Loss: 23.5976\n",
      "Processing batch 2055/11884 - Val_Loss: 25.8355\n",
      "Processing batch 2056/11884 - Val_Loss: 28.0053\n",
      "Processing batch 2057/11884 - Val_Loss: 25.7718\n",
      "Processing batch 2058/11884 - Val_Loss: 26.5266\n",
      "Processing batch 2059/11884 - Val_Loss: 25.5593\n",
      "Processing batch 2060/11884 - Val_Loss: 23.1181\n",
      "Processing batch 2061/11884 - Val_Loss: 26.4262\n",
      "Processing batch 2062/11884 - Val_Loss: 23.8646\n",
      "Processing batch 2063/11884 - Val_Loss: 24.6104\n",
      "Processing batch 2064/11884 - Val_Loss: 24.2040\n",
      "Processing batch 2065/11884 - Val_Loss: 24.0229\n",
      "Processing batch 2066/11884 - Val_Loss: 26.5597\n",
      "Processing batch 2067/11884 - Val_Loss: 24.3426\n",
      "Processing batch 2068/11884 - Val_Loss: 24.5400\n",
      "Processing batch 2069/11884 - Val_Loss: 23.9286\n",
      "Processing batch 2070/11884 - Val_Loss: 24.0823\n",
      "Processing batch 2071/11884 - Val_Loss: 26.2847\n",
      "Processing batch 2072/11884 - Val_Loss: 23.6116\n",
      "Processing batch 2073/11884 - Val_Loss: 25.2443\n",
      "Processing batch 2074/11884 - Val_Loss: 25.4067\n",
      "Processing batch 2075/11884 - Val_Loss: 23.9390\n",
      "Processing batch 2076/11884 - Val_Loss: 24.2113\n",
      "Processing batch 2077/11884 - Val_Loss: 26.9946\n",
      "Processing batch 2078/11884 - Val_Loss: 23.9320\n",
      "Processing batch 2079/11884 - Val_Loss: 25.5037\n",
      "Processing batch 2080/11884 - Val_Loss: 26.0350\n",
      "Processing batch 2081/11884 - Val_Loss: 23.8575\n",
      "Processing batch 2082/11884 - Val_Loss: 25.7043\n",
      "Processing batch 2083/11884 - Val_Loss: 23.3646\n",
      "Processing batch 2084/11884 - Val_Loss: 24.6581\n",
      "Processing batch 2085/11884 - Val_Loss: 22.5267\n",
      "Processing batch 2086/11884 - Val_Loss: 24.6358\n",
      "Processing batch 2087/11884 - Val_Loss: 26.4736\n",
      "Processing batch 2088/11884 - Val_Loss: 25.6208\n",
      "Processing batch 2089/11884 - Val_Loss: 27.0270\n",
      "Processing batch 2090/11884 - Val_Loss: 26.0204\n",
      "Processing batch 2091/11884 - Val_Loss: 22.7375\n",
      "Processing batch 2092/11884 - Val_Loss: 25.0364\n",
      "Processing batch 2093/11884 - Val_Loss: 26.6533\n",
      "Processing batch 2094/11884 - Val_Loss: 23.8807\n",
      "Processing batch 2095/11884 - Val_Loss: 24.2917\n",
      "Processing batch 2096/11884 - Val_Loss: 26.4066\n",
      "Processing batch 2097/11884 - Val_Loss: 29.4406\n",
      "Processing batch 2098/11884 - Val_Loss: 25.4876\n",
      "Processing batch 2099/11884 - Val_Loss: 26.6723\n",
      "Processing batch 2100/11884 - Val_Loss: 23.9899\n",
      "Processing batch 2101/11884 - Val_Loss: 28.8377\n",
      "Processing batch 2102/11884 - Val_Loss: 24.5553\n",
      "Processing batch 2103/11884 - Val_Loss: 26.6531\n",
      "Processing batch 2104/11884 - Val_Loss: 25.7419\n",
      "Processing batch 2105/11884 - Val_Loss: 25.9024\n",
      "Processing batch 2106/11884 - Val_Loss: 25.1852\n",
      "Processing batch 2107/11884 - Val_Loss: 26.2025\n",
      "Processing batch 2108/11884 - Val_Loss: 24.6723\n",
      "Processing batch 2109/11884 - Val_Loss: 24.1516\n",
      "Processing batch 2110/11884 - Val_Loss: 27.5703\n",
      "Processing batch 2111/11884 - Val_Loss: 24.4141\n",
      "Processing batch 2112/11884 - Val_Loss: 24.4128\n",
      "Processing batch 2113/11884 - Val_Loss: 25.0765\n",
      "Processing batch 2114/11884 - Val_Loss: 25.9377\n",
      "Processing batch 2115/11884 - Val_Loss: 23.4614\n",
      "Processing batch 2116/11884 - Val_Loss: 25.1012\n",
      "Processing batch 2117/11884 - Val_Loss: 22.7544\n",
      "Processing batch 2118/11884 - Val_Loss: 26.2724\n",
      "Processing batch 2119/11884 - Val_Loss: 21.6006\n",
      "Processing batch 2120/11884 - Val_Loss: 26.5345\n",
      "Processing batch 2121/11884 - Val_Loss: 24.8858\n",
      "Processing batch 2122/11884 - Val_Loss: 25.4076\n",
      "Processing batch 2123/11884 - Val_Loss: 27.6878\n",
      "Processing batch 2124/11884 - Val_Loss: 27.2929\n",
      "Processing batch 2125/11884 - Val_Loss: 24.1719\n",
      "Processing batch 2126/11884 - Val_Loss: 24.0974\n",
      "Processing batch 2127/11884 - Val_Loss: 26.7041\n",
      "Processing batch 2128/11884 - Val_Loss: 25.8786\n",
      "Processing batch 2129/11884 - Val_Loss: 28.6502\n",
      "Processing batch 2130/11884 - Val_Loss: 21.5019\n",
      "Processing batch 2131/11884 - Val_Loss: 26.3730\n",
      "Processing batch 2132/11884 - Val_Loss: 26.2055\n",
      "Processing batch 2133/11884 - Val_Loss: 27.7334\n",
      "Processing batch 2134/11884 - Val_Loss: 24.0929\n",
      "Processing batch 2135/11884 - Val_Loss: 26.6920\n",
      "Processing batch 2136/11884 - Val_Loss: 25.6605\n",
      "Processing batch 2137/11884 - Val_Loss: 27.4782\n",
      "Processing batch 2138/11884 - Val_Loss: 25.7903\n",
      "Processing batch 2139/11884 - Val_Loss: 23.6002\n",
      "Processing batch 2140/11884 - Val_Loss: 25.6397\n",
      "Processing batch 2141/11884 - Val_Loss: 22.3005\n",
      "Processing batch 2142/11884 - Val_Loss: 25.3618\n",
      "Processing batch 2143/11884 - Val_Loss: 27.8719\n",
      "Processing batch 2144/11884 - Val_Loss: 24.8245\n",
      "Processing batch 2145/11884 - Val_Loss: 28.4181\n",
      "Processing batch 2146/11884 - Val_Loss: 25.2546\n",
      "Processing batch 2147/11884 - Val_Loss: 22.3090\n",
      "Processing batch 2148/11884 - Val_Loss: 26.3573\n",
      "Processing batch 2149/11884 - Val_Loss: 24.0905\n",
      "Processing batch 2150/11884 - Val_Loss: 23.7176\n",
      "Processing batch 2151/11884 - Val_Loss: 27.0643\n",
      "Processing batch 2152/11884 - Val_Loss: 27.1430\n",
      "Processing batch 2153/11884 - Val_Loss: 24.0281\n",
      "Processing batch 2154/11884 - Val_Loss: 27.2385\n",
      "Processing batch 2155/11884 - Val_Loss: 22.3223\n",
      "Processing batch 2156/11884 - Val_Loss: 24.7942\n",
      "Processing batch 2157/11884 - Val_Loss: 27.6499\n",
      "Processing batch 2158/11884 - Val_Loss: 23.6661\n",
      "Processing batch 2159/11884 - Val_Loss: 25.1461\n",
      "Processing batch 2160/11884 - Val_Loss: 23.5924\n",
      "Processing batch 2161/11884 - Val_Loss: 28.1585\n",
      "Processing batch 2162/11884 - Val_Loss: 25.3507\n",
      "Processing batch 2163/11884 - Val_Loss: 25.5786\n",
      "Processing batch 2164/11884 - Val_Loss: 27.1642\n",
      "Processing batch 2165/11884 - Val_Loss: 25.2145\n",
      "Processing batch 2166/11884 - Val_Loss: 26.5854\n",
      "Processing batch 2167/11884 - Val_Loss: 27.3703\n",
      "Processing batch 2168/11884 - Val_Loss: 22.8584\n",
      "Processing batch 2169/11884 - Val_Loss: 25.7151\n",
      "Processing batch 2170/11884 - Val_Loss: 25.2452\n",
      "Processing batch 2171/11884 - Val_Loss: 25.1521\n",
      "Processing batch 2172/11884 - Val_Loss: 25.8287\n",
      "Processing batch 2173/11884 - Val_Loss: 25.8217\n",
      "Processing batch 2174/11884 - Val_Loss: 26.2244\n",
      "Processing batch 2175/11884 - Val_Loss: 25.1747\n",
      "Processing batch 2176/11884 - Val_Loss: 28.1198\n",
      "Processing batch 2177/11884 - Val_Loss: 25.7059\n",
      "Processing batch 2178/11884 - Val_Loss: 24.1174\n",
      "Processing batch 2179/11884 - Val_Loss: 24.5475\n",
      "Processing batch 2180/11884 - Val_Loss: 26.0759\n",
      "Processing batch 2181/11884 - Val_Loss: 24.3817\n",
      "Processing batch 2182/11884 - Val_Loss: 24.9870\n",
      "Processing batch 2183/11884 - Val_Loss: 25.1721\n",
      "Processing batch 2184/11884 - Val_Loss: 24.3111\n",
      "Processing batch 2185/11884 - Val_Loss: 25.8831\n",
      "Processing batch 2186/11884 - Val_Loss: 26.6839\n",
      "Processing batch 2187/11884 - Val_Loss: 26.6128\n",
      "Processing batch 2188/11884 - Val_Loss: 25.3492\n",
      "Processing batch 2189/11884 - Val_Loss: 27.2385\n",
      "Processing batch 2190/11884 - Val_Loss: 25.5970\n",
      "Processing batch 2191/11884 - Val_Loss: 23.3200\n",
      "Processing batch 2192/11884 - Val_Loss: 30.1190\n",
      "Processing batch 2193/11884 - Val_Loss: 23.5025\n",
      "Processing batch 2194/11884 - Val_Loss: 26.8605\n",
      "Processing batch 2195/11884 - Val_Loss: 25.0349\n",
      "Processing batch 2196/11884 - Val_Loss: 23.6342\n",
      "Processing batch 2197/11884 - Val_Loss: 27.0278\n",
      "Processing batch 2198/11884 - Val_Loss: 29.5824\n",
      "Processing batch 2199/11884 - Val_Loss: 25.1142\n",
      "Processing batch 2200/11884 - Val_Loss: 21.8146\n",
      "Processing batch 2201/11884 - Val_Loss: 26.2530\n",
      "Processing batch 2202/11884 - Val_Loss: 25.9849\n",
      "Processing batch 2203/11884 - Val_Loss: 25.6549\n",
      "Processing batch 2204/11884 - Val_Loss: 26.4827\n",
      "Processing batch 2205/11884 - Val_Loss: 23.0935\n",
      "Processing batch 2206/11884 - Val_Loss: 25.4301\n",
      "Processing batch 2207/11884 - Val_Loss: 24.6653\n",
      "Processing batch 2208/11884 - Val_Loss: 25.0482\n",
      "Processing batch 2209/11884 - Val_Loss: 25.8980\n",
      "Processing batch 2210/11884 - Val_Loss: 26.4635\n",
      "Processing batch 2211/11884 - Val_Loss: 22.0601\n",
      "Processing batch 2212/11884 - Val_Loss: 23.2008\n",
      "Processing batch 2213/11884 - Val_Loss: 27.2697\n",
      "Processing batch 2214/11884 - Val_Loss: 23.2851\n",
      "Processing batch 2215/11884 - Val_Loss: 26.4922\n",
      "Processing batch 2216/11884 - Val_Loss: 26.1392\n",
      "Processing batch 2217/11884 - Val_Loss: 28.0716\n",
      "Processing batch 2218/11884 - Val_Loss: 28.3918\n",
      "Processing batch 2219/11884 - Val_Loss: 27.0246\n",
      "Processing batch 2220/11884 - Val_Loss: 25.1582\n",
      "Processing batch 2221/11884 - Val_Loss: 24.4388\n",
      "Processing batch 2222/11884 - Val_Loss: 22.3307\n",
      "Processing batch 2223/11884 - Val_Loss: 27.1500\n",
      "Processing batch 2224/11884 - Val_Loss: 24.1295\n",
      "Processing batch 2225/11884 - Val_Loss: 30.9463\n",
      "Processing batch 2226/11884 - Val_Loss: 21.1187\n",
      "Processing batch 2227/11884 - Val_Loss: 25.0049\n",
      "Processing batch 2228/11884 - Val_Loss: 27.3531\n",
      "Processing batch 2229/11884 - Val_Loss: 27.8095\n",
      "Processing batch 2230/11884 - Val_Loss: 24.1639\n",
      "Processing batch 2231/11884 - Val_Loss: 22.4182\n",
      "Processing batch 2232/11884 - Val_Loss: 26.2172\n",
      "Processing batch 2233/11884 - Val_Loss: 24.8734\n",
      "Processing batch 2234/11884 - Val_Loss: 24.8297\n",
      "Processing batch 2235/11884 - Val_Loss: 25.7617\n",
      "Processing batch 2236/11884 - Val_Loss: 25.1546\n",
      "Processing batch 2237/11884 - Val_Loss: 27.4957\n",
      "Processing batch 2238/11884 - Val_Loss: 27.5826\n",
      "Processing batch 2239/11884 - Val_Loss: 26.0021\n",
      "Processing batch 2240/11884 - Val_Loss: 27.3176\n",
      "Processing batch 2241/11884 - Val_Loss: 30.0136\n",
      "Processing batch 2242/11884 - Val_Loss: 25.3047\n",
      "Processing batch 2243/11884 - Val_Loss: 25.7089\n",
      "Processing batch 2244/11884 - Val_Loss: 22.9698\n",
      "Processing batch 2245/11884 - Val_Loss: 24.5039\n",
      "Processing batch 2246/11884 - Val_Loss: 26.2738\n",
      "Processing batch 2247/11884 - Val_Loss: 23.3048\n",
      "Processing batch 2248/11884 - Val_Loss: 25.0741\n",
      "Processing batch 2249/11884 - Val_Loss: 23.7048\n",
      "Processing batch 2250/11884 - Val_Loss: 27.5685\n",
      "Processing batch 2251/11884 - Val_Loss: 27.0935\n",
      "Processing batch 2252/11884 - Val_Loss: 24.7388\n",
      "Processing batch 2253/11884 - Val_Loss: 25.6132\n",
      "Processing batch 2254/11884 - Val_Loss: 25.7940\n",
      "Processing batch 2255/11884 - Val_Loss: 22.4381\n",
      "Processing batch 2256/11884 - Val_Loss: 22.7483\n",
      "Processing batch 2257/11884 - Val_Loss: 26.2584\n",
      "Processing batch 2258/11884 - Val_Loss: 26.7746\n",
      "Processing batch 2259/11884 - Val_Loss: 25.1597\n",
      "Processing batch 2260/11884 - Val_Loss: 26.2729\n",
      "Processing batch 2261/11884 - Val_Loss: 25.1870\n",
      "Processing batch 2262/11884 - Val_Loss: 25.1741\n",
      "Processing batch 2263/11884 - Val_Loss: 26.2952\n",
      "Processing batch 2264/11884 - Val_Loss: 26.6049\n",
      "Processing batch 2265/11884 - Val_Loss: 26.1448\n",
      "Processing batch 2266/11884 - Val_Loss: 23.5520\n",
      "Processing batch 2267/11884 - Val_Loss: 22.0872\n",
      "Processing batch 2268/11884 - Val_Loss: 24.9394\n",
      "Processing batch 2269/11884 - Val_Loss: 24.0909\n",
      "Processing batch 2270/11884 - Val_Loss: 25.3949\n",
      "Processing batch 2271/11884 - Val_Loss: 23.0633\n",
      "Processing batch 2272/11884 - Val_Loss: 28.5185\n",
      "Processing batch 2273/11884 - Val_Loss: 29.2132\n",
      "Processing batch 2274/11884 - Val_Loss: 23.2414\n",
      "Processing batch 2275/11884 - Val_Loss: 23.8256\n",
      "Processing batch 2276/11884 - Val_Loss: 22.6883\n",
      "Processing batch 2277/11884 - Val_Loss: 27.3433\n",
      "Processing batch 2278/11884 - Val_Loss: 22.4185\n",
      "Processing batch 2279/11884 - Val_Loss: 25.2019\n",
      "Processing batch 2280/11884 - Val_Loss: 23.9181\n",
      "Processing batch 2281/11884 - Val_Loss: 23.7418\n",
      "Processing batch 2282/11884 - Val_Loss: 25.9329\n",
      "Processing batch 2283/11884 - Val_Loss: 25.6437\n",
      "Processing batch 2284/11884 - Val_Loss: 26.0861\n",
      "Processing batch 2285/11884 - Val_Loss: 23.8957\n",
      "Processing batch 2286/11884 - Val_Loss: 28.4810\n",
      "Processing batch 2287/11884 - Val_Loss: 25.7379\n",
      "Processing batch 2288/11884 - Val_Loss: 25.4089\n",
      "Processing batch 2289/11884 - Val_Loss: 24.7027\n",
      "Processing batch 2290/11884 - Val_Loss: 24.9280\n",
      "Processing batch 2291/11884 - Val_Loss: 25.0451\n",
      "Processing batch 2292/11884 - Val_Loss: 26.2650\n",
      "Processing batch 2293/11884 - Val_Loss: 26.4190\n",
      "Processing batch 2294/11884 - Val_Loss: 26.0720\n",
      "Processing batch 2295/11884 - Val_Loss: 24.5321\n",
      "Processing batch 2296/11884 - Val_Loss: 26.6426\n",
      "Processing batch 2297/11884 - Val_Loss: 25.1897\n",
      "Processing batch 2298/11884 - Val_Loss: 23.6696\n",
      "Processing batch 2299/11884 - Val_Loss: 25.8631\n",
      "Processing batch 2300/11884 - Val_Loss: 23.8837\n",
      "Processing batch 2301/11884 - Val_Loss: 26.2096\n",
      "Processing batch 2302/11884 - Val_Loss: 24.7877\n",
      "Processing batch 2303/11884 - Val_Loss: 28.6997\n",
      "Processing batch 2304/11884 - Val_Loss: 23.9570\n",
      "Processing batch 2305/11884 - Val_Loss: 27.7992\n",
      "Processing batch 2306/11884 - Val_Loss: 25.8632\n",
      "Processing batch 2307/11884 - Val_Loss: 25.3534\n",
      "Processing batch 2308/11884 - Val_Loss: 27.2948\n",
      "Processing batch 2309/11884 - Val_Loss: 25.4627\n",
      "Processing batch 2310/11884 - Val_Loss: 27.6688\n",
      "Processing batch 2311/11884 - Val_Loss: 23.5928\n",
      "Processing batch 2312/11884 - Val_Loss: 25.8154\n",
      "Processing batch 2313/11884 - Val_Loss: 26.3013\n",
      "Processing batch 2314/11884 - Val_Loss: 27.2053\n",
      "Processing batch 2315/11884 - Val_Loss: 26.1463\n",
      "Processing batch 2316/11884 - Val_Loss: 26.8412\n",
      "Processing batch 2317/11884 - Val_Loss: 27.3769\n",
      "Processing batch 2318/11884 - Val_Loss: 23.8358\n",
      "Processing batch 2319/11884 - Val_Loss: 26.3571\n",
      "Processing batch 2320/11884 - Val_Loss: 22.6585\n",
      "Processing batch 2321/11884 - Val_Loss: 24.7794\n",
      "Processing batch 2322/11884 - Val_Loss: 24.9624\n",
      "Processing batch 2323/11884 - Val_Loss: 24.9310\n",
      "Processing batch 2324/11884 - Val_Loss: 22.7778\n",
      "Processing batch 2325/11884 - Val_Loss: 25.7991\n",
      "Processing batch 2326/11884 - Val_Loss: 23.8940\n",
      "Processing batch 2327/11884 - Val_Loss: 22.8122\n",
      "Processing batch 2328/11884 - Val_Loss: 28.0191\n",
      "Processing batch 2329/11884 - Val_Loss: 25.9258\n",
      "Processing batch 2330/11884 - Val_Loss: 25.8897\n",
      "Processing batch 2331/11884 - Val_Loss: 27.0944\n",
      "Processing batch 2332/11884 - Val_Loss: 25.7025\n",
      "Processing batch 2333/11884 - Val_Loss: 25.1808\n",
      "Processing batch 2334/11884 - Val_Loss: 26.8739\n",
      "Processing batch 2335/11884 - Val_Loss: 26.0749\n",
      "Processing batch 2336/11884 - Val_Loss: 25.4261\n",
      "Processing batch 2337/11884 - Val_Loss: 27.4690\n",
      "Processing batch 2338/11884 - Val_Loss: 23.6585\n",
      "Processing batch 2339/11884 - Val_Loss: 27.6155\n",
      "Processing batch 2340/11884 - Val_Loss: 25.8223\n",
      "Processing batch 2341/11884 - Val_Loss: 24.0088\n",
      "Processing batch 2342/11884 - Val_Loss: 29.3469\n",
      "Processing batch 2343/11884 - Val_Loss: 26.8060\n",
      "Processing batch 2344/11884 - Val_Loss: 25.4838\n",
      "Processing batch 2345/11884 - Val_Loss: 27.7543\n",
      "Processing batch 2346/11884 - Val_Loss: 25.4119\n",
      "Processing batch 2347/11884 - Val_Loss: 24.0741\n",
      "Processing batch 2348/11884 - Val_Loss: 25.3275\n",
      "Processing batch 2349/11884 - Val_Loss: 28.0955\n",
      "Processing batch 2350/11884 - Val_Loss: 25.4587\n",
      "Processing batch 2351/11884 - Val_Loss: 26.3943\n",
      "Processing batch 2352/11884 - Val_Loss: 23.3696\n",
      "Processing batch 2353/11884 - Val_Loss: 27.3236\n",
      "Processing batch 2354/11884 - Val_Loss: 24.0004\n",
      "Processing batch 2355/11884 - Val_Loss: 24.8167\n",
      "Processing batch 2356/11884 - Val_Loss: 25.5271\n",
      "Processing batch 2357/11884 - Val_Loss: 25.2892\n",
      "Processing batch 2358/11884 - Val_Loss: 26.5226\n",
      "Processing batch 2359/11884 - Val_Loss: 26.7293\n",
      "Processing batch 2360/11884 - Val_Loss: 25.0645\n",
      "Processing batch 2361/11884 - Val_Loss: 24.7057\n",
      "Processing batch 2362/11884 - Val_Loss: 26.4970\n",
      "Processing batch 2363/11884 - Val_Loss: 25.9434\n",
      "Processing batch 2364/11884 - Val_Loss: 25.7789\n",
      "Processing batch 2365/11884 - Val_Loss: 25.8012\n",
      "Processing batch 2366/11884 - Val_Loss: 25.0629\n",
      "Processing batch 2367/11884 - Val_Loss: 27.2233\n",
      "Processing batch 2368/11884 - Val_Loss: 25.2783\n",
      "Processing batch 2369/11884 - Val_Loss: 23.2472\n",
      "Processing batch 2370/11884 - Val_Loss: 23.2185\n",
      "Processing batch 2371/11884 - Val_Loss: 22.4997\n",
      "Processing batch 2372/11884 - Val_Loss: 23.5464\n",
      "Processing batch 2373/11884 - Val_Loss: 23.0618\n",
      "Processing batch 2374/11884 - Val_Loss: 25.5723\n",
      "Processing batch 2375/11884 - Val_Loss: 25.9081\n",
      "Processing batch 2376/11884 - Val_Loss: 23.6830\n",
      "Processing batch 2377/11884 - Val_Loss: 24.2322\n",
      "Processing batch 2378/11884 - Val_Loss: 26.6078\n",
      "Processing batch 2379/11884 - Val_Loss: 24.6446\n",
      "Processing batch 2380/11884 - Val_Loss: 25.0958\n",
      "Processing batch 2381/11884 - Val_Loss: 26.5582\n",
      "Processing batch 2382/11884 - Val_Loss: 24.3289\n",
      "Processing batch 2383/11884 - Val_Loss: 23.8268\n",
      "Processing batch 2384/11884 - Val_Loss: 24.6374\n",
      "Processing batch 2385/11884 - Val_Loss: 24.1678\n",
      "Processing batch 2386/11884 - Val_Loss: 22.1731\n",
      "Processing batch 2387/11884 - Val_Loss: 22.9713\n",
      "Processing batch 2388/11884 - Val_Loss: 25.8615\n",
      "Processing batch 2389/11884 - Val_Loss: 25.5704\n",
      "Processing batch 2390/11884 - Val_Loss: 27.1577\n",
      "Processing batch 2391/11884 - Val_Loss: 26.4578\n",
      "Processing batch 2392/11884 - Val_Loss: 23.4021\n",
      "Processing batch 2393/11884 - Val_Loss: 24.8426\n",
      "Processing batch 2394/11884 - Val_Loss: 26.2396\n",
      "Processing batch 2395/11884 - Val_Loss: 24.5383\n",
      "Processing batch 2396/11884 - Val_Loss: 27.9261\n",
      "Processing batch 2397/11884 - Val_Loss: 24.8965\n",
      "Processing batch 2398/11884 - Val_Loss: 26.5853\n",
      "Processing batch 2399/11884 - Val_Loss: 25.8420\n",
      "Processing batch 2400/11884 - Val_Loss: 22.7622\n",
      "Processing batch 2401/11884 - Val_Loss: 26.8591\n",
      "Processing batch 2402/11884 - Val_Loss: 24.3643\n",
      "Processing batch 2403/11884 - Val_Loss: 26.8885\n",
      "Processing batch 2404/11884 - Val_Loss: 24.8543\n",
      "Processing batch 2405/11884 - Val_Loss: 22.9498\n",
      "Processing batch 2406/11884 - Val_Loss: 25.1855\n",
      "Processing batch 2407/11884 - Val_Loss: 24.1133\n",
      "Processing batch 2408/11884 - Val_Loss: 25.9136\n",
      "Processing batch 2409/11884 - Val_Loss: 24.2633\n",
      "Processing batch 2410/11884 - Val_Loss: 25.9036\n",
      "Processing batch 2411/11884 - Val_Loss: 24.8725\n",
      "Processing batch 2412/11884 - Val_Loss: 24.8840\n",
      "Processing batch 2413/11884 - Val_Loss: 23.9150\n",
      "Processing batch 2414/11884 - Val_Loss: 23.4310\n",
      "Processing batch 2415/11884 - Val_Loss: 24.0177\n",
      "Processing batch 2416/11884 - Val_Loss: 28.8469\n",
      "Processing batch 2417/11884 - Val_Loss: 23.5754\n",
      "Processing batch 2418/11884 - Val_Loss: 23.9028\n",
      "Processing batch 2419/11884 - Val_Loss: 25.4705\n",
      "Processing batch 2420/11884 - Val_Loss: 26.4511\n",
      "Processing batch 2421/11884 - Val_Loss: 27.6057\n",
      "Processing batch 2422/11884 - Val_Loss: 25.4292\n",
      "Processing batch 2423/11884 - Val_Loss: 26.4657\n",
      "Processing batch 2424/11884 - Val_Loss: 26.0838\n",
      "Processing batch 2425/11884 - Val_Loss: 25.0235\n",
      "Processing batch 2426/11884 - Val_Loss: 26.8396\n",
      "Processing batch 2427/11884 - Val_Loss: 24.4718\n",
      "Processing batch 2428/11884 - Val_Loss: 23.7840\n",
      "Processing batch 2429/11884 - Val_Loss: 25.1935\n",
      "Processing batch 2430/11884 - Val_Loss: 25.0833\n",
      "Processing batch 2431/11884 - Val_Loss: 23.5268\n",
      "Processing batch 2432/11884 - Val_Loss: 24.1799\n",
      "Processing batch 2433/11884 - Val_Loss: 25.4081\n",
      "Processing batch 2434/11884 - Val_Loss: 25.7094\n",
      "Processing batch 2435/11884 - Val_Loss: 26.2124\n",
      "Processing batch 2436/11884 - Val_Loss: 22.1048\n",
      "Processing batch 2437/11884 - Val_Loss: 29.6229\n",
      "Processing batch 2438/11884 - Val_Loss: 26.9594\n",
      "Processing batch 2439/11884 - Val_Loss: 24.4534\n",
      "Processing batch 2440/11884 - Val_Loss: 25.2801\n",
      "Processing batch 2441/11884 - Val_Loss: 27.1081\n",
      "Processing batch 2442/11884 - Val_Loss: 24.4791\n",
      "Processing batch 2443/11884 - Val_Loss: 25.5636\n",
      "Processing batch 2444/11884 - Val_Loss: 25.7744\n",
      "Processing batch 2445/11884 - Val_Loss: 24.4758\n",
      "Processing batch 2446/11884 - Val_Loss: 27.4493\n",
      "Processing batch 2447/11884 - Val_Loss: 26.1048\n",
      "Processing batch 2448/11884 - Val_Loss: 26.9150\n",
      "Processing batch 2449/11884 - Val_Loss: 24.6924\n",
      "Processing batch 2450/11884 - Val_Loss: 25.8309\n",
      "Processing batch 2451/11884 - Val_Loss: 26.5014\n",
      "Processing batch 2452/11884 - Val_Loss: 25.2195\n",
      "Processing batch 2453/11884 - Val_Loss: 26.7351\n",
      "Processing batch 2454/11884 - Val_Loss: 29.4915\n",
      "Processing batch 2455/11884 - Val_Loss: 22.4064\n",
      "Processing batch 2456/11884 - Val_Loss: 28.1015\n",
      "Processing batch 2457/11884 - Val_Loss: 27.7366\n",
      "Processing batch 2458/11884 - Val_Loss: 28.4844\n",
      "Processing batch 2459/11884 - Val_Loss: 24.0845\n",
      "Processing batch 2460/11884 - Val_Loss: 25.2893\n",
      "Processing batch 2461/11884 - Val_Loss: 22.5397\n",
      "Processing batch 2462/11884 - Val_Loss: 23.8051\n",
      "Processing batch 2463/11884 - Val_Loss: 26.3784\n",
      "Processing batch 2464/11884 - Val_Loss: 24.0810\n",
      "Processing batch 2465/11884 - Val_Loss: 23.0711\n",
      "Processing batch 2466/11884 - Val_Loss: 28.1909\n",
      "Processing batch 2467/11884 - Val_Loss: 24.5157\n",
      "Processing batch 2468/11884 - Val_Loss: 27.8382\n",
      "Processing batch 2469/11884 - Val_Loss: 27.4816\n",
      "Processing batch 2470/11884 - Val_Loss: 23.0536\n",
      "Processing batch 2471/11884 - Val_Loss: 23.1091\n",
      "Processing batch 2472/11884 - Val_Loss: 24.7292\n",
      "Processing batch 2473/11884 - Val_Loss: 26.4387\n",
      "Processing batch 2474/11884 - Val_Loss: 23.2461\n",
      "Processing batch 2475/11884 - Val_Loss: 28.3149\n",
      "Processing batch 2476/11884 - Val_Loss: 22.4188\n",
      "Processing batch 2477/11884 - Val_Loss: 25.1401\n",
      "Processing batch 2478/11884 - Val_Loss: 23.4458\n",
      "Processing batch 2479/11884 - Val_Loss: 27.0989\n",
      "Processing batch 2480/11884 - Val_Loss: 26.3516\n",
      "Processing batch 2481/11884 - Val_Loss: 26.8302\n",
      "Processing batch 2482/11884 - Val_Loss: 22.1755\n",
      "Processing batch 2483/11884 - Val_Loss: 25.9191\n",
      "Processing batch 2484/11884 - Val_Loss: 25.3945\n",
      "Processing batch 2485/11884 - Val_Loss: 23.6340\n",
      "Processing batch 2486/11884 - Val_Loss: 25.1200\n",
      "Processing batch 2487/11884 - Val_Loss: 27.3586\n",
      "Processing batch 2488/11884 - Val_Loss: 25.9505\n",
      "Processing batch 2489/11884 - Val_Loss: 23.5070\n",
      "Processing batch 2490/11884 - Val_Loss: 21.7054\n",
      "Processing batch 2491/11884 - Val_Loss: 24.1485\n",
      "Processing batch 2492/11884 - Val_Loss: 24.2102\n",
      "Processing batch 2493/11884 - Val_Loss: 21.9631\n",
      "Processing batch 2494/11884 - Val_Loss: 25.4210\n",
      "Processing batch 2495/11884 - Val_Loss: 25.6260\n",
      "Processing batch 2496/11884 - Val_Loss: 23.7985\n",
      "Processing batch 2497/11884 - Val_Loss: 24.7952\n",
      "Processing batch 2498/11884 - Val_Loss: 24.8493\n",
      "Processing batch 2499/11884 - Val_Loss: 26.3893\n",
      "Processing batch 2500/11884 - Val_Loss: 29.2169\n",
      "Processing batch 2501/11884 - Val_Loss: 29.1777\n",
      "Processing batch 2502/11884 - Val_Loss: 27.6673\n",
      "Processing batch 2503/11884 - Val_Loss: 25.9612\n",
      "Processing batch 2504/11884 - Val_Loss: 26.0356\n",
      "Processing batch 2505/11884 - Val_Loss: 26.5713\n",
      "Processing batch 2506/11884 - Val_Loss: 28.4200\n",
      "Processing batch 2507/11884 - Val_Loss: 24.8633\n",
      "Processing batch 2508/11884 - Val_Loss: 28.4419\n",
      "Processing batch 2509/11884 - Val_Loss: 27.6895\n",
      "Processing batch 2510/11884 - Val_Loss: 26.2253\n",
      "Processing batch 2511/11884 - Val_Loss: 26.6879\n",
      "Processing batch 2512/11884 - Val_Loss: 22.6549\n",
      "Processing batch 2513/11884 - Val_Loss: 26.0308\n",
      "Processing batch 2514/11884 - Val_Loss: 25.2562\n",
      "Processing batch 2515/11884 - Val_Loss: 26.0102\n",
      "Processing batch 2516/11884 - Val_Loss: 27.7221\n",
      "Processing batch 2517/11884 - Val_Loss: 26.5917\n",
      "Processing batch 2518/11884 - Val_Loss: 25.9418\n",
      "Processing batch 2519/11884 - Val_Loss: 23.9703\n",
      "Processing batch 2520/11884 - Val_Loss: 24.0428\n",
      "Processing batch 2521/11884 - Val_Loss: 25.0466\n",
      "Processing batch 2522/11884 - Val_Loss: 27.0534\n",
      "Processing batch 2523/11884 - Val_Loss: 25.0676\n",
      "Processing batch 2524/11884 - Val_Loss: 27.2340\n",
      "Processing batch 2525/11884 - Val_Loss: 24.2285\n",
      "Processing batch 2526/11884 - Val_Loss: 25.4589\n",
      "Processing batch 2527/11884 - Val_Loss: 23.6648\n",
      "Processing batch 2528/11884 - Val_Loss: 27.2427\n",
      "Processing batch 2529/11884 - Val_Loss: 22.2277\n",
      "Processing batch 2530/11884 - Val_Loss: 25.6270\n",
      "Processing batch 2531/11884 - Val_Loss: 26.7325\n",
      "Processing batch 2532/11884 - Val_Loss: 23.1464\n",
      "Processing batch 2533/11884 - Val_Loss: 25.5710\n",
      "Processing batch 2534/11884 - Val_Loss: 25.9461\n",
      "Processing batch 2535/11884 - Val_Loss: 23.9654\n",
      "Processing batch 2536/11884 - Val_Loss: 23.8474\n",
      "Processing batch 2537/11884 - Val_Loss: 24.3087\n",
      "Processing batch 2538/11884 - Val_Loss: 27.2115\n",
      "Processing batch 2539/11884 - Val_Loss: 24.9379\n",
      "Processing batch 2540/11884 - Val_Loss: 26.3405\n",
      "Processing batch 2541/11884 - Val_Loss: 27.9978\n",
      "Processing batch 2542/11884 - Val_Loss: 26.5012\n",
      "Processing batch 2543/11884 - Val_Loss: 26.0842\n",
      "Processing batch 2544/11884 - Val_Loss: 25.6430\n",
      "Processing batch 2545/11884 - Val_Loss: 26.1663\n",
      "Processing batch 2546/11884 - Val_Loss: 26.7605\n",
      "Processing batch 2547/11884 - Val_Loss: 22.6125\n",
      "Processing batch 2548/11884 - Val_Loss: 25.3148\n",
      "Processing batch 2549/11884 - Val_Loss: 26.2069\n",
      "Processing batch 2550/11884 - Val_Loss: 26.9750\n",
      "Processing batch 2551/11884 - Val_Loss: 26.4767\n",
      "Processing batch 2552/11884 - Val_Loss: 25.6419\n",
      "Processing batch 2553/11884 - Val_Loss: 25.2034\n",
      "Processing batch 2554/11884 - Val_Loss: 24.3030\n",
      "Processing batch 2555/11884 - Val_Loss: 23.6360\n",
      "Processing batch 2556/11884 - Val_Loss: 26.4549\n",
      "Processing batch 2557/11884 - Val_Loss: 25.4791\n",
      "Processing batch 2558/11884 - Val_Loss: 25.0059\n",
      "Processing batch 2559/11884 - Val_Loss: 27.4233\n",
      "Processing batch 2560/11884 - Val_Loss: 24.7551\n",
      "Processing batch 2561/11884 - Val_Loss: 29.9116\n",
      "Processing batch 2562/11884 - Val_Loss: 27.3771\n",
      "Processing batch 2563/11884 - Val_Loss: 24.6517\n",
      "Processing batch 2564/11884 - Val_Loss: 27.0903\n",
      "Processing batch 2565/11884 - Val_Loss: 27.6496\n",
      "Processing batch 2566/11884 - Val_Loss: 24.9948\n",
      "Processing batch 2567/11884 - Val_Loss: 24.0915\n",
      "Processing batch 2568/11884 - Val_Loss: 22.9331\n",
      "Processing batch 2569/11884 - Val_Loss: 27.7472\n",
      "Processing batch 2570/11884 - Val_Loss: 26.2129\n",
      "Processing batch 2571/11884 - Val_Loss: 26.3819\n",
      "Processing batch 2572/11884 - Val_Loss: 25.3597\n",
      "Processing batch 2573/11884 - Val_Loss: 25.9031\n",
      "Processing batch 2574/11884 - Val_Loss: 26.6025\n",
      "Processing batch 2575/11884 - Val_Loss: 28.3724\n",
      "Processing batch 2576/11884 - Val_Loss: 25.1180\n",
      "Processing batch 2577/11884 - Val_Loss: 25.5488\n",
      "Processing batch 2578/11884 - Val_Loss: 26.6140\n",
      "Processing batch 2579/11884 - Val_Loss: 27.5028\n",
      "Processing batch 2580/11884 - Val_Loss: 25.0459\n",
      "Processing batch 2581/11884 - Val_Loss: 24.5225\n",
      "Processing batch 2582/11884 - Val_Loss: 26.1269\n",
      "Processing batch 2583/11884 - Val_Loss: 25.0221\n",
      "Processing batch 2584/11884 - Val_Loss: 24.3301\n",
      "Processing batch 2585/11884 - Val_Loss: 24.1912\n",
      "Processing batch 2586/11884 - Val_Loss: 28.8064\n",
      "Processing batch 2587/11884 - Val_Loss: 22.6426\n",
      "Processing batch 2588/11884 - Val_Loss: 26.9079\n",
      "Processing batch 2589/11884 - Val_Loss: 24.6168\n",
      "Processing batch 2590/11884 - Val_Loss: 24.9278\n",
      "Processing batch 2591/11884 - Val_Loss: 25.5341\n",
      "Processing batch 2592/11884 - Val_Loss: 26.0295\n",
      "Processing batch 2593/11884 - Val_Loss: 26.6532\n",
      "Processing batch 2594/11884 - Val_Loss: 26.4729\n",
      "Processing batch 2595/11884 - Val_Loss: 28.1665\n",
      "Processing batch 2596/11884 - Val_Loss: 27.4166\n",
      "Processing batch 2597/11884 - Val_Loss: 25.7576\n",
      "Processing batch 2598/11884 - Val_Loss: 25.3033\n",
      "Processing batch 2599/11884 - Val_Loss: 26.2767\n",
      "Processing batch 2600/11884 - Val_Loss: 27.3240\n",
      "Processing batch 2601/11884 - Val_Loss: 28.1807\n",
      "Processing batch 2602/11884 - Val_Loss: 26.4241\n",
      "Processing batch 2603/11884 - Val_Loss: 23.4507\n",
      "Processing batch 2604/11884 - Val_Loss: 26.1474\n",
      "Processing batch 2605/11884 - Val_Loss: 27.2115\n",
      "Processing batch 2606/11884 - Val_Loss: 23.0226\n",
      "Processing batch 2607/11884 - Val_Loss: 24.9530\n",
      "Processing batch 2608/11884 - Val_Loss: 26.8176\n",
      "Processing batch 2609/11884 - Val_Loss: 27.8463\n",
      "Processing batch 2610/11884 - Val_Loss: 22.3202\n",
      "Processing batch 2611/11884 - Val_Loss: 24.3658\n",
      "Processing batch 2612/11884 - Val_Loss: 24.3117\n",
      "Processing batch 2613/11884 - Val_Loss: 24.0492\n",
      "Processing batch 2614/11884 - Val_Loss: 23.9191\n",
      "Processing batch 2615/11884 - Val_Loss: 23.9408\n",
      "Processing batch 2616/11884 - Val_Loss: 27.3665\n",
      "Processing batch 2617/11884 - Val_Loss: 24.4113\n",
      "Processing batch 2618/11884 - Val_Loss: 25.4470\n",
      "Processing batch 2619/11884 - Val_Loss: 28.4293\n",
      "Processing batch 2620/11884 - Val_Loss: 25.7536\n",
      "Processing batch 2621/11884 - Val_Loss: 26.0645\n",
      "Processing batch 2622/11884 - Val_Loss: 24.3685\n",
      "Processing batch 2623/11884 - Val_Loss: 24.8399\n",
      "Processing batch 2624/11884 - Val_Loss: 24.8330\n",
      "Processing batch 2625/11884 - Val_Loss: 27.0558\n",
      "Processing batch 2626/11884 - Val_Loss: 28.5223\n",
      "Processing batch 2627/11884 - Val_Loss: 24.6866\n",
      "Processing batch 2628/11884 - Val_Loss: 25.5188\n",
      "Processing batch 2629/11884 - Val_Loss: 28.4390\n",
      "Processing batch 2630/11884 - Val_Loss: 26.7569\n",
      "Processing batch 2631/11884 - Val_Loss: 22.9910\n",
      "Processing batch 2632/11884 - Val_Loss: 22.6340\n",
      "Processing batch 2633/11884 - Val_Loss: 26.1066\n",
      "Processing batch 2634/11884 - Val_Loss: 25.6505\n",
      "Processing batch 2635/11884 - Val_Loss: 27.7727\n",
      "Processing batch 2636/11884 - Val_Loss: 25.5888\n",
      "Processing batch 2637/11884 - Val_Loss: 27.5711\n",
      "Processing batch 2638/11884 - Val_Loss: 24.6968\n",
      "Processing batch 2639/11884 - Val_Loss: 26.1666\n",
      "Processing batch 2640/11884 - Val_Loss: 25.4447\n",
      "Processing batch 2641/11884 - Val_Loss: 24.4455\n",
      "Processing batch 2642/11884 - Val_Loss: 24.4020\n",
      "Processing batch 2643/11884 - Val_Loss: 24.8605\n",
      "Processing batch 2644/11884 - Val_Loss: 22.9716\n",
      "Processing batch 2645/11884 - Val_Loss: 26.3010\n",
      "Processing batch 2646/11884 - Val_Loss: 23.9256\n",
      "Processing batch 2647/11884 - Val_Loss: 26.5282\n",
      "Processing batch 2648/11884 - Val_Loss: 24.9466\n",
      "Processing batch 2649/11884 - Val_Loss: 25.6965\n",
      "Processing batch 2650/11884 - Val_Loss: 27.7990\n",
      "Processing batch 2651/11884 - Val_Loss: 23.9694\n",
      "Processing batch 2652/11884 - Val_Loss: 24.1041\n",
      "Processing batch 2653/11884 - Val_Loss: 24.8269\n",
      "Processing batch 2654/11884 - Val_Loss: 23.9218\n",
      "Processing batch 2655/11884 - Val_Loss: 26.2771\n",
      "Processing batch 2656/11884 - Val_Loss: 24.6719\n",
      "Processing batch 2657/11884 - Val_Loss: 21.4457\n",
      "Processing batch 2658/11884 - Val_Loss: 27.0529\n",
      "Processing batch 2659/11884 - Val_Loss: 24.0443\n",
      "Processing batch 2660/11884 - Val_Loss: 24.2289\n",
      "Processing batch 2661/11884 - Val_Loss: 25.3127\n",
      "Processing batch 2662/11884 - Val_Loss: 26.1750\n",
      "Processing batch 2663/11884 - Val_Loss: 25.1974\n",
      "Processing batch 2664/11884 - Val_Loss: 25.8200\n",
      "Processing batch 2665/11884 - Val_Loss: 25.3474\n",
      "Processing batch 2666/11884 - Val_Loss: 28.1566\n",
      "Processing batch 2667/11884 - Val_Loss: 29.0049\n",
      "Processing batch 2668/11884 - Val_Loss: 26.0196\n",
      "Processing batch 2669/11884 - Val_Loss: 24.8697\n",
      "Processing batch 2670/11884 - Val_Loss: 24.7379\n",
      "Processing batch 2671/11884 - Val_Loss: 24.7914\n",
      "Processing batch 2672/11884 - Val_Loss: 25.7943\n",
      "Processing batch 2673/11884 - Val_Loss: 27.3184\n",
      "Processing batch 2674/11884 - Val_Loss: 26.8842\n",
      "Processing batch 2675/11884 - Val_Loss: 25.7234\n",
      "Processing batch 2676/11884 - Val_Loss: 26.0429\n",
      "Processing batch 2677/11884 - Val_Loss: 24.2876\n",
      "Processing batch 2678/11884 - Val_Loss: 25.9118\n",
      "Processing batch 2679/11884 - Val_Loss: 24.6834\n",
      "Processing batch 2680/11884 - Val_Loss: 25.0172\n",
      "Processing batch 2681/11884 - Val_Loss: 28.4565\n",
      "Processing batch 2682/11884 - Val_Loss: 26.0392\n",
      "Processing batch 2683/11884 - Val_Loss: 27.0961\n",
      "Processing batch 2684/11884 - Val_Loss: 24.5748\n",
      "Processing batch 2685/11884 - Val_Loss: 22.8513\n",
      "Processing batch 2686/11884 - Val_Loss: 25.8390\n",
      "Processing batch 2687/11884 - Val_Loss: 25.4048\n",
      "Processing batch 2688/11884 - Val_Loss: 26.8094\n",
      "Processing batch 2689/11884 - Val_Loss: 25.8613\n",
      "Processing batch 2690/11884 - Val_Loss: 24.8622\n",
      "Processing batch 2691/11884 - Val_Loss: 24.8148\n",
      "Processing batch 2692/11884 - Val_Loss: 22.3273\n",
      "Processing batch 2693/11884 - Val_Loss: 22.3398\n",
      "Processing batch 2694/11884 - Val_Loss: 27.3751\n",
      "Processing batch 2695/11884 - Val_Loss: 25.4152\n",
      "Processing batch 2696/11884 - Val_Loss: 25.1628\n",
      "Processing batch 2697/11884 - Val_Loss: 23.3384\n",
      "Processing batch 2698/11884 - Val_Loss: 23.9676\n",
      "Processing batch 2699/11884 - Val_Loss: 25.6698\n",
      "Processing batch 2700/11884 - Val_Loss: 27.3949\n",
      "Processing batch 2701/11884 - Val_Loss: 29.5216\n",
      "Processing batch 2702/11884 - Val_Loss: 26.5880\n",
      "Processing batch 2703/11884 - Val_Loss: 26.1277\n",
      "Processing batch 2704/11884 - Val_Loss: 23.8904\n",
      "Processing batch 2705/11884 - Val_Loss: 28.0739\n",
      "Processing batch 2706/11884 - Val_Loss: 26.9862\n",
      "Processing batch 2707/11884 - Val_Loss: 25.4860\n",
      "Processing batch 2708/11884 - Val_Loss: 30.8968\n",
      "Processing batch 2709/11884 - Val_Loss: 25.9806\n",
      "Processing batch 2710/11884 - Val_Loss: 27.5270\n",
      "Processing batch 2711/11884 - Val_Loss: 24.5831\n",
      "Processing batch 2712/11884 - Val_Loss: 26.3257\n",
      "Processing batch 2713/11884 - Val_Loss: 26.2994\n",
      "Processing batch 2714/11884 - Val_Loss: 27.7091\n",
      "Processing batch 2715/11884 - Val_Loss: 25.0653\n",
      "Processing batch 2716/11884 - Val_Loss: 25.9871\n",
      "Processing batch 2717/11884 - Val_Loss: 27.3389\n",
      "Processing batch 2718/11884 - Val_Loss: 26.2098\n",
      "Processing batch 2719/11884 - Val_Loss: 24.7514\n",
      "Processing batch 2720/11884 - Val_Loss: 23.9749\n",
      "Processing batch 2721/11884 - Val_Loss: 26.6795\n",
      "Processing batch 2722/11884 - Val_Loss: 24.3297\n",
      "Processing batch 2723/11884 - Val_Loss: 24.7295\n",
      "Processing batch 2724/11884 - Val_Loss: 24.0637\n",
      "Processing batch 2725/11884 - Val_Loss: 22.9399\n",
      "Processing batch 2726/11884 - Val_Loss: 23.3200\n",
      "Processing batch 2727/11884 - Val_Loss: 27.7020\n",
      "Processing batch 2728/11884 - Val_Loss: 22.4467\n",
      "Processing batch 2729/11884 - Val_Loss: 23.3155\n",
      "Processing batch 2730/11884 - Val_Loss: 25.7835\n",
      "Processing batch 2731/11884 - Val_Loss: 25.6726\n",
      "Processing batch 2732/11884 - Val_Loss: 25.5718\n",
      "Processing batch 2733/11884 - Val_Loss: 28.2565\n",
      "Processing batch 2734/11884 - Val_Loss: 23.3770\n",
      "Processing batch 2735/11884 - Val_Loss: 26.1615\n",
      "Processing batch 2736/11884 - Val_Loss: 26.2210\n",
      "Processing batch 2737/11884 - Val_Loss: 24.5667\n",
      "Processing batch 2738/11884 - Val_Loss: 25.0854\n",
      "Processing batch 2739/11884 - Val_Loss: 24.2731\n",
      "Processing batch 2740/11884 - Val_Loss: 25.8331\n",
      "Processing batch 2741/11884 - Val_Loss: 25.7606\n",
      "Processing batch 2742/11884 - Val_Loss: 22.7283\n",
      "Processing batch 2743/11884 - Val_Loss: 27.1394\n",
      "Processing batch 2744/11884 - Val_Loss: 25.0848\n",
      "Processing batch 2745/11884 - Val_Loss: 24.6143\n",
      "Processing batch 2746/11884 - Val_Loss: 26.1470\n",
      "Processing batch 2747/11884 - Val_Loss: 27.3999\n",
      "Processing batch 2748/11884 - Val_Loss: 28.9631\n",
      "Processing batch 2749/11884 - Val_Loss: 26.5808\n",
      "Processing batch 2750/11884 - Val_Loss: 24.0774\n",
      "Processing batch 2751/11884 - Val_Loss: 27.1612\n",
      "Processing batch 2752/11884 - Val_Loss: 26.8069\n",
      "Processing batch 2753/11884 - Val_Loss: 23.7982\n",
      "Processing batch 2754/11884 - Val_Loss: 24.5865\n",
      "Processing batch 2755/11884 - Val_Loss: 26.0332\n",
      "Processing batch 2756/11884 - Val_Loss: 24.2392\n",
      "Processing batch 2757/11884 - Val_Loss: 23.3447\n",
      "Processing batch 2758/11884 - Val_Loss: 25.8860\n",
      "Processing batch 2759/11884 - Val_Loss: 27.5744\n",
      "Processing batch 2760/11884 - Val_Loss: 27.7410\n",
      "Processing batch 2761/11884 - Val_Loss: 25.8108\n",
      "Processing batch 2762/11884 - Val_Loss: 23.9590\n",
      "Processing batch 2763/11884 - Val_Loss: 22.8591\n",
      "Processing batch 2764/11884 - Val_Loss: 27.8474\n",
      "Processing batch 2765/11884 - Val_Loss: 25.7823\n",
      "Processing batch 2766/11884 - Val_Loss: 24.2854\n",
      "Processing batch 2767/11884 - Val_Loss: 25.9851\n",
      "Processing batch 2768/11884 - Val_Loss: 26.5914\n",
      "Processing batch 2769/11884 - Val_Loss: 24.5634\n",
      "Processing batch 2770/11884 - Val_Loss: 25.6455\n",
      "Processing batch 2771/11884 - Val_Loss: 25.2816\n",
      "Processing batch 2772/11884 - Val_Loss: 26.1566\n",
      "Processing batch 2773/11884 - Val_Loss: 24.9251\n",
      "Processing batch 2774/11884 - Val_Loss: 26.5711\n",
      "Processing batch 2775/11884 - Val_Loss: 21.7272\n",
      "Processing batch 2776/11884 - Val_Loss: 24.1802\n",
      "Processing batch 2777/11884 - Val_Loss: 23.5610\n",
      "Processing batch 2778/11884 - Val_Loss: 23.7676\n",
      "Processing batch 2779/11884 - Val_Loss: 26.8064\n",
      "Processing batch 2780/11884 - Val_Loss: 26.7095\n",
      "Processing batch 2781/11884 - Val_Loss: 27.2622\n",
      "Processing batch 2782/11884 - Val_Loss: 25.4762\n",
      "Processing batch 2783/11884 - Val_Loss: 22.4526\n",
      "Processing batch 2784/11884 - Val_Loss: 25.9722\n",
      "Processing batch 2785/11884 - Val_Loss: 22.3759\n",
      "Processing batch 2786/11884 - Val_Loss: 25.9929\n",
      "Processing batch 2787/11884 - Val_Loss: 23.3521\n",
      "Processing batch 2788/11884 - Val_Loss: 25.3149\n",
      "Processing batch 2789/11884 - Val_Loss: 25.5672\n",
      "Processing batch 2790/11884 - Val_Loss: 28.2286\n",
      "Processing batch 2791/11884 - Val_Loss: 22.6090\n",
      "Processing batch 2792/11884 - Val_Loss: 24.6713\n",
      "Processing batch 2793/11884 - Val_Loss: 24.5116\n",
      "Processing batch 2794/11884 - Val_Loss: 25.7149\n",
      "Processing batch 2795/11884 - Val_Loss: 24.0093\n",
      "Processing batch 2796/11884 - Val_Loss: 25.3767\n",
      "Processing batch 2797/11884 - Val_Loss: 24.1892\n",
      "Processing batch 2798/11884 - Val_Loss: 25.0159\n",
      "Processing batch 2799/11884 - Val_Loss: 26.0928\n",
      "Processing batch 2800/11884 - Val_Loss: 26.9682\n",
      "Processing batch 2801/11884 - Val_Loss: 24.8299\n",
      "Processing batch 2802/11884 - Val_Loss: 23.5844\n",
      "Processing batch 2803/11884 - Val_Loss: 23.5826\n",
      "Processing batch 2804/11884 - Val_Loss: 28.5948\n",
      "Processing batch 2805/11884 - Val_Loss: 24.2586\n",
      "Processing batch 2806/11884 - Val_Loss: 25.1535\n",
      "Processing batch 2807/11884 - Val_Loss: 27.1035\n",
      "Processing batch 2808/11884 - Val_Loss: 27.2360\n",
      "Processing batch 2809/11884 - Val_Loss: 26.4487\n",
      "Processing batch 2810/11884 - Val_Loss: 26.7212\n",
      "Processing batch 2811/11884 - Val_Loss: 26.3511\n",
      "Processing batch 2812/11884 - Val_Loss: 25.5439\n",
      "Processing batch 2813/11884 - Val_Loss: 24.6504\n",
      "Processing batch 2814/11884 - Val_Loss: 23.6877\n",
      "Processing batch 2815/11884 - Val_Loss: 27.8523\n",
      "Processing batch 2816/11884 - Val_Loss: 24.1771\n",
      "Processing batch 2817/11884 - Val_Loss: 24.3118\n",
      "Processing batch 2818/11884 - Val_Loss: 25.7338\n",
      "Processing batch 2819/11884 - Val_Loss: 25.7438\n",
      "Processing batch 2820/11884 - Val_Loss: 26.2569\n",
      "Processing batch 2821/11884 - Val_Loss: 25.9747\n",
      "Processing batch 2822/11884 - Val_Loss: 23.7350\n",
      "Processing batch 2823/11884 - Val_Loss: 21.0660\n",
      "Processing batch 2824/11884 - Val_Loss: 24.1186\n",
      "Processing batch 2825/11884 - Val_Loss: 23.3799\n",
      "Processing batch 2826/11884 - Val_Loss: 26.7410\n",
      "Processing batch 2827/11884 - Val_Loss: 24.6361\n",
      "Processing batch 2828/11884 - Val_Loss: 23.0966\n",
      "Processing batch 2829/11884 - Val_Loss: 24.4653\n",
      "Processing batch 2830/11884 - Val_Loss: 25.2594\n",
      "Processing batch 2831/11884 - Val_Loss: 28.0159\n",
      "Processing batch 2832/11884 - Val_Loss: 28.3424\n",
      "Processing batch 2833/11884 - Val_Loss: 23.5101\n",
      "Processing batch 2834/11884 - Val_Loss: 27.0707\n",
      "Processing batch 2835/11884 - Val_Loss: 23.9688\n",
      "Processing batch 2836/11884 - Val_Loss: 24.6085\n",
      "Processing batch 2837/11884 - Val_Loss: 23.8246\n",
      "Processing batch 2838/11884 - Val_Loss: 23.2828\n",
      "Processing batch 2839/11884 - Val_Loss: 23.9926\n",
      "Processing batch 2840/11884 - Val_Loss: 25.4032\n",
      "Processing batch 2841/11884 - Val_Loss: 25.2449\n",
      "Processing batch 2842/11884 - Val_Loss: 26.2314\n",
      "Processing batch 2843/11884 - Val_Loss: 23.3372\n",
      "Processing batch 2844/11884 - Val_Loss: 23.3052\n",
      "Processing batch 2845/11884 - Val_Loss: 26.3271\n",
      "Processing batch 2846/11884 - Val_Loss: 25.5414\n",
      "Processing batch 2847/11884 - Val_Loss: 26.5155\n",
      "Processing batch 2848/11884 - Val_Loss: 25.3381\n",
      "Processing batch 2849/11884 - Val_Loss: 21.8674\n",
      "Processing batch 2850/11884 - Val_Loss: 26.6919\n",
      "Processing batch 2851/11884 - Val_Loss: 23.2587\n",
      "Processing batch 2852/11884 - Val_Loss: 21.8194\n",
      "Processing batch 2853/11884 - Val_Loss: 27.2133\n",
      "Processing batch 2854/11884 - Val_Loss: 27.1972\n",
      "Processing batch 2855/11884 - Val_Loss: 27.8618\n",
      "Processing batch 2856/11884 - Val_Loss: 22.4149\n",
      "Processing batch 2857/11884 - Val_Loss: 28.2891\n",
      "Processing batch 2858/11884 - Val_Loss: 26.5443\n",
      "Processing batch 2859/11884 - Val_Loss: 26.6003\n",
      "Processing batch 2860/11884 - Val_Loss: 27.1915\n",
      "Processing batch 2861/11884 - Val_Loss: 24.6735\n",
      "Processing batch 2862/11884 - Val_Loss: 24.1914\n",
      "Processing batch 2863/11884 - Val_Loss: 27.3623\n",
      "Processing batch 2864/11884 - Val_Loss: 26.7405\n",
      "Processing batch 2865/11884 - Val_Loss: 28.2307\n",
      "Processing batch 2866/11884 - Val_Loss: 26.1132\n",
      "Processing batch 2867/11884 - Val_Loss: 24.6457\n",
      "Processing batch 2868/11884 - Val_Loss: 24.8863\n",
      "Processing batch 2869/11884 - Val_Loss: 26.2962\n",
      "Processing batch 2870/11884 - Val_Loss: 24.2125\n",
      "Processing batch 2871/11884 - Val_Loss: 29.8597\n",
      "Processing batch 2872/11884 - Val_Loss: 26.6382\n",
      "Processing batch 2873/11884 - Val_Loss: 25.9384\n",
      "Processing batch 2874/11884 - Val_Loss: 26.0137\n",
      "Processing batch 2875/11884 - Val_Loss: 22.9782\n",
      "Processing batch 2876/11884 - Val_Loss: 22.9999\n",
      "Processing batch 2877/11884 - Val_Loss: 21.9320\n",
      "Processing batch 2878/11884 - Val_Loss: 25.8981\n",
      "Processing batch 2879/11884 - Val_Loss: 25.7924\n",
      "Processing batch 2880/11884 - Val_Loss: 25.2939\n",
      "Processing batch 2881/11884 - Val_Loss: 26.3558\n",
      "Processing batch 2882/11884 - Val_Loss: 27.2199\n",
      "Processing batch 2883/11884 - Val_Loss: 26.7301\n",
      "Processing batch 2884/11884 - Val_Loss: 24.5988\n",
      "Processing batch 2885/11884 - Val_Loss: 26.3604\n",
      "Processing batch 2886/11884 - Val_Loss: 26.3577\n",
      "Processing batch 2887/11884 - Val_Loss: 27.6493\n",
      "Processing batch 2888/11884 - Val_Loss: 25.2965\n",
      "Processing batch 2889/11884 - Val_Loss: 23.4358\n",
      "Processing batch 2890/11884 - Val_Loss: 28.3822\n",
      "Processing batch 2891/11884 - Val_Loss: 25.4022\n",
      "Processing batch 2892/11884 - Val_Loss: 24.4465\n",
      "Processing batch 2893/11884 - Val_Loss: 28.0206\n",
      "Processing batch 2894/11884 - Val_Loss: 23.5925\n",
      "Processing batch 2895/11884 - Val_Loss: 24.9702\n",
      "Processing batch 2896/11884 - Val_Loss: 24.8563\n",
      "Processing batch 2897/11884 - Val_Loss: 24.5021\n",
      "Processing batch 2898/11884 - Val_Loss: 24.3438\n",
      "Processing batch 2899/11884 - Val_Loss: 26.5821\n",
      "Processing batch 2900/11884 - Val_Loss: 24.8641\n",
      "Processing batch 2901/11884 - Val_Loss: 21.2042\n",
      "Processing batch 2902/11884 - Val_Loss: 25.1391\n",
      "Processing batch 2903/11884 - Val_Loss: 26.6024\n",
      "Processing batch 2904/11884 - Val_Loss: 26.0484\n",
      "Processing batch 2905/11884 - Val_Loss: 24.6578\n",
      "Processing batch 2906/11884 - Val_Loss: 27.3689\n",
      "Processing batch 2907/11884 - Val_Loss: 26.8808\n",
      "Processing batch 2908/11884 - Val_Loss: 26.7543\n",
      "Processing batch 2909/11884 - Val_Loss: 26.1237\n",
      "Processing batch 2910/11884 - Val_Loss: 29.1658\n",
      "Processing batch 2911/11884 - Val_Loss: 26.0544\n",
      "Processing batch 2912/11884 - Val_Loss: 25.4074\n",
      "Processing batch 2913/11884 - Val_Loss: 25.8271\n",
      "Processing batch 2914/11884 - Val_Loss: 24.3832\n",
      "Processing batch 2915/11884 - Val_Loss: 28.5961\n",
      "Processing batch 2916/11884 - Val_Loss: 24.3165\n",
      "Processing batch 2917/11884 - Val_Loss: 24.1801\n",
      "Processing batch 2918/11884 - Val_Loss: 25.1173\n",
      "Processing batch 2919/11884 - Val_Loss: 28.4916\n",
      "Processing batch 2920/11884 - Val_Loss: 26.1858\n",
      "Processing batch 2921/11884 - Val_Loss: 25.3450\n",
      "Processing batch 2922/11884 - Val_Loss: 23.9237\n",
      "Processing batch 2923/11884 - Val_Loss: 26.4137\n",
      "Processing batch 2924/11884 - Val_Loss: 21.8593\n",
      "Processing batch 2925/11884 - Val_Loss: 25.7327\n",
      "Processing batch 2926/11884 - Val_Loss: 23.2153\n",
      "Processing batch 2927/11884 - Val_Loss: 23.0487\n",
      "Processing batch 2928/11884 - Val_Loss: 26.0417\n",
      "Processing batch 2929/11884 - Val_Loss: 26.9082\n",
      "Processing batch 2930/11884 - Val_Loss: 26.3700\n",
      "Processing batch 2931/11884 - Val_Loss: 24.4809\n",
      "Processing batch 2932/11884 - Val_Loss: 27.4620\n",
      "Processing batch 2933/11884 - Val_Loss: 25.0499\n",
      "Processing batch 2934/11884 - Val_Loss: 26.4060\n",
      "Processing batch 2935/11884 - Val_Loss: 26.9359\n",
      "Processing batch 2936/11884 - Val_Loss: 25.8750\n",
      "Processing batch 2937/11884 - Val_Loss: 21.8015\n",
      "Processing batch 2938/11884 - Val_Loss: 24.0308\n",
      "Processing batch 2939/11884 - Val_Loss: 24.4604\n",
      "Processing batch 2940/11884 - Val_Loss: 26.7044\n",
      "Processing batch 2941/11884 - Val_Loss: 26.4675\n",
      "Processing batch 2942/11884 - Val_Loss: 25.6095\n",
      "Processing batch 2943/11884 - Val_Loss: 26.4036\n",
      "Processing batch 2944/11884 - Val_Loss: 24.8533\n",
      "Processing batch 2945/11884 - Val_Loss: 24.4044\n",
      "Processing batch 2946/11884 - Val_Loss: 22.9654\n",
      "Processing batch 2947/11884 - Val_Loss: 26.4868\n",
      "Processing batch 2948/11884 - Val_Loss: 24.4540\n",
      "Processing batch 2949/11884 - Val_Loss: 24.3807\n",
      "Processing batch 2950/11884 - Val_Loss: 25.6202\n",
      "Processing batch 2951/11884 - Val_Loss: 25.8480\n",
      "Processing batch 2952/11884 - Val_Loss: 23.8861\n",
      "Processing batch 2953/11884 - Val_Loss: 22.2398\n",
      "Processing batch 2954/11884 - Val_Loss: 23.8847\n",
      "Processing batch 2955/11884 - Val_Loss: 24.8754\n",
      "Processing batch 2956/11884 - Val_Loss: 25.1092\n",
      "Processing batch 2957/11884 - Val_Loss: 23.6002\n",
      "Processing batch 2958/11884 - Val_Loss: 25.8383\n",
      "Processing batch 2959/11884 - Val_Loss: 22.4631\n",
      "Processing batch 2960/11884 - Val_Loss: 27.0756\n",
      "Processing batch 2961/11884 - Val_Loss: 25.5007\n",
      "Processing batch 2962/11884 - Val_Loss: 23.4353\n",
      "Processing batch 2963/11884 - Val_Loss: 26.2647\n",
      "Processing batch 2964/11884 - Val_Loss: 25.9059\n",
      "Processing batch 2965/11884 - Val_Loss: 24.8620\n",
      "Processing batch 2966/11884 - Val_Loss: 25.4338\n",
      "Processing batch 2967/11884 - Val_Loss: 25.2196\n",
      "Processing batch 2968/11884 - Val_Loss: 25.0763\n",
      "Processing batch 2969/11884 - Val_Loss: 25.3224\n",
      "Processing batch 2970/11884 - Val_Loss: 23.1278\n",
      "Processing batch 2971/11884 - Val_Loss: 25.2134\n",
      "Processing batch 2972/11884 - Val_Loss: 24.9206\n",
      "Processing batch 2973/11884 - Val_Loss: 27.1101\n",
      "Processing batch 2974/11884 - Val_Loss: 25.1666\n",
      "Processing batch 2975/11884 - Val_Loss: 24.7281\n",
      "Processing batch 2976/11884 - Val_Loss: 25.0050\n",
      "Processing batch 2977/11884 - Val_Loss: 22.2682\n",
      "Processing batch 2978/11884 - Val_Loss: 24.3105\n",
      "Processing batch 2979/11884 - Val_Loss: 25.2097\n",
      "Processing batch 2980/11884 - Val_Loss: 23.7031\n",
      "Processing batch 2981/11884 - Val_Loss: 28.6145\n",
      "Processing batch 2982/11884 - Val_Loss: 27.3439\n",
      "Processing batch 2983/11884 - Val_Loss: 30.7103\n",
      "Processing batch 2984/11884 - Val_Loss: 24.2646\n",
      "Processing batch 2985/11884 - Val_Loss: 25.5762\n",
      "Processing batch 2986/11884 - Val_Loss: 25.8258\n",
      "Processing batch 2987/11884 - Val_Loss: 25.8853\n",
      "Processing batch 2988/11884 - Val_Loss: 26.0779\n",
      "Processing batch 2989/11884 - Val_Loss: 24.2606\n",
      "Processing batch 2990/11884 - Val_Loss: 27.1689\n",
      "Processing batch 2991/11884 - Val_Loss: 25.8353\n",
      "Processing batch 2992/11884 - Val_Loss: 27.2231\n",
      "Processing batch 2993/11884 - Val_Loss: 27.3022\n",
      "Processing batch 2994/11884 - Val_Loss: 24.9848\n",
      "Processing batch 2995/11884 - Val_Loss: 26.7225\n",
      "Processing batch 2996/11884 - Val_Loss: 25.2498\n",
      "Processing batch 2997/11884 - Val_Loss: 22.6686\n",
      "Processing batch 2998/11884 - Val_Loss: 24.2348\n",
      "Processing batch 2999/11884 - Val_Loss: 26.8009\n",
      "Processing batch 3000/11884 - Val_Loss: 23.7604\n",
      "Processing batch 3001/11884 - Val_Loss: 26.1894\n",
      "Processing batch 3002/11884 - Val_Loss: 23.9780\n",
      "Processing batch 3003/11884 - Val_Loss: 24.2973\n",
      "Processing batch 3004/11884 - Val_Loss: 24.4945\n",
      "Processing batch 3005/11884 - Val_Loss: 23.6043\n",
      "Processing batch 3006/11884 - Val_Loss: 24.7836\n",
      "Processing batch 3007/11884 - Val_Loss: 26.2892\n",
      "Processing batch 3008/11884 - Val_Loss: 27.5481\n",
      "Processing batch 3009/11884 - Val_Loss: 26.0113\n",
      "Processing batch 3010/11884 - Val_Loss: 23.7039\n",
      "Processing batch 3011/11884 - Val_Loss: 26.2058\n",
      "Processing batch 3012/11884 - Val_Loss: 25.0898\n",
      "Processing batch 3013/11884 - Val_Loss: 24.0043\n",
      "Processing batch 3014/11884 - Val_Loss: 25.1394\n",
      "Processing batch 3015/11884 - Val_Loss: 24.4552\n",
      "Processing batch 3016/11884 - Val_Loss: 25.8231\n",
      "Processing batch 3017/11884 - Val_Loss: 24.9607\n",
      "Processing batch 3018/11884 - Val_Loss: 26.1019\n",
      "Processing batch 3019/11884 - Val_Loss: 22.5468\n",
      "Processing batch 3020/11884 - Val_Loss: 27.5767\n",
      "Processing batch 3021/11884 - Val_Loss: 26.6758\n",
      "Processing batch 3022/11884 - Val_Loss: 26.3944\n",
      "Processing batch 3023/11884 - Val_Loss: 24.9751\n",
      "Processing batch 3024/11884 - Val_Loss: 24.0968\n",
      "Processing batch 3025/11884 - Val_Loss: 24.3827\n",
      "Processing batch 3026/11884 - Val_Loss: 27.1383\n",
      "Processing batch 3027/11884 - Val_Loss: 29.4659\n",
      "Processing batch 3028/11884 - Val_Loss: 23.8219\n",
      "Processing batch 3029/11884 - Val_Loss: 27.5838\n",
      "Processing batch 3030/11884 - Val_Loss: 26.6425\n",
      "Processing batch 3031/11884 - Val_Loss: 26.1880\n",
      "Processing batch 3032/11884 - Val_Loss: 26.5640\n",
      "Processing batch 3033/11884 - Val_Loss: 26.1153\n",
      "Processing batch 3034/11884 - Val_Loss: 28.1803\n",
      "Processing batch 3035/11884 - Val_Loss: 26.9637\n",
      "Processing batch 3036/11884 - Val_Loss: 26.3631\n",
      "Processing batch 3037/11884 - Val_Loss: 23.1253\n",
      "Processing batch 3038/11884 - Val_Loss: 25.2413\n",
      "Processing batch 3039/11884 - Val_Loss: 28.7788\n",
      "Processing batch 3040/11884 - Val_Loss: 27.3285\n",
      "Processing batch 3041/11884 - Val_Loss: 25.6832\n",
      "Processing batch 3042/11884 - Val_Loss: 24.2521\n",
      "Processing batch 3043/11884 - Val_Loss: 25.1031\n",
      "Processing batch 3044/11884 - Val_Loss: 27.2317\n",
      "Processing batch 3045/11884 - Val_Loss: 26.6180\n",
      "Processing batch 3046/11884 - Val_Loss: 24.0179\n",
      "Processing batch 3047/11884 - Val_Loss: 25.1030\n",
      "Processing batch 3048/11884 - Val_Loss: 24.1790\n",
      "Processing batch 3049/11884 - Val_Loss: 26.6465\n",
      "Processing batch 3050/11884 - Val_Loss: 26.1416\n",
      "Processing batch 3051/11884 - Val_Loss: 26.1669\n",
      "Processing batch 3052/11884 - Val_Loss: 25.5763\n",
      "Processing batch 3053/11884 - Val_Loss: 25.8496\n",
      "Processing batch 3054/11884 - Val_Loss: 27.2591\n",
      "Processing batch 3055/11884 - Val_Loss: 24.4149\n",
      "Processing batch 3056/11884 - Val_Loss: 22.9724\n",
      "Processing batch 3057/11884 - Val_Loss: 26.6240\n",
      "Processing batch 3058/11884 - Val_Loss: 29.1186\n",
      "Processing batch 3059/11884 - Val_Loss: 29.5664\n",
      "Processing batch 3060/11884 - Val_Loss: 24.6901\n",
      "Processing batch 3061/11884 - Val_Loss: 24.1733\n",
      "Processing batch 3062/11884 - Val_Loss: 25.8363\n",
      "Processing batch 3063/11884 - Val_Loss: 25.8936\n",
      "Processing batch 3064/11884 - Val_Loss: 25.9134\n",
      "Processing batch 3065/11884 - Val_Loss: 23.9587\n",
      "Processing batch 3066/11884 - Val_Loss: 24.3208\n",
      "Processing batch 3067/11884 - Val_Loss: 26.4993\n",
      "Processing batch 3068/11884 - Val_Loss: 25.9511\n",
      "Processing batch 3069/11884 - Val_Loss: 25.5916\n",
      "Processing batch 3070/11884 - Val_Loss: 24.3076\n",
      "Processing batch 3071/11884 - Val_Loss: 24.4389\n",
      "Processing batch 3072/11884 - Val_Loss: 36.7037\n",
      "Processing batch 3073/11884 - Val_Loss: 25.9095\n",
      "Processing batch 3074/11884 - Val_Loss: 28.7784\n",
      "Processing batch 3075/11884 - Val_Loss: 27.1311\n",
      "Processing batch 3076/11884 - Val_Loss: 24.1786\n",
      "Processing batch 3077/11884 - Val_Loss: 23.4638\n",
      "Processing batch 3078/11884 - Val_Loss: 24.8277\n",
      "Processing batch 3079/11884 - Val_Loss: 28.6137\n",
      "Processing batch 3080/11884 - Val_Loss: 25.1333\n",
      "Processing batch 3081/11884 - Val_Loss: 28.2510\n",
      "Processing batch 3082/11884 - Val_Loss: 22.4146\n",
      "Processing batch 3083/11884 - Val_Loss: 25.1197\n",
      "Processing batch 3084/11884 - Val_Loss: 26.0820\n",
      "Processing batch 3085/11884 - Val_Loss: 23.6281\n",
      "Processing batch 3086/11884 - Val_Loss: 24.7413\n",
      "Processing batch 3087/11884 - Val_Loss: 24.6701\n",
      "Processing batch 3088/11884 - Val_Loss: 25.3088\n",
      "Processing batch 3089/11884 - Val_Loss: 25.9746\n",
      "Processing batch 3090/11884 - Val_Loss: 27.0149\n",
      "Processing batch 3091/11884 - Val_Loss: 28.3726\n",
      "Processing batch 3092/11884 - Val_Loss: 27.0266\n",
      "Processing batch 3093/11884 - Val_Loss: 24.8986\n",
      "Processing batch 3094/11884 - Val_Loss: 24.2072\n",
      "Processing batch 3095/11884 - Val_Loss: 23.9536\n",
      "Processing batch 3096/11884 - Val_Loss: 24.3359\n",
      "Processing batch 3097/11884 - Val_Loss: 22.4267\n",
      "Processing batch 3098/11884 - Val_Loss: 24.4935\n",
      "Processing batch 3099/11884 - Val_Loss: 26.0247\n",
      "Processing batch 3100/11884 - Val_Loss: 23.5219\n",
      "Processing batch 3101/11884 - Val_Loss: 26.1293\n",
      "Processing batch 3102/11884 - Val_Loss: 26.2908\n",
      "Processing batch 3103/11884 - Val_Loss: 24.3414\n",
      "Processing batch 3104/11884 - Val_Loss: 25.8167\n",
      "Processing batch 3105/11884 - Val_Loss: 25.7285\n",
      "Processing batch 3106/11884 - Val_Loss: 24.0303\n",
      "Processing batch 3107/11884 - Val_Loss: 22.8012\n",
      "Processing batch 3108/11884 - Val_Loss: 25.0837\n",
      "Processing batch 3109/11884 - Val_Loss: 27.4317\n",
      "Processing batch 3110/11884 - Val_Loss: 26.2405\n",
      "Processing batch 3111/11884 - Val_Loss: 21.8474\n",
      "Processing batch 3112/11884 - Val_Loss: 25.5111\n",
      "Processing batch 3113/11884 - Val_Loss: 23.9275\n",
      "Processing batch 3114/11884 - Val_Loss: 27.2650\n",
      "Processing batch 3115/11884 - Val_Loss: 25.3754\n",
      "Processing batch 3116/11884 - Val_Loss: 26.3568\n",
      "Processing batch 3117/11884 - Val_Loss: 26.1465\n",
      "Processing batch 3118/11884 - Val_Loss: 25.9828\n",
      "Processing batch 3119/11884 - Val_Loss: 26.9704\n",
      "Processing batch 3120/11884 - Val_Loss: 27.1067\n",
      "Processing batch 3121/11884 - Val_Loss: 26.3241\n",
      "Processing batch 3122/11884 - Val_Loss: 22.2245\n",
      "Processing batch 3123/11884 - Val_Loss: 23.4771\n",
      "Processing batch 3124/11884 - Val_Loss: 25.6443\n",
      "Processing batch 3125/11884 - Val_Loss: 25.8626\n",
      "Processing batch 3126/11884 - Val_Loss: 23.4720\n",
      "Processing batch 3127/11884 - Val_Loss: 24.4268\n",
      "Processing batch 3128/11884 - Val_Loss: 24.3274\n",
      "Processing batch 3129/11884 - Val_Loss: 29.6848\n",
      "Processing batch 3130/11884 - Val_Loss: 24.8073\n",
      "Processing batch 3131/11884 - Val_Loss: 24.0508\n",
      "Processing batch 3132/11884 - Val_Loss: 25.8130\n",
      "Processing batch 3133/11884 - Val_Loss: 26.0000\n",
      "Processing batch 3134/11884 - Val_Loss: 24.6217\n",
      "Processing batch 3135/11884 - Val_Loss: 25.2444\n",
      "Processing batch 3136/11884 - Val_Loss: 25.2563\n",
      "Processing batch 3137/11884 - Val_Loss: 26.3250\n",
      "Processing batch 3138/11884 - Val_Loss: 25.5675\n",
      "Processing batch 3139/11884 - Val_Loss: 24.5928\n",
      "Processing batch 3140/11884 - Val_Loss: 25.1336\n",
      "Processing batch 3141/11884 - Val_Loss: 25.3121\n",
      "Processing batch 3142/11884 - Val_Loss: 24.0294\n",
      "Processing batch 3143/11884 - Val_Loss: 25.4460\n",
      "Processing batch 3144/11884 - Val_Loss: 24.2402\n",
      "Processing batch 3145/11884 - Val_Loss: 23.8092\n",
      "Processing batch 3146/11884 - Val_Loss: 26.1925\n",
      "Processing batch 3147/11884 - Val_Loss: 23.6912\n",
      "Processing batch 3148/11884 - Val_Loss: 25.4680\n",
      "Processing batch 3149/11884 - Val_Loss: 28.0118\n",
      "Processing batch 3150/11884 - Val_Loss: 25.6483\n",
      "Processing batch 3151/11884 - Val_Loss: 26.9506\n",
      "Processing batch 3152/11884 - Val_Loss: 23.7476\n",
      "Processing batch 3153/11884 - Val_Loss: 24.1183\n",
      "Processing batch 3154/11884 - Val_Loss: 25.6037\n",
      "Processing batch 3155/11884 - Val_Loss: 24.3593\n",
      "Processing batch 3156/11884 - Val_Loss: 24.7735\n",
      "Processing batch 3157/11884 - Val_Loss: 25.8250\n",
      "Processing batch 3158/11884 - Val_Loss: 24.1702\n",
      "Processing batch 3159/11884 - Val_Loss: 27.8810\n",
      "Processing batch 3160/11884 - Val_Loss: 21.5970\n",
      "Processing batch 3161/11884 - Val_Loss: 23.7395\n",
      "Processing batch 3162/11884 - Val_Loss: 26.7113\n",
      "Processing batch 3163/11884 - Val_Loss: 25.7803\n",
      "Processing batch 3164/11884 - Val_Loss: 26.4730\n",
      "Processing batch 3165/11884 - Val_Loss: 28.3681\n",
      "Processing batch 3166/11884 - Val_Loss: 25.6425\n",
      "Processing batch 3167/11884 - Val_Loss: 25.8223\n",
      "Processing batch 3168/11884 - Val_Loss: 27.3064\n",
      "Processing batch 3169/11884 - Val_Loss: 26.2815\n",
      "Processing batch 3170/11884 - Val_Loss: 29.0345\n",
      "Processing batch 3171/11884 - Val_Loss: 22.9884\n",
      "Processing batch 3172/11884 - Val_Loss: 26.5013\n",
      "Processing batch 3173/11884 - Val_Loss: 23.9395\n",
      "Processing batch 3174/11884 - Val_Loss: 25.2395\n",
      "Processing batch 3175/11884 - Val_Loss: 23.6593\n",
      "Processing batch 3176/11884 - Val_Loss: 27.9280\n",
      "Processing batch 3177/11884 - Val_Loss: 23.1208\n",
      "Processing batch 3178/11884 - Val_Loss: 26.5682\n",
      "Processing batch 3179/11884 - Val_Loss: 24.9392\n",
      "Processing batch 3180/11884 - Val_Loss: 25.6162\n",
      "Processing batch 3181/11884 - Val_Loss: 25.9116\n",
      "Processing batch 3182/11884 - Val_Loss: 23.9864\n",
      "Processing batch 3183/11884 - Val_Loss: 26.8166\n",
      "Processing batch 3184/11884 - Val_Loss: 27.9366\n",
      "Processing batch 3185/11884 - Val_Loss: 26.2781\n",
      "Processing batch 3186/11884 - Val_Loss: 24.2815\n",
      "Processing batch 3187/11884 - Val_Loss: 24.8131\n",
      "Processing batch 3188/11884 - Val_Loss: 27.7626\n",
      "Processing batch 3189/11884 - Val_Loss: 25.0400\n",
      "Processing batch 3190/11884 - Val_Loss: 23.7727\n",
      "Processing batch 3191/11884 - Val_Loss: 25.5304\n",
      "Processing batch 3192/11884 - Val_Loss: 25.4772\n",
      "Processing batch 3193/11884 - Val_Loss: 24.0099\n",
      "Processing batch 3194/11884 - Val_Loss: 26.7726\n",
      "Processing batch 3195/11884 - Val_Loss: 23.4383\n",
      "Processing batch 3196/11884 - Val_Loss: 21.8239\n",
      "Processing batch 3197/11884 - Val_Loss: 26.9095\n",
      "Processing batch 3198/11884 - Val_Loss: 24.6495\n",
      "Processing batch 3199/11884 - Val_Loss: 25.9498\n",
      "Processing batch 3200/11884 - Val_Loss: 25.2825\n",
      "Processing batch 3201/11884 - Val_Loss: 25.7838\n",
      "Processing batch 3202/11884 - Val_Loss: 24.3080\n",
      "Processing batch 3203/11884 - Val_Loss: 25.1853\n",
      "Processing batch 3204/11884 - Val_Loss: 26.1792\n",
      "Processing batch 3205/11884 - Val_Loss: 27.8197\n",
      "Processing batch 3206/11884 - Val_Loss: 26.5045\n",
      "Processing batch 3207/11884 - Val_Loss: 29.6098\n",
      "Processing batch 3208/11884 - Val_Loss: 27.0059\n",
      "Processing batch 3209/11884 - Val_Loss: 26.1221\n",
      "Processing batch 3210/11884 - Val_Loss: 25.4750\n",
      "Processing batch 3211/11884 - Val_Loss: 23.5448\n",
      "Processing batch 3212/11884 - Val_Loss: 24.8846\n",
      "Processing batch 3213/11884 - Val_Loss: 26.4081\n",
      "Processing batch 3214/11884 - Val_Loss: 24.7548\n",
      "Processing batch 3215/11884 - Val_Loss: 26.2968\n",
      "Processing batch 3216/11884 - Val_Loss: 26.7051\n",
      "Processing batch 3217/11884 - Val_Loss: 25.3449\n",
      "Processing batch 3218/11884 - Val_Loss: 28.8181\n",
      "Processing batch 3219/11884 - Val_Loss: 26.3914\n",
      "Processing batch 3220/11884 - Val_Loss: 24.5059\n",
      "Processing batch 3221/11884 - Val_Loss: 24.5607\n",
      "Processing batch 3222/11884 - Val_Loss: 27.0668\n",
      "Processing batch 3223/11884 - Val_Loss: 25.3991\n",
      "Processing batch 3224/11884 - Val_Loss: 24.4193\n",
      "Processing batch 3225/11884 - Val_Loss: 27.7751\n",
      "Processing batch 3226/11884 - Val_Loss: 24.0902\n",
      "Processing batch 3227/11884 - Val_Loss: 26.8784\n",
      "Processing batch 3228/11884 - Val_Loss: 26.7498\n",
      "Processing batch 3229/11884 - Val_Loss: 26.7998\n",
      "Processing batch 3230/11884 - Val_Loss: 26.3308\n",
      "Processing batch 3231/11884 - Val_Loss: 24.3554\n",
      "Processing batch 3232/11884 - Val_Loss: 22.2992\n",
      "Processing batch 3233/11884 - Val_Loss: 27.1643\n",
      "Processing batch 3234/11884 - Val_Loss: 26.3745\n",
      "Processing batch 3235/11884 - Val_Loss: 29.0444\n",
      "Processing batch 3236/11884 - Val_Loss: 26.5968\n",
      "Processing batch 3237/11884 - Val_Loss: 26.5585\n",
      "Processing batch 3238/11884 - Val_Loss: 27.5978\n",
      "Processing batch 3239/11884 - Val_Loss: 24.2724\n",
      "Processing batch 3240/11884 - Val_Loss: 26.4017\n",
      "Processing batch 3241/11884 - Val_Loss: 22.3616\n",
      "Processing batch 3242/11884 - Val_Loss: 27.2593\n",
      "Processing batch 3243/11884 - Val_Loss: 26.0213\n",
      "Processing batch 3244/11884 - Val_Loss: 22.8109\n",
      "Processing batch 3245/11884 - Val_Loss: 22.9986\n",
      "Processing batch 3246/11884 - Val_Loss: 26.4364\n",
      "Processing batch 3247/11884 - Val_Loss: 28.1738\n",
      "Processing batch 3248/11884 - Val_Loss: 25.1047\n",
      "Processing batch 3249/11884 - Val_Loss: 25.9932\n",
      "Processing batch 3250/11884 - Val_Loss: 24.3098\n",
      "Processing batch 3251/11884 - Val_Loss: 23.3434\n",
      "Processing batch 3252/11884 - Val_Loss: 23.7774\n",
      "Processing batch 3253/11884 - Val_Loss: 29.0772\n",
      "Processing batch 3254/11884 - Val_Loss: 26.4815\n",
      "Processing batch 3255/11884 - Val_Loss: 27.0103\n",
      "Processing batch 3256/11884 - Val_Loss: 23.6574\n",
      "Processing batch 3257/11884 - Val_Loss: 27.5884\n",
      "Processing batch 3258/11884 - Val_Loss: 27.1052\n",
      "Processing batch 3259/11884 - Val_Loss: 23.2369\n",
      "Processing batch 3260/11884 - Val_Loss: 26.9848\n",
      "Processing batch 3261/11884 - Val_Loss: 26.6810\n",
      "Processing batch 3262/11884 - Val_Loss: 27.0267\n",
      "Processing batch 3263/11884 - Val_Loss: 23.5474\n",
      "Processing batch 3264/11884 - Val_Loss: 26.5192\n",
      "Processing batch 3265/11884 - Val_Loss: 25.7750\n",
      "Processing batch 3266/11884 - Val_Loss: 24.7303\n",
      "Processing batch 3267/11884 - Val_Loss: 24.6581\n",
      "Processing batch 3268/11884 - Val_Loss: 26.0883\n",
      "Processing batch 3269/11884 - Val_Loss: 22.7041\n",
      "Processing batch 3270/11884 - Val_Loss: 23.6602\n",
      "Processing batch 3271/11884 - Val_Loss: 29.2334\n",
      "Processing batch 3272/11884 - Val_Loss: 28.0340\n",
      "Processing batch 3273/11884 - Val_Loss: 28.4650\n",
      "Processing batch 3274/11884 - Val_Loss: 24.1052\n",
      "Processing batch 3275/11884 - Val_Loss: 26.2866\n",
      "Processing batch 3276/11884 - Val_Loss: 25.6005\n",
      "Processing batch 3277/11884 - Val_Loss: 25.9299\n",
      "Processing batch 3278/11884 - Val_Loss: 29.5794\n",
      "Processing batch 3279/11884 - Val_Loss: 25.6461\n",
      "Processing batch 3280/11884 - Val_Loss: 26.1807\n",
      "Processing batch 3281/11884 - Val_Loss: 24.1621\n",
      "Processing batch 3282/11884 - Val_Loss: 22.2378\n",
      "Processing batch 3283/11884 - Val_Loss: 26.4691\n",
      "Processing batch 3284/11884 - Val_Loss: 23.2123\n",
      "Processing batch 3285/11884 - Val_Loss: 26.7761\n",
      "Processing batch 3286/11884 - Val_Loss: 23.8213\n",
      "Processing batch 3287/11884 - Val_Loss: 24.1230\n",
      "Processing batch 3288/11884 - Val_Loss: 25.7899\n",
      "Processing batch 3289/11884 - Val_Loss: 24.5044\n",
      "Processing batch 3290/11884 - Val_Loss: 26.2961\n",
      "Processing batch 3291/11884 - Val_Loss: 25.4993\n",
      "Processing batch 3292/11884 - Val_Loss: 24.9524\n",
      "Processing batch 3293/11884 - Val_Loss: 24.4363\n",
      "Processing batch 3294/11884 - Val_Loss: 25.8645\n",
      "Processing batch 3295/11884 - Val_Loss: 25.3819\n",
      "Processing batch 3296/11884 - Val_Loss: 23.4848\n",
      "Processing batch 3297/11884 - Val_Loss: 24.4334\n",
      "Processing batch 3298/11884 - Val_Loss: 24.8535\n",
      "Processing batch 3299/11884 - Val_Loss: 25.2837\n",
      "Processing batch 3300/11884 - Val_Loss: 29.8374\n",
      "Processing batch 3301/11884 - Val_Loss: 23.9393\n",
      "Processing batch 3302/11884 - Val_Loss: 23.8125\n",
      "Processing batch 3303/11884 - Val_Loss: 24.0505\n",
      "Processing batch 3304/11884 - Val_Loss: 25.5876\n",
      "Processing batch 3305/11884 - Val_Loss: 27.9247\n",
      "Processing batch 3306/11884 - Val_Loss: 26.3405\n",
      "Processing batch 3307/11884 - Val_Loss: 25.6473\n",
      "Processing batch 3308/11884 - Val_Loss: 23.8566\n",
      "Processing batch 3309/11884 - Val_Loss: 25.9444\n",
      "Processing batch 3310/11884 - Val_Loss: 28.8500\n",
      "Processing batch 3311/11884 - Val_Loss: 26.0284\n",
      "Processing batch 3312/11884 - Val_Loss: 24.1956\n",
      "Processing batch 3313/11884 - Val_Loss: 22.6821\n",
      "Processing batch 3314/11884 - Val_Loss: 24.0947\n",
      "Processing batch 3315/11884 - Val_Loss: 23.6668\n",
      "Processing batch 3316/11884 - Val_Loss: 23.5469\n",
      "Processing batch 3317/11884 - Val_Loss: 28.0789\n",
      "Processing batch 3318/11884 - Val_Loss: 25.4341\n",
      "Processing batch 3319/11884 - Val_Loss: 25.1761\n",
      "Processing batch 3320/11884 - Val_Loss: 27.2957\n",
      "Processing batch 3321/11884 - Val_Loss: 23.7596\n",
      "Processing batch 3322/11884 - Val_Loss: 26.1953\n",
      "Processing batch 3323/11884 - Val_Loss: 25.3199\n",
      "Processing batch 3324/11884 - Val_Loss: 23.5853\n",
      "Processing batch 3325/11884 - Val_Loss: 26.4115\n",
      "Processing batch 3326/11884 - Val_Loss: 24.7172\n",
      "Processing batch 3327/11884 - Val_Loss: 24.6785\n",
      "Processing batch 3328/11884 - Val_Loss: 25.5024\n",
      "Processing batch 3329/11884 - Val_Loss: 25.9969\n",
      "Processing batch 3330/11884 - Val_Loss: 25.2913\n",
      "Processing batch 3331/11884 - Val_Loss: 27.5792\n",
      "Processing batch 3332/11884 - Val_Loss: 24.2288\n",
      "Processing batch 3333/11884 - Val_Loss: 28.6666\n",
      "Processing batch 3334/11884 - Val_Loss: 25.5438\n",
      "Processing batch 3335/11884 - Val_Loss: 28.3933\n",
      "Processing batch 3336/11884 - Val_Loss: 23.2711\n",
      "Processing batch 3337/11884 - Val_Loss: 24.4356\n",
      "Processing batch 3338/11884 - Val_Loss: 26.6988\n",
      "Processing batch 3339/11884 - Val_Loss: 24.8617\n",
      "Processing batch 3340/11884 - Val_Loss: 26.1620\n",
      "Processing batch 3341/11884 - Val_Loss: 25.8025\n",
      "Processing batch 3342/11884 - Val_Loss: 25.6432\n",
      "Processing batch 3343/11884 - Val_Loss: 28.0559\n",
      "Processing batch 3344/11884 - Val_Loss: 27.7962\n",
      "Processing batch 3345/11884 - Val_Loss: 27.2192\n",
      "Processing batch 3346/11884 - Val_Loss: 22.7434\n",
      "Processing batch 3347/11884 - Val_Loss: 26.4160\n",
      "Processing batch 3348/11884 - Val_Loss: 23.8564\n",
      "Processing batch 3349/11884 - Val_Loss: 23.4077\n",
      "Processing batch 3350/11884 - Val_Loss: 24.5574\n",
      "Processing batch 3351/11884 - Val_Loss: 24.6887\n",
      "Processing batch 3352/11884 - Val_Loss: 25.4249\n",
      "Processing batch 3353/11884 - Val_Loss: 25.8639\n",
      "Processing batch 3354/11884 - Val_Loss: 24.5101\n",
      "Processing batch 3355/11884 - Val_Loss: 23.7537\n",
      "Processing batch 3356/11884 - Val_Loss: 24.0658\n",
      "Processing batch 3357/11884 - Val_Loss: 22.6460\n",
      "Processing batch 3358/11884 - Val_Loss: 25.1228\n",
      "Processing batch 3359/11884 - Val_Loss: 25.0250\n",
      "Processing batch 3360/11884 - Val_Loss: 24.8666\n",
      "Processing batch 3361/11884 - Val_Loss: 26.6083\n",
      "Processing batch 3362/11884 - Val_Loss: 23.3765\n",
      "Processing batch 3363/11884 - Val_Loss: 25.6834\n",
      "Processing batch 3364/11884 - Val_Loss: 25.8140\n",
      "Processing batch 3365/11884 - Val_Loss: 28.4847\n",
      "Processing batch 3366/11884 - Val_Loss: 26.6268\n",
      "Processing batch 3367/11884 - Val_Loss: 25.6142\n",
      "Processing batch 3368/11884 - Val_Loss: 23.8087\n",
      "Processing batch 3369/11884 - Val_Loss: 25.7707\n",
      "Processing batch 3370/11884 - Val_Loss: 26.6509\n",
      "Processing batch 3371/11884 - Val_Loss: 24.6104\n",
      "Processing batch 3372/11884 - Val_Loss: 23.8876\n",
      "Processing batch 3373/11884 - Val_Loss: 25.1469\n",
      "Processing batch 3374/11884 - Val_Loss: 28.6147\n",
      "Processing batch 3375/11884 - Val_Loss: 26.2637\n",
      "Processing batch 3376/11884 - Val_Loss: 24.0305\n",
      "Processing batch 3377/11884 - Val_Loss: 28.2483\n",
      "Processing batch 3378/11884 - Val_Loss: 27.2302\n",
      "Processing batch 3379/11884 - Val_Loss: 27.8076\n",
      "Processing batch 3380/11884 - Val_Loss: 24.4994\n",
      "Processing batch 3381/11884 - Val_Loss: 24.8842\n",
      "Processing batch 3382/11884 - Val_Loss: 27.1103\n",
      "Processing batch 3383/11884 - Val_Loss: 23.9661\n",
      "Processing batch 3384/11884 - Val_Loss: 26.7170\n",
      "Processing batch 3385/11884 - Val_Loss: 24.8591\n",
      "Processing batch 3386/11884 - Val_Loss: 24.9935\n",
      "Processing batch 3387/11884 - Val_Loss: 23.3953\n",
      "Processing batch 3388/11884 - Val_Loss: 26.2704\n",
      "Processing batch 3389/11884 - Val_Loss: 25.4645\n",
      "Processing batch 3390/11884 - Val_Loss: 25.1634\n",
      "Processing batch 3391/11884 - Val_Loss: 23.6361\n",
      "Processing batch 3392/11884 - Val_Loss: 26.1226\n",
      "Processing batch 3393/11884 - Val_Loss: 23.3188\n",
      "Processing batch 3394/11884 - Val_Loss: 25.0354\n",
      "Processing batch 3395/11884 - Val_Loss: 25.8949\n",
      "Processing batch 3396/11884 - Val_Loss: 25.7167\n",
      "Processing batch 3397/11884 - Val_Loss: 26.4820\n",
      "Processing batch 3398/11884 - Val_Loss: 25.4061\n",
      "Processing batch 3399/11884 - Val_Loss: 27.9074\n",
      "Processing batch 3400/11884 - Val_Loss: 26.0170\n",
      "Processing batch 3401/11884 - Val_Loss: 25.7416\n",
      "Processing batch 3402/11884 - Val_Loss: 25.3579\n",
      "Processing batch 3403/11884 - Val_Loss: 26.1143\n",
      "Processing batch 3404/11884 - Val_Loss: 25.2076\n",
      "Processing batch 3405/11884 - Val_Loss: 27.1948\n",
      "Processing batch 3406/11884 - Val_Loss: 24.1876\n",
      "Processing batch 3407/11884 - Val_Loss: 25.6430\n",
      "Processing batch 3408/11884 - Val_Loss: 24.0732\n",
      "Processing batch 3409/11884 - Val_Loss: 25.0068\n",
      "Processing batch 3410/11884 - Val_Loss: 25.6713\n",
      "Processing batch 3411/11884 - Val_Loss: 25.0252\n",
      "Processing batch 3412/11884 - Val_Loss: 24.5555\n",
      "Processing batch 3413/11884 - Val_Loss: 26.6674\n",
      "Processing batch 3414/11884 - Val_Loss: 24.3790\n",
      "Processing batch 3415/11884 - Val_Loss: 26.6393\n",
      "Processing batch 3416/11884 - Val_Loss: 27.1232\n",
      "Processing batch 3417/11884 - Val_Loss: 22.5994\n",
      "Processing batch 3418/11884 - Val_Loss: 23.2042\n",
      "Processing batch 3419/11884 - Val_Loss: 27.5735\n",
      "Processing batch 3420/11884 - Val_Loss: 25.1264\n",
      "Processing batch 3421/11884 - Val_Loss: 28.4881\n",
      "Processing batch 3422/11884 - Val_Loss: 24.4565\n",
      "Processing batch 3423/11884 - Val_Loss: 25.1670\n",
      "Processing batch 3424/11884 - Val_Loss: 27.3717\n",
      "Processing batch 3425/11884 - Val_Loss: 24.6779\n",
      "Processing batch 3426/11884 - Val_Loss: 26.0555\n",
      "Processing batch 3427/11884 - Val_Loss: 24.8894\n",
      "Processing batch 3428/11884 - Val_Loss: 23.0728\n",
      "Processing batch 3429/11884 - Val_Loss: 27.7976\n",
      "Processing batch 3430/11884 - Val_Loss: 22.6584\n",
      "Processing batch 3431/11884 - Val_Loss: 26.7731\n",
      "Processing batch 3432/11884 - Val_Loss: 25.2358\n",
      "Processing batch 3433/11884 - Val_Loss: 23.8697\n",
      "Processing batch 3434/11884 - Val_Loss: 24.6365\n",
      "Processing batch 3435/11884 - Val_Loss: 24.6308\n",
      "Processing batch 3436/11884 - Val_Loss: 22.2798\n",
      "Processing batch 3437/11884 - Val_Loss: 24.5147\n",
      "Processing batch 3438/11884 - Val_Loss: 24.4572\n",
      "Processing batch 3439/11884 - Val_Loss: 23.2351\n",
      "Processing batch 3440/11884 - Val_Loss: 25.8697\n",
      "Processing batch 3441/11884 - Val_Loss: 22.6634\n",
      "Processing batch 3442/11884 - Val_Loss: 28.9826\n",
      "Processing batch 3443/11884 - Val_Loss: 27.6823\n",
      "Processing batch 3444/11884 - Val_Loss: 25.9396\n",
      "Processing batch 3445/11884 - Val_Loss: 25.2208\n",
      "Processing batch 3446/11884 - Val_Loss: 24.8030\n",
      "Processing batch 3447/11884 - Val_Loss: 26.5077\n",
      "Processing batch 3448/11884 - Val_Loss: 24.2435\n",
      "Processing batch 3449/11884 - Val_Loss: 25.1084\n",
      "Processing batch 3450/11884 - Val_Loss: 23.2182\n",
      "Processing batch 3451/11884 - Val_Loss: 26.8104\n",
      "Processing batch 3452/11884 - Val_Loss: 22.4549\n",
      "Processing batch 3453/11884 - Val_Loss: 24.9879\n",
      "Processing batch 3454/11884 - Val_Loss: 28.1874\n",
      "Processing batch 3455/11884 - Val_Loss: 24.8109\n",
      "Processing batch 3456/11884 - Val_Loss: 28.2378\n",
      "Processing batch 3457/11884 - Val_Loss: 26.8978\n",
      "Processing batch 3458/11884 - Val_Loss: 23.8774\n",
      "Processing batch 3459/11884 - Val_Loss: 25.2386\n",
      "Processing batch 3460/11884 - Val_Loss: 25.7232\n",
      "Processing batch 3461/11884 - Val_Loss: 23.0361\n",
      "Processing batch 3462/11884 - Val_Loss: 26.7226\n",
      "Processing batch 3463/11884 - Val_Loss: 28.2326\n",
      "Processing batch 3464/11884 - Val_Loss: 25.2116\n",
      "Processing batch 3465/11884 - Val_Loss: 27.5477\n",
      "Processing batch 3466/11884 - Val_Loss: 27.1481\n",
      "Processing batch 3467/11884 - Val_Loss: 29.0560\n",
      "Processing batch 3468/11884 - Val_Loss: 24.2829\n",
      "Processing batch 3469/11884 - Val_Loss: 26.7734\n",
      "Processing batch 3470/11884 - Val_Loss: 23.3798\n",
      "Processing batch 3471/11884 - Val_Loss: 24.8968\n",
      "Processing batch 3472/11884 - Val_Loss: 25.8692\n",
      "Processing batch 3473/11884 - Val_Loss: 25.6798\n",
      "Processing batch 3474/11884 - Val_Loss: 24.6573\n",
      "Processing batch 3475/11884 - Val_Loss: 24.0175\n",
      "Processing batch 3476/11884 - Val_Loss: 28.3883\n",
      "Processing batch 3477/11884 - Val_Loss: 27.4459\n",
      "Processing batch 3478/11884 - Val_Loss: 26.5434\n",
      "Processing batch 3479/11884 - Val_Loss: 27.3359\n",
      "Processing batch 3480/11884 - Val_Loss: 28.7635\n",
      "Processing batch 3481/11884 - Val_Loss: 26.5117\n",
      "Processing batch 3482/11884 - Val_Loss: 25.8765\n",
      "Processing batch 3483/11884 - Val_Loss: 23.7928\n",
      "Processing batch 3484/11884 - Val_Loss: 26.3305\n",
      "Processing batch 3485/11884 - Val_Loss: 25.7171\n",
      "Processing batch 3486/11884 - Val_Loss: 26.9562\n",
      "Processing batch 3487/11884 - Val_Loss: 26.2247\n",
      "Processing batch 3488/11884 - Val_Loss: 28.6506\n",
      "Processing batch 3489/11884 - Val_Loss: 27.4456\n",
      "Processing batch 3490/11884 - Val_Loss: 24.5260\n",
      "Processing batch 3491/11884 - Val_Loss: 27.1477\n",
      "Processing batch 3492/11884 - Val_Loss: 28.7667\n",
      "Processing batch 3493/11884 - Val_Loss: 28.1528\n",
      "Processing batch 3494/11884 - Val_Loss: 28.2047\n",
      "Processing batch 3495/11884 - Val_Loss: 23.0737\n",
      "Processing batch 3496/11884 - Val_Loss: 25.4747\n",
      "Processing batch 3497/11884 - Val_Loss: 25.2942\n",
      "Processing batch 3498/11884 - Val_Loss: 27.4580\n",
      "Processing batch 3499/11884 - Val_Loss: 25.7306\n",
      "Processing batch 3500/11884 - Val_Loss: 24.6983\n",
      "Processing batch 3501/11884 - Val_Loss: 23.1545\n",
      "Processing batch 3502/11884 - Val_Loss: 27.3134\n",
      "Processing batch 3503/11884 - Val_Loss: 25.5318\n",
      "Processing batch 3504/11884 - Val_Loss: 27.5131\n",
      "Processing batch 3505/11884 - Val_Loss: 25.5385\n",
      "Processing batch 3506/11884 - Val_Loss: 23.1689\n",
      "Processing batch 3507/11884 - Val_Loss: 26.5817\n",
      "Processing batch 3508/11884 - Val_Loss: 24.7984\n",
      "Processing batch 3509/11884 - Val_Loss: 27.2611\n",
      "Processing batch 3510/11884 - Val_Loss: 25.1112\n",
      "Processing batch 3511/11884 - Val_Loss: 23.9304\n",
      "Processing batch 3512/11884 - Val_Loss: 26.2198\n",
      "Processing batch 3513/11884 - Val_Loss: 27.0772\n",
      "Processing batch 3514/11884 - Val_Loss: 29.2174\n",
      "Processing batch 3515/11884 - Val_Loss: 24.8262\n",
      "Processing batch 3516/11884 - Val_Loss: 27.0537\n",
      "Processing batch 3517/11884 - Val_Loss: 25.5629\n",
      "Processing batch 3518/11884 - Val_Loss: 26.9458\n",
      "Processing batch 3519/11884 - Val_Loss: 24.6983\n",
      "Processing batch 3520/11884 - Val_Loss: 24.5022\n",
      "Processing batch 3521/11884 - Val_Loss: 25.0396\n",
      "Processing batch 3522/11884 - Val_Loss: 25.6560\n",
      "Processing batch 3523/11884 - Val_Loss: 27.8058\n",
      "Processing batch 3524/11884 - Val_Loss: 24.5156\n",
      "Processing batch 3525/11884 - Val_Loss: 25.7580\n",
      "Processing batch 3526/11884 - Val_Loss: 27.0291\n",
      "Processing batch 3527/11884 - Val_Loss: 25.5773\n",
      "Processing batch 3528/11884 - Val_Loss: 26.4513\n",
      "Processing batch 3529/11884 - Val_Loss: 26.0456\n",
      "Processing batch 3530/11884 - Val_Loss: 26.2121\n",
      "Processing batch 3531/11884 - Val_Loss: 25.0651\n",
      "Processing batch 3532/11884 - Val_Loss: 26.6253\n",
      "Processing batch 3533/11884 - Val_Loss: 27.2713\n",
      "Processing batch 3534/11884 - Val_Loss: 23.9373\n",
      "Processing batch 3535/11884 - Val_Loss: 22.6088\n",
      "Processing batch 3536/11884 - Val_Loss: 22.7639\n",
      "Processing batch 3537/11884 - Val_Loss: 25.1839\n",
      "Processing batch 3538/11884 - Val_Loss: 23.2337\n",
      "Processing batch 3539/11884 - Val_Loss: 25.2949\n",
      "Processing batch 3540/11884 - Val_Loss: 26.0476\n",
      "Processing batch 3541/11884 - Val_Loss: 24.6211\n",
      "Processing batch 3542/11884 - Val_Loss: 25.7186\n",
      "Processing batch 3543/11884 - Val_Loss: 26.9776\n",
      "Processing batch 3544/11884 - Val_Loss: 23.7999\n",
      "Processing batch 3545/11884 - Val_Loss: 27.5276\n",
      "Processing batch 3546/11884 - Val_Loss: 23.6409\n",
      "Processing batch 3547/11884 - Val_Loss: 26.7845\n",
      "Processing batch 3548/11884 - Val_Loss: 24.3405\n",
      "Processing batch 3549/11884 - Val_Loss: 28.5126\n",
      "Processing batch 3550/11884 - Val_Loss: 26.2106\n",
      "Processing batch 3551/11884 - Val_Loss: 26.7276\n",
      "Processing batch 3552/11884 - Val_Loss: 22.8561\n",
      "Processing batch 3553/11884 - Val_Loss: 26.0818\n",
      "Processing batch 3554/11884 - Val_Loss: 24.4607\n",
      "Processing batch 3555/11884 - Val_Loss: 23.3051\n",
      "Processing batch 3556/11884 - Val_Loss: 22.0926\n",
      "Processing batch 3557/11884 - Val_Loss: 25.8866\n",
      "Processing batch 3558/11884 - Val_Loss: 24.8952\n",
      "Processing batch 3559/11884 - Val_Loss: 28.9003\n",
      "Processing batch 3560/11884 - Val_Loss: 26.9030\n",
      "Processing batch 3561/11884 - Val_Loss: 24.9561\n",
      "Processing batch 3562/11884 - Val_Loss: 26.1122\n",
      "Processing batch 3563/11884 - Val_Loss: 24.4135\n",
      "Processing batch 3564/11884 - Val_Loss: 26.6319\n",
      "Processing batch 3565/11884 - Val_Loss: 25.9523\n",
      "Processing batch 3566/11884 - Val_Loss: 25.2225\n",
      "Processing batch 3567/11884 - Val_Loss: 27.0073\n",
      "Processing batch 3568/11884 - Val_Loss: 29.0282\n",
      "Processing batch 3569/11884 - Val_Loss: 26.9576\n",
      "Processing batch 3570/11884 - Val_Loss: 27.4125\n",
      "Processing batch 3571/11884 - Val_Loss: 25.2347\n",
      "Processing batch 3572/11884 - Val_Loss: 27.1420\n",
      "Processing batch 3573/11884 - Val_Loss: 27.7484\n",
      "Processing batch 3574/11884 - Val_Loss: 23.0784\n",
      "Processing batch 3575/11884 - Val_Loss: 28.0989\n",
      "Processing batch 3576/11884 - Val_Loss: 25.3812\n",
      "Processing batch 3577/11884 - Val_Loss: 24.5970\n",
      "Processing batch 3578/11884 - Val_Loss: 27.1130\n",
      "Processing batch 3579/11884 - Val_Loss: 26.1842\n",
      "Processing batch 3580/11884 - Val_Loss: 25.1932\n",
      "Processing batch 3581/11884 - Val_Loss: 25.2567\n",
      "Processing batch 3582/11884 - Val_Loss: 26.8948\n",
      "Processing batch 3583/11884 - Val_Loss: 26.0716\n",
      "Processing batch 3584/11884 - Val_Loss: 26.6407\n",
      "Processing batch 3585/11884 - Val_Loss: 24.8410\n",
      "Processing batch 3586/11884 - Val_Loss: 23.9175\n",
      "Processing batch 3587/11884 - Val_Loss: 27.6668\n",
      "Processing batch 3588/11884 - Val_Loss: 26.0562\n",
      "Processing batch 3589/11884 - Val_Loss: 21.3493\n",
      "Processing batch 3590/11884 - Val_Loss: 25.6006\n",
      "Processing batch 3591/11884 - Val_Loss: 23.0436\n",
      "Processing batch 3592/11884 - Val_Loss: 26.0258\n",
      "Processing batch 3593/11884 - Val_Loss: 25.5232\n",
      "Processing batch 3594/11884 - Val_Loss: 25.6762\n",
      "Processing batch 3595/11884 - Val_Loss: 23.0973\n",
      "Processing batch 3596/11884 - Val_Loss: 24.7792\n",
      "Processing batch 3597/11884 - Val_Loss: 24.4998\n",
      "Processing batch 3598/11884 - Val_Loss: 25.7558\n",
      "Processing batch 3599/11884 - Val_Loss: 27.8721\n",
      "Processing batch 3600/11884 - Val_Loss: 27.4609\n",
      "Processing batch 3601/11884 - Val_Loss: 22.5304\n",
      "Processing batch 3602/11884 - Val_Loss: 25.7873\n",
      "Processing batch 3603/11884 - Val_Loss: 22.7264\n",
      "Processing batch 3604/11884 - Val_Loss: 25.8214\n",
      "Processing batch 3605/11884 - Val_Loss: 25.7594\n",
      "Processing batch 3606/11884 - Val_Loss: 24.4686\n",
      "Processing batch 3607/11884 - Val_Loss: 26.8788\n",
      "Processing batch 3608/11884 - Val_Loss: 24.8836\n",
      "Processing batch 3609/11884 - Val_Loss: 25.5894\n",
      "Processing batch 3610/11884 - Val_Loss: 24.0439\n",
      "Processing batch 3611/11884 - Val_Loss: 24.6783\n",
      "Processing batch 3612/11884 - Val_Loss: 22.3571\n",
      "Processing batch 3613/11884 - Val_Loss: 25.6844\n",
      "Processing batch 3614/11884 - Val_Loss: 26.2180\n",
      "Processing batch 3615/11884 - Val_Loss: 25.2491\n",
      "Processing batch 3616/11884 - Val_Loss: 25.9549\n",
      "Processing batch 3617/11884 - Val_Loss: 24.5797\n",
      "Processing batch 3618/11884 - Val_Loss: 25.8537\n",
      "Processing batch 3619/11884 - Val_Loss: 23.4655\n",
      "Processing batch 3620/11884 - Val_Loss: 25.5624\n",
      "Processing batch 3621/11884 - Val_Loss: 24.8248\n",
      "Processing batch 3622/11884 - Val_Loss: 26.4237\n",
      "Processing batch 3623/11884 - Val_Loss: 26.1860\n",
      "Processing batch 3624/11884 - Val_Loss: 23.7787\n",
      "Processing batch 3625/11884 - Val_Loss: 26.3069\n",
      "Processing batch 3626/11884 - Val_Loss: 24.6449\n",
      "Processing batch 3627/11884 - Val_Loss: 24.3438\n",
      "Processing batch 3628/11884 - Val_Loss: 23.5199\n",
      "Processing batch 3629/11884 - Val_Loss: 25.5547\n",
      "Processing batch 3630/11884 - Val_Loss: 27.0155\n",
      "Processing batch 3631/11884 - Val_Loss: 25.1212\n",
      "Processing batch 3632/11884 - Val_Loss: 27.2630\n",
      "Processing batch 3633/11884 - Val_Loss: 26.4060\n",
      "Processing batch 3634/11884 - Val_Loss: 25.9868\n",
      "Processing batch 3635/11884 - Val_Loss: 28.1303\n",
      "Processing batch 3636/11884 - Val_Loss: 26.2938\n",
      "Processing batch 3637/11884 - Val_Loss: 25.8249\n",
      "Processing batch 3638/11884 - Val_Loss: 26.3901\n",
      "Processing batch 3639/11884 - Val_Loss: 24.4344\n",
      "Processing batch 3640/11884 - Val_Loss: 28.8175\n",
      "Processing batch 3641/11884 - Val_Loss: 25.5311\n",
      "Processing batch 3642/11884 - Val_Loss: 24.7921\n",
      "Processing batch 3643/11884 - Val_Loss: 25.5146\n",
      "Processing batch 3644/11884 - Val_Loss: 26.5224\n",
      "Processing batch 3645/11884 - Val_Loss: 23.4333\n",
      "Processing batch 3646/11884 - Val_Loss: 26.3144\n",
      "Processing batch 3647/11884 - Val_Loss: 23.7913\n",
      "Processing batch 3648/11884 - Val_Loss: 24.0217\n",
      "Processing batch 3649/11884 - Val_Loss: 25.5291\n",
      "Processing batch 3650/11884 - Val_Loss: 24.2920\n",
      "Processing batch 3651/11884 - Val_Loss: 24.8978\n",
      "Processing batch 3652/11884 - Val_Loss: 28.8678\n",
      "Processing batch 3653/11884 - Val_Loss: 25.5182\n",
      "Processing batch 3654/11884 - Val_Loss: 25.2756\n",
      "Processing batch 3655/11884 - Val_Loss: 25.3844\n",
      "Processing batch 3656/11884 - Val_Loss: 25.7618\n",
      "Processing batch 3657/11884 - Val_Loss: 26.4213\n",
      "Processing batch 3658/11884 - Val_Loss: 23.2758\n",
      "Processing batch 3659/11884 - Val_Loss: 24.7956\n",
      "Processing batch 3660/11884 - Val_Loss: 24.4662\n",
      "Processing batch 3661/11884 - Val_Loss: 26.9899\n",
      "Processing batch 3662/11884 - Val_Loss: 25.0176\n",
      "Processing batch 3663/11884 - Val_Loss: 22.9865\n",
      "Processing batch 3664/11884 - Val_Loss: 24.1667\n",
      "Processing batch 3665/11884 - Val_Loss: 24.2639\n",
      "Processing batch 3666/11884 - Val_Loss: 23.7376\n",
      "Processing batch 3667/11884 - Val_Loss: 28.5124\n",
      "Processing batch 3668/11884 - Val_Loss: 25.5285\n",
      "Processing batch 3669/11884 - Val_Loss: 22.7158\n",
      "Processing batch 3670/11884 - Val_Loss: 25.8337\n",
      "Processing batch 3671/11884 - Val_Loss: 29.7731\n",
      "Processing batch 3672/11884 - Val_Loss: 26.8392\n",
      "Processing batch 3673/11884 - Val_Loss: 25.6925\n",
      "Processing batch 3674/11884 - Val_Loss: 25.1482\n",
      "Processing batch 3675/11884 - Val_Loss: 24.2972\n",
      "Processing batch 3676/11884 - Val_Loss: 24.1641\n",
      "Processing batch 3677/11884 - Val_Loss: 24.4088\n",
      "Processing batch 3678/11884 - Val_Loss: 21.3777\n",
      "Processing batch 3679/11884 - Val_Loss: 25.2877\n",
      "Processing batch 3680/11884 - Val_Loss: 26.4685\n",
      "Processing batch 3681/11884 - Val_Loss: 24.5005\n",
      "Processing batch 3682/11884 - Val_Loss: 27.5013\n",
      "Processing batch 3683/11884 - Val_Loss: 25.3703\n",
      "Processing batch 3684/11884 - Val_Loss: 25.5887\n",
      "Processing batch 3685/11884 - Val_Loss: 23.0737\n",
      "Processing batch 3686/11884 - Val_Loss: 24.7108\n",
      "Processing batch 3687/11884 - Val_Loss: 24.8400\n",
      "Processing batch 3688/11884 - Val_Loss: 24.0109\n",
      "Processing batch 3689/11884 - Val_Loss: 22.4337\n",
      "Processing batch 3690/11884 - Val_Loss: 24.4282\n",
      "Processing batch 3691/11884 - Val_Loss: 23.6183\n",
      "Processing batch 3692/11884 - Val_Loss: 26.3864\n",
      "Processing batch 3693/11884 - Val_Loss: 24.0121\n",
      "Processing batch 3694/11884 - Val_Loss: 27.4151\n",
      "Processing batch 3695/11884 - Val_Loss: 25.5381\n",
      "Processing batch 3696/11884 - Val_Loss: 25.6582\n",
      "Processing batch 3697/11884 - Val_Loss: 25.6838\n",
      "Processing batch 3698/11884 - Val_Loss: 25.7001\n",
      "Processing batch 3699/11884 - Val_Loss: 20.5795\n",
      "Processing batch 3700/11884 - Val_Loss: 24.6344\n",
      "Processing batch 3701/11884 - Val_Loss: 22.4252\n",
      "Processing batch 3702/11884 - Val_Loss: 25.5770\n",
      "Processing batch 3703/11884 - Val_Loss: 23.3319\n",
      "Processing batch 3704/11884 - Val_Loss: 24.8947\n",
      "Processing batch 3705/11884 - Val_Loss: 24.2925\n",
      "Processing batch 3706/11884 - Val_Loss: 25.4439\n",
      "Processing batch 3707/11884 - Val_Loss: 24.9376\n",
      "Processing batch 3708/11884 - Val_Loss: 25.9392\n",
      "Processing batch 3709/11884 - Val_Loss: 22.6610\n",
      "Processing batch 3710/11884 - Val_Loss: 29.8207\n",
      "Processing batch 3711/11884 - Val_Loss: 27.0939\n",
      "Processing batch 3712/11884 - Val_Loss: 24.0607\n",
      "Processing batch 3713/11884 - Val_Loss: 24.9477\n",
      "Processing batch 3714/11884 - Val_Loss: 24.8399\n",
      "Processing batch 3715/11884 - Val_Loss: 22.8550\n",
      "Processing batch 3716/11884 - Val_Loss: 22.0835\n",
      "Processing batch 3717/11884 - Val_Loss: 25.1425\n",
      "Processing batch 3718/11884 - Val_Loss: 24.6730\n",
      "Processing batch 3719/11884 - Val_Loss: 26.6231\n",
      "Processing batch 3720/11884 - Val_Loss: 24.2294\n",
      "Processing batch 3721/11884 - Val_Loss: 24.9376\n",
      "Processing batch 3722/11884 - Val_Loss: 23.8303\n",
      "Processing batch 3723/11884 - Val_Loss: 23.2357\n",
      "Processing batch 3724/11884 - Val_Loss: 25.3048\n",
      "Processing batch 3725/11884 - Val_Loss: 27.3018\n",
      "Processing batch 3726/11884 - Val_Loss: 27.3296\n",
      "Processing batch 3727/11884 - Val_Loss: 27.4549\n",
      "Processing batch 3728/11884 - Val_Loss: 24.1565\n",
      "Processing batch 3729/11884 - Val_Loss: 25.5325\n",
      "Processing batch 3730/11884 - Val_Loss: 27.2832\n",
      "Processing batch 3731/11884 - Val_Loss: 28.3201\n",
      "Processing batch 3732/11884 - Val_Loss: 25.8796\n",
      "Processing batch 3733/11884 - Val_Loss: 26.6672\n",
      "Processing batch 3734/11884 - Val_Loss: 23.9665\n",
      "Processing batch 3735/11884 - Val_Loss: 27.0007\n",
      "Processing batch 3736/11884 - Val_Loss: 25.2670\n",
      "Processing batch 3737/11884 - Val_Loss: 23.7792\n",
      "Processing batch 3738/11884 - Val_Loss: 25.6767\n",
      "Processing batch 3739/11884 - Val_Loss: 23.1649\n",
      "Processing batch 3740/11884 - Val_Loss: 26.7532\n",
      "Processing batch 3741/11884 - Val_Loss: 24.2619\n",
      "Processing batch 3742/11884 - Val_Loss: 26.4188\n",
      "Processing batch 3743/11884 - Val_Loss: 26.9583\n",
      "Processing batch 3744/11884 - Val_Loss: 28.5543\n",
      "Processing batch 3745/11884 - Val_Loss: 24.3278\n",
      "Processing batch 3746/11884 - Val_Loss: 24.1767\n",
      "Processing batch 3747/11884 - Val_Loss: 24.4026\n",
      "Processing batch 3748/11884 - Val_Loss: 23.8033\n",
      "Processing batch 3749/11884 - Val_Loss: 23.0818\n",
      "Processing batch 3750/11884 - Val_Loss: 23.9425\n",
      "Processing batch 3751/11884 - Val_Loss: 26.9681\n",
      "Processing batch 3752/11884 - Val_Loss: 23.4365\n",
      "Processing batch 3753/11884 - Val_Loss: 26.1374\n",
      "Processing batch 3754/11884 - Val_Loss: 25.1950\n",
      "Processing batch 3755/11884 - Val_Loss: 26.6291\n",
      "Processing batch 3756/11884 - Val_Loss: 25.1058\n",
      "Processing batch 3757/11884 - Val_Loss: 23.1826\n",
      "Processing batch 3758/11884 - Val_Loss: 28.0021\n",
      "Processing batch 3759/11884 - Val_Loss: 26.2575\n",
      "Processing batch 3760/11884 - Val_Loss: 25.0146\n",
      "Processing batch 3761/11884 - Val_Loss: 27.8619\n",
      "Processing batch 3762/11884 - Val_Loss: 27.2163\n",
      "Processing batch 3763/11884 - Val_Loss: 24.2608\n",
      "Processing batch 3764/11884 - Val_Loss: 25.7231\n",
      "Processing batch 3765/11884 - Val_Loss: 25.0150\n",
      "Processing batch 3766/11884 - Val_Loss: 25.4167\n",
      "Processing batch 3767/11884 - Val_Loss: 25.6874\n",
      "Processing batch 3768/11884 - Val_Loss: 26.3978\n",
      "Processing batch 3769/11884 - Val_Loss: 24.5888\n",
      "Processing batch 3770/11884 - Val_Loss: 26.2924\n",
      "Processing batch 3771/11884 - Val_Loss: 26.9792\n",
      "Processing batch 3772/11884 - Val_Loss: 24.2844\n",
      "Processing batch 3773/11884 - Val_Loss: 27.3174\n",
      "Processing batch 3774/11884 - Val_Loss: 24.9437\n",
      "Processing batch 3775/11884 - Val_Loss: 25.1805\n",
      "Processing batch 3776/11884 - Val_Loss: 24.4644\n",
      "Processing batch 3777/11884 - Val_Loss: 28.2006\n",
      "Processing batch 3778/11884 - Val_Loss: 25.6550\n",
      "Processing batch 3779/11884 - Val_Loss: 23.3697\n",
      "Processing batch 3780/11884 - Val_Loss: 25.7899\n",
      "Processing batch 3781/11884 - Val_Loss: 26.3081\n",
      "Processing batch 3782/11884 - Val_Loss: 23.9720\n",
      "Processing batch 3783/11884 - Val_Loss: 25.8873\n",
      "Processing batch 3784/11884 - Val_Loss: 25.7016\n",
      "Processing batch 3785/11884 - Val_Loss: 27.1468\n",
      "Processing batch 3786/11884 - Val_Loss: 25.7595\n",
      "Processing batch 3787/11884 - Val_Loss: 23.8227\n",
      "Processing batch 3788/11884 - Val_Loss: 23.6535\n",
      "Processing batch 3789/11884 - Val_Loss: 21.1722\n",
      "Processing batch 3790/11884 - Val_Loss: 24.0310\n",
      "Processing batch 3791/11884 - Val_Loss: 25.8503\n",
      "Processing batch 3792/11884 - Val_Loss: 26.4598\n",
      "Processing batch 3793/11884 - Val_Loss: 23.8624\n",
      "Processing batch 3794/11884 - Val_Loss: 27.6793\n",
      "Processing batch 3795/11884 - Val_Loss: 26.0736\n",
      "Processing batch 3796/11884 - Val_Loss: 26.4133\n",
      "Processing batch 3797/11884 - Val_Loss: 23.6057\n",
      "Processing batch 3798/11884 - Val_Loss: 24.0532\n",
      "Processing batch 3799/11884 - Val_Loss: 25.2219\n",
      "Processing batch 3800/11884 - Val_Loss: 23.6232\n",
      "Processing batch 3801/11884 - Val_Loss: 25.4026\n",
      "Processing batch 3802/11884 - Val_Loss: 25.2632\n",
      "Processing batch 3803/11884 - Val_Loss: 27.5904\n",
      "Processing batch 3804/11884 - Val_Loss: 26.3739\n",
      "Processing batch 3805/11884 - Val_Loss: 26.1290\n",
      "Processing batch 3806/11884 - Val_Loss: 25.1120\n",
      "Processing batch 3807/11884 - Val_Loss: 26.3678\n",
      "Processing batch 3808/11884 - Val_Loss: 27.8047\n",
      "Processing batch 3809/11884 - Val_Loss: 25.6594\n",
      "Processing batch 3810/11884 - Val_Loss: 25.5889\n",
      "Processing batch 3811/11884 - Val_Loss: 25.5299\n",
      "Processing batch 3812/11884 - Val_Loss: 23.6547\n",
      "Processing batch 3813/11884 - Val_Loss: 27.4464\n",
      "Processing batch 3814/11884 - Val_Loss: 25.7323\n",
      "Processing batch 3815/11884 - Val_Loss: 26.5291\n",
      "Processing batch 3816/11884 - Val_Loss: 26.6432\n",
      "Processing batch 3817/11884 - Val_Loss: 26.2469\n",
      "Processing batch 3818/11884 - Val_Loss: 23.7793\n",
      "Processing batch 3819/11884 - Val_Loss: 23.7153\n",
      "Processing batch 3820/11884 - Val_Loss: 25.2808\n",
      "Processing batch 3821/11884 - Val_Loss: 28.4024\n",
      "Processing batch 3822/11884 - Val_Loss: 24.9165\n",
      "Processing batch 3823/11884 - Val_Loss: 24.9022\n",
      "Processing batch 3824/11884 - Val_Loss: 26.4567\n",
      "Processing batch 3825/11884 - Val_Loss: 26.4063\n",
      "Processing batch 3826/11884 - Val_Loss: 28.5942\n",
      "Processing batch 3827/11884 - Val_Loss: 26.4591\n",
      "Processing batch 3828/11884 - Val_Loss: 26.6921\n",
      "Processing batch 3829/11884 - Val_Loss: 23.9555\n",
      "Processing batch 3830/11884 - Val_Loss: 27.6693\n",
      "Processing batch 3831/11884 - Val_Loss: 22.2577\n",
      "Processing batch 3832/11884 - Val_Loss: 27.4300\n",
      "Processing batch 3833/11884 - Val_Loss: 25.7630\n",
      "Processing batch 3834/11884 - Val_Loss: 26.5479\n",
      "Processing batch 3835/11884 - Val_Loss: 26.0861\n",
      "Processing batch 3836/11884 - Val_Loss: 25.7980\n",
      "Processing batch 3837/11884 - Val_Loss: 22.0487\n",
      "Processing batch 3838/11884 - Val_Loss: 21.4656\n",
      "Processing batch 3839/11884 - Val_Loss: 25.1861\n",
      "Processing batch 3840/11884 - Val_Loss: 24.1407\n",
      "Processing batch 3841/11884 - Val_Loss: 25.7467\n",
      "Processing batch 3842/11884 - Val_Loss: 25.1868\n",
      "Processing batch 3843/11884 - Val_Loss: 24.0728\n",
      "Processing batch 3844/11884 - Val_Loss: 25.0723\n",
      "Processing batch 3845/11884 - Val_Loss: 25.6617\n",
      "Processing batch 3846/11884 - Val_Loss: 23.5798\n",
      "Processing batch 3847/11884 - Val_Loss: 27.2362\n",
      "Processing batch 3848/11884 - Val_Loss: 25.2467\n",
      "Processing batch 3849/11884 - Val_Loss: 25.9812\n",
      "Processing batch 3850/11884 - Val_Loss: 27.2416\n",
      "Processing batch 3851/11884 - Val_Loss: 25.2780\n",
      "Processing batch 3852/11884 - Val_Loss: 23.4772\n",
      "Processing batch 3853/11884 - Val_Loss: 26.5902\n",
      "Processing batch 3854/11884 - Val_Loss: 28.3964\n",
      "Processing batch 3855/11884 - Val_Loss: 25.2709\n",
      "Processing batch 3856/11884 - Val_Loss: 25.5691\n",
      "Processing batch 3857/11884 - Val_Loss: 20.0967\n",
      "Processing batch 3858/11884 - Val_Loss: 25.7604\n",
      "Processing batch 3859/11884 - Val_Loss: 23.6134\n",
      "Processing batch 3860/11884 - Val_Loss: 28.7360\n",
      "Processing batch 3861/11884 - Val_Loss: 24.0368\n",
      "Processing batch 3862/11884 - Val_Loss: 25.4159\n",
      "Processing batch 3863/11884 - Val_Loss: 26.9239\n",
      "Processing batch 3864/11884 - Val_Loss: 28.6447\n",
      "Processing batch 3865/11884 - Val_Loss: 27.3994\n",
      "Processing batch 3866/11884 - Val_Loss: 25.8775\n",
      "Processing batch 3867/11884 - Val_Loss: 23.9880\n",
      "Processing batch 3868/11884 - Val_Loss: 27.1858\n",
      "Processing batch 3869/11884 - Val_Loss: 24.9370\n",
      "Processing batch 3870/11884 - Val_Loss: 25.9573\n",
      "Processing batch 3871/11884 - Val_Loss: 26.6817\n",
      "Processing batch 3872/11884 - Val_Loss: 24.2931\n",
      "Processing batch 3873/11884 - Val_Loss: 24.4889\n",
      "Processing batch 3874/11884 - Val_Loss: 25.1962\n",
      "Processing batch 3875/11884 - Val_Loss: 27.0988\n",
      "Processing batch 3876/11884 - Val_Loss: 27.5814\n",
      "Processing batch 3877/11884 - Val_Loss: 24.3203\n",
      "Processing batch 3878/11884 - Val_Loss: 25.5383\n",
      "Processing batch 3879/11884 - Val_Loss: 25.1830\n",
      "Processing batch 3880/11884 - Val_Loss: 22.3961\n",
      "Processing batch 3881/11884 - Val_Loss: 26.4872\n",
      "Processing batch 3882/11884 - Val_Loss: 27.9774\n",
      "Processing batch 3883/11884 - Val_Loss: 22.7966\n",
      "Processing batch 3884/11884 - Val_Loss: 27.0239\n",
      "Processing batch 3885/11884 - Val_Loss: 25.7875\n",
      "Processing batch 3886/11884 - Val_Loss: 26.9610\n",
      "Processing batch 3887/11884 - Val_Loss: 26.7928\n",
      "Processing batch 3888/11884 - Val_Loss: 27.0502\n",
      "Processing batch 3889/11884 - Val_Loss: 26.2310\n",
      "Processing batch 3890/11884 - Val_Loss: 23.9736\n",
      "Processing batch 3891/11884 - Val_Loss: 22.0632\n",
      "Processing batch 3892/11884 - Val_Loss: 25.6802\n",
      "Processing batch 3893/11884 - Val_Loss: 24.3323\n",
      "Processing batch 3894/11884 - Val_Loss: 27.8309\n",
      "Processing batch 3895/11884 - Val_Loss: 24.6506\n",
      "Processing batch 3896/11884 - Val_Loss: 23.6692\n",
      "Processing batch 3897/11884 - Val_Loss: 26.0929\n",
      "Processing batch 3898/11884 - Val_Loss: 22.2167\n",
      "Processing batch 3899/11884 - Val_Loss: 25.6147\n",
      "Processing batch 3900/11884 - Val_Loss: 27.8920\n",
      "Processing batch 3901/11884 - Val_Loss: 26.1538\n",
      "Processing batch 3902/11884 - Val_Loss: 25.2945\n",
      "Processing batch 3903/11884 - Val_Loss: 25.7466\n",
      "Processing batch 3904/11884 - Val_Loss: 26.3334\n",
      "Processing batch 3905/11884 - Val_Loss: 25.7831\n",
      "Processing batch 3906/11884 - Val_Loss: 27.5063\n",
      "Processing batch 3907/11884 - Val_Loss: 28.0599\n",
      "Processing batch 3908/11884 - Val_Loss: 28.2571\n",
      "Processing batch 3909/11884 - Val_Loss: 21.4867\n",
      "Processing batch 3910/11884 - Val_Loss: 26.2373\n",
      "Processing batch 3911/11884 - Val_Loss: 28.5353\n",
      "Processing batch 3912/11884 - Val_Loss: 26.7634\n",
      "Processing batch 3913/11884 - Val_Loss: 27.0290\n",
      "Processing batch 3914/11884 - Val_Loss: 28.6395\n",
      "Processing batch 3915/11884 - Val_Loss: 24.6293\n",
      "Processing batch 3916/11884 - Val_Loss: 27.5103\n",
      "Processing batch 3917/11884 - Val_Loss: 28.4212\n",
      "Processing batch 3918/11884 - Val_Loss: 24.1753\n",
      "Processing batch 3919/11884 - Val_Loss: 27.4018\n",
      "Processing batch 3920/11884 - Val_Loss: 26.6613\n",
      "Processing batch 3921/11884 - Val_Loss: 23.3341\n",
      "Processing batch 3922/11884 - Val_Loss: 24.9017\n",
      "Processing batch 3923/11884 - Val_Loss: 24.0012\n",
      "Processing batch 3924/11884 - Val_Loss: 23.7715\n",
      "Processing batch 3925/11884 - Val_Loss: 25.7846\n",
      "Processing batch 3926/11884 - Val_Loss: 27.0553\n",
      "Processing batch 3927/11884 - Val_Loss: 26.1740\n",
      "Processing batch 3928/11884 - Val_Loss: 23.8201\n",
      "Processing batch 3929/11884 - Val_Loss: 22.9169\n",
      "Processing batch 3930/11884 - Val_Loss: 23.6884\n",
      "Processing batch 3931/11884 - Val_Loss: 21.6449\n",
      "Processing batch 3932/11884 - Val_Loss: 24.3303\n",
      "Processing batch 3933/11884 - Val_Loss: 24.9585\n",
      "Processing batch 3934/11884 - Val_Loss: 23.4704\n",
      "Processing batch 3935/11884 - Val_Loss: 27.6958\n",
      "Processing batch 3936/11884 - Val_Loss: 23.9958\n",
      "Processing batch 3937/11884 - Val_Loss: 22.7596\n",
      "Processing batch 3938/11884 - Val_Loss: 24.4828\n",
      "Processing batch 3939/11884 - Val_Loss: 26.8980\n",
      "Processing batch 3940/11884 - Val_Loss: 24.4281\n",
      "Processing batch 3941/11884 - Val_Loss: 23.3637\n",
      "Processing batch 3942/11884 - Val_Loss: 23.8622\n",
      "Processing batch 3943/11884 - Val_Loss: 25.6958\n",
      "Processing batch 3944/11884 - Val_Loss: 27.6123\n",
      "Processing batch 3945/11884 - Val_Loss: 26.4491\n",
      "Processing batch 3946/11884 - Val_Loss: 26.1708\n",
      "Processing batch 3947/11884 - Val_Loss: 23.3977\n",
      "Processing batch 3948/11884 - Val_Loss: 25.1576\n",
      "Processing batch 3949/11884 - Val_Loss: 23.6681\n",
      "Processing batch 3950/11884 - Val_Loss: 25.1164\n",
      "Processing batch 3951/11884 - Val_Loss: 24.3580\n",
      "Processing batch 3952/11884 - Val_Loss: 25.4650\n",
      "Processing batch 3953/11884 - Val_Loss: 27.3691\n",
      "Processing batch 3954/11884 - Val_Loss: 24.0597\n",
      "Processing batch 3955/11884 - Val_Loss: 24.5082\n",
      "Processing batch 3956/11884 - Val_Loss: 25.6337\n",
      "Processing batch 3957/11884 - Val_Loss: 25.6131\n",
      "Processing batch 3958/11884 - Val_Loss: 24.7136\n",
      "Processing batch 3959/11884 - Val_Loss: 25.9919\n",
      "Processing batch 3960/11884 - Val_Loss: 24.5632\n",
      "Processing batch 3961/11884 - Val_Loss: 24.0725\n",
      "Processing batch 3962/11884 - Val_Loss: 23.7063\n",
      "Processing batch 3963/11884 - Val_Loss: 24.7878\n",
      "Processing batch 3964/11884 - Val_Loss: 24.0194\n",
      "Processing batch 3965/11884 - Val_Loss: 27.6068\n",
      "Processing batch 3966/11884 - Val_Loss: 26.4941\n",
      "Processing batch 3967/11884 - Val_Loss: 22.5772\n",
      "Processing batch 3968/11884 - Val_Loss: 25.1726\n",
      "Processing batch 3969/11884 - Val_Loss: 23.5522\n",
      "Processing batch 3970/11884 - Val_Loss: 26.1530\n",
      "Processing batch 3971/11884 - Val_Loss: 25.7002\n",
      "Processing batch 3972/11884 - Val_Loss: 23.7639\n",
      "Processing batch 3973/11884 - Val_Loss: 24.9733\n",
      "Processing batch 3974/11884 - Val_Loss: 23.8656\n",
      "Processing batch 3975/11884 - Val_Loss: 23.9154\n",
      "Processing batch 3976/11884 - Val_Loss: 25.3997\n",
      "Processing batch 3977/11884 - Val_Loss: 24.7441\n",
      "Processing batch 3978/11884 - Val_Loss: 23.2970\n",
      "Processing batch 3979/11884 - Val_Loss: 27.8189\n",
      "Processing batch 3980/11884 - Val_Loss: 24.6957\n",
      "Processing batch 3981/11884 - Val_Loss: 24.8304\n",
      "Processing batch 3982/11884 - Val_Loss: 24.8715\n",
      "Processing batch 3983/11884 - Val_Loss: 24.7711\n",
      "Processing batch 3984/11884 - Val_Loss: 25.6399\n",
      "Processing batch 3985/11884 - Val_Loss: 26.0400\n",
      "Processing batch 3986/11884 - Val_Loss: 25.6876\n",
      "Processing batch 3987/11884 - Val_Loss: 25.4882\n",
      "Processing batch 3988/11884 - Val_Loss: 26.9776\n",
      "Processing batch 3989/11884 - Val_Loss: 25.3968\n",
      "Processing batch 3990/11884 - Val_Loss: 24.3422\n",
      "Processing batch 3991/11884 - Val_Loss: 24.0556\n",
      "Processing batch 3992/11884 - Val_Loss: 26.0683\n",
      "Processing batch 3993/11884 - Val_Loss: 26.3887\n",
      "Processing batch 3994/11884 - Val_Loss: 21.8690\n",
      "Processing batch 3995/11884 - Val_Loss: 24.0488\n",
      "Processing batch 3996/11884 - Val_Loss: 24.0879\n",
      "Processing batch 3997/11884 - Val_Loss: 24.4891\n",
      "Processing batch 3998/11884 - Val_Loss: 24.0430\n",
      "Processing batch 3999/11884 - Val_Loss: 28.0045\n",
      "Processing batch 4000/11884 - Val_Loss: 26.4046\n",
      "Processing batch 4001/11884 - Val_Loss: 24.6214\n",
      "Processing batch 4002/11884 - Val_Loss: 27.7697\n",
      "Processing batch 4003/11884 - Val_Loss: 24.5992\n",
      "Processing batch 4004/11884 - Val_Loss: 22.9516\n",
      "Processing batch 4005/11884 - Val_Loss: 26.6023\n",
      "Processing batch 4006/11884 - Val_Loss: 28.3552\n",
      "Processing batch 4007/11884 - Val_Loss: 27.8751\n",
      "Processing batch 4008/11884 - Val_Loss: 26.5542\n",
      "Processing batch 4009/11884 - Val_Loss: 27.1404\n",
      "Processing batch 4010/11884 - Val_Loss: 24.2041\n",
      "Processing batch 4011/11884 - Val_Loss: 26.9033\n",
      "Processing batch 4012/11884 - Val_Loss: 24.2660\n",
      "Processing batch 4013/11884 - Val_Loss: 23.1838\n",
      "Processing batch 4014/11884 - Val_Loss: 28.9227\n",
      "Processing batch 4015/11884 - Val_Loss: 27.7217\n",
      "Processing batch 4016/11884 - Val_Loss: 25.9998\n",
      "Processing batch 4017/11884 - Val_Loss: 23.9763\n",
      "Processing batch 4018/11884 - Val_Loss: 23.8516\n",
      "Processing batch 4019/11884 - Val_Loss: 26.6190\n",
      "Processing batch 4020/11884 - Val_Loss: 23.5233\n",
      "Processing batch 4021/11884 - Val_Loss: 26.3763\n",
      "Processing batch 4022/11884 - Val_Loss: 25.2881\n",
      "Processing batch 4023/11884 - Val_Loss: 24.6957\n",
      "Processing batch 4024/11884 - Val_Loss: 23.2067\n",
      "Processing batch 4025/11884 - Val_Loss: 27.0847\n",
      "Processing batch 4026/11884 - Val_Loss: 26.9790\n",
      "Processing batch 4027/11884 - Val_Loss: 27.5744\n",
      "Processing batch 4028/11884 - Val_Loss: 24.4780\n",
      "Processing batch 4029/11884 - Val_Loss: 22.0980\n",
      "Processing batch 4030/11884 - Val_Loss: 26.8980\n",
      "Processing batch 4031/11884 - Val_Loss: 30.3514\n",
      "Processing batch 4032/11884 - Val_Loss: 25.0990\n",
      "Processing batch 4033/11884 - Val_Loss: 33.6826\n",
      "Processing batch 4034/11884 - Val_Loss: 23.4846\n",
      "Processing batch 4035/11884 - Val_Loss: 27.1207\n",
      "Processing batch 4036/11884 - Val_Loss: 24.8740\n",
      "Processing batch 4037/11884 - Val_Loss: 24.4083\n",
      "Processing batch 4038/11884 - Val_Loss: 25.2789\n",
      "Processing batch 4039/11884 - Val_Loss: 22.9200\n",
      "Processing batch 4040/11884 - Val_Loss: 25.6901\n",
      "Processing batch 4041/11884 - Val_Loss: 25.6453\n",
      "Processing batch 4042/11884 - Val_Loss: 27.5675\n",
      "Processing batch 4043/11884 - Val_Loss: 24.9227\n",
      "Processing batch 4044/11884 - Val_Loss: 27.4115\n",
      "Processing batch 4045/11884 - Val_Loss: 23.9386\n",
      "Processing batch 4046/11884 - Val_Loss: 25.3788\n",
      "Processing batch 4047/11884 - Val_Loss: 27.7140\n",
      "Processing batch 4048/11884 - Val_Loss: 24.4908\n",
      "Processing batch 4049/11884 - Val_Loss: 24.0810\n",
      "Processing batch 4050/11884 - Val_Loss: 27.6868\n",
      "Processing batch 4051/11884 - Val_Loss: 26.4923\n",
      "Processing batch 4052/11884 - Val_Loss: 26.5934\n",
      "Processing batch 4053/11884 - Val_Loss: 22.0504\n",
      "Processing batch 4054/11884 - Val_Loss: 24.6761\n",
      "Processing batch 4055/11884 - Val_Loss: 29.3232\n",
      "Processing batch 4056/11884 - Val_Loss: 25.6363\n",
      "Processing batch 4057/11884 - Val_Loss: 25.1966\n",
      "Processing batch 4058/11884 - Val_Loss: 25.0795\n",
      "Processing batch 4059/11884 - Val_Loss: 24.0284\n",
      "Processing batch 4060/11884 - Val_Loss: 26.3012\n",
      "Processing batch 4061/11884 - Val_Loss: 28.0003\n",
      "Processing batch 4062/11884 - Val_Loss: 25.7126\n",
      "Processing batch 4063/11884 - Val_Loss: 26.3645\n",
      "Processing batch 4064/11884 - Val_Loss: 25.9177\n",
      "Processing batch 4065/11884 - Val_Loss: 24.9634\n",
      "Processing batch 4066/11884 - Val_Loss: 22.6981\n",
      "Processing batch 4067/11884 - Val_Loss: 25.1959\n",
      "Processing batch 4068/11884 - Val_Loss: 30.7876\n",
      "Processing batch 4069/11884 - Val_Loss: 26.6355\n",
      "Processing batch 4070/11884 - Val_Loss: 23.9478\n",
      "Processing batch 4071/11884 - Val_Loss: 25.0240\n",
      "Processing batch 4072/11884 - Val_Loss: 23.2857\n",
      "Processing batch 4073/11884 - Val_Loss: 26.1952\n",
      "Processing batch 4074/11884 - Val_Loss: 23.5448\n",
      "Processing batch 4075/11884 - Val_Loss: 26.5843\n",
      "Processing batch 4076/11884 - Val_Loss: 25.7108\n",
      "Processing batch 4077/11884 - Val_Loss: 26.1678\n",
      "Processing batch 4078/11884 - Val_Loss: 24.7749\n",
      "Processing batch 4079/11884 - Val_Loss: 26.6611\n",
      "Processing batch 4080/11884 - Val_Loss: 25.6466\n",
      "Processing batch 4081/11884 - Val_Loss: 23.3518\n",
      "Processing batch 4082/11884 - Val_Loss: 24.7340\n",
      "Processing batch 4083/11884 - Val_Loss: 24.9403\n",
      "Processing batch 4084/11884 - Val_Loss: 26.0515\n",
      "Processing batch 4085/11884 - Val_Loss: 26.9756\n",
      "Processing batch 4086/11884 - Val_Loss: 24.6716\n",
      "Processing batch 4087/11884 - Val_Loss: 27.9223\n",
      "Processing batch 4088/11884 - Val_Loss: 24.3887\n",
      "Processing batch 4089/11884 - Val_Loss: 23.4510\n",
      "Processing batch 4090/11884 - Val_Loss: 25.4481\n",
      "Processing batch 4091/11884 - Val_Loss: 25.4774\n",
      "Processing batch 4092/11884 - Val_Loss: 27.5775\n",
      "Processing batch 4093/11884 - Val_Loss: 22.5826\n",
      "Processing batch 4094/11884 - Val_Loss: 23.2464\n",
      "Processing batch 4095/11884 - Val_Loss: 26.7433\n",
      "Processing batch 4096/11884 - Val_Loss: 25.4633\n",
      "Processing batch 4097/11884 - Val_Loss: 25.5362\n",
      "Processing batch 4098/11884 - Val_Loss: 24.9798\n",
      "Processing batch 4099/11884 - Val_Loss: 26.9748\n",
      "Processing batch 4100/11884 - Val_Loss: 24.1907\n",
      "Processing batch 4101/11884 - Val_Loss: 24.4613\n",
      "Processing batch 4102/11884 - Val_Loss: 28.3123\n",
      "Processing batch 4103/11884 - Val_Loss: 25.5197\n",
      "Processing batch 4104/11884 - Val_Loss: 24.2523\n",
      "Processing batch 4105/11884 - Val_Loss: 28.3385\n",
      "Processing batch 4106/11884 - Val_Loss: 25.3036\n",
      "Processing batch 4107/11884 - Val_Loss: 24.1760\n",
      "Processing batch 4108/11884 - Val_Loss: 24.0330\n",
      "Processing batch 4109/11884 - Val_Loss: 27.0583\n",
      "Processing batch 4110/11884 - Val_Loss: 26.2710\n",
      "Processing batch 4111/11884 - Val_Loss: 29.5183\n",
      "Processing batch 4112/11884 - Val_Loss: 24.8098\n",
      "Processing batch 4113/11884 - Val_Loss: 25.9455\n",
      "Processing batch 4114/11884 - Val_Loss: 27.2587\n",
      "Processing batch 4115/11884 - Val_Loss: 28.3854\n",
      "Processing batch 4116/11884 - Val_Loss: 24.9358\n",
      "Processing batch 4117/11884 - Val_Loss: 24.6316\n",
      "Processing batch 4118/11884 - Val_Loss: 25.2061\n",
      "Processing batch 4119/11884 - Val_Loss: 25.8223\n",
      "Processing batch 4120/11884 - Val_Loss: 26.2101\n",
      "Processing batch 4121/11884 - Val_Loss: 24.9058\n",
      "Processing batch 4122/11884 - Val_Loss: 23.9245\n",
      "Processing batch 4123/11884 - Val_Loss: 26.4682\n",
      "Processing batch 4124/11884 - Val_Loss: 26.5650\n",
      "Processing batch 4125/11884 - Val_Loss: 26.4722\n",
      "Processing batch 4126/11884 - Val_Loss: 22.2981\n",
      "Processing batch 4127/11884 - Val_Loss: 27.2948\n",
      "Processing batch 4128/11884 - Val_Loss: 23.7461\n",
      "Processing batch 4129/11884 - Val_Loss: 24.7322\n",
      "Processing batch 4130/11884 - Val_Loss: 24.5789\n",
      "Processing batch 4131/11884 - Val_Loss: 27.3680\n",
      "Processing batch 4132/11884 - Val_Loss: 26.1269\n",
      "Processing batch 4133/11884 - Val_Loss: 24.9030\n",
      "Processing batch 4134/11884 - Val_Loss: 28.3419\n",
      "Processing batch 4135/11884 - Val_Loss: 22.9613\n",
      "Processing batch 4136/11884 - Val_Loss: 23.7470\n",
      "Processing batch 4137/11884 - Val_Loss: 26.6222\n",
      "Processing batch 4138/11884 - Val_Loss: 24.3357\n",
      "Processing batch 4139/11884 - Val_Loss: 24.1693\n",
      "Processing batch 4140/11884 - Val_Loss: 23.9923\n",
      "Processing batch 4141/11884 - Val_Loss: 26.1042\n",
      "Processing batch 4142/11884 - Val_Loss: 29.4564\n",
      "Processing batch 4143/11884 - Val_Loss: 23.6327\n",
      "Processing batch 4144/11884 - Val_Loss: 24.4879\n",
      "Processing batch 4145/11884 - Val_Loss: 26.3892\n",
      "Processing batch 4146/11884 - Val_Loss: 25.1977\n",
      "Processing batch 4147/11884 - Val_Loss: 26.0681\n",
      "Processing batch 4148/11884 - Val_Loss: 26.3738\n",
      "Processing batch 4149/11884 - Val_Loss: 26.5148\n",
      "Processing batch 4150/11884 - Val_Loss: 27.1655\n",
      "Processing batch 4151/11884 - Val_Loss: 24.9426\n",
      "Processing batch 4152/11884 - Val_Loss: 24.4901\n",
      "Processing batch 4153/11884 - Val_Loss: 24.9214\n",
      "Processing batch 4154/11884 - Val_Loss: 24.4435\n",
      "Processing batch 4155/11884 - Val_Loss: 26.0063\n",
      "Processing batch 4156/11884 - Val_Loss: 27.4273\n",
      "Processing batch 4157/11884 - Val_Loss: 25.5694\n",
      "Processing batch 4158/11884 - Val_Loss: 27.4034\n",
      "Processing batch 4159/11884 - Val_Loss: 23.5900\n",
      "Processing batch 4160/11884 - Val_Loss: 24.4704\n",
      "Processing batch 4161/11884 - Val_Loss: 25.6142\n",
      "Processing batch 4162/11884 - Val_Loss: 25.6591\n",
      "Processing batch 4163/11884 - Val_Loss: 25.1791\n",
      "Processing batch 4164/11884 - Val_Loss: 23.0495\n",
      "Processing batch 4165/11884 - Val_Loss: 25.9625\n",
      "Processing batch 4166/11884 - Val_Loss: 25.0858\n",
      "Processing batch 4167/11884 - Val_Loss: 25.9448\n",
      "Processing batch 4168/11884 - Val_Loss: 24.7680\n",
      "Processing batch 4169/11884 - Val_Loss: 25.8607\n",
      "Processing batch 4170/11884 - Val_Loss: 27.1318\n",
      "Processing batch 4171/11884 - Val_Loss: 26.6961\n",
      "Processing batch 4172/11884 - Val_Loss: 25.9598\n",
      "Processing batch 4173/11884 - Val_Loss: 25.9082\n",
      "Processing batch 4174/11884 - Val_Loss: 24.4767\n",
      "Processing batch 4175/11884 - Val_Loss: 24.6443\n",
      "Processing batch 4176/11884 - Val_Loss: 24.9129\n",
      "Processing batch 4177/11884 - Val_Loss: 23.4520\n",
      "Processing batch 4178/11884 - Val_Loss: 24.8585\n",
      "Processing batch 4179/11884 - Val_Loss: 28.4539\n",
      "Processing batch 4180/11884 - Val_Loss: 28.4837\n",
      "Processing batch 4181/11884 - Val_Loss: 27.0684\n",
      "Processing batch 4182/11884 - Val_Loss: 23.1670\n",
      "Processing batch 4183/11884 - Val_Loss: 25.1119\n",
      "Processing batch 4184/11884 - Val_Loss: 25.7070\n",
      "Processing batch 4185/11884 - Val_Loss: 23.7122\n",
      "Processing batch 4186/11884 - Val_Loss: 26.4697\n",
      "Processing batch 4187/11884 - Val_Loss: 24.8465\n",
      "Processing batch 4188/11884 - Val_Loss: 24.9255\n",
      "Processing batch 4189/11884 - Val_Loss: 26.0528\n",
      "Processing batch 4190/11884 - Val_Loss: 25.4040\n",
      "Processing batch 4191/11884 - Val_Loss: 28.8125\n",
      "Processing batch 4192/11884 - Val_Loss: 23.7084\n",
      "Processing batch 4193/11884 - Val_Loss: 27.3051\n",
      "Processing batch 4194/11884 - Val_Loss: 27.6734\n",
      "Processing batch 4195/11884 - Val_Loss: 24.3846\n",
      "Processing batch 4196/11884 - Val_Loss: 23.8120\n",
      "Processing batch 4197/11884 - Val_Loss: 25.1554\n",
      "Processing batch 4198/11884 - Val_Loss: 26.5385\n",
      "Processing batch 4199/11884 - Val_Loss: 26.0596\n",
      "Processing batch 4200/11884 - Val_Loss: 25.6599\n",
      "Processing batch 4201/11884 - Val_Loss: 25.8854\n",
      "Processing batch 4202/11884 - Val_Loss: 24.4501\n",
      "Processing batch 4203/11884 - Val_Loss: 27.3253\n",
      "Processing batch 4204/11884 - Val_Loss: 26.2844\n",
      "Processing batch 4205/11884 - Val_Loss: 25.2987\n",
      "Processing batch 4206/11884 - Val_Loss: 28.8761\n",
      "Processing batch 4207/11884 - Val_Loss: 23.5103\n",
      "Processing batch 4208/11884 - Val_Loss: 24.9854\n",
      "Processing batch 4209/11884 - Val_Loss: 26.8814\n",
      "Processing batch 4210/11884 - Val_Loss: 25.6128\n",
      "Processing batch 4211/11884 - Val_Loss: 26.5012\n",
      "Processing batch 4212/11884 - Val_Loss: 26.6961\n",
      "Processing batch 4213/11884 - Val_Loss: 26.4331\n",
      "Processing batch 4214/11884 - Val_Loss: 22.6304\n",
      "Processing batch 4215/11884 - Val_Loss: 26.8483\n",
      "Processing batch 4216/11884 - Val_Loss: 27.1497\n",
      "Processing batch 4217/11884 - Val_Loss: 26.3262\n",
      "Processing batch 4218/11884 - Val_Loss: 26.1521\n",
      "Processing batch 4219/11884 - Val_Loss: 21.0398\n",
      "Processing batch 4220/11884 - Val_Loss: 28.3086\n",
      "Processing batch 4221/11884 - Val_Loss: 27.4543\n",
      "Processing batch 4222/11884 - Val_Loss: 26.9925\n",
      "Processing batch 4223/11884 - Val_Loss: 25.3118\n",
      "Processing batch 4224/11884 - Val_Loss: 26.7190\n",
      "Processing batch 4225/11884 - Val_Loss: 26.3542\n",
      "Processing batch 4226/11884 - Val_Loss: 29.3261\n",
      "Processing batch 4227/11884 - Val_Loss: 26.0495\n",
      "Processing batch 4228/11884 - Val_Loss: 25.4930\n",
      "Processing batch 4229/11884 - Val_Loss: 26.6497\n",
      "Processing batch 4230/11884 - Val_Loss: 24.8975\n",
      "Processing batch 4231/11884 - Val_Loss: 26.1751\n",
      "Processing batch 4232/11884 - Val_Loss: 25.8010\n",
      "Processing batch 4233/11884 - Val_Loss: 23.9224\n",
      "Processing batch 4234/11884 - Val_Loss: 24.6708\n",
      "Processing batch 4235/11884 - Val_Loss: 23.3627\n",
      "Processing batch 4236/11884 - Val_Loss: 27.2731\n",
      "Processing batch 4237/11884 - Val_Loss: 25.8682\n",
      "Processing batch 4238/11884 - Val_Loss: 27.0714\n",
      "Processing batch 4239/11884 - Val_Loss: 25.9623\n",
      "Processing batch 4240/11884 - Val_Loss: 25.3251\n",
      "Processing batch 4241/11884 - Val_Loss: 25.2210\n",
      "Processing batch 4242/11884 - Val_Loss: 27.4563\n",
      "Processing batch 4243/11884 - Val_Loss: 26.5149\n",
      "Processing batch 4244/11884 - Val_Loss: 24.6719\n",
      "Processing batch 4245/11884 - Val_Loss: 27.3385\n",
      "Processing batch 4246/11884 - Val_Loss: 24.7508\n",
      "Processing batch 4247/11884 - Val_Loss: 26.0410\n",
      "Processing batch 4248/11884 - Val_Loss: 25.1878\n",
      "Processing batch 4249/11884 - Val_Loss: 24.9280\n",
      "Processing batch 4250/11884 - Val_Loss: 25.1134\n",
      "Processing batch 4251/11884 - Val_Loss: 25.8449\n",
      "Processing batch 4252/11884 - Val_Loss: 25.4107\n",
      "Processing batch 4253/11884 - Val_Loss: 24.6248\n",
      "Processing batch 4254/11884 - Val_Loss: 27.9846\n",
      "Processing batch 4255/11884 - Val_Loss: 26.1133\n",
      "Processing batch 4256/11884 - Val_Loss: 26.2352\n",
      "Processing batch 4257/11884 - Val_Loss: 25.4188\n",
      "Processing batch 4258/11884 - Val_Loss: 26.5858\n",
      "Processing batch 4259/11884 - Val_Loss: 25.5184\n",
      "Processing batch 4260/11884 - Val_Loss: 25.0563\n",
      "Processing batch 4261/11884 - Val_Loss: 25.4488\n",
      "Processing batch 4262/11884 - Val_Loss: 25.7199\n",
      "Processing batch 4263/11884 - Val_Loss: 24.4623\n",
      "Processing batch 4264/11884 - Val_Loss: 26.4526\n",
      "Processing batch 4265/11884 - Val_Loss: 22.5486\n",
      "Processing batch 4266/11884 - Val_Loss: 24.9674\n",
      "Processing batch 4267/11884 - Val_Loss: 26.8217\n",
      "Processing batch 4268/11884 - Val_Loss: 25.7691\n",
      "Processing batch 4269/11884 - Val_Loss: 27.8289\n",
      "Processing batch 4270/11884 - Val_Loss: 24.2811\n",
      "Processing batch 4271/11884 - Val_Loss: 26.7458\n",
      "Processing batch 4272/11884 - Val_Loss: 26.8880\n",
      "Processing batch 4273/11884 - Val_Loss: 25.8116\n",
      "Processing batch 4274/11884 - Val_Loss: 24.5729\n",
      "Processing batch 4275/11884 - Val_Loss: 23.6953\n",
      "Processing batch 4276/11884 - Val_Loss: 23.1719\n",
      "Processing batch 4277/11884 - Val_Loss: 26.8539\n",
      "Processing batch 4278/11884 - Val_Loss: 24.3933\n",
      "Processing batch 4279/11884 - Val_Loss: 26.0893\n",
      "Processing batch 4280/11884 - Val_Loss: 23.6394\n",
      "Processing batch 4281/11884 - Val_Loss: 23.4252\n",
      "Processing batch 4282/11884 - Val_Loss: 24.4041\n",
      "Processing batch 4283/11884 - Val_Loss: 23.7257\n",
      "Processing batch 4284/11884 - Val_Loss: 24.4573\n",
      "Processing batch 4285/11884 - Val_Loss: 27.6575\n",
      "Processing batch 4286/11884 - Val_Loss: 26.1357\n",
      "Processing batch 4287/11884 - Val_Loss: 23.1760\n",
      "Processing batch 4288/11884 - Val_Loss: 26.4363\n",
      "Processing batch 4289/11884 - Val_Loss: 26.9333\n",
      "Processing batch 4290/11884 - Val_Loss: 26.3043\n",
      "Processing batch 4291/11884 - Val_Loss: 24.7450\n",
      "Processing batch 4292/11884 - Val_Loss: 24.8860\n",
      "Processing batch 4293/11884 - Val_Loss: 26.4720\n",
      "Processing batch 4294/11884 - Val_Loss: 24.5305\n",
      "Processing batch 4295/11884 - Val_Loss: 24.9234\n",
      "Processing batch 4296/11884 - Val_Loss: 24.2202\n",
      "Processing batch 4297/11884 - Val_Loss: 24.3274\n",
      "Processing batch 4298/11884 - Val_Loss: 26.2343\n",
      "Processing batch 4299/11884 - Val_Loss: 21.5339\n",
      "Processing batch 4300/11884 - Val_Loss: 25.0843\n",
      "Processing batch 4301/11884 - Val_Loss: 23.5132\n",
      "Processing batch 4302/11884 - Val_Loss: 24.6044\n",
      "Processing batch 4303/11884 - Val_Loss: 23.3814\n",
      "Processing batch 4304/11884 - Val_Loss: 26.3216\n",
      "Processing batch 4305/11884 - Val_Loss: 26.0949\n",
      "Processing batch 4306/11884 - Val_Loss: 27.0854\n",
      "Processing batch 4307/11884 - Val_Loss: 27.9937\n",
      "Processing batch 4308/11884 - Val_Loss: 23.8816\n",
      "Processing batch 4309/11884 - Val_Loss: 26.3955\n",
      "Processing batch 4310/11884 - Val_Loss: 23.2182\n",
      "Processing batch 4311/11884 - Val_Loss: 26.7868\n",
      "Processing batch 4312/11884 - Val_Loss: 26.8789\n",
      "Processing batch 4313/11884 - Val_Loss: 25.8171\n",
      "Processing batch 4314/11884 - Val_Loss: 27.1421\n",
      "Processing batch 4315/11884 - Val_Loss: 26.2505\n",
      "Processing batch 4316/11884 - Val_Loss: 23.7283\n",
      "Processing batch 4317/11884 - Val_Loss: 23.7100\n",
      "Processing batch 4318/11884 - Val_Loss: 23.0077\n",
      "Processing batch 4319/11884 - Val_Loss: 23.3919\n",
      "Processing batch 4320/11884 - Val_Loss: 24.5763\n",
      "Processing batch 4321/11884 - Val_Loss: 26.8351\n",
      "Processing batch 4322/11884 - Val_Loss: 25.6561\n",
      "Processing batch 4323/11884 - Val_Loss: 25.6865\n",
      "Processing batch 4324/11884 - Val_Loss: 27.8161\n",
      "Processing batch 4325/11884 - Val_Loss: 22.9467\n",
      "Processing batch 4326/11884 - Val_Loss: 26.0337\n",
      "Processing batch 4327/11884 - Val_Loss: 24.5045\n",
      "Processing batch 4328/11884 - Val_Loss: 25.5424\n",
      "Processing batch 4329/11884 - Val_Loss: 25.1687\n",
      "Processing batch 4330/11884 - Val_Loss: 24.6193\n",
      "Processing batch 4331/11884 - Val_Loss: 27.6870\n",
      "Processing batch 4332/11884 - Val_Loss: 27.4139\n",
      "Processing batch 4333/11884 - Val_Loss: 22.8959\n",
      "Processing batch 4334/11884 - Val_Loss: 26.2464\n",
      "Processing batch 4335/11884 - Val_Loss: 23.5992\n",
      "Processing batch 4336/11884 - Val_Loss: 25.9656\n",
      "Processing batch 4337/11884 - Val_Loss: 26.1659\n",
      "Processing batch 4338/11884 - Val_Loss: 26.1419\n",
      "Processing batch 4339/11884 - Val_Loss: 23.4923\n",
      "Processing batch 4340/11884 - Val_Loss: 30.0013\n",
      "Processing batch 4341/11884 - Val_Loss: 26.4677\n",
      "Processing batch 4342/11884 - Val_Loss: 24.1171\n",
      "Processing batch 4343/11884 - Val_Loss: 25.0050\n",
      "Processing batch 4344/11884 - Val_Loss: 26.5122\n",
      "Processing batch 4345/11884 - Val_Loss: 26.8607\n",
      "Processing batch 4346/11884 - Val_Loss: 26.5681\n",
      "Processing batch 4347/11884 - Val_Loss: 24.3195\n",
      "Processing batch 4348/11884 - Val_Loss: 24.3441\n",
      "Processing batch 4349/11884 - Val_Loss: 26.6486\n",
      "Processing batch 4350/11884 - Val_Loss: 24.3192\n",
      "Processing batch 4351/11884 - Val_Loss: 24.2496\n",
      "Processing batch 4352/11884 - Val_Loss: 25.3209\n",
      "Processing batch 4353/11884 - Val_Loss: 26.6166\n",
      "Processing batch 4354/11884 - Val_Loss: 26.6093\n",
      "Processing batch 4355/11884 - Val_Loss: 24.6021\n",
      "Processing batch 4356/11884 - Val_Loss: 25.8389\n",
      "Processing batch 4357/11884 - Val_Loss: 24.6609\n",
      "Processing batch 4358/11884 - Val_Loss: 26.2865\n",
      "Processing batch 4359/11884 - Val_Loss: 26.9566\n",
      "Processing batch 4360/11884 - Val_Loss: 28.9191\n",
      "Processing batch 4361/11884 - Val_Loss: 27.6584\n",
      "Processing batch 4362/11884 - Val_Loss: 24.6813\n",
      "Processing batch 4363/11884 - Val_Loss: 24.4373\n",
      "Processing batch 4364/11884 - Val_Loss: 29.5666\n",
      "Processing batch 4365/11884 - Val_Loss: 26.0568\n",
      "Processing batch 4366/11884 - Val_Loss: 24.9088\n",
      "Processing batch 4367/11884 - Val_Loss: 26.9741\n",
      "Processing batch 4368/11884 - Val_Loss: 20.0903\n",
      "Processing batch 4369/11884 - Val_Loss: 23.1940\n",
      "Processing batch 4370/11884 - Val_Loss: 24.1903\n",
      "Processing batch 4371/11884 - Val_Loss: 27.2763\n",
      "Processing batch 4372/11884 - Val_Loss: 26.0843\n",
      "Processing batch 4373/11884 - Val_Loss: 25.9737\n",
      "Processing batch 4374/11884 - Val_Loss: 25.7681\n",
      "Processing batch 4375/11884 - Val_Loss: 24.0004\n",
      "Processing batch 4376/11884 - Val_Loss: 24.8651\n",
      "Processing batch 4377/11884 - Val_Loss: 24.2372\n",
      "Processing batch 4378/11884 - Val_Loss: 24.9510\n",
      "Processing batch 4379/11884 - Val_Loss: 24.6660\n",
      "Processing batch 4380/11884 - Val_Loss: 25.4308\n",
      "Processing batch 4381/11884 - Val_Loss: 24.4326\n",
      "Processing batch 4382/11884 - Val_Loss: 25.9337\n",
      "Processing batch 4383/11884 - Val_Loss: 26.0725\n",
      "Processing batch 4384/11884 - Val_Loss: 23.8176\n",
      "Processing batch 4385/11884 - Val_Loss: 26.2895\n",
      "Processing batch 4386/11884 - Val_Loss: 25.8268\n",
      "Processing batch 4387/11884 - Val_Loss: 25.5608\n",
      "Processing batch 4388/11884 - Val_Loss: 26.3560\n",
      "Processing batch 4389/11884 - Val_Loss: 25.0135\n",
      "Processing batch 4390/11884 - Val_Loss: 24.7593\n",
      "Processing batch 4391/11884 - Val_Loss: 24.5799\n",
      "Processing batch 4392/11884 - Val_Loss: 24.1244\n",
      "Processing batch 4393/11884 - Val_Loss: 23.7140\n",
      "Processing batch 4394/11884 - Val_Loss: 24.7652\n",
      "Processing batch 4395/11884 - Val_Loss: 25.7876\n",
      "Processing batch 4396/11884 - Val_Loss: 28.2823\n",
      "Processing batch 4397/11884 - Val_Loss: 22.9378\n",
      "Processing batch 4398/11884 - Val_Loss: 25.1066\n",
      "Processing batch 4399/11884 - Val_Loss: 26.1554\n",
      "Processing batch 4400/11884 - Val_Loss: 25.8531\n",
      "Processing batch 4401/11884 - Val_Loss: 28.2987\n",
      "Processing batch 4402/11884 - Val_Loss: 25.1761\n",
      "Processing batch 4403/11884 - Val_Loss: 27.4479\n",
      "Processing batch 4404/11884 - Val_Loss: 26.7927\n",
      "Processing batch 4405/11884 - Val_Loss: 24.6450\n",
      "Processing batch 4406/11884 - Val_Loss: 24.3695\n",
      "Processing batch 4407/11884 - Val_Loss: 24.8484\n",
      "Processing batch 4408/11884 - Val_Loss: 24.2658\n",
      "Processing batch 4409/11884 - Val_Loss: 26.7829\n",
      "Processing batch 4410/11884 - Val_Loss: 25.3244\n",
      "Processing batch 4411/11884 - Val_Loss: 23.7985\n",
      "Processing batch 4412/11884 - Val_Loss: 26.0936\n",
      "Processing batch 4413/11884 - Val_Loss: 25.3940\n",
      "Processing batch 4414/11884 - Val_Loss: 25.2883\n",
      "Processing batch 4415/11884 - Val_Loss: 26.5485\n",
      "Processing batch 4416/11884 - Val_Loss: 26.8553\n",
      "Processing batch 4417/11884 - Val_Loss: 25.2185\n",
      "Processing batch 4418/11884 - Val_Loss: 27.2021\n",
      "Processing batch 4419/11884 - Val_Loss: 26.6739\n",
      "Processing batch 4420/11884 - Val_Loss: 25.3361\n",
      "Processing batch 4421/11884 - Val_Loss: 24.6770\n",
      "Processing batch 4422/11884 - Val_Loss: 26.1393\n",
      "Processing batch 4423/11884 - Val_Loss: 25.3327\n",
      "Processing batch 4424/11884 - Val_Loss: 25.4783\n",
      "Processing batch 4425/11884 - Val_Loss: 23.4097\n",
      "Processing batch 4426/11884 - Val_Loss: 26.4785\n",
      "Processing batch 4427/11884 - Val_Loss: 27.7392\n",
      "Processing batch 4428/11884 - Val_Loss: 24.1042\n",
      "Processing batch 4429/11884 - Val_Loss: 24.7622\n",
      "Processing batch 4430/11884 - Val_Loss: 22.3351\n",
      "Processing batch 4431/11884 - Val_Loss: 21.9272\n",
      "Processing batch 4432/11884 - Val_Loss: 24.1401\n",
      "Processing batch 4433/11884 - Val_Loss: 29.7869\n",
      "Processing batch 4434/11884 - Val_Loss: 24.8006\n",
      "Processing batch 4435/11884 - Val_Loss: 26.1976\n",
      "Processing batch 4436/11884 - Val_Loss: 26.3334\n",
      "Processing batch 4437/11884 - Val_Loss: 23.0843\n",
      "Processing batch 4438/11884 - Val_Loss: 26.6272\n",
      "Processing batch 4439/11884 - Val_Loss: 28.4159\n",
      "Processing batch 4440/11884 - Val_Loss: 26.5355\n",
      "Processing batch 4441/11884 - Val_Loss: 24.7402\n",
      "Processing batch 4442/11884 - Val_Loss: 24.4919\n",
      "Processing batch 4443/11884 - Val_Loss: 26.1991\n",
      "Processing batch 4444/11884 - Val_Loss: 25.3560\n",
      "Processing batch 4445/11884 - Val_Loss: 25.8374\n",
      "Processing batch 4446/11884 - Val_Loss: 23.5793\n",
      "Processing batch 4447/11884 - Val_Loss: 24.8452\n",
      "Processing batch 4448/11884 - Val_Loss: 25.5976\n",
      "Processing batch 4449/11884 - Val_Loss: 25.3204\n",
      "Processing batch 4450/11884 - Val_Loss: 27.2025\n",
      "Processing batch 4451/11884 - Val_Loss: 26.7584\n",
      "Processing batch 4452/11884 - Val_Loss: 24.4182\n",
      "Processing batch 4453/11884 - Val_Loss: 26.9972\n",
      "Processing batch 4454/11884 - Val_Loss: 23.5999\n",
      "Processing batch 4455/11884 - Val_Loss: 27.4074\n",
      "Processing batch 4456/11884 - Val_Loss: 23.3992\n",
      "Processing batch 4457/11884 - Val_Loss: 25.6294\n",
      "Processing batch 4458/11884 - Val_Loss: 26.1842\n",
      "Processing batch 4459/11884 - Val_Loss: 24.1321\n",
      "Processing batch 4460/11884 - Val_Loss: 26.1077\n",
      "Processing batch 4461/11884 - Val_Loss: 25.1942\n",
      "Processing batch 4462/11884 - Val_Loss: 27.8787\n",
      "Processing batch 4463/11884 - Val_Loss: 23.0731\n",
      "Processing batch 4464/11884 - Val_Loss: 25.2096\n",
      "Processing batch 4465/11884 - Val_Loss: 24.2557\n",
      "Processing batch 4466/11884 - Val_Loss: 23.0073\n",
      "Processing batch 4467/11884 - Val_Loss: 24.3331\n",
      "Processing batch 4468/11884 - Val_Loss: 25.4812\n",
      "Processing batch 4469/11884 - Val_Loss: 23.8964\n",
      "Processing batch 4470/11884 - Val_Loss: 23.1905\n",
      "Processing batch 4471/11884 - Val_Loss: 28.2161\n",
      "Processing batch 4472/11884 - Val_Loss: 22.5535\n",
      "Processing batch 4473/11884 - Val_Loss: 27.2304\n",
      "Processing batch 4474/11884 - Val_Loss: 24.8690\n",
      "Processing batch 4475/11884 - Val_Loss: 25.3083\n",
      "Processing batch 4476/11884 - Val_Loss: 25.2934\n",
      "Processing batch 4477/11884 - Val_Loss: 26.0414\n",
      "Processing batch 4478/11884 - Val_Loss: 22.4396\n",
      "Processing batch 4479/11884 - Val_Loss: 24.2764\n",
      "Processing batch 4480/11884 - Val_Loss: 28.0651\n",
      "Processing batch 4481/11884 - Val_Loss: 25.4261\n",
      "Processing batch 4482/11884 - Val_Loss: 23.1279\n",
      "Processing batch 4483/11884 - Val_Loss: 24.9401\n",
      "Processing batch 4484/11884 - Val_Loss: 24.5809\n",
      "Processing batch 4485/11884 - Val_Loss: 26.6235\n",
      "Processing batch 4486/11884 - Val_Loss: 25.9148\n",
      "Processing batch 4487/11884 - Val_Loss: 22.6059\n",
      "Processing batch 4488/11884 - Val_Loss: 24.0356\n",
      "Processing batch 4489/11884 - Val_Loss: 25.9682\n",
      "Processing batch 4490/11884 - Val_Loss: 24.3283\n",
      "Processing batch 4491/11884 - Val_Loss: 27.1132\n",
      "Processing batch 4492/11884 - Val_Loss: 26.7839\n",
      "Processing batch 4493/11884 - Val_Loss: 22.3463\n",
      "Processing batch 4494/11884 - Val_Loss: 21.0982\n",
      "Processing batch 4495/11884 - Val_Loss: 26.2784\n",
      "Processing batch 4496/11884 - Val_Loss: 23.7863\n",
      "Processing batch 4497/11884 - Val_Loss: 25.3470\n",
      "Processing batch 4498/11884 - Val_Loss: 27.6011\n",
      "Processing batch 4499/11884 - Val_Loss: 21.7083\n",
      "Processing batch 4500/11884 - Val_Loss: 25.9264\n",
      "Processing batch 4501/11884 - Val_Loss: 27.7869\n",
      "Processing batch 4502/11884 - Val_Loss: 23.2836\n",
      "Processing batch 4503/11884 - Val_Loss: 24.2497\n",
      "Processing batch 4504/11884 - Val_Loss: 25.1721\n",
      "Processing batch 4505/11884 - Val_Loss: 26.8205\n",
      "Processing batch 4506/11884 - Val_Loss: 23.6743\n",
      "Processing batch 4507/11884 - Val_Loss: 27.0764\n",
      "Processing batch 4508/11884 - Val_Loss: 26.2193\n",
      "Processing batch 4509/11884 - Val_Loss: 25.5201\n",
      "Processing batch 4510/11884 - Val_Loss: 28.0301\n",
      "Processing batch 4511/11884 - Val_Loss: 25.9030\n",
      "Processing batch 4512/11884 - Val_Loss: 24.9021\n",
      "Processing batch 4513/11884 - Val_Loss: 26.4007\n",
      "Processing batch 4514/11884 - Val_Loss: 27.8760\n",
      "Processing batch 4515/11884 - Val_Loss: 26.4273\n",
      "Processing batch 4516/11884 - Val_Loss: 24.2611\n",
      "Processing batch 4517/11884 - Val_Loss: 24.5686\n",
      "Processing batch 4518/11884 - Val_Loss: 25.1047\n",
      "Processing batch 4519/11884 - Val_Loss: 26.1995\n",
      "Processing batch 4520/11884 - Val_Loss: 25.6080\n",
      "Processing batch 4521/11884 - Val_Loss: 25.0918\n",
      "Processing batch 4522/11884 - Val_Loss: 26.1419\n",
      "Processing batch 4523/11884 - Val_Loss: 25.0821\n",
      "Processing batch 4524/11884 - Val_Loss: 27.5785\n",
      "Processing batch 4525/11884 - Val_Loss: 26.4705\n",
      "Processing batch 4526/11884 - Val_Loss: 25.3919\n",
      "Processing batch 4527/11884 - Val_Loss: 25.3802\n",
      "Processing batch 4528/11884 - Val_Loss: 26.7038\n",
      "Processing batch 4529/11884 - Val_Loss: 24.4010\n",
      "Processing batch 4530/11884 - Val_Loss: 24.8382\n",
      "Processing batch 4531/11884 - Val_Loss: 23.8738\n",
      "Processing batch 4532/11884 - Val_Loss: 27.6394\n",
      "Processing batch 4533/11884 - Val_Loss: 26.5415\n",
      "Processing batch 4534/11884 - Val_Loss: 24.4869\n",
      "Processing batch 4535/11884 - Val_Loss: 24.8801\n",
      "Processing batch 4536/11884 - Val_Loss: 28.8506\n",
      "Processing batch 4537/11884 - Val_Loss: 22.0681\n",
      "Processing batch 4538/11884 - Val_Loss: 24.3504\n",
      "Processing batch 4539/11884 - Val_Loss: 24.3384\n",
      "Processing batch 4540/11884 - Val_Loss: 25.5792\n",
      "Processing batch 4541/11884 - Val_Loss: 25.1717\n",
      "Processing batch 4542/11884 - Val_Loss: 25.3173\n",
      "Processing batch 4543/11884 - Val_Loss: 23.2609\n",
      "Processing batch 4544/11884 - Val_Loss: 27.7105\n",
      "Processing batch 4545/11884 - Val_Loss: 24.3030\n",
      "Processing batch 4546/11884 - Val_Loss: 23.6762\n",
      "Processing batch 4547/11884 - Val_Loss: 22.5934\n",
      "Processing batch 4548/11884 - Val_Loss: 26.6521\n",
      "Processing batch 4549/11884 - Val_Loss: 26.6388\n",
      "Processing batch 4550/11884 - Val_Loss: 26.8136\n",
      "Processing batch 4551/11884 - Val_Loss: 21.2719\n",
      "Processing batch 4552/11884 - Val_Loss: 26.4965\n",
      "Processing batch 4553/11884 - Val_Loss: 25.9922\n",
      "Processing batch 4554/11884 - Val_Loss: 25.0370\n",
      "Processing batch 4555/11884 - Val_Loss: 22.4626\n",
      "Processing batch 4556/11884 - Val_Loss: 22.5428\n",
      "Processing batch 4557/11884 - Val_Loss: 23.5782\n",
      "Processing batch 4558/11884 - Val_Loss: 26.6834\n",
      "Processing batch 4559/11884 - Val_Loss: 25.9993\n",
      "Processing batch 4560/11884 - Val_Loss: 25.9211\n",
      "Processing batch 4561/11884 - Val_Loss: 28.8912\n",
      "Processing batch 4562/11884 - Val_Loss: 27.3200\n",
      "Processing batch 4563/11884 - Val_Loss: 26.8935\n",
      "Processing batch 4564/11884 - Val_Loss: 24.6503\n",
      "Processing batch 4565/11884 - Val_Loss: 24.3873\n",
      "Processing batch 4566/11884 - Val_Loss: 25.7967\n",
      "Processing batch 4567/11884 - Val_Loss: 25.5311\n",
      "Processing batch 4568/11884 - Val_Loss: 28.1863\n",
      "Processing batch 4569/11884 - Val_Loss: 24.3178\n",
      "Processing batch 4570/11884 - Val_Loss: 23.8967\n",
      "Processing batch 4571/11884 - Val_Loss: 26.4186\n",
      "Processing batch 4572/11884 - Val_Loss: 25.5957\n",
      "Processing batch 4573/11884 - Val_Loss: 25.1177\n",
      "Processing batch 4574/11884 - Val_Loss: 26.1077\n",
      "Processing batch 4575/11884 - Val_Loss: 24.6434\n",
      "Processing batch 4576/11884 - Val_Loss: 23.3869\n",
      "Processing batch 4577/11884 - Val_Loss: 22.7922\n",
      "Processing batch 4578/11884 - Val_Loss: 27.7022\n",
      "Processing batch 4579/11884 - Val_Loss: 26.5338\n",
      "Processing batch 4580/11884 - Val_Loss: 25.0039\n",
      "Processing batch 4581/11884 - Val_Loss: 27.4466\n",
      "Processing batch 4582/11884 - Val_Loss: 27.1237\n",
      "Processing batch 4583/11884 - Val_Loss: 25.5768\n",
      "Processing batch 4584/11884 - Val_Loss: 24.9376\n",
      "Processing batch 4585/11884 - Val_Loss: 22.3228\n",
      "Processing batch 4586/11884 - Val_Loss: 27.5995\n",
      "Processing batch 4587/11884 - Val_Loss: 27.0328\n",
      "Processing batch 4588/11884 - Val_Loss: 29.1819\n",
      "Processing batch 4589/11884 - Val_Loss: 24.9423\n",
      "Processing batch 4590/11884 - Val_Loss: 25.8443\n",
      "Processing batch 4591/11884 - Val_Loss: 22.1285\n",
      "Processing batch 4592/11884 - Val_Loss: 26.7563\n",
      "Processing batch 4593/11884 - Val_Loss: 28.3789\n",
      "Processing batch 4594/11884 - Val_Loss: 26.4433\n",
      "Processing batch 4595/11884 - Val_Loss: 24.2729\n",
      "Processing batch 4596/11884 - Val_Loss: 23.0855\n",
      "Processing batch 4597/11884 - Val_Loss: 25.6248\n",
      "Processing batch 4598/11884 - Val_Loss: 23.9637\n",
      "Processing batch 4599/11884 - Val_Loss: 23.8676\n",
      "Processing batch 4600/11884 - Val_Loss: 25.5012\n",
      "Processing batch 4601/11884 - Val_Loss: 24.8104\n",
      "Processing batch 4602/11884 - Val_Loss: 24.2979\n",
      "Processing batch 4603/11884 - Val_Loss: 25.9665\n",
      "Processing batch 4604/11884 - Val_Loss: 23.5058\n",
      "Processing batch 4605/11884 - Val_Loss: 25.4554\n",
      "Processing batch 4606/11884 - Val_Loss: 25.2830\n",
      "Processing batch 4607/11884 - Val_Loss: 26.2783\n",
      "Processing batch 4608/11884 - Val_Loss: 23.5608\n",
      "Processing batch 4609/11884 - Val_Loss: 27.1477\n",
      "Processing batch 4610/11884 - Val_Loss: 25.7960\n",
      "Processing batch 4611/11884 - Val_Loss: 24.9102\n",
      "Processing batch 4612/11884 - Val_Loss: 27.6934\n",
      "Processing batch 4613/11884 - Val_Loss: 25.6021\n",
      "Processing batch 4614/11884 - Val_Loss: 23.9071\n",
      "Processing batch 4615/11884 - Val_Loss: 25.1198\n",
      "Processing batch 4616/11884 - Val_Loss: 29.6593\n",
      "Processing batch 4617/11884 - Val_Loss: 27.5067\n",
      "Processing batch 4618/11884 - Val_Loss: 24.5745\n",
      "Processing batch 4619/11884 - Val_Loss: 26.2866\n",
      "Processing batch 4620/11884 - Val_Loss: 22.9070\n",
      "Processing batch 4621/11884 - Val_Loss: 28.8696\n",
      "Processing batch 4622/11884 - Val_Loss: 25.7417\n",
      "Processing batch 4623/11884 - Val_Loss: 25.9306\n",
      "Processing batch 4624/11884 - Val_Loss: 22.4397\n",
      "Processing batch 4625/11884 - Val_Loss: 24.7178\n",
      "Processing batch 4626/11884 - Val_Loss: 26.1197\n",
      "Processing batch 4627/11884 - Val_Loss: 24.8019\n",
      "Processing batch 4628/11884 - Val_Loss: 25.2727\n",
      "Processing batch 4629/11884 - Val_Loss: 22.7843\n",
      "Processing batch 4630/11884 - Val_Loss: 24.2540\n",
      "Processing batch 4631/11884 - Val_Loss: 24.3969\n",
      "Processing batch 4632/11884 - Val_Loss: 26.0672\n",
      "Processing batch 4633/11884 - Val_Loss: 25.2439\n",
      "Processing batch 4634/11884 - Val_Loss: 26.8614\n",
      "Processing batch 4635/11884 - Val_Loss: 24.5510\n",
      "Processing batch 4636/11884 - Val_Loss: 23.9942\n",
      "Processing batch 4637/11884 - Val_Loss: 25.8065\n",
      "Processing batch 4638/11884 - Val_Loss: 23.7763\n",
      "Processing batch 4639/11884 - Val_Loss: 26.1208\n",
      "Processing batch 4640/11884 - Val_Loss: 24.9913\n",
      "Processing batch 4641/11884 - Val_Loss: 24.4151\n",
      "Processing batch 4642/11884 - Val_Loss: 24.9697\n",
      "Processing batch 4643/11884 - Val_Loss: 25.6875\n",
      "Processing batch 4644/11884 - Val_Loss: 25.9416\n",
      "Processing batch 4645/11884 - Val_Loss: 24.1635\n",
      "Processing batch 4646/11884 - Val_Loss: 23.1838\n",
      "Processing batch 4647/11884 - Val_Loss: 27.9787\n",
      "Processing batch 4648/11884 - Val_Loss: 26.0194\n",
      "Processing batch 4649/11884 - Val_Loss: 27.8802\n",
      "Processing batch 4650/11884 - Val_Loss: 27.8849\n",
      "Processing batch 4651/11884 - Val_Loss: 27.1022\n",
      "Processing batch 4652/11884 - Val_Loss: 25.9848\n",
      "Processing batch 4653/11884 - Val_Loss: 26.8746\n",
      "Processing batch 4654/11884 - Val_Loss: 29.1855\n",
      "Processing batch 4655/11884 - Val_Loss: 22.4013\n",
      "Processing batch 4656/11884 - Val_Loss: 23.5839\n",
      "Processing batch 4657/11884 - Val_Loss: 25.9782\n",
      "Processing batch 4658/11884 - Val_Loss: 23.7972\n",
      "Processing batch 4659/11884 - Val_Loss: 23.5737\n",
      "Processing batch 4660/11884 - Val_Loss: 26.9974\n",
      "Processing batch 4661/11884 - Val_Loss: 24.8152\n",
      "Processing batch 4662/11884 - Val_Loss: 26.5557\n",
      "Processing batch 4663/11884 - Val_Loss: 25.8423\n",
      "Processing batch 4664/11884 - Val_Loss: 25.4620\n",
      "Processing batch 4665/11884 - Val_Loss: 27.6677\n",
      "Processing batch 4666/11884 - Val_Loss: 27.3273\n",
      "Processing batch 4667/11884 - Val_Loss: 27.3993\n",
      "Processing batch 4668/11884 - Val_Loss: 25.8485\n",
      "Processing batch 4669/11884 - Val_Loss: 23.2077\n",
      "Processing batch 4670/11884 - Val_Loss: 25.7794\n",
      "Processing batch 4671/11884 - Val_Loss: 22.4614\n",
      "Processing batch 4672/11884 - Val_Loss: 27.1278\n",
      "Processing batch 4673/11884 - Val_Loss: 25.0135\n",
      "Processing batch 4674/11884 - Val_Loss: 25.8508\n",
      "Processing batch 4675/11884 - Val_Loss: 26.1971\n",
      "Processing batch 4676/11884 - Val_Loss: 24.7395\n",
      "Processing batch 4677/11884 - Val_Loss: 26.0734\n",
      "Processing batch 4678/11884 - Val_Loss: 23.1233\n",
      "Processing batch 4679/11884 - Val_Loss: 27.0424\n",
      "Processing batch 4680/11884 - Val_Loss: 25.5207\n",
      "Processing batch 4681/11884 - Val_Loss: 25.7017\n",
      "Processing batch 4682/11884 - Val_Loss: 22.9488\n",
      "Processing batch 4683/11884 - Val_Loss: 24.3747\n",
      "Processing batch 4684/11884 - Val_Loss: 24.8141\n",
      "Processing batch 4685/11884 - Val_Loss: 27.8475\n",
      "Processing batch 4686/11884 - Val_Loss: 26.1654\n",
      "Processing batch 4687/11884 - Val_Loss: 25.6104\n",
      "Processing batch 4688/11884 - Val_Loss: 26.2355\n",
      "Processing batch 4689/11884 - Val_Loss: 23.3261\n",
      "Processing batch 4690/11884 - Val_Loss: 26.6812\n",
      "Processing batch 4691/11884 - Val_Loss: 25.0990\n",
      "Processing batch 4692/11884 - Val_Loss: 25.7330\n",
      "Processing batch 4693/11884 - Val_Loss: 27.5336\n",
      "Processing batch 4694/11884 - Val_Loss: 25.6649\n",
      "Processing batch 4695/11884 - Val_Loss: 26.0763\n",
      "Processing batch 4696/11884 - Val_Loss: 25.4585\n",
      "Processing batch 4697/11884 - Val_Loss: 26.5132\n",
      "Processing batch 4698/11884 - Val_Loss: 26.5388\n",
      "Processing batch 4699/11884 - Val_Loss: 24.5045\n",
      "Processing batch 4700/11884 - Val_Loss: 25.4960\n",
      "Processing batch 4701/11884 - Val_Loss: 24.6710\n",
      "Processing batch 4702/11884 - Val_Loss: 26.2222\n",
      "Processing batch 4703/11884 - Val_Loss: 24.2229\n",
      "Processing batch 4704/11884 - Val_Loss: 27.2966\n",
      "Processing batch 4705/11884 - Val_Loss: 24.6733\n",
      "Processing batch 4706/11884 - Val_Loss: 25.2427\n",
      "Processing batch 4707/11884 - Val_Loss: 27.8470\n",
      "Processing batch 4708/11884 - Val_Loss: 25.6323\n",
      "Processing batch 4709/11884 - Val_Loss: 29.4965\n",
      "Processing batch 4710/11884 - Val_Loss: 24.8862\n",
      "Processing batch 4711/11884 - Val_Loss: 26.4379\n",
      "Processing batch 4712/11884 - Val_Loss: 27.4057\n",
      "Processing batch 4713/11884 - Val_Loss: 25.5024\n",
      "Processing batch 4714/11884 - Val_Loss: 26.1684\n",
      "Processing batch 4715/11884 - Val_Loss: 25.0345\n",
      "Processing batch 4716/11884 - Val_Loss: 29.4671\n",
      "Processing batch 4717/11884 - Val_Loss: 25.4943\n",
      "Processing batch 4718/11884 - Val_Loss: 26.4900\n",
      "Processing batch 4719/11884 - Val_Loss: 25.9847\n",
      "Processing batch 4720/11884 - Val_Loss: 23.8330\n",
      "Processing batch 4721/11884 - Val_Loss: 25.6892\n",
      "Processing batch 4722/11884 - Val_Loss: 26.2453\n",
      "Processing batch 4723/11884 - Val_Loss: 26.9654\n",
      "Processing batch 4724/11884 - Val_Loss: 25.0341\n",
      "Processing batch 4725/11884 - Val_Loss: 26.1928\n",
      "Processing batch 4726/11884 - Val_Loss: 26.5756\n",
      "Processing batch 4727/11884 - Val_Loss: 26.8308\n",
      "Processing batch 4728/11884 - Val_Loss: 25.7377\n",
      "Processing batch 4729/11884 - Val_Loss: 26.1315\n",
      "Processing batch 4730/11884 - Val_Loss: 24.8899\n",
      "Processing batch 4731/11884 - Val_Loss: 28.9556\n",
      "Processing batch 4732/11884 - Val_Loss: 23.0400\n",
      "Processing batch 4733/11884 - Val_Loss: 27.2422\n",
      "Processing batch 4734/11884 - Val_Loss: 30.2354\n",
      "Processing batch 4735/11884 - Val_Loss: 25.3055\n",
      "Processing batch 4736/11884 - Val_Loss: 25.3672\n",
      "Processing batch 4737/11884 - Val_Loss: 26.2962\n",
      "Processing batch 4738/11884 - Val_Loss: 24.2740\n",
      "Processing batch 4739/11884 - Val_Loss: 25.1795\n",
      "Processing batch 4740/11884 - Val_Loss: 25.7957\n",
      "Processing batch 4741/11884 - Val_Loss: 22.9925\n",
      "Processing batch 4742/11884 - Val_Loss: 26.8536\n",
      "Processing batch 4743/11884 - Val_Loss: 24.7275\n",
      "Processing batch 4744/11884 - Val_Loss: 27.6349\n",
      "Processing batch 4745/11884 - Val_Loss: 28.5959\n",
      "Processing batch 4746/11884 - Val_Loss: 28.4021\n",
      "Processing batch 4747/11884 - Val_Loss: 25.9320\n",
      "Processing batch 4748/11884 - Val_Loss: 25.7028\n",
      "Processing batch 4749/11884 - Val_Loss: 24.5373\n",
      "Processing batch 4750/11884 - Val_Loss: 24.5724\n",
      "Processing batch 4751/11884 - Val_Loss: 25.7865\n",
      "Processing batch 4752/11884 - Val_Loss: 23.5485\n",
      "Processing batch 4753/11884 - Val_Loss: 25.5674\n",
      "Processing batch 4754/11884 - Val_Loss: 29.7768\n",
      "Processing batch 4755/11884 - Val_Loss: 26.9772\n",
      "Processing batch 4756/11884 - Val_Loss: 29.6699\n",
      "Processing batch 4757/11884 - Val_Loss: 25.0931\n",
      "Processing batch 4758/11884 - Val_Loss: 27.2821\n",
      "Processing batch 4759/11884 - Val_Loss: 23.8320\n",
      "Processing batch 4760/11884 - Val_Loss: 25.6941\n",
      "Processing batch 4761/11884 - Val_Loss: 25.7751\n",
      "Processing batch 4762/11884 - Val_Loss: 26.7148\n",
      "Processing batch 4763/11884 - Val_Loss: 26.5246\n",
      "Processing batch 4764/11884 - Val_Loss: 23.4438\n",
      "Processing batch 4765/11884 - Val_Loss: 24.2921\n",
      "Processing batch 4766/11884 - Val_Loss: 24.9488\n",
      "Processing batch 4767/11884 - Val_Loss: 26.2405\n",
      "Processing batch 4768/11884 - Val_Loss: 25.1508\n",
      "Processing batch 4769/11884 - Val_Loss: 26.2160\n",
      "Processing batch 4770/11884 - Val_Loss: 25.0162\n",
      "Processing batch 4771/11884 - Val_Loss: 23.2447\n",
      "Processing batch 4772/11884 - Val_Loss: 25.0048\n",
      "Processing batch 4773/11884 - Val_Loss: 24.6440\n",
      "Processing batch 4774/11884 - Val_Loss: 27.2870\n",
      "Processing batch 4775/11884 - Val_Loss: 27.1260\n",
      "Processing batch 4776/11884 - Val_Loss: 26.8115\n",
      "Processing batch 4777/11884 - Val_Loss: 24.2841\n",
      "Processing batch 4778/11884 - Val_Loss: 23.3800\n",
      "Processing batch 4779/11884 - Val_Loss: 22.6215\n",
      "Processing batch 4780/11884 - Val_Loss: 27.2526\n",
      "Processing batch 4781/11884 - Val_Loss: 24.4787\n",
      "Processing batch 4782/11884 - Val_Loss: 25.7095\n",
      "Processing batch 4783/11884 - Val_Loss: 25.8149\n",
      "Processing batch 4784/11884 - Val_Loss: 25.6986\n",
      "Processing batch 4785/11884 - Val_Loss: 26.2668\n",
      "Processing batch 4786/11884 - Val_Loss: 23.8555\n",
      "Processing batch 4787/11884 - Val_Loss: 24.2583\n",
      "Processing batch 4788/11884 - Val_Loss: 25.8858\n",
      "Processing batch 4789/11884 - Val_Loss: 26.5467\n",
      "Processing batch 4790/11884 - Val_Loss: 25.5060\n",
      "Processing batch 4791/11884 - Val_Loss: 24.8996\n",
      "Processing batch 4792/11884 - Val_Loss: 24.4086\n",
      "Processing batch 4793/11884 - Val_Loss: 26.4915\n",
      "Processing batch 4794/11884 - Val_Loss: 23.6474\n",
      "Processing batch 4795/11884 - Val_Loss: 26.7518\n",
      "Processing batch 4796/11884 - Val_Loss: 28.5765\n",
      "Processing batch 4797/11884 - Val_Loss: 25.3205\n",
      "Processing batch 4798/11884 - Val_Loss: 27.8882\n",
      "Processing batch 4799/11884 - Val_Loss: 25.2720\n",
      "Processing batch 4800/11884 - Val_Loss: 26.7416\n",
      "Processing batch 4801/11884 - Val_Loss: 26.3776\n",
      "Processing batch 4802/11884 - Val_Loss: 29.0422\n",
      "Processing batch 4803/11884 - Val_Loss: 26.2525\n",
      "Processing batch 4804/11884 - Val_Loss: 24.5248\n",
      "Processing batch 4805/11884 - Val_Loss: 26.8490\n",
      "Processing batch 4806/11884 - Val_Loss: 26.9883\n",
      "Processing batch 4807/11884 - Val_Loss: 26.6158\n",
      "Processing batch 4808/11884 - Val_Loss: 24.7915\n",
      "Processing batch 4809/11884 - Val_Loss: 26.9442\n",
      "Processing batch 4810/11884 - Val_Loss: 26.4062\n",
      "Processing batch 4811/11884 - Val_Loss: 27.4993\n",
      "Processing batch 4812/11884 - Val_Loss: 25.4310\n",
      "Processing batch 4813/11884 - Val_Loss: 25.7693\n",
      "Processing batch 4814/11884 - Val_Loss: 27.6160\n",
      "Processing batch 4815/11884 - Val_Loss: 24.5714\n",
      "Processing batch 4816/11884 - Val_Loss: 24.8753\n",
      "Processing batch 4817/11884 - Val_Loss: 24.0804\n",
      "Processing batch 4818/11884 - Val_Loss: 26.8828\n",
      "Processing batch 4819/11884 - Val_Loss: 27.4316\n",
      "Processing batch 4820/11884 - Val_Loss: 27.2328\n",
      "Processing batch 4821/11884 - Val_Loss: 28.1748\n",
      "Processing batch 4822/11884 - Val_Loss: 26.1427\n",
      "Processing batch 4823/11884 - Val_Loss: 25.6843\n",
      "Processing batch 4824/11884 - Val_Loss: 27.0789\n",
      "Processing batch 4825/11884 - Val_Loss: 25.5642\n",
      "Processing batch 4826/11884 - Val_Loss: 25.0211\n",
      "Processing batch 4827/11884 - Val_Loss: 24.5383\n",
      "Processing batch 4828/11884 - Val_Loss: 25.7013\n",
      "Processing batch 4829/11884 - Val_Loss: 26.1650\n",
      "Processing batch 4830/11884 - Val_Loss: 25.7754\n",
      "Processing batch 4831/11884 - Val_Loss: 23.3228\n",
      "Processing batch 4832/11884 - Val_Loss: 24.3437\n",
      "Processing batch 4833/11884 - Val_Loss: 23.7346\n",
      "Processing batch 4834/11884 - Val_Loss: 25.5130\n",
      "Processing batch 4835/11884 - Val_Loss: 22.5649\n",
      "Processing batch 4836/11884 - Val_Loss: 25.8378\n",
      "Processing batch 4837/11884 - Val_Loss: 23.5094\n",
      "Processing batch 4838/11884 - Val_Loss: 25.6319\n",
      "Processing batch 4839/11884 - Val_Loss: 25.2137\n",
      "Processing batch 4840/11884 - Val_Loss: 24.1564\n",
      "Processing batch 4841/11884 - Val_Loss: 26.5781\n",
      "Processing batch 4842/11884 - Val_Loss: 25.7165\n",
      "Processing batch 4843/11884 - Val_Loss: 25.5562\n",
      "Processing batch 4844/11884 - Val_Loss: 26.2581\n",
      "Processing batch 4845/11884 - Val_Loss: 29.2036\n",
      "Processing batch 4846/11884 - Val_Loss: 27.2843\n",
      "Processing batch 4847/11884 - Val_Loss: 24.8674\n",
      "Processing batch 4848/11884 - Val_Loss: 27.4161\n",
      "Processing batch 4849/11884 - Val_Loss: 25.9496\n",
      "Processing batch 4850/11884 - Val_Loss: 25.5760\n",
      "Processing batch 4851/11884 - Val_Loss: 28.4545\n",
      "Processing batch 4852/11884 - Val_Loss: 25.6056\n",
      "Processing batch 4853/11884 - Val_Loss: 24.6535\n",
      "Processing batch 4854/11884 - Val_Loss: 22.8793\n",
      "Processing batch 4855/11884 - Val_Loss: 25.7726\n",
      "Processing batch 4856/11884 - Val_Loss: 25.3265\n",
      "Processing batch 4857/11884 - Val_Loss: 24.1589\n",
      "Processing batch 4858/11884 - Val_Loss: 27.0345\n",
      "Processing batch 4859/11884 - Val_Loss: 23.7238\n",
      "Processing batch 4860/11884 - Val_Loss: 25.5059\n",
      "Processing batch 4861/11884 - Val_Loss: 30.1837\n",
      "Processing batch 4862/11884 - Val_Loss: 25.2630\n",
      "Processing batch 4863/11884 - Val_Loss: 26.7656\n",
      "Processing batch 4864/11884 - Val_Loss: 26.4535\n",
      "Processing batch 4865/11884 - Val_Loss: 27.8820\n",
      "Processing batch 4866/11884 - Val_Loss: 25.0952\n",
      "Processing batch 4867/11884 - Val_Loss: 25.2951\n",
      "Processing batch 4868/11884 - Val_Loss: 26.2692\n",
      "Processing batch 4869/11884 - Val_Loss: 26.1314\n",
      "Processing batch 4870/11884 - Val_Loss: 24.4142\n",
      "Processing batch 4871/11884 - Val_Loss: 23.1417\n",
      "Processing batch 4872/11884 - Val_Loss: 27.1249\n",
      "Processing batch 4873/11884 - Val_Loss: 26.1128\n",
      "Processing batch 4874/11884 - Val_Loss: 24.1705\n",
      "Processing batch 4875/11884 - Val_Loss: 25.5589\n",
      "Processing batch 4876/11884 - Val_Loss: 27.2293\n",
      "Processing batch 4877/11884 - Val_Loss: 25.5170\n",
      "Processing batch 4878/11884 - Val_Loss: 25.0025\n",
      "Processing batch 4879/11884 - Val_Loss: 25.0631\n",
      "Processing batch 4880/11884 - Val_Loss: 24.9691\n",
      "Processing batch 4881/11884 - Val_Loss: 26.3985\n",
      "Processing batch 4882/11884 - Val_Loss: 22.3218\n",
      "Processing batch 4883/11884 - Val_Loss: 25.0600\n",
      "Processing batch 4884/11884 - Val_Loss: 23.2404\n",
      "Processing batch 4885/11884 - Val_Loss: 25.3711\n",
      "Processing batch 4886/11884 - Val_Loss: 27.2907\n",
      "Processing batch 4887/11884 - Val_Loss: 28.2652\n",
      "Processing batch 4888/11884 - Val_Loss: 28.1833\n",
      "Processing batch 4889/11884 - Val_Loss: 28.0593\n",
      "Processing batch 4890/11884 - Val_Loss: 23.5713\n",
      "Processing batch 4891/11884 - Val_Loss: 24.9385\n",
      "Processing batch 4892/11884 - Val_Loss: 26.9279\n",
      "Processing batch 4893/11884 - Val_Loss: 26.5949\n",
      "Processing batch 4894/11884 - Val_Loss: 27.5875\n",
      "Processing batch 4895/11884 - Val_Loss: 24.5095\n",
      "Processing batch 4896/11884 - Val_Loss: 29.7096\n",
      "Processing batch 4897/11884 - Val_Loss: 28.1220\n",
      "Processing batch 4898/11884 - Val_Loss: 23.9215\n",
      "Processing batch 4899/11884 - Val_Loss: 26.4732\n",
      "Processing batch 4900/11884 - Val_Loss: 23.5887\n",
      "Processing batch 4901/11884 - Val_Loss: 26.2901\n",
      "Processing batch 4902/11884 - Val_Loss: 27.8584\n",
      "Processing batch 4903/11884 - Val_Loss: 25.0140\n",
      "Processing batch 4904/11884 - Val_Loss: 26.5387\n",
      "Processing batch 4905/11884 - Val_Loss: 26.9234\n",
      "Processing batch 4906/11884 - Val_Loss: 27.6772\n",
      "Processing batch 4907/11884 - Val_Loss: 26.9524\n",
      "Processing batch 4908/11884 - Val_Loss: 25.2165\n",
      "Processing batch 4909/11884 - Val_Loss: 24.8819\n",
      "Processing batch 4910/11884 - Val_Loss: 26.6456\n",
      "Processing batch 4911/11884 - Val_Loss: 26.5713\n",
      "Processing batch 4912/11884 - Val_Loss: 24.4922\n",
      "Processing batch 4913/11884 - Val_Loss: 24.3868\n",
      "Processing batch 4914/11884 - Val_Loss: 26.1577\n",
      "Processing batch 4915/11884 - Val_Loss: 25.4666\n",
      "Processing batch 4916/11884 - Val_Loss: 25.1898\n",
      "Processing batch 4917/11884 - Val_Loss: 28.7382\n",
      "Processing batch 4918/11884 - Val_Loss: 23.7643\n",
      "Processing batch 4919/11884 - Val_Loss: 26.8120\n",
      "Processing batch 4920/11884 - Val_Loss: 25.6192\n",
      "Processing batch 4921/11884 - Val_Loss: 23.6295\n",
      "Processing batch 4922/11884 - Val_Loss: 23.1655\n",
      "Processing batch 4923/11884 - Val_Loss: 25.4323\n",
      "Processing batch 4924/11884 - Val_Loss: 23.8092\n",
      "Processing batch 4925/11884 - Val_Loss: 27.4926\n",
      "Processing batch 4926/11884 - Val_Loss: 25.4177\n",
      "Processing batch 4927/11884 - Val_Loss: 25.2984\n",
      "Processing batch 4928/11884 - Val_Loss: 27.3281\n",
      "Processing batch 4929/11884 - Val_Loss: 23.6392\n",
      "Processing batch 4930/11884 - Val_Loss: 28.9364\n",
      "Processing batch 4931/11884 - Val_Loss: 22.5124\n",
      "Processing batch 4932/11884 - Val_Loss: 24.2205\n",
      "Processing batch 4933/11884 - Val_Loss: 24.1653\n",
      "Processing batch 4934/11884 - Val_Loss: 25.8932\n",
      "Processing batch 4935/11884 - Val_Loss: 26.1418\n",
      "Processing batch 4936/11884 - Val_Loss: 25.4501\n",
      "Processing batch 4937/11884 - Val_Loss: 25.4489\n",
      "Processing batch 4938/11884 - Val_Loss: 26.1979\n",
      "Processing batch 4939/11884 - Val_Loss: 23.4328\n",
      "Processing batch 4940/11884 - Val_Loss: 25.5240\n",
      "Processing batch 4941/11884 - Val_Loss: 26.7152\n",
      "Processing batch 4942/11884 - Val_Loss: 25.5385\n",
      "Processing batch 4943/11884 - Val_Loss: 24.2516\n",
      "Processing batch 4944/11884 - Val_Loss: 26.1914\n",
      "Processing batch 4945/11884 - Val_Loss: 25.4223\n",
      "Processing batch 4946/11884 - Val_Loss: 27.4014\n",
      "Processing batch 4947/11884 - Val_Loss: 27.1494\n",
      "Processing batch 4948/11884 - Val_Loss: 28.6949\n",
      "Processing batch 4949/11884 - Val_Loss: 25.4616\n",
      "Processing batch 4950/11884 - Val_Loss: 26.2095\n",
      "Processing batch 4951/11884 - Val_Loss: 24.6741\n",
      "Processing batch 4952/11884 - Val_Loss: 25.9319\n",
      "Processing batch 4953/11884 - Val_Loss: 26.8068\n",
      "Processing batch 4954/11884 - Val_Loss: 27.1135\n",
      "Processing batch 4955/11884 - Val_Loss: 26.8536\n",
      "Processing batch 4956/11884 - Val_Loss: 21.1854\n",
      "Processing batch 4957/11884 - Val_Loss: 29.4058\n",
      "Processing batch 4958/11884 - Val_Loss: 25.1941\n",
      "Processing batch 4959/11884 - Val_Loss: 24.0787\n",
      "Processing batch 4960/11884 - Val_Loss: 26.3877\n",
      "Processing batch 4961/11884 - Val_Loss: 24.5659\n",
      "Processing batch 4962/11884 - Val_Loss: 25.1102\n",
      "Processing batch 4963/11884 - Val_Loss: 23.7292\n",
      "Processing batch 4964/11884 - Val_Loss: 24.0720\n",
      "Processing batch 4965/11884 - Val_Loss: 25.2412\n",
      "Processing batch 4966/11884 - Val_Loss: 24.2524\n",
      "Processing batch 4967/11884 - Val_Loss: 26.9321\n",
      "Processing batch 4968/11884 - Val_Loss: 21.6038\n",
      "Processing batch 4969/11884 - Val_Loss: 26.7609\n",
      "Processing batch 4970/11884 - Val_Loss: 26.3062\n",
      "Processing batch 4971/11884 - Val_Loss: 26.9175\n",
      "Processing batch 4972/11884 - Val_Loss: 24.3557\n",
      "Processing batch 4973/11884 - Val_Loss: 25.8475\n",
      "Processing batch 4974/11884 - Val_Loss: 25.0659\n",
      "Processing batch 4975/11884 - Val_Loss: 26.6402\n",
      "Processing batch 4976/11884 - Val_Loss: 27.5107\n",
      "Processing batch 4977/11884 - Val_Loss: 27.8286\n",
      "Processing batch 4978/11884 - Val_Loss: 22.0631\n",
      "Processing batch 4979/11884 - Val_Loss: 25.4027\n",
      "Processing batch 4980/11884 - Val_Loss: 24.2950\n",
      "Processing batch 4981/11884 - Val_Loss: 25.0432\n",
      "Processing batch 4982/11884 - Val_Loss: 24.3995\n",
      "Processing batch 4983/11884 - Val_Loss: 24.5362\n",
      "Processing batch 4984/11884 - Val_Loss: 24.7029\n",
      "Processing batch 4985/11884 - Val_Loss: 24.0401\n",
      "Processing batch 4986/11884 - Val_Loss: 26.0986\n",
      "Processing batch 4987/11884 - Val_Loss: 26.9921\n",
      "Processing batch 4988/11884 - Val_Loss: 29.6976\n",
      "Processing batch 4989/11884 - Val_Loss: 24.3167\n",
      "Processing batch 4990/11884 - Val_Loss: 29.4341\n",
      "Processing batch 4991/11884 - Val_Loss: 27.5957\n",
      "Processing batch 4992/11884 - Val_Loss: 23.8574\n",
      "Processing batch 4993/11884 - Val_Loss: 22.9520\n",
      "Processing batch 4994/11884 - Val_Loss: 27.5532\n",
      "Processing batch 4995/11884 - Val_Loss: 25.5810\n",
      "Processing batch 4996/11884 - Val_Loss: 24.7657\n",
      "Processing batch 4997/11884 - Val_Loss: 23.5191\n",
      "Processing batch 4998/11884 - Val_Loss: 26.5280\n",
      "Processing batch 4999/11884 - Val_Loss: 28.2839\n",
      "Processing batch 5000/11884 - Val_Loss: 22.1417\n",
      "Processing batch 5001/11884 - Val_Loss: 28.1071\n",
      "Processing batch 5002/11884 - Val_Loss: 24.5284\n",
      "Processing batch 5003/11884 - Val_Loss: 25.1589\n",
      "Processing batch 5004/11884 - Val_Loss: 26.0663\n",
      "Processing batch 5005/11884 - Val_Loss: 28.6187\n",
      "Processing batch 5006/11884 - Val_Loss: 26.9722\n",
      "Processing batch 5007/11884 - Val_Loss: 24.6683\n",
      "Processing batch 5008/11884 - Val_Loss: 26.2953\n",
      "Processing batch 5009/11884 - Val_Loss: 23.9905\n",
      "Processing batch 5010/11884 - Val_Loss: 24.9989\n",
      "Processing batch 5011/11884 - Val_Loss: 25.3393\n",
      "Processing batch 5012/11884 - Val_Loss: 25.8519\n",
      "Processing batch 5013/11884 - Val_Loss: 23.7836\n",
      "Processing batch 5014/11884 - Val_Loss: 29.0590\n",
      "Processing batch 5015/11884 - Val_Loss: 23.4898\n",
      "Processing batch 5016/11884 - Val_Loss: 28.2638\n",
      "Processing batch 5017/11884 - Val_Loss: 25.1345\n",
      "Processing batch 5018/11884 - Val_Loss: 25.5631\n",
      "Processing batch 5019/11884 - Val_Loss: 25.4582\n",
      "Processing batch 5020/11884 - Val_Loss: 26.6412\n",
      "Processing batch 5021/11884 - Val_Loss: 25.5949\n",
      "Processing batch 5022/11884 - Val_Loss: 26.0087\n",
      "Processing batch 5023/11884 - Val_Loss: 26.8168\n",
      "Processing batch 5024/11884 - Val_Loss: 23.3679\n",
      "Processing batch 5025/11884 - Val_Loss: 25.4892\n",
      "Processing batch 5026/11884 - Val_Loss: 26.9618\n",
      "Processing batch 5027/11884 - Val_Loss: 24.7962\n",
      "Processing batch 5028/11884 - Val_Loss: 26.3090\n",
      "Processing batch 5029/11884 - Val_Loss: 25.3370\n",
      "Processing batch 5030/11884 - Val_Loss: 25.3235\n",
      "Processing batch 5031/11884 - Val_Loss: 24.1873\n",
      "Processing batch 5032/11884 - Val_Loss: 26.7698\n",
      "Processing batch 5033/11884 - Val_Loss: 23.0028\n",
      "Processing batch 5034/11884 - Val_Loss: 22.0713\n",
      "Processing batch 5035/11884 - Val_Loss: 25.2637\n",
      "Processing batch 5036/11884 - Val_Loss: 27.3252\n",
      "Processing batch 5037/11884 - Val_Loss: 24.5617\n",
      "Processing batch 5038/11884 - Val_Loss: 25.9765\n",
      "Processing batch 5039/11884 - Val_Loss: 23.7435\n",
      "Processing batch 5040/11884 - Val_Loss: 22.6538\n",
      "Processing batch 5041/11884 - Val_Loss: 24.2552\n",
      "Processing batch 5042/11884 - Val_Loss: 24.1622\n",
      "Processing batch 5043/11884 - Val_Loss: 23.5696\n",
      "Processing batch 5044/11884 - Val_Loss: 27.5785\n",
      "Processing batch 5045/11884 - Val_Loss: 26.6477\n",
      "Processing batch 5046/11884 - Val_Loss: 24.8816\n",
      "Processing batch 5047/11884 - Val_Loss: 25.4913\n",
      "Processing batch 5048/11884 - Val_Loss: 24.0101\n",
      "Processing batch 5049/11884 - Val_Loss: 27.6673\n",
      "Processing batch 5050/11884 - Val_Loss: 24.6605\n",
      "Processing batch 5051/11884 - Val_Loss: 25.0570\n",
      "Processing batch 5052/11884 - Val_Loss: 25.2948\n",
      "Processing batch 5053/11884 - Val_Loss: 26.0208\n",
      "Processing batch 5054/11884 - Val_Loss: 28.1539\n",
      "Processing batch 5055/11884 - Val_Loss: 28.0194\n",
      "Processing batch 5056/11884 - Val_Loss: 25.1891\n",
      "Processing batch 5057/11884 - Val_Loss: 25.5783\n",
      "Processing batch 5058/11884 - Val_Loss: 24.3818\n",
      "Processing batch 5059/11884 - Val_Loss: 27.2499\n",
      "Processing batch 5060/11884 - Val_Loss: 25.0999\n",
      "Processing batch 5061/11884 - Val_Loss: 28.2146\n",
      "Processing batch 5062/11884 - Val_Loss: 21.9526\n",
      "Processing batch 5063/11884 - Val_Loss: 21.3762\n",
      "Processing batch 5064/11884 - Val_Loss: 26.3893\n",
      "Processing batch 5065/11884 - Val_Loss: 23.5745\n",
      "Processing batch 5066/11884 - Val_Loss: 24.8957\n",
      "Processing batch 5067/11884 - Val_Loss: 25.3421\n",
      "Processing batch 5068/11884 - Val_Loss: 26.8152\n",
      "Processing batch 5069/11884 - Val_Loss: 24.7383\n",
      "Processing batch 5070/11884 - Val_Loss: 23.5674\n",
      "Processing batch 5071/11884 - Val_Loss: 28.2111\n",
      "Processing batch 5072/11884 - Val_Loss: 23.8331\n",
      "Processing batch 5073/11884 - Val_Loss: 26.3257\n",
      "Processing batch 5074/11884 - Val_Loss: 26.7788\n",
      "Processing batch 5075/11884 - Val_Loss: 26.1848\n",
      "Processing batch 5076/11884 - Val_Loss: 24.9770\n",
      "Processing batch 5077/11884 - Val_Loss: 24.2607\n",
      "Processing batch 5078/11884 - Val_Loss: 25.4965\n",
      "Processing batch 5079/11884 - Val_Loss: 26.6426\n",
      "Processing batch 5080/11884 - Val_Loss: 25.2001\n",
      "Processing batch 5081/11884 - Val_Loss: 23.4900\n",
      "Processing batch 5082/11884 - Val_Loss: 26.6134\n",
      "Processing batch 5083/11884 - Val_Loss: 24.0130\n",
      "Processing batch 5084/11884 - Val_Loss: 23.6494\n",
      "Processing batch 5085/11884 - Val_Loss: 25.6656\n",
      "Processing batch 5086/11884 - Val_Loss: 28.7279\n",
      "Processing batch 5087/11884 - Val_Loss: 26.1426\n",
      "Processing batch 5088/11884 - Val_Loss: 24.7824\n",
      "Processing batch 5089/11884 - Val_Loss: 27.3994\n",
      "Processing batch 5090/11884 - Val_Loss: 22.8687\n",
      "Processing batch 5091/11884 - Val_Loss: 24.8884\n",
      "Processing batch 5092/11884 - Val_Loss: 24.8672\n",
      "Processing batch 5093/11884 - Val_Loss: 25.9516\n",
      "Processing batch 5094/11884 - Val_Loss: 27.0272\n",
      "Processing batch 5095/11884 - Val_Loss: 24.8758\n",
      "Processing batch 5096/11884 - Val_Loss: 27.5620\n",
      "Processing batch 5097/11884 - Val_Loss: 25.7859\n",
      "Processing batch 5098/11884 - Val_Loss: 25.0433\n",
      "Processing batch 5099/11884 - Val_Loss: 23.9881\n",
      "Processing batch 5100/11884 - Val_Loss: 25.8486\n",
      "Processing batch 5101/11884 - Val_Loss: 23.9026\n",
      "Processing batch 5102/11884 - Val_Loss: 25.7521\n",
      "Processing batch 5103/11884 - Val_Loss: 26.4286\n",
      "Processing batch 5104/11884 - Val_Loss: 26.1057\n",
      "Processing batch 5105/11884 - Val_Loss: 25.6436\n",
      "Processing batch 5106/11884 - Val_Loss: 23.5480\n",
      "Processing batch 5107/11884 - Val_Loss: 22.1326\n",
      "Processing batch 5108/11884 - Val_Loss: 21.5665\n",
      "Processing batch 5109/11884 - Val_Loss: 27.0519\n",
      "Processing batch 5110/11884 - Val_Loss: 26.3697\n",
      "Processing batch 5111/11884 - Val_Loss: 27.8807\n",
      "Processing batch 5112/11884 - Val_Loss: 23.1042\n",
      "Processing batch 5113/11884 - Val_Loss: 29.0734\n",
      "Processing batch 5114/11884 - Val_Loss: 24.5272\n",
      "Processing batch 5115/11884 - Val_Loss: 25.8925\n",
      "Processing batch 5116/11884 - Val_Loss: 26.3802\n",
      "Processing batch 5117/11884 - Val_Loss: 24.5934\n",
      "Processing batch 5118/11884 - Val_Loss: 28.8681\n",
      "Processing batch 5119/11884 - Val_Loss: 27.4219\n",
      "Processing batch 5120/11884 - Val_Loss: 23.1639\n",
      "Processing batch 5121/11884 - Val_Loss: 25.2570\n",
      "Processing batch 5122/11884 - Val_Loss: 26.6588\n",
      "Processing batch 5123/11884 - Val_Loss: 26.9564\n",
      "Processing batch 5124/11884 - Val_Loss: 25.0462\n",
      "Processing batch 5125/11884 - Val_Loss: 25.5554\n",
      "Processing batch 5126/11884 - Val_Loss: 25.9600\n",
      "Processing batch 5127/11884 - Val_Loss: 26.3038\n",
      "Processing batch 5128/11884 - Val_Loss: 27.9311\n",
      "Processing batch 5129/11884 - Val_Loss: 27.1744\n",
      "Processing batch 5130/11884 - Val_Loss: 24.9572\n",
      "Processing batch 5131/11884 - Val_Loss: 24.6763\n",
      "Processing batch 5132/11884 - Val_Loss: 28.2185\n",
      "Processing batch 5133/11884 - Val_Loss: 28.5967\n",
      "Processing batch 5134/11884 - Val_Loss: 25.8810\n",
      "Processing batch 5135/11884 - Val_Loss: 23.0148\n",
      "Processing batch 5136/11884 - Val_Loss: 22.9683\n",
      "Processing batch 5137/11884 - Val_Loss: 29.1394\n",
      "Processing batch 5138/11884 - Val_Loss: 23.8949\n",
      "Processing batch 5139/11884 - Val_Loss: 24.8128\n",
      "Processing batch 5140/11884 - Val_Loss: 28.9289\n",
      "Processing batch 5141/11884 - Val_Loss: 26.2844\n",
      "Processing batch 5142/11884 - Val_Loss: 24.2868\n",
      "Processing batch 5143/11884 - Val_Loss: 24.2437\n",
      "Processing batch 5144/11884 - Val_Loss: 27.0534\n",
      "Processing batch 5145/11884 - Val_Loss: 27.7395\n",
      "Processing batch 5146/11884 - Val_Loss: 24.3934\n",
      "Processing batch 5147/11884 - Val_Loss: 26.0605\n",
      "Processing batch 5148/11884 - Val_Loss: 26.2377\n",
      "Processing batch 5149/11884 - Val_Loss: 28.0080\n",
      "Processing batch 5150/11884 - Val_Loss: 28.2184\n",
      "Processing batch 5151/11884 - Val_Loss: 25.7420\n",
      "Processing batch 5152/11884 - Val_Loss: 24.5348\n",
      "Processing batch 5153/11884 - Val_Loss: 26.5065\n",
      "Processing batch 5154/11884 - Val_Loss: 24.7209\n",
      "Processing batch 5155/11884 - Val_Loss: 25.4813\n",
      "Processing batch 5156/11884 - Val_Loss: 26.4319\n",
      "Processing batch 5157/11884 - Val_Loss: 28.3123\n",
      "Processing batch 5158/11884 - Val_Loss: 27.0756\n",
      "Processing batch 5159/11884 - Val_Loss: 23.3190\n",
      "Processing batch 5160/11884 - Val_Loss: 27.4086\n",
      "Processing batch 5161/11884 - Val_Loss: 26.0409\n",
      "Processing batch 5162/11884 - Val_Loss: 25.5935\n",
      "Processing batch 5163/11884 - Val_Loss: 25.7789\n",
      "Processing batch 5164/11884 - Val_Loss: 25.0655\n",
      "Processing batch 5165/11884 - Val_Loss: 27.2679\n",
      "Processing batch 5166/11884 - Val_Loss: 24.8632\n",
      "Processing batch 5167/11884 - Val_Loss: 25.0807\n",
      "Processing batch 5168/11884 - Val_Loss: 24.3670\n",
      "Processing batch 5169/11884 - Val_Loss: 24.5676\n",
      "Processing batch 5170/11884 - Val_Loss: 22.6332\n",
      "Processing batch 5171/11884 - Val_Loss: 25.1646\n",
      "Processing batch 5172/11884 - Val_Loss: 27.8322\n",
      "Processing batch 5173/11884 - Val_Loss: 26.4748\n",
      "Processing batch 5174/11884 - Val_Loss: 22.5526\n",
      "Processing batch 5175/11884 - Val_Loss: 25.1282\n",
      "Processing batch 5176/11884 - Val_Loss: 23.6797\n",
      "Processing batch 5177/11884 - Val_Loss: 25.8439\n",
      "Processing batch 5178/11884 - Val_Loss: 24.2410\n",
      "Processing batch 5179/11884 - Val_Loss: 26.7432\n",
      "Processing batch 5180/11884 - Val_Loss: 24.2144\n",
      "Processing batch 5181/11884 - Val_Loss: 24.2976\n",
      "Processing batch 5182/11884 - Val_Loss: 26.1322\n",
      "Processing batch 5183/11884 - Val_Loss: 26.7871\n",
      "Processing batch 5184/11884 - Val_Loss: 26.9981\n",
      "Processing batch 5185/11884 - Val_Loss: 25.1425\n",
      "Processing batch 5186/11884 - Val_Loss: 27.4687\n",
      "Processing batch 5187/11884 - Val_Loss: 24.1912\n",
      "Processing batch 5188/11884 - Val_Loss: 22.7254\n",
      "Processing batch 5189/11884 - Val_Loss: 27.5421\n",
      "Processing batch 5190/11884 - Val_Loss: 22.8723\n",
      "Processing batch 5191/11884 - Val_Loss: 25.6035\n",
      "Processing batch 5192/11884 - Val_Loss: 23.7719\n",
      "Processing batch 5193/11884 - Val_Loss: 23.9034\n",
      "Processing batch 5194/11884 - Val_Loss: 26.0501\n",
      "Processing batch 5195/11884 - Val_Loss: 29.3671\n",
      "Processing batch 5196/11884 - Val_Loss: 24.1633\n",
      "Processing batch 5197/11884 - Val_Loss: 23.8252\n",
      "Processing batch 5198/11884 - Val_Loss: 28.5181\n",
      "Processing batch 5199/11884 - Val_Loss: 27.8288\n",
      "Processing batch 5200/11884 - Val_Loss: 25.1565\n",
      "Processing batch 5201/11884 - Val_Loss: 26.3714\n",
      "Processing batch 5202/11884 - Val_Loss: 25.4920\n",
      "Processing batch 5203/11884 - Val_Loss: 27.2471\n",
      "Processing batch 5204/11884 - Val_Loss: 22.7600\n",
      "Processing batch 5205/11884 - Val_Loss: 21.5535\n",
      "Processing batch 5206/11884 - Val_Loss: 25.5545\n",
      "Processing batch 5207/11884 - Val_Loss: 26.0786\n",
      "Processing batch 5208/11884 - Val_Loss: 25.1803\n",
      "Processing batch 5209/11884 - Val_Loss: 25.3320\n",
      "Processing batch 5210/11884 - Val_Loss: 26.0489\n",
      "Processing batch 5211/11884 - Val_Loss: 24.3212\n",
      "Processing batch 5212/11884 - Val_Loss: 24.2246\n",
      "Processing batch 5213/11884 - Val_Loss: 25.4458\n",
      "Processing batch 5214/11884 - Val_Loss: 23.5700\n",
      "Processing batch 5215/11884 - Val_Loss: 26.2819\n",
      "Processing batch 5216/11884 - Val_Loss: 28.5879\n",
      "Processing batch 5217/11884 - Val_Loss: 24.9193\n",
      "Processing batch 5218/11884 - Val_Loss: 27.0695\n",
      "Processing batch 5219/11884 - Val_Loss: 25.6920\n",
      "Processing batch 5220/11884 - Val_Loss: 23.8052\n",
      "Processing batch 5221/11884 - Val_Loss: 25.7432\n",
      "Processing batch 5222/11884 - Val_Loss: 25.2563\n",
      "Processing batch 5223/11884 - Val_Loss: 26.4595\n",
      "Processing batch 5224/11884 - Val_Loss: 27.7559\n",
      "Processing batch 5225/11884 - Val_Loss: 24.3041\n",
      "Processing batch 5226/11884 - Val_Loss: 24.8157\n",
      "Processing batch 5227/11884 - Val_Loss: 24.0527\n",
      "Processing batch 5228/11884 - Val_Loss: 26.0629\n",
      "Processing batch 5229/11884 - Val_Loss: 24.5974\n",
      "Processing batch 5230/11884 - Val_Loss: 24.2655\n",
      "Processing batch 5231/11884 - Val_Loss: 26.0352\n",
      "Processing batch 5232/11884 - Val_Loss: 24.6385\n",
      "Processing batch 5233/11884 - Val_Loss: 25.7120\n",
      "Processing batch 5234/11884 - Val_Loss: 26.4576\n",
      "Processing batch 5235/11884 - Val_Loss: 25.5319\n",
      "Processing batch 5236/11884 - Val_Loss: 25.6930\n",
      "Processing batch 5237/11884 - Val_Loss: 25.4684\n",
      "Processing batch 5238/11884 - Val_Loss: 27.2122\n",
      "Processing batch 5239/11884 - Val_Loss: 23.5109\n",
      "Processing batch 5240/11884 - Val_Loss: 25.7439\n",
      "Processing batch 5241/11884 - Val_Loss: 25.3591\n",
      "Processing batch 5242/11884 - Val_Loss: 30.9865\n",
      "Processing batch 5243/11884 - Val_Loss: 23.4787\n",
      "Processing batch 5244/11884 - Val_Loss: 26.2895\n",
      "Processing batch 5245/11884 - Val_Loss: 24.8266\n",
      "Processing batch 5246/11884 - Val_Loss: 26.1397\n",
      "Processing batch 5247/11884 - Val_Loss: 24.2969\n",
      "Processing batch 5248/11884 - Val_Loss: 28.8910\n",
      "Processing batch 5249/11884 - Val_Loss: 24.1794\n",
      "Processing batch 5250/11884 - Val_Loss: 27.8903\n",
      "Processing batch 5251/11884 - Val_Loss: 26.5889\n",
      "Processing batch 5252/11884 - Val_Loss: 26.0493\n",
      "Processing batch 5253/11884 - Val_Loss: 23.3982\n",
      "Processing batch 5254/11884 - Val_Loss: 25.8473\n",
      "Processing batch 5255/11884 - Val_Loss: 23.0649\n",
      "Processing batch 5256/11884 - Val_Loss: 25.6594\n",
      "Processing batch 5257/11884 - Val_Loss: 24.1508\n",
      "Processing batch 5258/11884 - Val_Loss: 24.0686\n",
      "Processing batch 5259/11884 - Val_Loss: 25.3600\n",
      "Processing batch 5260/11884 - Val_Loss: 22.3092\n",
      "Processing batch 5261/11884 - Val_Loss: 23.0893\n",
      "Processing batch 5262/11884 - Val_Loss: 26.0590\n",
      "Processing batch 5263/11884 - Val_Loss: 25.4509\n",
      "Processing batch 5264/11884 - Val_Loss: 24.0612\n",
      "Processing batch 5265/11884 - Val_Loss: 22.2665\n",
      "Processing batch 5266/11884 - Val_Loss: 25.7954\n",
      "Processing batch 5267/11884 - Val_Loss: 25.1521\n",
      "Processing batch 5268/11884 - Val_Loss: 27.6229\n",
      "Processing batch 5269/11884 - Val_Loss: 23.8259\n",
      "Processing batch 5270/11884 - Val_Loss: 26.7250\n",
      "Processing batch 5271/11884 - Val_Loss: 26.4504\n",
      "Processing batch 5272/11884 - Val_Loss: 24.9881\n",
      "Processing batch 5273/11884 - Val_Loss: 25.5445\n",
      "Processing batch 5274/11884 - Val_Loss: 26.2782\n",
      "Processing batch 5275/11884 - Val_Loss: 23.4129\n",
      "Processing batch 5276/11884 - Val_Loss: 24.9558\n",
      "Processing batch 5277/11884 - Val_Loss: 23.7884\n",
      "Processing batch 5278/11884 - Val_Loss: 24.6381\n",
      "Processing batch 5279/11884 - Val_Loss: 25.3865\n",
      "Processing batch 5280/11884 - Val_Loss: 25.9652\n",
      "Processing batch 5281/11884 - Val_Loss: 26.1922\n",
      "Processing batch 5282/11884 - Val_Loss: 25.2538\n",
      "Processing batch 5283/11884 - Val_Loss: 23.3634\n",
      "Processing batch 5284/11884 - Val_Loss: 24.3762\n",
      "Processing batch 5285/11884 - Val_Loss: 27.1365\n",
      "Processing batch 5286/11884 - Val_Loss: 22.5722\n",
      "Processing batch 5287/11884 - Val_Loss: 26.1126\n",
      "Processing batch 5288/11884 - Val_Loss: 28.1191\n",
      "Processing batch 5289/11884 - Val_Loss: 25.3327\n",
      "Processing batch 5290/11884 - Val_Loss: 23.0702\n",
      "Processing batch 5291/11884 - Val_Loss: 27.6591\n",
      "Processing batch 5292/11884 - Val_Loss: 23.7589\n",
      "Processing batch 5293/11884 - Val_Loss: 23.7948\n",
      "Processing batch 5294/11884 - Val_Loss: 25.0304\n",
      "Processing batch 5295/11884 - Val_Loss: 26.1872\n",
      "Processing batch 5296/11884 - Val_Loss: 26.3284\n",
      "Processing batch 5297/11884 - Val_Loss: 23.7621\n",
      "Processing batch 5298/11884 - Val_Loss: 28.0232\n",
      "Processing batch 5299/11884 - Val_Loss: 29.6082\n",
      "Processing batch 5300/11884 - Val_Loss: 23.3958\n",
      "Processing batch 5301/11884 - Val_Loss: 25.7213\n",
      "Processing batch 5302/11884 - Val_Loss: 27.2200\n",
      "Processing batch 5303/11884 - Val_Loss: 26.0480\n",
      "Processing batch 5304/11884 - Val_Loss: 23.8105\n",
      "Processing batch 5305/11884 - Val_Loss: 24.7938\n",
      "Processing batch 5306/11884 - Val_Loss: 25.0073\n",
      "Processing batch 5307/11884 - Val_Loss: 25.6641\n",
      "Processing batch 5308/11884 - Val_Loss: 23.8844\n",
      "Processing batch 5309/11884 - Val_Loss: 25.8619\n",
      "Processing batch 5310/11884 - Val_Loss: 26.1269\n",
      "Processing batch 5311/11884 - Val_Loss: 25.6507\n",
      "Processing batch 5312/11884 - Val_Loss: 29.6619\n",
      "Processing batch 5313/11884 - Val_Loss: 26.1040\n",
      "Processing batch 5314/11884 - Val_Loss: 27.6410\n",
      "Processing batch 5315/11884 - Val_Loss: 27.9171\n",
      "Processing batch 5316/11884 - Val_Loss: 27.2289\n",
      "Processing batch 5317/11884 - Val_Loss: 26.8700\n",
      "Processing batch 5318/11884 - Val_Loss: 27.3060\n",
      "Processing batch 5319/11884 - Val_Loss: 26.0669\n",
      "Processing batch 5320/11884 - Val_Loss: 25.7696\n",
      "Processing batch 5321/11884 - Val_Loss: 22.8885\n",
      "Processing batch 5322/11884 - Val_Loss: 26.2503\n",
      "Processing batch 5323/11884 - Val_Loss: 24.1606\n",
      "Processing batch 5324/11884 - Val_Loss: 28.0920\n",
      "Processing batch 5325/11884 - Val_Loss: 23.0428\n",
      "Processing batch 5326/11884 - Val_Loss: 24.4261\n",
      "Processing batch 5327/11884 - Val_Loss: 24.3568\n",
      "Processing batch 5328/11884 - Val_Loss: 25.4618\n",
      "Processing batch 5329/11884 - Val_Loss: 25.3156\n",
      "Processing batch 5330/11884 - Val_Loss: 24.9952\n",
      "Processing batch 5331/11884 - Val_Loss: 26.2160\n",
      "Processing batch 5332/11884 - Val_Loss: 24.0048\n",
      "Processing batch 5333/11884 - Val_Loss: 23.2973\n",
      "Processing batch 5334/11884 - Val_Loss: 25.5609\n",
      "Processing batch 5335/11884 - Val_Loss: 23.7493\n",
      "Processing batch 5336/11884 - Val_Loss: 25.2773\n",
      "Processing batch 5337/11884 - Val_Loss: 24.8059\n",
      "Processing batch 5338/11884 - Val_Loss: 27.2991\n",
      "Processing batch 5339/11884 - Val_Loss: 27.9312\n",
      "Processing batch 5340/11884 - Val_Loss: 24.5186\n",
      "Processing batch 5341/11884 - Val_Loss: 26.2801\n",
      "Processing batch 5342/11884 - Val_Loss: 26.9173\n",
      "Processing batch 5343/11884 - Val_Loss: 26.3049\n",
      "Processing batch 5344/11884 - Val_Loss: 28.2215\n",
      "Processing batch 5345/11884 - Val_Loss: 21.7096\n",
      "Processing batch 5346/11884 - Val_Loss: 24.7968\n",
      "Processing batch 5347/11884 - Val_Loss: 25.2769\n",
      "Processing batch 5348/11884 - Val_Loss: 25.3720\n",
      "Processing batch 5349/11884 - Val_Loss: 23.6885\n",
      "Processing batch 5350/11884 - Val_Loss: 25.7831\n",
      "Processing batch 5351/11884 - Val_Loss: 25.1988\n",
      "Processing batch 5352/11884 - Val_Loss: 24.6883\n",
      "Processing batch 5353/11884 - Val_Loss: 25.8666\n",
      "Processing batch 5354/11884 - Val_Loss: 23.0248\n",
      "Processing batch 5355/11884 - Val_Loss: 26.4995\n",
      "Processing batch 5356/11884 - Val_Loss: 26.1300\n",
      "Processing batch 5357/11884 - Val_Loss: 26.2223\n",
      "Processing batch 5358/11884 - Val_Loss: 29.5348\n",
      "Processing batch 5359/11884 - Val_Loss: 25.1068\n",
      "Processing batch 5360/11884 - Val_Loss: 22.5340\n",
      "Processing batch 5361/11884 - Val_Loss: 26.2889\n",
      "Processing batch 5362/11884 - Val_Loss: 26.2180\n",
      "Processing batch 5363/11884 - Val_Loss: 26.5492\n",
      "Processing batch 5364/11884 - Val_Loss: 23.6287\n",
      "Processing batch 5365/11884 - Val_Loss: 24.4110\n",
      "Processing batch 5366/11884 - Val_Loss: 25.2723\n",
      "Processing batch 5367/11884 - Val_Loss: 26.2165\n",
      "Processing batch 5368/11884 - Val_Loss: 25.5124\n",
      "Processing batch 5369/11884 - Val_Loss: 28.9775\n",
      "Processing batch 5370/11884 - Val_Loss: 23.7641\n",
      "Processing batch 5371/11884 - Val_Loss: 25.3903\n",
      "Processing batch 5372/11884 - Val_Loss: 28.0290\n",
      "Processing batch 5373/11884 - Val_Loss: 26.1434\n",
      "Processing batch 5374/11884 - Val_Loss: 23.6280\n",
      "Processing batch 5375/11884 - Val_Loss: 27.0804\n",
      "Processing batch 5376/11884 - Val_Loss: 25.8843\n",
      "Processing batch 5377/11884 - Val_Loss: 25.9734\n",
      "Processing batch 5378/11884 - Val_Loss: 24.9011\n",
      "Processing batch 5379/11884 - Val_Loss: 26.0189\n",
      "Processing batch 5380/11884 - Val_Loss: 23.9080\n",
      "Processing batch 5381/11884 - Val_Loss: 26.6892\n",
      "Processing batch 5382/11884 - Val_Loss: 27.2832\n",
      "Processing batch 5383/11884 - Val_Loss: 25.4073\n",
      "Processing batch 5384/11884 - Val_Loss: 26.5950\n",
      "Processing batch 5385/11884 - Val_Loss: 23.8226\n",
      "Processing batch 5386/11884 - Val_Loss: 22.6429\n",
      "Processing batch 5387/11884 - Val_Loss: 21.7493\n",
      "Processing batch 5388/11884 - Val_Loss: 25.7642\n",
      "Processing batch 5389/11884 - Val_Loss: 26.8936\n",
      "Processing batch 5390/11884 - Val_Loss: 25.3200\n",
      "Processing batch 5391/11884 - Val_Loss: 27.6682\n",
      "Processing batch 5392/11884 - Val_Loss: 25.4131\n",
      "Processing batch 5393/11884 - Val_Loss: 23.0900\n",
      "Processing batch 5394/11884 - Val_Loss: 26.4009\n",
      "Processing batch 5395/11884 - Val_Loss: 25.8716\n",
      "Processing batch 5396/11884 - Val_Loss: 22.5641\n",
      "Processing batch 5397/11884 - Val_Loss: 26.3936\n",
      "Processing batch 5398/11884 - Val_Loss: 22.1470\n",
      "Processing batch 5399/11884 - Val_Loss: 24.2408\n",
      "Processing batch 5400/11884 - Val_Loss: 25.9025\n",
      "Processing batch 5401/11884 - Val_Loss: 26.6328\n",
      "Processing batch 5402/11884 - Val_Loss: 26.9911\n",
      "Processing batch 5403/11884 - Val_Loss: 27.6074\n",
      "Processing batch 5404/11884 - Val_Loss: 25.2597\n",
      "Processing batch 5405/11884 - Val_Loss: 24.7370\n",
      "Processing batch 5406/11884 - Val_Loss: 27.2697\n",
      "Processing batch 5407/11884 - Val_Loss: 28.3450\n",
      "Processing batch 5408/11884 - Val_Loss: 25.9627\n",
      "Processing batch 5409/11884 - Val_Loss: 29.3802\n",
      "Processing batch 5410/11884 - Val_Loss: 26.5101\n",
      "Processing batch 5411/11884 - Val_Loss: 25.7281\n",
      "Processing batch 5412/11884 - Val_Loss: 25.8808\n",
      "Processing batch 5413/11884 - Val_Loss: 25.5627\n",
      "Processing batch 5414/11884 - Val_Loss: 23.9433\n",
      "Processing batch 5415/11884 - Val_Loss: 25.4857\n",
      "Processing batch 5416/11884 - Val_Loss: 25.8203\n",
      "Processing batch 5417/11884 - Val_Loss: 25.7299\n",
      "Processing batch 5418/11884 - Val_Loss: 27.0375\n",
      "Processing batch 5419/11884 - Val_Loss: 26.8585\n",
      "Processing batch 5420/11884 - Val_Loss: 25.2626\n",
      "Processing batch 5421/11884 - Val_Loss: 26.2767\n",
      "Processing batch 5422/11884 - Val_Loss: 25.3164\n",
      "Processing batch 5423/11884 - Val_Loss: 27.9742\n",
      "Processing batch 5424/11884 - Val_Loss: 23.6863\n",
      "Processing batch 5425/11884 - Val_Loss: 24.8165\n",
      "Processing batch 5426/11884 - Val_Loss: 25.5669\n",
      "Processing batch 5427/11884 - Val_Loss: 25.6797\n",
      "Processing batch 5428/11884 - Val_Loss: 25.6613\n",
      "Processing batch 5429/11884 - Val_Loss: 24.8447\n",
      "Processing batch 5430/11884 - Val_Loss: 23.2226\n",
      "Processing batch 5431/11884 - Val_Loss: 25.7398\n",
      "Processing batch 5432/11884 - Val_Loss: 28.6545\n",
      "Processing batch 5433/11884 - Val_Loss: 26.3605\n",
      "Processing batch 5434/11884 - Val_Loss: 24.4838\n",
      "Processing batch 5435/11884 - Val_Loss: 24.7186\n",
      "Processing batch 5436/11884 - Val_Loss: 23.5775\n",
      "Processing batch 5437/11884 - Val_Loss: 29.0523\n",
      "Processing batch 5438/11884 - Val_Loss: 25.1585\n",
      "Processing batch 5439/11884 - Val_Loss: 28.2959\n",
      "Processing batch 5440/11884 - Val_Loss: 22.8293\n",
      "Processing batch 5441/11884 - Val_Loss: 24.9002\n",
      "Processing batch 5442/11884 - Val_Loss: 23.5671\n",
      "Processing batch 5443/11884 - Val_Loss: 27.6223\n",
      "Processing batch 5444/11884 - Val_Loss: 24.3141\n",
      "Processing batch 5445/11884 - Val_Loss: 26.2323\n",
      "Processing batch 5446/11884 - Val_Loss: 24.5170\n",
      "Processing batch 5447/11884 - Val_Loss: 24.0875\n",
      "Processing batch 5448/11884 - Val_Loss: 24.3294\n",
      "Processing batch 5449/11884 - Val_Loss: 24.6678\n",
      "Processing batch 5450/11884 - Val_Loss: 24.3017\n",
      "Processing batch 5451/11884 - Val_Loss: 24.9599\n",
      "Processing batch 5452/11884 - Val_Loss: 25.7470\n",
      "Processing batch 5453/11884 - Val_Loss: 25.7610\n",
      "Processing batch 5454/11884 - Val_Loss: 26.2358\n",
      "Processing batch 5455/11884 - Val_Loss: 24.6609\n",
      "Processing batch 5456/11884 - Val_Loss: 26.9042\n",
      "Processing batch 5457/11884 - Val_Loss: 24.5599\n",
      "Processing batch 5458/11884 - Val_Loss: 25.2238\n",
      "Processing batch 5459/11884 - Val_Loss: 25.7804\n",
      "Processing batch 5460/11884 - Val_Loss: 24.6281\n",
      "Processing batch 5461/11884 - Val_Loss: 24.6007\n",
      "Processing batch 5462/11884 - Val_Loss: 24.4919\n",
      "Processing batch 5463/11884 - Val_Loss: 27.7556\n",
      "Processing batch 5464/11884 - Val_Loss: 24.4558\n",
      "Processing batch 5465/11884 - Val_Loss: 26.4534\n",
      "Processing batch 5466/11884 - Val_Loss: 27.1528\n",
      "Processing batch 5467/11884 - Val_Loss: 24.9967\n",
      "Processing batch 5468/11884 - Val_Loss: 23.3549\n",
      "Processing batch 5469/11884 - Val_Loss: 25.2585\n",
      "Processing batch 5470/11884 - Val_Loss: 26.2275\n",
      "Processing batch 5471/11884 - Val_Loss: 23.6708\n",
      "Processing batch 5472/11884 - Val_Loss: 26.4416\n",
      "Processing batch 5473/11884 - Val_Loss: 25.9985\n",
      "Processing batch 5474/11884 - Val_Loss: 23.9242\n",
      "Processing batch 5475/11884 - Val_Loss: 24.2227\n",
      "Processing batch 5476/11884 - Val_Loss: 27.0037\n",
      "Processing batch 5477/11884 - Val_Loss: 25.6264\n",
      "Processing batch 5478/11884 - Val_Loss: 24.6324\n",
      "Processing batch 5479/11884 - Val_Loss: 24.6721\n",
      "Processing batch 5480/11884 - Val_Loss: 26.2484\n",
      "Processing batch 5481/11884 - Val_Loss: 23.5406\n",
      "Processing batch 5482/11884 - Val_Loss: 25.9951\n",
      "Processing batch 5483/11884 - Val_Loss: 23.9503\n",
      "Processing batch 5484/11884 - Val_Loss: 25.4157\n",
      "Processing batch 5485/11884 - Val_Loss: 26.1796\n",
      "Processing batch 5486/11884 - Val_Loss: 25.7590\n",
      "Processing batch 5487/11884 - Val_Loss: 25.8522\n",
      "Processing batch 5488/11884 - Val_Loss: 19.8427\n",
      "Processing batch 5489/11884 - Val_Loss: 26.3176\n",
      "Processing batch 5490/11884 - Val_Loss: 23.9108\n",
      "Processing batch 5491/11884 - Val_Loss: 28.8485\n",
      "Processing batch 5492/11884 - Val_Loss: 22.1019\n",
      "Processing batch 5493/11884 - Val_Loss: 27.9288\n",
      "Processing batch 5494/11884 - Val_Loss: 25.1681\n",
      "Processing batch 5495/11884 - Val_Loss: 23.1701\n",
      "Processing batch 5496/11884 - Val_Loss: 26.8975\n",
      "Processing batch 5497/11884 - Val_Loss: 28.8789\n",
      "Processing batch 5498/11884 - Val_Loss: 28.2311\n",
      "Processing batch 5499/11884 - Val_Loss: 27.5190\n",
      "Processing batch 5500/11884 - Val_Loss: 26.0451\n",
      "Processing batch 5501/11884 - Val_Loss: 26.3242\n",
      "Processing batch 5502/11884 - Val_Loss: 25.1899\n",
      "Processing batch 5503/11884 - Val_Loss: 26.3426\n",
      "Processing batch 5504/11884 - Val_Loss: 27.5271\n",
      "Processing batch 5505/11884 - Val_Loss: 23.7183\n",
      "Processing batch 5506/11884 - Val_Loss: 25.8642\n",
      "Processing batch 5507/11884 - Val_Loss: 25.7268\n",
      "Processing batch 5508/11884 - Val_Loss: 24.9508\n",
      "Processing batch 5509/11884 - Val_Loss: 23.9778\n",
      "Processing batch 5510/11884 - Val_Loss: 26.6370\n",
      "Processing batch 5511/11884 - Val_Loss: 28.3292\n",
      "Processing batch 5512/11884 - Val_Loss: 26.0238\n",
      "Processing batch 5513/11884 - Val_Loss: 24.0496\n",
      "Processing batch 5514/11884 - Val_Loss: 26.1130\n",
      "Processing batch 5515/11884 - Val_Loss: 25.0513\n",
      "Processing batch 5516/11884 - Val_Loss: 23.5038\n",
      "Processing batch 5517/11884 - Val_Loss: 25.6103\n",
      "Processing batch 5518/11884 - Val_Loss: 24.0819\n",
      "Processing batch 5519/11884 - Val_Loss: 26.7313\n",
      "Processing batch 5520/11884 - Val_Loss: 23.4250\n",
      "Processing batch 5521/11884 - Val_Loss: 24.2627\n",
      "Processing batch 5522/11884 - Val_Loss: 28.1120\n",
      "Processing batch 5523/11884 - Val_Loss: 29.1937\n",
      "Processing batch 5524/11884 - Val_Loss: 23.2672\n",
      "Processing batch 5525/11884 - Val_Loss: 25.3389\n",
      "Processing batch 5526/11884 - Val_Loss: 25.8718\n",
      "Processing batch 5527/11884 - Val_Loss: 23.7525\n",
      "Processing batch 5528/11884 - Val_Loss: 27.4503\n",
      "Processing batch 5529/11884 - Val_Loss: 26.3734\n",
      "Processing batch 5530/11884 - Val_Loss: 26.9548\n",
      "Processing batch 5531/11884 - Val_Loss: 25.8851\n",
      "Processing batch 5532/11884 - Val_Loss: 25.2763\n",
      "Processing batch 5533/11884 - Val_Loss: 24.3784\n",
      "Processing batch 5534/11884 - Val_Loss: 24.9039\n",
      "Processing batch 5535/11884 - Val_Loss: 26.8004\n",
      "Processing batch 5536/11884 - Val_Loss: 26.1879\n",
      "Processing batch 5537/11884 - Val_Loss: 26.0215\n",
      "Processing batch 5538/11884 - Val_Loss: 26.9068\n",
      "Processing batch 5539/11884 - Val_Loss: 25.6011\n",
      "Processing batch 5540/11884 - Val_Loss: 24.8661\n",
      "Processing batch 5541/11884 - Val_Loss: 24.2259\n",
      "Processing batch 5542/11884 - Val_Loss: 23.8632\n",
      "Processing batch 5543/11884 - Val_Loss: 23.3094\n",
      "Processing batch 5544/11884 - Val_Loss: 23.5158\n",
      "Processing batch 5545/11884 - Val_Loss: 27.0885\n",
      "Processing batch 5546/11884 - Val_Loss: 25.8822\n",
      "Processing batch 5547/11884 - Val_Loss: 24.4854\n",
      "Processing batch 5548/11884 - Val_Loss: 24.5164\n",
      "Processing batch 5549/11884 - Val_Loss: 26.4095\n",
      "Processing batch 5550/11884 - Val_Loss: 27.0075\n",
      "Processing batch 5551/11884 - Val_Loss: 26.7553\n",
      "Processing batch 5552/11884 - Val_Loss: 26.4335\n",
      "Processing batch 5553/11884 - Val_Loss: 28.6398\n",
      "Processing batch 5554/11884 - Val_Loss: 26.2077\n",
      "Processing batch 5555/11884 - Val_Loss: 26.8185\n",
      "Processing batch 5556/11884 - Val_Loss: 29.0922\n",
      "Processing batch 5557/11884 - Val_Loss: 25.0367\n",
      "Processing batch 5558/11884 - Val_Loss: 23.7512\n",
      "Processing batch 5559/11884 - Val_Loss: 25.7711\n",
      "Processing batch 5560/11884 - Val_Loss: 29.0622\n",
      "Processing batch 5561/11884 - Val_Loss: 27.6315\n",
      "Processing batch 5562/11884 - Val_Loss: 25.1574\n",
      "Processing batch 5563/11884 - Val_Loss: 24.5712\n",
      "Processing batch 5564/11884 - Val_Loss: 26.2203\n",
      "Processing batch 5565/11884 - Val_Loss: 24.8934\n",
      "Processing batch 5566/11884 - Val_Loss: 23.5170\n",
      "Processing batch 5567/11884 - Val_Loss: 23.5940\n",
      "Processing batch 5568/11884 - Val_Loss: 28.3560\n",
      "Processing batch 5569/11884 - Val_Loss: 25.7902\n",
      "Processing batch 5570/11884 - Val_Loss: 26.1445\n",
      "Processing batch 5571/11884 - Val_Loss: 24.0605\n",
      "Processing batch 5572/11884 - Val_Loss: 25.9738\n",
      "Processing batch 5573/11884 - Val_Loss: 26.1622\n",
      "Processing batch 5574/11884 - Val_Loss: 25.9330\n",
      "Processing batch 5575/11884 - Val_Loss: 27.1835\n",
      "Processing batch 5576/11884 - Val_Loss: 24.8545\n",
      "Processing batch 5577/11884 - Val_Loss: 25.6489\n",
      "Processing batch 5578/11884 - Val_Loss: 21.7595\n",
      "Processing batch 5579/11884 - Val_Loss: 26.9900\n",
      "Processing batch 5580/11884 - Val_Loss: 25.1383\n",
      "Processing batch 5581/11884 - Val_Loss: 24.7366\n",
      "Processing batch 5582/11884 - Val_Loss: 27.7846\n",
      "Processing batch 5583/11884 - Val_Loss: 24.6590\n",
      "Processing batch 5584/11884 - Val_Loss: 28.0291\n",
      "Processing batch 5585/11884 - Val_Loss: 23.7870\n",
      "Processing batch 5586/11884 - Val_Loss: 25.6692\n",
      "Processing batch 5587/11884 - Val_Loss: 26.1908\n",
      "Processing batch 5588/11884 - Val_Loss: 27.1646\n",
      "Processing batch 5589/11884 - Val_Loss: 23.8707\n",
      "Processing batch 5590/11884 - Val_Loss: 26.5318\n",
      "Processing batch 5591/11884 - Val_Loss: 24.0644\n",
      "Processing batch 5592/11884 - Val_Loss: 23.4709\n",
      "Processing batch 5593/11884 - Val_Loss: 27.2452\n",
      "Processing batch 5594/11884 - Val_Loss: 26.0156\n",
      "Processing batch 5595/11884 - Val_Loss: 25.1026\n",
      "Processing batch 5596/11884 - Val_Loss: 25.7543\n",
      "Processing batch 5597/11884 - Val_Loss: 25.5649\n",
      "Processing batch 5598/11884 - Val_Loss: 23.6427\n",
      "Processing batch 5599/11884 - Val_Loss: 24.8303\n",
      "Processing batch 5600/11884 - Val_Loss: 21.6693\n",
      "Processing batch 5601/11884 - Val_Loss: 24.8756\n",
      "Processing batch 5602/11884 - Val_Loss: 23.7956\n",
      "Processing batch 5603/11884 - Val_Loss: 26.0513\n",
      "Processing batch 5604/11884 - Val_Loss: 27.2306\n",
      "Processing batch 5605/11884 - Val_Loss: 25.1949\n",
      "Processing batch 5606/11884 - Val_Loss: 23.1189\n",
      "Processing batch 5607/11884 - Val_Loss: 26.6311\n",
      "Processing batch 5608/11884 - Val_Loss: 24.1054\n",
      "Processing batch 5609/11884 - Val_Loss: 24.9255\n",
      "Processing batch 5610/11884 - Val_Loss: 27.2545\n",
      "Processing batch 5611/11884 - Val_Loss: 27.0290\n",
      "Processing batch 5612/11884 - Val_Loss: 23.3452\n",
      "Processing batch 5613/11884 - Val_Loss: 26.5418\n",
      "Processing batch 5614/11884 - Val_Loss: 25.8760\n",
      "Processing batch 5615/11884 - Val_Loss: 26.1129\n",
      "Processing batch 5616/11884 - Val_Loss: 23.2381\n",
      "Processing batch 5617/11884 - Val_Loss: 27.3334\n",
      "Processing batch 5618/11884 - Val_Loss: 26.4290\n",
      "Processing batch 5619/11884 - Val_Loss: 21.9345\n",
      "Processing batch 5620/11884 - Val_Loss: 22.7465\n",
      "Processing batch 5621/11884 - Val_Loss: 21.9310\n",
      "Processing batch 5622/11884 - Val_Loss: 24.9918\n",
      "Processing batch 5623/11884 - Val_Loss: 25.4842\n",
      "Processing batch 5624/11884 - Val_Loss: 25.7562\n",
      "Processing batch 5625/11884 - Val_Loss: 25.9649\n",
      "Processing batch 5626/11884 - Val_Loss: 25.1613\n",
      "Processing batch 5627/11884 - Val_Loss: 25.0115\n",
      "Processing batch 5628/11884 - Val_Loss: 25.6398\n",
      "Processing batch 5629/11884 - Val_Loss: 24.7719\n",
      "Processing batch 5630/11884 - Val_Loss: 24.4483\n",
      "Processing batch 5631/11884 - Val_Loss: 28.1679\n",
      "Processing batch 5632/11884 - Val_Loss: 27.7732\n",
      "Processing batch 5633/11884 - Val_Loss: 25.4285\n",
      "Processing batch 5634/11884 - Val_Loss: 24.8913\n",
      "Processing batch 5635/11884 - Val_Loss: 26.4392\n",
      "Processing batch 5636/11884 - Val_Loss: 25.5270\n",
      "Processing batch 5637/11884 - Val_Loss: 26.0072\n",
      "Processing batch 5638/11884 - Val_Loss: 23.7022\n",
      "Processing batch 5639/11884 - Val_Loss: 28.6587\n",
      "Processing batch 5640/11884 - Val_Loss: 26.6564\n",
      "Processing batch 5641/11884 - Val_Loss: 23.2183\n",
      "Processing batch 5642/11884 - Val_Loss: 27.5372\n",
      "Processing batch 5643/11884 - Val_Loss: 28.3418\n",
      "Processing batch 5644/11884 - Val_Loss: 25.4476\n",
      "Processing batch 5645/11884 - Val_Loss: 24.1068\n",
      "Processing batch 5646/11884 - Val_Loss: 24.6641\n",
      "Processing batch 5647/11884 - Val_Loss: 28.2058\n",
      "Processing batch 5648/11884 - Val_Loss: 25.5094\n",
      "Processing batch 5649/11884 - Val_Loss: 26.0326\n",
      "Processing batch 5650/11884 - Val_Loss: 26.2816\n",
      "Processing batch 5651/11884 - Val_Loss: 24.0188\n",
      "Processing batch 5652/11884 - Val_Loss: 24.7237\n",
      "Processing batch 5653/11884 - Val_Loss: 22.8952\n",
      "Processing batch 5654/11884 - Val_Loss: 24.6949\n",
      "Processing batch 5655/11884 - Val_Loss: 28.3458\n",
      "Processing batch 5656/11884 - Val_Loss: 27.3596\n",
      "Processing batch 5657/11884 - Val_Loss: 24.7657\n",
      "Processing batch 5658/11884 - Val_Loss: 25.5588\n",
      "Processing batch 5659/11884 - Val_Loss: 25.8071\n",
      "Processing batch 5660/11884 - Val_Loss: 25.4512\n",
      "Processing batch 5661/11884 - Val_Loss: 23.9776\n",
      "Processing batch 5662/11884 - Val_Loss: 27.6414\n",
      "Processing batch 5663/11884 - Val_Loss: 24.6807\n",
      "Processing batch 5664/11884 - Val_Loss: 24.2389\n",
      "Processing batch 5665/11884 - Val_Loss: 23.7747\n",
      "Processing batch 5666/11884 - Val_Loss: 26.3331\n",
      "Processing batch 5667/11884 - Val_Loss: 28.7627\n",
      "Processing batch 5668/11884 - Val_Loss: 24.0133\n",
      "Processing batch 5669/11884 - Val_Loss: 27.5717\n",
      "Processing batch 5670/11884 - Val_Loss: 24.3153\n",
      "Processing batch 5671/11884 - Val_Loss: 27.8204\n",
      "Processing batch 5672/11884 - Val_Loss: 26.0272\n",
      "Processing batch 5673/11884 - Val_Loss: 26.4548\n",
      "Processing batch 5674/11884 - Val_Loss: 23.8462\n",
      "Processing batch 5675/11884 - Val_Loss: 27.0602\n",
      "Processing batch 5676/11884 - Val_Loss: 27.3284\n",
      "Processing batch 5677/11884 - Val_Loss: 25.0606\n",
      "Processing batch 5678/11884 - Val_Loss: 24.4082\n",
      "Processing batch 5679/11884 - Val_Loss: 24.5265\n",
      "Processing batch 5680/11884 - Val_Loss: 27.2418\n",
      "Processing batch 5681/11884 - Val_Loss: 25.8569\n",
      "Processing batch 5682/11884 - Val_Loss: 27.6644\n",
      "Processing batch 5683/11884 - Val_Loss: 26.9990\n",
      "Processing batch 5684/11884 - Val_Loss: 25.6681\n",
      "Processing batch 5685/11884 - Val_Loss: 24.8134\n",
      "Processing batch 5686/11884 - Val_Loss: 24.4579\n",
      "Processing batch 5687/11884 - Val_Loss: 27.5465\n",
      "Processing batch 5688/11884 - Val_Loss: 25.8981\n",
      "Processing batch 5689/11884 - Val_Loss: 27.4982\n",
      "Processing batch 5690/11884 - Val_Loss: 27.0508\n",
      "Processing batch 5691/11884 - Val_Loss: 25.0682\n",
      "Processing batch 5692/11884 - Val_Loss: 27.3321\n",
      "Processing batch 5693/11884 - Val_Loss: 23.4734\n",
      "Processing batch 5694/11884 - Val_Loss: 26.2285\n",
      "Processing batch 5695/11884 - Val_Loss: 27.9783\n",
      "Processing batch 5696/11884 - Val_Loss: 25.3339\n",
      "Processing batch 5697/11884 - Val_Loss: 26.5397\n",
      "Processing batch 5698/11884 - Val_Loss: 26.1209\n",
      "Processing batch 5699/11884 - Val_Loss: 24.7580\n",
      "Processing batch 5700/11884 - Val_Loss: 29.9874\n",
      "Processing batch 5701/11884 - Val_Loss: 25.9338\n",
      "Processing batch 5702/11884 - Val_Loss: 24.8720\n",
      "Processing batch 5703/11884 - Val_Loss: 26.4027\n",
      "Processing batch 5704/11884 - Val_Loss: 26.4096\n",
      "Processing batch 5705/11884 - Val_Loss: 22.4167\n",
      "Processing batch 5706/11884 - Val_Loss: 28.5114\n",
      "Processing batch 5707/11884 - Val_Loss: 29.2269\n",
      "Processing batch 5708/11884 - Val_Loss: 24.1230\n",
      "Processing batch 5709/11884 - Val_Loss: 26.4203\n",
      "Processing batch 5710/11884 - Val_Loss: 25.2462\n",
      "Processing batch 5711/11884 - Val_Loss: 26.0505\n",
      "Processing batch 5712/11884 - Val_Loss: 23.5462\n",
      "Processing batch 5713/11884 - Val_Loss: 25.8311\n",
      "Processing batch 5714/11884 - Val_Loss: 25.3899\n",
      "Processing batch 5715/11884 - Val_Loss: 23.7593\n",
      "Processing batch 5716/11884 - Val_Loss: 25.9474\n",
      "Processing batch 5717/11884 - Val_Loss: 25.1171\n",
      "Processing batch 5718/11884 - Val_Loss: 25.3258\n",
      "Processing batch 5719/11884 - Val_Loss: 24.0632\n",
      "Processing batch 5720/11884 - Val_Loss: 25.7127\n",
      "Processing batch 5721/11884 - Val_Loss: 26.0307\n",
      "Processing batch 5722/11884 - Val_Loss: 23.8047\n",
      "Processing batch 5723/11884 - Val_Loss: 24.8703\n",
      "Processing batch 5724/11884 - Val_Loss: 31.0899\n",
      "Processing batch 5725/11884 - Val_Loss: 23.9461\n",
      "Processing batch 5726/11884 - Val_Loss: 25.9844\n",
      "Processing batch 5727/11884 - Val_Loss: 25.6264\n",
      "Processing batch 5728/11884 - Val_Loss: 26.8583\n",
      "Processing batch 5729/11884 - Val_Loss: 28.0730\n",
      "Processing batch 5730/11884 - Val_Loss: 27.2308\n",
      "Processing batch 5731/11884 - Val_Loss: 24.1036\n",
      "Processing batch 5732/11884 - Val_Loss: 20.8225\n",
      "Processing batch 5733/11884 - Val_Loss: 25.6439\n",
      "Processing batch 5734/11884 - Val_Loss: 23.7461\n",
      "Processing batch 5735/11884 - Val_Loss: 26.9113\n",
      "Processing batch 5736/11884 - Val_Loss: 28.5548\n",
      "Processing batch 5737/11884 - Val_Loss: 24.9178\n",
      "Processing batch 5738/11884 - Val_Loss: 27.6175\n",
      "Processing batch 5739/11884 - Val_Loss: 27.0774\n",
      "Processing batch 5740/11884 - Val_Loss: 26.7835\n",
      "Processing batch 5741/11884 - Val_Loss: 25.8044\n",
      "Processing batch 5742/11884 - Val_Loss: 23.2486\n",
      "Processing batch 5743/11884 - Val_Loss: 21.3863\n",
      "Processing batch 5744/11884 - Val_Loss: 25.0972\n",
      "Processing batch 5745/11884 - Val_Loss: 29.1183\n",
      "Processing batch 5746/11884 - Val_Loss: 25.3546\n",
      "Processing batch 5747/11884 - Val_Loss: 26.0524\n",
      "Processing batch 5748/11884 - Val_Loss: 23.8776\n",
      "Processing batch 5749/11884 - Val_Loss: 25.0362\n",
      "Processing batch 5750/11884 - Val_Loss: 23.7410\n",
      "Processing batch 5751/11884 - Val_Loss: 23.7158\n",
      "Processing batch 5752/11884 - Val_Loss: 25.1820\n",
      "Processing batch 5753/11884 - Val_Loss: 24.6237\n",
      "Processing batch 5754/11884 - Val_Loss: 28.1431\n",
      "Processing batch 5755/11884 - Val_Loss: 24.7820\n",
      "Processing batch 5756/11884 - Val_Loss: 23.2049\n",
      "Processing batch 5757/11884 - Val_Loss: 27.0930\n",
      "Processing batch 5758/11884 - Val_Loss: 24.8333\n",
      "Processing batch 5759/11884 - Val_Loss: 21.7659\n",
      "Processing batch 5760/11884 - Val_Loss: 27.6185\n",
      "Processing batch 5761/11884 - Val_Loss: 27.4776\n",
      "Processing batch 5762/11884 - Val_Loss: 27.1808\n",
      "Processing batch 5763/11884 - Val_Loss: 26.4742\n",
      "Processing batch 5764/11884 - Val_Loss: 24.8819\n",
      "Processing batch 5765/11884 - Val_Loss: 27.3559\n",
      "Processing batch 5766/11884 - Val_Loss: 24.2425\n",
      "Processing batch 5767/11884 - Val_Loss: 24.2287\n",
      "Processing batch 5768/11884 - Val_Loss: 25.6252\n",
      "Processing batch 5769/11884 - Val_Loss: 26.3097\n",
      "Processing batch 5770/11884 - Val_Loss: 25.5994\n",
      "Processing batch 5771/11884 - Val_Loss: 25.9831\n",
      "Processing batch 5772/11884 - Val_Loss: 23.5427\n",
      "Processing batch 5773/11884 - Val_Loss: 25.5950\n",
      "Processing batch 5774/11884 - Val_Loss: 26.0042\n",
      "Processing batch 5775/11884 - Val_Loss: 27.9552\n",
      "Processing batch 5776/11884 - Val_Loss: 29.1246\n",
      "Processing batch 5777/11884 - Val_Loss: 24.4605\n",
      "Processing batch 5778/11884 - Val_Loss: 29.1931\n",
      "Processing batch 5779/11884 - Val_Loss: 24.6676\n",
      "Processing batch 5780/11884 - Val_Loss: 25.9093\n",
      "Processing batch 5781/11884 - Val_Loss: 23.6997\n",
      "Processing batch 5782/11884 - Val_Loss: 23.9391\n",
      "Processing batch 5783/11884 - Val_Loss: 24.1122\n",
      "Processing batch 5784/11884 - Val_Loss: 25.9730\n",
      "Processing batch 5785/11884 - Val_Loss: 25.0360\n",
      "Processing batch 5786/11884 - Val_Loss: 24.4452\n",
      "Processing batch 5787/11884 - Val_Loss: 24.7476\n",
      "Processing batch 5788/11884 - Val_Loss: 28.2887\n",
      "Processing batch 5789/11884 - Val_Loss: 28.2861\n",
      "Processing batch 5790/11884 - Val_Loss: 27.2081\n",
      "Processing batch 5791/11884 - Val_Loss: 25.3755\n",
      "Processing batch 5792/11884 - Val_Loss: 27.6457\n",
      "Processing batch 5793/11884 - Val_Loss: 27.5765\n",
      "Processing batch 5794/11884 - Val_Loss: 24.0629\n",
      "Processing batch 5795/11884 - Val_Loss: 22.7598\n",
      "Processing batch 5796/11884 - Val_Loss: 25.5009\n",
      "Processing batch 5797/11884 - Val_Loss: 24.9611\n",
      "Processing batch 5798/11884 - Val_Loss: 22.5642\n",
      "Processing batch 5799/11884 - Val_Loss: 29.6604\n",
      "Processing batch 5800/11884 - Val_Loss: 24.5026\n",
      "Processing batch 5801/11884 - Val_Loss: 25.7696\n",
      "Processing batch 5802/11884 - Val_Loss: 28.2792\n",
      "Processing batch 5803/11884 - Val_Loss: 25.0901\n",
      "Processing batch 5804/11884 - Val_Loss: 23.5717\n",
      "Processing batch 5805/11884 - Val_Loss: 25.2839\n",
      "Processing batch 5806/11884 - Val_Loss: 25.0871\n",
      "Processing batch 5807/11884 - Val_Loss: 25.6020\n",
      "Processing batch 5808/11884 - Val_Loss: 25.2920\n",
      "Processing batch 5809/11884 - Val_Loss: 26.3970\n",
      "Processing batch 5810/11884 - Val_Loss: 25.1734\n",
      "Processing batch 5811/11884 - Val_Loss: 26.6643\n",
      "Processing batch 5812/11884 - Val_Loss: 24.8498\n",
      "Processing batch 5813/11884 - Val_Loss: 24.8753\n",
      "Processing batch 5814/11884 - Val_Loss: 22.7003\n",
      "Processing batch 5815/11884 - Val_Loss: 22.4652\n",
      "Processing batch 5816/11884 - Val_Loss: 25.5252\n",
      "Processing batch 5817/11884 - Val_Loss: 26.7093\n",
      "Processing batch 5818/11884 - Val_Loss: 24.8827\n",
      "Processing batch 5819/11884 - Val_Loss: 25.6653\n",
      "Processing batch 5820/11884 - Val_Loss: 25.3948\n",
      "Processing batch 5821/11884 - Val_Loss: 26.8862\n",
      "Processing batch 5822/11884 - Val_Loss: 24.8376\n",
      "Processing batch 5823/11884 - Val_Loss: 24.7153\n",
      "Processing batch 5824/11884 - Val_Loss: 24.1688\n",
      "Processing batch 5825/11884 - Val_Loss: 24.2551\n",
      "Processing batch 5826/11884 - Val_Loss: 32.7854\n",
      "Processing batch 5827/11884 - Val_Loss: 24.9135\n",
      "Processing batch 5828/11884 - Val_Loss: 22.7814\n",
      "Processing batch 5829/11884 - Val_Loss: 28.1552\n",
      "Processing batch 5830/11884 - Val_Loss: 24.0409\n",
      "Processing batch 5831/11884 - Val_Loss: 24.1804\n",
      "Processing batch 5832/11884 - Val_Loss: 28.5908\n",
      "Processing batch 5833/11884 - Val_Loss: 29.1325\n",
      "Processing batch 5834/11884 - Val_Loss: 27.7521\n",
      "Processing batch 5835/11884 - Val_Loss: 25.7385\n",
      "Processing batch 5836/11884 - Val_Loss: 26.6262\n",
      "Processing batch 5837/11884 - Val_Loss: 25.4925\n",
      "Processing batch 5838/11884 - Val_Loss: 27.2137\n",
      "Processing batch 5839/11884 - Val_Loss: 22.6092\n",
      "Processing batch 5840/11884 - Val_Loss: 27.2331\n",
      "Processing batch 5841/11884 - Val_Loss: 26.2864\n",
      "Processing batch 5842/11884 - Val_Loss: 28.4812\n",
      "Processing batch 5843/11884 - Val_Loss: 24.2877\n",
      "Processing batch 5844/11884 - Val_Loss: 26.7091\n",
      "Processing batch 5845/11884 - Val_Loss: 26.9423\n",
      "Processing batch 5846/11884 - Val_Loss: 23.6516\n",
      "Processing batch 5847/11884 - Val_Loss: 23.6425\n",
      "Processing batch 5848/11884 - Val_Loss: 27.7392\n",
      "Processing batch 5849/11884 - Val_Loss: 25.8504\n",
      "Processing batch 5850/11884 - Val_Loss: 25.8040\n",
      "Processing batch 5851/11884 - Val_Loss: 22.5149\n",
      "Processing batch 5852/11884 - Val_Loss: 23.7766\n",
      "Processing batch 5853/11884 - Val_Loss: 27.5840\n",
      "Processing batch 5854/11884 - Val_Loss: 27.7331\n",
      "Processing batch 5855/11884 - Val_Loss: 26.6922\n",
      "Processing batch 5856/11884 - Val_Loss: 27.3808\n",
      "Processing batch 5857/11884 - Val_Loss: 27.2020\n",
      "Processing batch 5858/11884 - Val_Loss: 26.6221\n",
      "Processing batch 5859/11884 - Val_Loss: 22.1864\n",
      "Processing batch 5860/11884 - Val_Loss: 26.7368\n",
      "Processing batch 5861/11884 - Val_Loss: 24.2853\n",
      "Processing batch 5862/11884 - Val_Loss: 22.2846\n",
      "Processing batch 5863/11884 - Val_Loss: 28.3867\n",
      "Processing batch 5864/11884 - Val_Loss: 24.4973\n",
      "Epoch 2/5 - Train loss: 30.1206 - Val loss: 25.5200\n",
      "Processing batch 1/11884 - Loss: 29.7404\n",
      "Processing batch 2/11884 - Loss: 30.7409\n",
      "Processing batch 3/11884 - Loss: 28.8824\n",
      "Processing batch 4/11884 - Loss: 29.3578\n",
      "Processing batch 5/11884 - Loss: 30.1123\n",
      "Processing batch 6/11884 - Loss: 29.8495\n",
      "Processing batch 7/11884 - Loss: 29.6820\n",
      "Processing batch 8/11884 - Loss: 30.5266\n",
      "Processing batch 9/11884 - Loss: 30.5077\n",
      "Processing batch 10/11884 - Loss: 30.9205\n",
      "Processing batch 11/11884 - Loss: 31.1183\n",
      "Processing batch 12/11884 - Loss: 31.1402\n",
      "Processing batch 13/11884 - Loss: 29.6971\n",
      "Processing batch 14/11884 - Loss: 29.4896\n",
      "Processing batch 15/11884 - Loss: 29.9220\n",
      "Processing batch 16/11884 - Loss: 30.0454\n",
      "Processing batch 17/11884 - Loss: 29.4738\n",
      "Processing batch 18/11884 - Loss: 30.6367\n",
      "Processing batch 19/11884 - Loss: 29.6421\n",
      "Processing batch 20/11884 - Loss: 30.6030\n",
      "Processing batch 21/11884 - Loss: 30.1095\n",
      "Processing batch 22/11884 - Loss: 29.4834\n",
      "Processing batch 23/11884 - Loss: 29.1239\n",
      "Processing batch 24/11884 - Loss: 31.4838\n",
      "Processing batch 25/11884 - Loss: 29.0673\n",
      "Processing batch 26/11884 - Loss: 30.0425\n",
      "Processing batch 27/11884 - Loss: 29.4580\n",
      "Processing batch 28/11884 - Loss: 30.2802\n",
      "Processing batch 29/11884 - Loss: 30.3483\n",
      "Processing batch 30/11884 - Loss: 30.1679\n",
      "Processing batch 31/11884 - Loss: 28.9996\n",
      "Processing batch 32/11884 - Loss: 28.5618\n",
      "Processing batch 33/11884 - Loss: 29.7860\n",
      "Processing batch 34/11884 - Loss: 30.9389\n",
      "Processing batch 35/11884 - Loss: 29.0669\n",
      "Processing batch 36/11884 - Loss: 30.6469\n",
      "Processing batch 37/11884 - Loss: 29.9730\n",
      "Processing batch 38/11884 - Loss: 29.6752\n",
      "Processing batch 39/11884 - Loss: 30.3621\n",
      "Processing batch 40/11884 - Loss: 30.4602\n",
      "Processing batch 41/11884 - Loss: 28.7393\n",
      "Processing batch 42/11884 - Loss: 29.4234\n",
      "Processing batch 43/11884 - Loss: 29.8923\n",
      "Processing batch 44/11884 - Loss: 29.7114\n",
      "Processing batch 45/11884 - Loss: 30.5123\n",
      "Processing batch 46/11884 - Loss: 28.6092\n",
      "Processing batch 47/11884 - Loss: 30.7682\n",
      "Processing batch 48/11884 - Loss: 29.9432\n",
      "Processing batch 49/11884 - Loss: 29.4618\n",
      "Processing batch 50/11884 - Loss: 28.9729\n",
      "Processing batch 51/11884 - Loss: 30.3882\n",
      "Processing batch 52/11884 - Loss: 29.3699\n",
      "Processing batch 53/11884 - Loss: 30.6915\n",
      "Processing batch 54/11884 - Loss: 29.6299\n",
      "Processing batch 55/11884 - Loss: 29.9184\n",
      "Processing batch 56/11884 - Loss: 30.3224\n",
      "Processing batch 57/11884 - Loss: 29.7727\n",
      "Processing batch 58/11884 - Loss: 29.3492\n",
      "Processing batch 59/11884 - Loss: 29.3096\n",
      "Processing batch 60/11884 - Loss: 30.4505\n",
      "Processing batch 61/11884 - Loss: 28.7746\n",
      "Processing batch 62/11884 - Loss: 28.8448\n",
      "Processing batch 63/11884 - Loss: 29.4350\n",
      "Processing batch 64/11884 - Loss: 30.5408\n",
      "Processing batch 65/11884 - Loss: 29.4692\n",
      "Processing batch 66/11884 - Loss: 30.1401\n",
      "Processing batch 67/11884 - Loss: 31.6055\n",
      "Processing batch 68/11884 - Loss: 29.5999\n",
      "Processing batch 69/11884 - Loss: 31.1081\n",
      "Processing batch 70/11884 - Loss: 31.8545\n",
      "Processing batch 71/11884 - Loss: 30.1980\n",
      "Processing batch 72/11884 - Loss: 30.2228\n",
      "Processing batch 73/11884 - Loss: 29.1735\n",
      "Processing batch 74/11884 - Loss: 31.3516\n",
      "Processing batch 75/11884 - Loss: 29.3695\n",
      "Processing batch 76/11884 - Loss: 30.1185\n",
      "Processing batch 77/11884 - Loss: 31.4093\n",
      "Processing batch 78/11884 - Loss: 31.3762\n",
      "Processing batch 79/11884 - Loss: 29.2499\n",
      "Processing batch 80/11884 - Loss: 29.4135\n",
      "Processing batch 81/11884 - Loss: 29.9224\n",
      "Processing batch 82/11884 - Loss: 28.8526\n",
      "Processing batch 83/11884 - Loss: 29.5237\n",
      "Processing batch 84/11884 - Loss: 30.6483\n",
      "Processing batch 85/11884 - Loss: 30.2934\n",
      "Processing batch 86/11884 - Loss: 31.2939\n",
      "Processing batch 87/11884 - Loss: 29.9125\n",
      "Processing batch 88/11884 - Loss: 29.7750\n",
      "Processing batch 89/11884 - Loss: 29.0098\n",
      "Processing batch 90/11884 - Loss: 29.6566\n",
      "Processing batch 91/11884 - Loss: 29.4088\n",
      "Processing batch 92/11884 - Loss: 30.1296\n",
      "Processing batch 93/11884 - Loss: 30.4586\n",
      "Processing batch 94/11884 - Loss: 28.4092\n",
      "Processing batch 95/11884 - Loss: 31.9581\n",
      "Processing batch 96/11884 - Loss: 30.2691\n",
      "Processing batch 97/11884 - Loss: 30.0550\n",
      "Processing batch 98/11884 - Loss: 29.7420\n",
      "Processing batch 99/11884 - Loss: 31.2364\n",
      "Processing batch 100/11884 - Loss: 28.6308\n",
      "Processing batch 101/11884 - Loss: 29.9334\n",
      "Processing batch 102/11884 - Loss: 30.1320\n",
      "Processing batch 103/11884 - Loss: 29.7516\n",
      "Processing batch 104/11884 - Loss: 29.7529\n",
      "Processing batch 105/11884 - Loss: 30.3428\n",
      "Processing batch 106/11884 - Loss: 29.2925\n",
      "Processing batch 107/11884 - Loss: 29.4109\n",
      "Processing batch 108/11884 - Loss: 31.2931\n",
      "Processing batch 109/11884 - Loss: 30.0351\n",
      "Processing batch 110/11884 - Loss: 30.5156\n",
      "Processing batch 111/11884 - Loss: 28.1154\n",
      "Processing batch 112/11884 - Loss: 29.0783\n",
      "Processing batch 113/11884 - Loss: 30.3476\n",
      "Processing batch 114/11884 - Loss: 29.1841\n",
      "Processing batch 115/11884 - Loss: 30.1531\n",
      "Processing batch 116/11884 - Loss: 30.6586\n",
      "Processing batch 117/11884 - Loss: 30.3010\n",
      "Processing batch 118/11884 - Loss: 30.0780\n",
      "Processing batch 119/11884 - Loss: 28.5994\n",
      "Processing batch 120/11884 - Loss: 28.9103\n",
      "Processing batch 121/11884 - Loss: 28.6548\n",
      "Processing batch 122/11884 - Loss: 30.0759\n",
      "Processing batch 123/11884 - Loss: 30.2711\n",
      "Processing batch 124/11884 - Loss: 30.2446\n",
      "Processing batch 125/11884 - Loss: 29.7255\n",
      "Processing batch 126/11884 - Loss: 30.2064\n",
      "Processing batch 127/11884 - Loss: 30.0421\n",
      "Processing batch 128/11884 - Loss: 29.9905\n",
      "Processing batch 129/11884 - Loss: 30.0578\n",
      "Processing batch 130/11884 - Loss: 30.2205\n",
      "Processing batch 131/11884 - Loss: 29.5123\n",
      "Processing batch 132/11884 - Loss: 30.5161\n",
      "Processing batch 133/11884 - Loss: 30.0848\n",
      "Processing batch 134/11884 - Loss: 29.0227\n",
      "Processing batch 135/11884 - Loss: 30.3916\n",
      "Processing batch 136/11884 - Loss: 30.1044\n",
      "Processing batch 137/11884 - Loss: 30.3092\n",
      "Processing batch 138/11884 - Loss: 30.5347\n",
      "Processing batch 139/11884 - Loss: 30.6252\n",
      "Processing batch 140/11884 - Loss: 29.0056\n",
      "Processing batch 141/11884 - Loss: 29.2880\n",
      "Processing batch 142/11884 - Loss: 30.2787\n",
      "Processing batch 143/11884 - Loss: 28.7398\n",
      "Processing batch 144/11884 - Loss: 30.6464\n",
      "Processing batch 145/11884 - Loss: 30.0349\n",
      "Processing batch 146/11884 - Loss: 29.6464\n",
      "Processing batch 147/11884 - Loss: 29.6792\n",
      "Processing batch 148/11884 - Loss: 30.2085\n",
      "Processing batch 149/11884 - Loss: 30.3091\n",
      "Processing batch 150/11884 - Loss: 30.0667\n",
      "Processing batch 151/11884 - Loss: 28.0767\n",
      "Processing batch 152/11884 - Loss: 29.0962\n",
      "Processing batch 153/11884 - Loss: 30.5276\n",
      "Processing batch 154/11884 - Loss: 30.4416\n",
      "Processing batch 155/11884 - Loss: 29.1102\n",
      "Processing batch 156/11884 - Loss: 29.9925\n",
      "Processing batch 157/11884 - Loss: 31.3217\n",
      "Processing batch 158/11884 - Loss: 29.9844\n",
      "Processing batch 159/11884 - Loss: 30.3285\n",
      "Processing batch 160/11884 - Loss: 30.1205\n",
      "Processing batch 161/11884 - Loss: 30.4343\n",
      "Processing batch 162/11884 - Loss: 29.8939\n",
      "Processing batch 163/11884 - Loss: 31.0934\n",
      "Processing batch 164/11884 - Loss: 29.9165\n",
      "Processing batch 165/11884 - Loss: 29.4990\n",
      "Processing batch 166/11884 - Loss: 29.4990\n",
      "Processing batch 167/11884 - Loss: 29.0281\n",
      "Processing batch 168/11884 - Loss: 29.6640\n",
      "Processing batch 169/11884 - Loss: 28.8388\n",
      "Processing batch 170/11884 - Loss: 30.1093\n",
      "Processing batch 171/11884 - Loss: 30.6158\n",
      "Processing batch 172/11884 - Loss: 29.7633\n",
      "Processing batch 173/11884 - Loss: 30.9213\n",
      "Processing batch 174/11884 - Loss: 29.7895\n",
      "Processing batch 175/11884 - Loss: 30.6494\n",
      "Processing batch 176/11884 - Loss: 31.2476\n",
      "Processing batch 177/11884 - Loss: 29.7395\n",
      "Processing batch 178/11884 - Loss: 30.3310\n",
      "Processing batch 179/11884 - Loss: 27.9672\n",
      "Processing batch 180/11884 - Loss: 29.8046\n",
      "Processing batch 181/11884 - Loss: 30.9938\n",
      "Processing batch 182/11884 - Loss: 28.3360\n",
      "Processing batch 183/11884 - Loss: 30.1159\n",
      "Processing batch 184/11884 - Loss: 30.2817\n",
      "Processing batch 185/11884 - Loss: 29.0918\n",
      "Processing batch 186/11884 - Loss: 29.7594\n",
      "Processing batch 187/11884 - Loss: 29.1843\n",
      "Processing batch 188/11884 - Loss: 30.4836\n",
      "Processing batch 189/11884 - Loss: 29.3192\n",
      "Processing batch 190/11884 - Loss: 31.3266\n",
      "Processing batch 191/11884 - Loss: 29.1261\n",
      "Processing batch 192/11884 - Loss: 30.3906\n",
      "Processing batch 193/11884 - Loss: 30.0176\n",
      "Processing batch 194/11884 - Loss: 30.2489\n",
      "Processing batch 195/11884 - Loss: 29.8398\n",
      "Processing batch 196/11884 - Loss: 28.9574\n",
      "Processing batch 197/11884 - Loss: 30.1417\n",
      "Processing batch 198/11884 - Loss: 29.6043\n",
      "Processing batch 199/11884 - Loss: 29.5361\n",
      "Processing batch 200/11884 - Loss: 29.5079\n",
      "Processing batch 201/11884 - Loss: 29.6577\n",
      "Processing batch 202/11884 - Loss: 28.9580\n",
      "Processing batch 203/11884 - Loss: 30.9792\n",
      "Processing batch 204/11884 - Loss: 27.8140\n",
      "Processing batch 205/11884 - Loss: 28.9685\n",
      "Processing batch 206/11884 - Loss: 30.0939\n",
      "Processing batch 207/11884 - Loss: 29.4988\n",
      "Processing batch 208/11884 - Loss: 29.7915\n",
      "Processing batch 209/11884 - Loss: 29.6024\n",
      "Processing batch 210/11884 - Loss: 31.4604\n",
      "Processing batch 211/11884 - Loss: 30.2323\n",
      "Processing batch 212/11884 - Loss: 29.2978\n",
      "Processing batch 213/11884 - Loss: 29.6782\n",
      "Processing batch 214/11884 - Loss: 30.3451\n",
      "Processing batch 215/11884 - Loss: 29.8274\n",
      "Processing batch 216/11884 - Loss: 30.0707\n",
      "Processing batch 217/11884 - Loss: 29.6156\n",
      "Processing batch 218/11884 - Loss: 30.2431\n",
      "Processing batch 219/11884 - Loss: 30.6957\n",
      "Processing batch 220/11884 - Loss: 30.7459\n",
      "Processing batch 221/11884 - Loss: 29.9482\n",
      "Processing batch 222/11884 - Loss: 28.4810\n",
      "Processing batch 223/11884 - Loss: 29.2630\n",
      "Processing batch 224/11884 - Loss: 29.7173\n",
      "Processing batch 225/11884 - Loss: 30.9156\n",
      "Processing batch 226/11884 - Loss: 28.7569\n",
      "Processing batch 227/11884 - Loss: 30.4370\n",
      "Processing batch 228/11884 - Loss: 29.1928\n",
      "Processing batch 229/11884 - Loss: 30.1063\n",
      "Processing batch 230/11884 - Loss: 30.8616\n",
      "Processing batch 231/11884 - Loss: 29.9701\n",
      "Processing batch 232/11884 - Loss: 30.0721\n",
      "Processing batch 233/11884 - Loss: 29.6325\n",
      "Processing batch 234/11884 - Loss: 29.7278\n",
      "Processing batch 235/11884 - Loss: 29.7474\n",
      "Processing batch 236/11884 - Loss: 29.7544\n",
      "Processing batch 237/11884 - Loss: 30.2056\n",
      "Processing batch 238/11884 - Loss: 29.7401\n",
      "Processing batch 239/11884 - Loss: 29.2936\n",
      "Processing batch 240/11884 - Loss: 29.3358\n",
      "Processing batch 241/11884 - Loss: 29.1554\n",
      "Processing batch 242/11884 - Loss: 30.1895\n",
      "Processing batch 243/11884 - Loss: 29.8270\n",
      "Processing batch 244/11884 - Loss: 31.1177\n",
      "Processing batch 245/11884 - Loss: 29.1634\n",
      "Processing batch 246/11884 - Loss: 28.8910\n",
      "Processing batch 247/11884 - Loss: 29.6691\n",
      "Processing batch 248/11884 - Loss: 29.4033\n",
      "Processing batch 249/11884 - Loss: 31.3684\n",
      "Processing batch 250/11884 - Loss: 29.2825\n",
      "Processing batch 251/11884 - Loss: 30.2234\n",
      "Processing batch 252/11884 - Loss: 29.7235\n",
      "Processing batch 253/11884 - Loss: 30.7230\n",
      "Processing batch 254/11884 - Loss: 30.4434\n",
      "Processing batch 255/11884 - Loss: 29.3305\n",
      "Processing batch 256/11884 - Loss: 31.4872\n",
      "Processing batch 257/11884 - Loss: 30.3022\n",
      "Processing batch 258/11884 - Loss: 30.3260\n",
      "Processing batch 259/11884 - Loss: 31.0229\n",
      "Processing batch 260/11884 - Loss: 30.9494\n",
      "Processing batch 261/11884 - Loss: 28.7605\n",
      "Processing batch 262/11884 - Loss: 29.3999\n",
      "Processing batch 263/11884 - Loss: 29.9508\n",
      "Processing batch 264/11884 - Loss: 29.2884\n",
      "Processing batch 265/11884 - Loss: 30.8003\n",
      "Processing batch 266/11884 - Loss: 30.8888\n",
      "Processing batch 267/11884 - Loss: 29.9450\n",
      "Processing batch 268/11884 - Loss: 29.1391\n",
      "Processing batch 269/11884 - Loss: 29.8023\n",
      "Processing batch 270/11884 - Loss: 29.3454\n",
      "Processing batch 271/11884 - Loss: 29.6939\n",
      "Processing batch 272/11884 - Loss: 30.3627\n",
      "Processing batch 273/11884 - Loss: 29.1441\n",
      "Processing batch 274/11884 - Loss: 30.5859\n",
      "Processing batch 275/11884 - Loss: 29.5835\n",
      "Processing batch 276/11884 - Loss: 29.1246\n",
      "Processing batch 277/11884 - Loss: 28.9605\n",
      "Processing batch 278/11884 - Loss: 29.5445\n",
      "Processing batch 279/11884 - Loss: 29.9594\n",
      "Processing batch 280/11884 - Loss: 29.6816\n",
      "Processing batch 281/11884 - Loss: 31.7022\n",
      "Processing batch 282/11884 - Loss: 29.0407\n",
      "Processing batch 283/11884 - Loss: 29.6384\n",
      "Processing batch 284/11884 - Loss: 31.4855\n",
      "Processing batch 285/11884 - Loss: 30.0882\n",
      "Processing batch 286/11884 - Loss: 31.1151\n",
      "Processing batch 287/11884 - Loss: 30.9125\n",
      "Processing batch 288/11884 - Loss: 30.9289\n",
      "Processing batch 289/11884 - Loss: 29.0317\n",
      "Processing batch 290/11884 - Loss: 29.8027\n",
      "Processing batch 291/11884 - Loss: 28.6826\n",
      "Processing batch 292/11884 - Loss: 30.0780\n",
      "Processing batch 293/11884 - Loss: 30.8029\n",
      "Processing batch 294/11884 - Loss: 29.7195\n",
      "Processing batch 295/11884 - Loss: 29.6728\n",
      "Processing batch 296/11884 - Loss: 28.7607\n",
      "Processing batch 297/11884 - Loss: 29.5657\n",
      "Processing batch 298/11884 - Loss: 29.7316\n",
      "Processing batch 299/11884 - Loss: 30.7421\n",
      "Processing batch 300/11884 - Loss: 29.8757\n",
      "Processing batch 301/11884 - Loss: 29.8099\n",
      "Processing batch 302/11884 - Loss: 29.2339\n",
      "Processing batch 303/11884 - Loss: 30.7402\n",
      "Processing batch 304/11884 - Loss: 28.8896\n",
      "Processing batch 305/11884 - Loss: 29.6328\n",
      "Processing batch 306/11884 - Loss: 29.5719\n",
      "Processing batch 307/11884 - Loss: 28.9386\n",
      "Processing batch 308/11884 - Loss: 29.2400\n",
      "Processing batch 309/11884 - Loss: 29.4912\n",
      "Processing batch 310/11884 - Loss: 28.5430\n",
      "Processing batch 311/11884 - Loss: 29.8297\n",
      "Processing batch 312/11884 - Loss: 30.2698\n",
      "Processing batch 313/11884 - Loss: 30.1133\n",
      "Processing batch 314/11884 - Loss: 29.8069\n",
      "Processing batch 315/11884 - Loss: 29.2054\n",
      "Processing batch 316/11884 - Loss: 29.7718\n",
      "Processing batch 317/11884 - Loss: 29.3863\n",
      "Processing batch 318/11884 - Loss: 30.0794\n",
      "Processing batch 319/11884 - Loss: 29.0570\n",
      "Processing batch 320/11884 - Loss: 30.3416\n",
      "Processing batch 321/11884 - Loss: 30.3881\n",
      "Processing batch 322/11884 - Loss: 30.6420\n",
      "Processing batch 323/11884 - Loss: 29.6786\n",
      "Processing batch 324/11884 - Loss: 29.9375\n",
      "Processing batch 325/11884 - Loss: 29.7395\n",
      "Processing batch 326/11884 - Loss: 28.4929\n",
      "Processing batch 327/11884 - Loss: 29.7854\n",
      "Processing batch 328/11884 - Loss: 28.2716\n",
      "Processing batch 329/11884 - Loss: 30.2445\n",
      "Processing batch 330/11884 - Loss: 30.6715\n",
      "Processing batch 331/11884 - Loss: 29.9438\n",
      "Processing batch 332/11884 - Loss: 29.3922\n",
      "Processing batch 333/11884 - Loss: 28.5192\n",
      "Processing batch 334/11884 - Loss: 31.0012\n",
      "Processing batch 335/11884 - Loss: 30.4739\n",
      "Processing batch 336/11884 - Loss: 30.9561\n",
      "Processing batch 337/11884 - Loss: 29.7605\n",
      "Processing batch 338/11884 - Loss: 28.3785\n",
      "Processing batch 339/11884 - Loss: 28.7069\n",
      "Processing batch 340/11884 - Loss: 29.8667\n",
      "Processing batch 341/11884 - Loss: 30.2778\n",
      "Processing batch 342/11884 - Loss: 29.9763\n",
      "Processing batch 343/11884 - Loss: 28.3503\n",
      "Processing batch 344/11884 - Loss: 31.7928\n",
      "Processing batch 345/11884 - Loss: 29.6531\n",
      "Processing batch 346/11884 - Loss: 29.0162\n",
      "Processing batch 347/11884 - Loss: 29.4382\n",
      "Processing batch 348/11884 - Loss: 28.8114\n",
      "Processing batch 349/11884 - Loss: 30.7744\n",
      "Processing batch 350/11884 - Loss: 30.3063\n",
      "Processing batch 351/11884 - Loss: 29.5012\n",
      "Processing batch 352/11884 - Loss: 30.2300\n",
      "Processing batch 353/11884 - Loss: 30.5833\n",
      "Processing batch 354/11884 - Loss: 29.8815\n",
      "Processing batch 355/11884 - Loss: 29.1643\n",
      "Processing batch 356/11884 - Loss: 29.3068\n",
      "Processing batch 357/11884 - Loss: 28.6990\n",
      "Processing batch 358/11884 - Loss: 31.7378\n",
      "Processing batch 359/11884 - Loss: 29.7428\n",
      "Processing batch 360/11884 - Loss: 30.4576\n",
      "Processing batch 361/11884 - Loss: 29.5093\n",
      "Processing batch 362/11884 - Loss: 28.8913\n",
      "Processing batch 363/11884 - Loss: 29.8986\n",
      "Processing batch 364/11884 - Loss: 30.5450\n",
      "Processing batch 365/11884 - Loss: 30.7646\n",
      "Processing batch 366/11884 - Loss: 29.5331\n",
      "Processing batch 367/11884 - Loss: 30.7815\n",
      "Processing batch 368/11884 - Loss: 29.3015\n",
      "Processing batch 369/11884 - Loss: 29.7300\n",
      "Processing batch 370/11884 - Loss: 30.1809\n",
      "Processing batch 371/11884 - Loss: 30.6131\n",
      "Processing batch 372/11884 - Loss: 30.0292\n",
      "Processing batch 373/11884 - Loss: 29.9635\n",
      "Processing batch 374/11884 - Loss: 28.9011\n",
      "Processing batch 375/11884 - Loss: 29.7712\n",
      "Processing batch 376/11884 - Loss: 30.4792\n",
      "Processing batch 377/11884 - Loss: 30.5174\n",
      "Processing batch 378/11884 - Loss: 29.7215\n",
      "Processing batch 379/11884 - Loss: 30.6527\n",
      "Processing batch 380/11884 - Loss: 30.3339\n",
      "Processing batch 381/11884 - Loss: 30.2973\n",
      "Processing batch 382/11884 - Loss: 30.7086\n",
      "Processing batch 383/11884 - Loss: 30.0786\n",
      "Processing batch 384/11884 - Loss: 30.0592\n",
      "Processing batch 385/11884 - Loss: 29.9615\n",
      "Processing batch 386/11884 - Loss: 30.3032\n",
      "Processing batch 387/11884 - Loss: 31.1876\n",
      "Processing batch 388/11884 - Loss: 31.1670\n",
      "Processing batch 389/11884 - Loss: 30.4513\n",
      "Processing batch 390/11884 - Loss: 28.8055\n",
      "Processing batch 391/11884 - Loss: 29.5899\n",
      "Processing batch 392/11884 - Loss: 29.2319\n",
      "Processing batch 393/11884 - Loss: 29.9465\n",
      "Processing batch 394/11884 - Loss: 31.0686\n",
      "Processing batch 395/11884 - Loss: 29.4768\n",
      "Processing batch 396/11884 - Loss: 29.0546\n",
      "Processing batch 397/11884 - Loss: 29.7996\n",
      "Processing batch 398/11884 - Loss: 29.5156\n",
      "Processing batch 399/11884 - Loss: 29.1246\n",
      "Processing batch 400/11884 - Loss: 29.3189\n",
      "Processing batch 401/11884 - Loss: 29.5666\n",
      "Processing batch 402/11884 - Loss: 30.8193\n",
      "Processing batch 403/11884 - Loss: 30.4716\n",
      "Processing batch 404/11884 - Loss: 28.9658\n",
      "Processing batch 405/11884 - Loss: 29.8748\n",
      "Processing batch 406/11884 - Loss: 29.3721\n",
      "Processing batch 407/11884 - Loss: 29.1122\n",
      "Processing batch 408/11884 - Loss: 29.7742\n",
      "Processing batch 409/11884 - Loss: 29.1896\n",
      "Processing batch 410/11884 - Loss: 30.0564\n",
      "Processing batch 411/11884 - Loss: 30.0545\n",
      "Processing batch 412/11884 - Loss: 29.2548\n",
      "Processing batch 413/11884 - Loss: 30.7620\n",
      "Processing batch 414/11884 - Loss: 29.4046\n",
      "Processing batch 415/11884 - Loss: 30.3614\n",
      "Processing batch 416/11884 - Loss: 30.1292\n",
      "Processing batch 417/11884 - Loss: 29.9743\n",
      "Processing batch 418/11884 - Loss: 31.5612\n",
      "Processing batch 419/11884 - Loss: 29.8066\n",
      "Processing batch 420/11884 - Loss: 30.2958\n",
      "Processing batch 421/11884 - Loss: 29.9599\n",
      "Processing batch 422/11884 - Loss: 29.2903\n",
      "Processing batch 423/11884 - Loss: 29.5953\n",
      "Processing batch 424/11884 - Loss: 31.3501\n",
      "Processing batch 425/11884 - Loss: 30.8400\n",
      "Processing batch 426/11884 - Loss: 29.9391\n",
      "Processing batch 427/11884 - Loss: 30.4392\n",
      "Processing batch 428/11884 - Loss: 30.1740\n",
      "Processing batch 429/11884 - Loss: 30.4080\n",
      "Processing batch 430/11884 - Loss: 30.7753\n",
      "Processing batch 431/11884 - Loss: 29.8125\n",
      "Processing batch 432/11884 - Loss: 29.5197\n",
      "Processing batch 433/11884 - Loss: 29.4732\n",
      "Processing batch 434/11884 - Loss: 30.2395\n",
      "Processing batch 435/11884 - Loss: 29.9517\n",
      "Processing batch 436/11884 - Loss: 29.9303\n",
      "Processing batch 437/11884 - Loss: 30.6480\n",
      "Processing batch 438/11884 - Loss: 29.8214\n",
      "Processing batch 439/11884 - Loss: 31.1667\n",
      "Processing batch 440/11884 - Loss: 30.5718\n",
      "Processing batch 441/11884 - Loss: 30.2568\n",
      "Processing batch 442/11884 - Loss: 29.9277\n",
      "Processing batch 443/11884 - Loss: 29.4441\n",
      "Processing batch 444/11884 - Loss: 29.6377\n",
      "Processing batch 445/11884 - Loss: 29.3142\n",
      "Processing batch 446/11884 - Loss: 28.7876\n",
      "Processing batch 447/11884 - Loss: 30.5795\n",
      "Processing batch 448/11884 - Loss: 29.9594\n",
      "Processing batch 449/11884 - Loss: 30.2905\n",
      "Processing batch 450/11884 - Loss: 29.5018\n",
      "Processing batch 451/11884 - Loss: 29.2053\n",
      "Processing batch 452/11884 - Loss: 29.5614\n",
      "Processing batch 453/11884 - Loss: 29.2167\n",
      "Processing batch 454/11884 - Loss: 29.3844\n",
      "Processing batch 455/11884 - Loss: 29.5428\n",
      "Processing batch 456/11884 - Loss: 29.9365\n",
      "Processing batch 457/11884 - Loss: 30.6511\n",
      "Processing batch 458/11884 - Loss: 29.7556\n",
      "Processing batch 459/11884 - Loss: 29.2953\n",
      "Processing batch 460/11884 - Loss: 29.7529\n",
      "Processing batch 461/11884 - Loss: 29.7559\n",
      "Processing batch 462/11884 - Loss: 30.0964\n",
      "Processing batch 463/11884 - Loss: 30.6328\n",
      "Processing batch 464/11884 - Loss: 30.2044\n",
      "Processing batch 465/11884 - Loss: 28.6888\n",
      "Processing batch 466/11884 - Loss: 30.3419\n",
      "Processing batch 467/11884 - Loss: 30.8864\n",
      "Processing batch 468/11884 - Loss: 28.3062\n",
      "Processing batch 469/11884 - Loss: 29.8034\n",
      "Processing batch 470/11884 - Loss: 29.3142\n",
      "Processing batch 471/11884 - Loss: 27.7294\n",
      "Processing batch 472/11884 - Loss: 28.0382\n",
      "Processing batch 473/11884 - Loss: 29.3085\n",
      "Processing batch 474/11884 - Loss: 30.1681\n",
      "Processing batch 475/11884 - Loss: 31.2624\n",
      "Processing batch 476/11884 - Loss: 30.3476\n",
      "Processing batch 477/11884 - Loss: 30.4538\n",
      "Processing batch 478/11884 - Loss: 28.9477\n",
      "Processing batch 479/11884 - Loss: 29.5530\n",
      "Processing batch 480/11884 - Loss: 29.2456\n",
      "Processing batch 481/11884 - Loss: 29.3924\n",
      "Processing batch 482/11884 - Loss: 29.5964\n",
      "Processing batch 483/11884 - Loss: 29.9521\n",
      "Processing batch 484/11884 - Loss: 29.3908\n",
      "Processing batch 485/11884 - Loss: 31.5848\n",
      "Processing batch 486/11884 - Loss: 29.9347\n",
      "Processing batch 487/11884 - Loss: 29.4568\n",
      "Processing batch 488/11884 - Loss: 30.0811\n",
      "Processing batch 489/11884 - Loss: 29.8457\n",
      "Processing batch 490/11884 - Loss: 29.7426\n",
      "Processing batch 491/11884 - Loss: 29.8283\n",
      "Processing batch 492/11884 - Loss: 30.2784\n",
      "Processing batch 493/11884 - Loss: 29.3788\n",
      "Processing batch 494/11884 - Loss: 30.7067\n",
      "Processing batch 495/11884 - Loss: 29.2073\n",
      "Processing batch 496/11884 - Loss: 29.9698\n",
      "Processing batch 497/11884 - Loss: 29.6332\n",
      "Processing batch 498/11884 - Loss: 28.5719\n",
      "Processing batch 499/11884 - Loss: 29.7100\n",
      "Processing batch 500/11884 - Loss: 29.9520\n",
      "Processing batch 501/11884 - Loss: 30.0734\n",
      "Processing batch 502/11884 - Loss: 28.3036\n",
      "Processing batch 503/11884 - Loss: 29.1068\n",
      "Processing batch 504/11884 - Loss: 32.1148\n",
      "Processing batch 505/11884 - Loss: 29.9355\n",
      "Processing batch 506/11884 - Loss: 30.9034\n",
      "Processing batch 507/11884 - Loss: 31.1503\n",
      "Processing batch 508/11884 - Loss: 29.1730\n",
      "Processing batch 509/11884 - Loss: 29.2976\n",
      "Processing batch 510/11884 - Loss: 29.4917\n",
      "Processing batch 511/11884 - Loss: 31.2552\n",
      "Processing batch 512/11884 - Loss: 29.0302\n",
      "Processing batch 513/11884 - Loss: 29.2581\n",
      "Processing batch 514/11884 - Loss: 29.2492\n",
      "Processing batch 515/11884 - Loss: 30.4978\n",
      "Processing batch 516/11884 - Loss: 30.6197\n",
      "Processing batch 517/11884 - Loss: 30.0992\n",
      "Processing batch 518/11884 - Loss: 30.9268\n",
      "Processing batch 519/11884 - Loss: 30.0015\n",
      "Processing batch 520/11884 - Loss: 28.2806\n",
      "Processing batch 521/11884 - Loss: 29.7932\n",
      "Processing batch 522/11884 - Loss: 30.9487\n",
      "Processing batch 523/11884 - Loss: 30.2359\n",
      "Processing batch 524/11884 - Loss: 28.6058\n",
      "Processing batch 525/11884 - Loss: 31.0682\n",
      "Processing batch 526/11884 - Loss: 30.5139\n",
      "Processing batch 527/11884 - Loss: 30.2592\n",
      "Processing batch 528/11884 - Loss: 30.0942\n",
      "Processing batch 529/11884 - Loss: 29.5833\n",
      "Processing batch 530/11884 - Loss: 29.2450\n",
      "Processing batch 531/11884 - Loss: 30.3491\n",
      "Processing batch 532/11884 - Loss: 30.2689\n",
      "Processing batch 533/11884 - Loss: 29.1940\n",
      "Processing batch 534/11884 - Loss: 30.4101\n",
      "Processing batch 535/11884 - Loss: 30.9827\n",
      "Processing batch 536/11884 - Loss: 29.7623\n",
      "Processing batch 537/11884 - Loss: 29.3325\n",
      "Processing batch 538/11884 - Loss: 30.2367\n",
      "Processing batch 539/11884 - Loss: 28.7465\n",
      "Processing batch 540/11884 - Loss: 29.6334\n",
      "Processing batch 541/11884 - Loss: 30.2122\n",
      "Processing batch 542/11884 - Loss: 30.5462\n",
      "Processing batch 543/11884 - Loss: 28.7252\n",
      "Processing batch 544/11884 - Loss: 30.4798\n",
      "Processing batch 545/11884 - Loss: 30.8389\n",
      "Processing batch 546/11884 - Loss: 30.6618\n",
      "Processing batch 547/11884 - Loss: 29.7345\n",
      "Processing batch 548/11884 - Loss: 29.7169\n",
      "Processing batch 549/11884 - Loss: 29.6320\n",
      "Processing batch 550/11884 - Loss: 29.5523\n",
      "Processing batch 551/11884 - Loss: 30.5341\n",
      "Processing batch 552/11884 - Loss: 28.1866\n",
      "Processing batch 553/11884 - Loss: 29.4768\n",
      "Processing batch 554/11884 - Loss: 30.2744\n",
      "Processing batch 555/11884 - Loss: 29.2408\n",
      "Processing batch 556/11884 - Loss: 30.2819\n",
      "Processing batch 557/11884 - Loss: 29.7173\n",
      "Processing batch 558/11884 - Loss: 30.0888\n",
      "Processing batch 559/11884 - Loss: 31.3943\n",
      "Processing batch 560/11884 - Loss: 29.7672\n",
      "Processing batch 561/11884 - Loss: 29.3631\n",
      "Processing batch 562/11884 - Loss: 29.8634\n",
      "Processing batch 563/11884 - Loss: 29.7902\n",
      "Processing batch 564/11884 - Loss: 29.3976\n",
      "Processing batch 565/11884 - Loss: 30.8543\n",
      "Processing batch 566/11884 - Loss: 29.9741\n",
      "Processing batch 567/11884 - Loss: 29.6708\n",
      "Processing batch 568/11884 - Loss: 29.7047\n",
      "Processing batch 569/11884 - Loss: 29.0171\n",
      "Processing batch 570/11884 - Loss: 29.9437\n",
      "Processing batch 571/11884 - Loss: 29.4137\n",
      "Processing batch 572/11884 - Loss: 29.3603\n",
      "Processing batch 573/11884 - Loss: 29.4028\n",
      "Processing batch 574/11884 - Loss: 30.7563\n",
      "Processing batch 575/11884 - Loss: 30.2193\n",
      "Processing batch 576/11884 - Loss: 29.9292\n",
      "Processing batch 577/11884 - Loss: 30.3828\n",
      "Processing batch 578/11884 - Loss: 30.9197\n",
      "Processing batch 579/11884 - Loss: 30.2634\n",
      "Processing batch 580/11884 - Loss: 29.4618\n",
      "Processing batch 581/11884 - Loss: 28.8054\n",
      "Processing batch 582/11884 - Loss: 31.9353\n",
      "Processing batch 583/11884 - Loss: 28.6665\n",
      "Processing batch 584/11884 - Loss: 29.0337\n",
      "Processing batch 585/11884 - Loss: 29.9217\n",
      "Processing batch 586/11884 - Loss: 29.7177\n",
      "Processing batch 587/11884 - Loss: 30.4388\n",
      "Processing batch 588/11884 - Loss: 29.3267\n",
      "Processing batch 589/11884 - Loss: 29.0499\n",
      "Processing batch 590/11884 - Loss: 29.5048\n",
      "Processing batch 591/11884 - Loss: 29.7008\n",
      "Processing batch 592/11884 - Loss: 30.9317\n",
      "Processing batch 593/11884 - Loss: 28.6100\n",
      "Processing batch 594/11884 - Loss: 30.4310\n",
      "Processing batch 595/11884 - Loss: 29.4892\n",
      "Processing batch 596/11884 - Loss: 29.5236\n",
      "Processing batch 597/11884 - Loss: 28.9140\n",
      "Processing batch 598/11884 - Loss: 29.6814\n",
      "Processing batch 599/11884 - Loss: 29.6079\n",
      "Processing batch 600/11884 - Loss: 30.0895\n",
      "Processing batch 601/11884 - Loss: 29.9908\n",
      "Processing batch 602/11884 - Loss: 30.5972\n",
      "Processing batch 603/11884 - Loss: 30.6414\n",
      "Processing batch 604/11884 - Loss: 28.9019\n",
      "Processing batch 605/11884 - Loss: 28.4243\n",
      "Processing batch 606/11884 - Loss: 30.3245\n",
      "Processing batch 607/11884 - Loss: 29.9484\n",
      "Processing batch 608/11884 - Loss: 31.4455\n",
      "Processing batch 609/11884 - Loss: 30.8226\n",
      "Processing batch 610/11884 - Loss: 29.1082\n",
      "Processing batch 611/11884 - Loss: 28.9197\n",
      "Processing batch 612/11884 - Loss: 30.4224\n",
      "Processing batch 613/11884 - Loss: 30.0070\n",
      "Processing batch 614/11884 - Loss: 29.0198\n",
      "Processing batch 615/11884 - Loss: 29.2281\n",
      "Processing batch 616/11884 - Loss: 29.7399\n",
      "Processing batch 617/11884 - Loss: 30.0942\n",
      "Processing batch 618/11884 - Loss: 30.0747\n",
      "Processing batch 619/11884 - Loss: 30.0591\n",
      "Processing batch 620/11884 - Loss: 29.6480\n",
      "Processing batch 621/11884 - Loss: 29.9073\n",
      "Processing batch 622/11884 - Loss: 29.8533\n",
      "Processing batch 623/11884 - Loss: 29.6241\n",
      "Processing batch 624/11884 - Loss: 30.4387\n",
      "Processing batch 625/11884 - Loss: 29.7565\n",
      "Processing batch 626/11884 - Loss: 29.4849\n",
      "Processing batch 627/11884 - Loss: 30.7716\n",
      "Processing batch 628/11884 - Loss: 29.3524\n",
      "Processing batch 629/11884 - Loss: 30.9719\n",
      "Processing batch 630/11884 - Loss: 28.8257\n",
      "Processing batch 631/11884 - Loss: 30.2431\n",
      "Processing batch 632/11884 - Loss: 31.1772\n",
      "Processing batch 633/11884 - Loss: 29.1702\n",
      "Processing batch 634/11884 - Loss: 30.6582\n",
      "Processing batch 635/11884 - Loss: 30.7403\n",
      "Processing batch 636/11884 - Loss: 29.7659\n",
      "Processing batch 637/11884 - Loss: 30.9420\n",
      "Processing batch 638/11884 - Loss: 31.0599\n",
      "Processing batch 639/11884 - Loss: 30.2681\n",
      "Processing batch 640/11884 - Loss: 29.6250\n",
      "Processing batch 641/11884 - Loss: 29.7098\n",
      "Processing batch 642/11884 - Loss: 29.6809\n",
      "Processing batch 643/11884 - Loss: 30.3486\n",
      "Processing batch 644/11884 - Loss: 28.4479\n",
      "Processing batch 645/11884 - Loss: 28.8625\n",
      "Processing batch 646/11884 - Loss: 29.4997\n",
      "Processing batch 647/11884 - Loss: 29.0156\n",
      "Processing batch 648/11884 - Loss: 30.0801\n",
      "Processing batch 649/11884 - Loss: 31.1658\n",
      "Processing batch 650/11884 - Loss: 30.9741\n",
      "Processing batch 651/11884 - Loss: 30.6222\n",
      "Processing batch 652/11884 - Loss: 30.3178\n",
      "Processing batch 653/11884 - Loss: 29.7374\n",
      "Processing batch 654/11884 - Loss: 31.0399\n",
      "Processing batch 655/11884 - Loss: 29.4009\n",
      "Processing batch 656/11884 - Loss: 31.1423\n",
      "Processing batch 657/11884 - Loss: 28.6934\n",
      "Processing batch 658/11884 - Loss: 29.3996\n",
      "Processing batch 659/11884 - Loss: 28.0764\n",
      "Processing batch 660/11884 - Loss: 32.0470\n",
      "Processing batch 661/11884 - Loss: 29.4396\n",
      "Processing batch 662/11884 - Loss: 30.3307\n",
      "Processing batch 663/11884 - Loss: 30.3294\n",
      "Processing batch 664/11884 - Loss: 31.3585\n",
      "Processing batch 665/11884 - Loss: 29.2215\n",
      "Processing batch 666/11884 - Loss: 29.6216\n",
      "Processing batch 667/11884 - Loss: 29.2154\n",
      "Processing batch 668/11884 - Loss: 30.5562\n",
      "Processing batch 669/11884 - Loss: 30.1374\n",
      "Processing batch 670/11884 - Loss: 29.1492\n",
      "Processing batch 671/11884 - Loss: 29.7765\n",
      "Processing batch 672/11884 - Loss: 29.2303\n",
      "Processing batch 673/11884 - Loss: 29.3745\n",
      "Processing batch 674/11884 - Loss: 29.2775\n",
      "Processing batch 675/11884 - Loss: 31.0845\n",
      "Processing batch 676/11884 - Loss: 29.5272\n",
      "Processing batch 677/11884 - Loss: 30.5379\n",
      "Processing batch 678/11884 - Loss: 30.5668\n",
      "Processing batch 679/11884 - Loss: 30.3167\n",
      "Processing batch 680/11884 - Loss: 29.2411\n",
      "Processing batch 681/11884 - Loss: 29.1935\n",
      "Processing batch 682/11884 - Loss: 29.4496\n",
      "Processing batch 683/11884 - Loss: 29.7173\n",
      "Processing batch 684/11884 - Loss: 31.1255\n",
      "Processing batch 685/11884 - Loss: 30.6147\n",
      "Processing batch 686/11884 - Loss: 28.8641\n",
      "Processing batch 687/11884 - Loss: 28.2321\n",
      "Processing batch 688/11884 - Loss: 30.6881\n",
      "Processing batch 689/11884 - Loss: 30.0671\n",
      "Processing batch 690/11884 - Loss: 30.6051\n",
      "Processing batch 691/11884 - Loss: 29.5170\n",
      "Processing batch 692/11884 - Loss: 30.0292\n",
      "Processing batch 693/11884 - Loss: 30.0386\n",
      "Processing batch 694/11884 - Loss: 30.4967\n",
      "Processing batch 695/11884 - Loss: 30.2195\n",
      "Processing batch 696/11884 - Loss: 30.5197\n",
      "Processing batch 697/11884 - Loss: 29.6986\n",
      "Processing batch 698/11884 - Loss: 29.9613\n",
      "Processing batch 699/11884 - Loss: 29.9492\n",
      "Processing batch 700/11884 - Loss: 30.2925\n",
      "Processing batch 701/11884 - Loss: 31.6458\n",
      "Processing batch 702/11884 - Loss: 29.3192\n",
      "Processing batch 703/11884 - Loss: 29.9826\n",
      "Processing batch 704/11884 - Loss: 30.1979\n",
      "Processing batch 705/11884 - Loss: 30.3126\n",
      "Processing batch 706/11884 - Loss: 29.1543\n",
      "Processing batch 707/11884 - Loss: 29.8621\n",
      "Processing batch 708/11884 - Loss: 29.8451\n",
      "Processing batch 709/11884 - Loss: 31.9513\n",
      "Processing batch 710/11884 - Loss: 29.5197\n",
      "Processing batch 711/11884 - Loss: 29.9784\n",
      "Processing batch 712/11884 - Loss: 30.0502\n",
      "Processing batch 713/11884 - Loss: 30.7651\n",
      "Processing batch 714/11884 - Loss: 29.5306\n",
      "Processing batch 715/11884 - Loss: 30.3376\n",
      "Processing batch 716/11884 - Loss: 30.2989\n",
      "Processing batch 717/11884 - Loss: 29.3475\n",
      "Processing batch 718/11884 - Loss: 28.8164\n",
      "Processing batch 719/11884 - Loss: 29.9619\n",
      "Processing batch 720/11884 - Loss: 30.1841\n",
      "Processing batch 721/11884 - Loss: 29.9909\n",
      "Processing batch 722/11884 - Loss: 31.8202\n",
      "Processing batch 723/11884 - Loss: 29.8032\n",
      "Processing batch 724/11884 - Loss: 28.7896\n",
      "Processing batch 725/11884 - Loss: 30.3374\n",
      "Processing batch 726/11884 - Loss: 29.4987\n",
      "Processing batch 727/11884 - Loss: 29.3676\n",
      "Processing batch 728/11884 - Loss: 31.0827\n",
      "Processing batch 729/11884 - Loss: 30.2923\n",
      "Processing batch 730/11884 - Loss: 29.8928\n",
      "Processing batch 731/11884 - Loss: 29.8428\n",
      "Processing batch 732/11884 - Loss: 29.9291\n",
      "Processing batch 733/11884 - Loss: 29.7588\n",
      "Processing batch 734/11884 - Loss: 30.1984\n",
      "Processing batch 735/11884 - Loss: 30.2726\n",
      "Processing batch 736/11884 - Loss: 30.6774\n",
      "Processing batch 737/11884 - Loss: 29.8542\n",
      "Processing batch 738/11884 - Loss: 29.6025\n",
      "Processing batch 739/11884 - Loss: 29.2323\n",
      "Processing batch 740/11884 - Loss: 30.0484\n",
      "Processing batch 741/11884 - Loss: 31.8080\n",
      "Processing batch 742/11884 - Loss: 29.6060\n",
      "Processing batch 743/11884 - Loss: 29.3994\n",
      "Processing batch 744/11884 - Loss: 31.0198\n",
      "Processing batch 745/11884 - Loss: 29.3550\n",
      "Processing batch 746/11884 - Loss: 31.3628\n",
      "Processing batch 747/11884 - Loss: 29.3080\n",
      "Processing batch 748/11884 - Loss: 29.0955\n",
      "Processing batch 749/11884 - Loss: 29.7863\n",
      "Processing batch 750/11884 - Loss: 30.1259\n",
      "Processing batch 751/11884 - Loss: 30.1288\n",
      "Processing batch 752/11884 - Loss: 30.9222\n",
      "Processing batch 753/11884 - Loss: 31.4282\n",
      "Processing batch 754/11884 - Loss: 31.0261\n",
      "Processing batch 755/11884 - Loss: 30.1927\n",
      "Processing batch 756/11884 - Loss: 29.3454\n",
      "Processing batch 757/11884 - Loss: 29.9504\n",
      "Processing batch 758/11884 - Loss: 29.0321\n",
      "Processing batch 759/11884 - Loss: 29.1031\n",
      "Processing batch 760/11884 - Loss: 31.5678\n",
      "Processing batch 761/11884 - Loss: 29.3841\n",
      "Processing batch 762/11884 - Loss: 30.8129\n",
      "Processing batch 763/11884 - Loss: 30.3312\n",
      "Processing batch 764/11884 - Loss: 29.5143\n",
      "Processing batch 765/11884 - Loss: 31.5274\n",
      "Processing batch 766/11884 - Loss: 30.0855\n",
      "Processing batch 767/11884 - Loss: 29.5214\n",
      "Processing batch 768/11884 - Loss: 29.5018\n",
      "Processing batch 769/11884 - Loss: 30.8428\n",
      "Processing batch 770/11884 - Loss: 29.1190\n",
      "Processing batch 771/11884 - Loss: 28.5294\n",
      "Processing batch 772/11884 - Loss: 30.4818\n",
      "Processing batch 773/11884 - Loss: 30.4239\n",
      "Processing batch 774/11884 - Loss: 29.5653\n",
      "Processing batch 775/11884 - Loss: 29.0865\n",
      "Processing batch 776/11884 - Loss: 30.2516\n",
      "Processing batch 777/11884 - Loss: 30.7599\n",
      "Processing batch 778/11884 - Loss: 29.7569\n",
      "Processing batch 779/11884 - Loss: 29.3848\n",
      "Processing batch 780/11884 - Loss: 30.3398\n",
      "Processing batch 781/11884 - Loss: 28.9690\n",
      "Processing batch 782/11884 - Loss: 30.0636\n",
      "Processing batch 783/11884 - Loss: 30.6077\n",
      "Processing batch 784/11884 - Loss: 29.8887\n",
      "Processing batch 785/11884 - Loss: 30.5489\n",
      "Processing batch 786/11884 - Loss: 29.7780\n",
      "Processing batch 787/11884 - Loss: 30.0867\n",
      "Processing batch 788/11884 - Loss: 29.3637\n",
      "Processing batch 789/11884 - Loss: 29.5569\n",
      "Processing batch 790/11884 - Loss: 29.9862\n",
      "Processing batch 791/11884 - Loss: 29.2031\n",
      "Processing batch 792/11884 - Loss: 29.7438\n",
      "Processing batch 793/11884 - Loss: 29.7186\n",
      "Processing batch 794/11884 - Loss: 30.4207\n",
      "Processing batch 795/11884 - Loss: 29.8320\n",
      "Processing batch 796/11884 - Loss: 30.0224\n",
      "Processing batch 797/11884 - Loss: 29.6162\n",
      "Processing batch 798/11884 - Loss: 28.7386\n",
      "Processing batch 799/11884 - Loss: 30.1062\n",
      "Processing batch 800/11884 - Loss: 27.6236\n",
      "Processing batch 801/11884 - Loss: 31.4674\n",
      "Processing batch 802/11884 - Loss: 29.9070\n",
      "Processing batch 803/11884 - Loss: 30.1335\n",
      "Processing batch 804/11884 - Loss: 29.8852\n",
      "Processing batch 805/11884 - Loss: 30.1158\n",
      "Processing batch 806/11884 - Loss: 28.8564\n",
      "Processing batch 807/11884 - Loss: 29.9571\n",
      "Processing batch 808/11884 - Loss: 31.0887\n",
      "Processing batch 809/11884 - Loss: 30.7122\n",
      "Processing batch 810/11884 - Loss: 30.8328\n",
      "Processing batch 811/11884 - Loss: 29.5124\n",
      "Processing batch 812/11884 - Loss: 28.9703\n",
      "Processing batch 813/11884 - Loss: 30.2103\n",
      "Processing batch 814/11884 - Loss: 31.1334\n",
      "Processing batch 815/11884 - Loss: 29.2153\n",
      "Processing batch 816/11884 - Loss: 29.7047\n",
      "Processing batch 817/11884 - Loss: 30.8168\n",
      "Processing batch 818/11884 - Loss: 30.9247\n",
      "Processing batch 819/11884 - Loss: 29.8566\n",
      "Processing batch 820/11884 - Loss: 31.3278\n",
      "Processing batch 821/11884 - Loss: 30.2863\n",
      "Processing batch 822/11884 - Loss: 29.4162\n",
      "Processing batch 823/11884 - Loss: 30.5642\n",
      "Processing batch 824/11884 - Loss: 28.3830\n",
      "Processing batch 825/11884 - Loss: 29.9665\n",
      "Processing batch 826/11884 - Loss: 28.3015\n",
      "Processing batch 827/11884 - Loss: 30.0478\n",
      "Processing batch 828/11884 - Loss: 30.0193\n",
      "Processing batch 829/11884 - Loss: 30.9968\n",
      "Processing batch 830/11884 - Loss: 29.7168\n",
      "Processing batch 831/11884 - Loss: 30.5985\n",
      "Processing batch 832/11884 - Loss: 30.6953\n",
      "Processing batch 833/11884 - Loss: 30.1469\n",
      "Processing batch 834/11884 - Loss: 29.7608\n",
      "Processing batch 835/11884 - Loss: 28.7938\n",
      "Processing batch 836/11884 - Loss: 30.5895\n",
      "Processing batch 837/11884 - Loss: 30.6491\n",
      "Processing batch 838/11884 - Loss: 28.8126\n",
      "Processing batch 839/11884 - Loss: 29.9919\n",
      "Processing batch 840/11884 - Loss: 30.3416\n",
      "Processing batch 841/11884 - Loss: 29.4430\n",
      "Processing batch 842/11884 - Loss: 30.8980\n",
      "Processing batch 843/11884 - Loss: 28.4614\n",
      "Processing batch 844/11884 - Loss: 29.5656\n",
      "Processing batch 845/11884 - Loss: 30.9082\n",
      "Processing batch 846/11884 - Loss: 31.0947\n",
      "Processing batch 847/11884 - Loss: 30.3618\n",
      "Processing batch 848/11884 - Loss: 29.8931\n",
      "Processing batch 849/11884 - Loss: 29.0551\n",
      "Processing batch 850/11884 - Loss: 29.6274\n",
      "Processing batch 851/11884 - Loss: 28.9474\n",
      "Processing batch 852/11884 - Loss: 29.4370\n",
      "Processing batch 853/11884 - Loss: 30.1244\n",
      "Processing batch 854/11884 - Loss: 30.4403\n",
      "Processing batch 855/11884 - Loss: 29.4598\n",
      "Processing batch 856/11884 - Loss: 29.9304\n",
      "Processing batch 857/11884 - Loss: 29.1565\n",
      "Processing batch 858/11884 - Loss: 30.9961\n",
      "Processing batch 859/11884 - Loss: 28.6916\n",
      "Processing batch 860/11884 - Loss: 30.6340\n",
      "Processing batch 861/11884 - Loss: 29.6466\n",
      "Processing batch 862/11884 - Loss: 30.4995\n",
      "Processing batch 863/11884 - Loss: 30.7759\n",
      "Processing batch 864/11884 - Loss: 28.1609\n",
      "Processing batch 865/11884 - Loss: 29.4768\n",
      "Processing batch 866/11884 - Loss: 30.5269\n",
      "Processing batch 867/11884 - Loss: 29.4040\n",
      "Processing batch 868/11884 - Loss: 30.4555\n",
      "Processing batch 869/11884 - Loss: 29.0423\n",
      "Processing batch 870/11884 - Loss: 29.5551\n",
      "Processing batch 871/11884 - Loss: 30.2744\n",
      "Processing batch 872/11884 - Loss: 30.3788\n",
      "Processing batch 873/11884 - Loss: 29.6555\n",
      "Processing batch 874/11884 - Loss: 29.5716\n",
      "Processing batch 875/11884 - Loss: 29.2274\n",
      "Processing batch 876/11884 - Loss: 30.4462\n",
      "Processing batch 877/11884 - Loss: 29.6327\n",
      "Processing batch 878/11884 - Loss: 29.1189\n",
      "Processing batch 879/11884 - Loss: 30.6737\n",
      "Processing batch 880/11884 - Loss: 29.6399\n",
      "Processing batch 881/11884 - Loss: 29.2480\n",
      "Processing batch 882/11884 - Loss: 29.9918\n",
      "Processing batch 883/11884 - Loss: 29.2999\n",
      "Processing batch 884/11884 - Loss: 29.7608\n",
      "Processing batch 885/11884 - Loss: 29.6355\n",
      "Processing batch 886/11884 - Loss: 30.9879\n",
      "Processing batch 887/11884 - Loss: 30.3608\n",
      "Processing batch 888/11884 - Loss: 28.4171\n",
      "Processing batch 889/11884 - Loss: 29.5616\n",
      "Processing batch 890/11884 - Loss: 29.3586\n",
      "Processing batch 891/11884 - Loss: 29.8473\n",
      "Processing batch 892/11884 - Loss: 28.8803\n",
      "Processing batch 893/11884 - Loss: 30.1977\n",
      "Processing batch 894/11884 - Loss: 30.0733\n",
      "Processing batch 895/11884 - Loss: 29.6649\n",
      "Processing batch 896/11884 - Loss: 29.3137\n",
      "Processing batch 897/11884 - Loss: 30.7370\n",
      "Processing batch 898/11884 - Loss: 30.5526\n",
      "Processing batch 899/11884 - Loss: 30.8170\n",
      "Processing batch 900/11884 - Loss: 29.2657\n",
      "Processing batch 901/11884 - Loss: 28.5212\n",
      "Processing batch 902/11884 - Loss: 29.9911\n",
      "Processing batch 903/11884 - Loss: 30.8378\n",
      "Processing batch 904/11884 - Loss: 31.1193\n",
      "Processing batch 905/11884 - Loss: 30.3504\n",
      "Processing batch 906/11884 - Loss: 29.1472\n",
      "Processing batch 907/11884 - Loss: 29.8454\n",
      "Processing batch 908/11884 - Loss: 30.6529\n",
      "Processing batch 909/11884 - Loss: 29.8833\n",
      "Processing batch 910/11884 - Loss: 29.5534\n",
      "Processing batch 911/11884 - Loss: 31.2049\n",
      "Processing batch 912/11884 - Loss: 30.2233\n",
      "Processing batch 913/11884 - Loss: 29.5135\n",
      "Processing batch 914/11884 - Loss: 30.0419\n",
      "Processing batch 915/11884 - Loss: 29.9784\n",
      "Processing batch 916/11884 - Loss: 30.4850\n",
      "Processing batch 917/11884 - Loss: 29.0405\n",
      "Processing batch 918/11884 - Loss: 29.3912\n",
      "Processing batch 919/11884 - Loss: 29.9947\n",
      "Processing batch 920/11884 - Loss: 29.8519\n",
      "Processing batch 921/11884 - Loss: 28.4171\n",
      "Processing batch 922/11884 - Loss: 31.0483\n",
      "Processing batch 923/11884 - Loss: 30.2101\n",
      "Processing batch 924/11884 - Loss: 29.2004\n",
      "Processing batch 925/11884 - Loss: 29.9526\n",
      "Processing batch 926/11884 - Loss: 30.1824\n",
      "Processing batch 927/11884 - Loss: 30.5896\n",
      "Processing batch 928/11884 - Loss: 28.9889\n",
      "Processing batch 929/11884 - Loss: 30.7446\n",
      "Processing batch 930/11884 - Loss: 27.5224\n",
      "Processing batch 931/11884 - Loss: 29.0934\n",
      "Processing batch 932/11884 - Loss: 30.5177\n",
      "Processing batch 933/11884 - Loss: 29.8502\n",
      "Processing batch 934/11884 - Loss: 29.8047\n",
      "Processing batch 935/11884 - Loss: 30.2238\n",
      "Processing batch 936/11884 - Loss: 29.7481\n",
      "Processing batch 937/11884 - Loss: 30.2520\n",
      "Processing batch 938/11884 - Loss: 29.2219\n",
      "Processing batch 939/11884 - Loss: 27.6824\n",
      "Processing batch 940/11884 - Loss: 30.9522\n",
      "Processing batch 941/11884 - Loss: 31.9877\n",
      "Processing batch 942/11884 - Loss: 31.3824\n",
      "Processing batch 943/11884 - Loss: 29.9400\n",
      "Processing batch 944/11884 - Loss: 29.5641\n",
      "Processing batch 945/11884 - Loss: 29.8630\n",
      "Processing batch 946/11884 - Loss: 30.1280\n",
      "Processing batch 947/11884 - Loss: 29.4751\n",
      "Processing batch 948/11884 - Loss: 28.8305\n",
      "Processing batch 949/11884 - Loss: 29.1028\n",
      "Processing batch 950/11884 - Loss: 29.1428\n",
      "Processing batch 951/11884 - Loss: 28.3587\n",
      "Processing batch 952/11884 - Loss: 30.7402\n",
      "Processing batch 953/11884 - Loss: 29.2248\n",
      "Processing batch 954/11884 - Loss: 30.7796\n",
      "Processing batch 955/11884 - Loss: 29.4668\n",
      "Processing batch 956/11884 - Loss: 30.1935\n",
      "Processing batch 957/11884 - Loss: 29.6046\n",
      "Processing batch 958/11884 - Loss: 30.1071\n",
      "Processing batch 959/11884 - Loss: 27.8730\n",
      "Processing batch 960/11884 - Loss: 30.8317\n",
      "Processing batch 961/11884 - Loss: 30.2194\n",
      "Processing batch 962/11884 - Loss: 29.5251\n",
      "Processing batch 963/11884 - Loss: 30.2208\n",
      "Processing batch 964/11884 - Loss: 29.6453\n",
      "Processing batch 965/11884 - Loss: 29.8586\n",
      "Processing batch 966/11884 - Loss: 29.8035\n",
      "Processing batch 967/11884 - Loss: 29.2750\n",
      "Processing batch 968/11884 - Loss: 30.4427\n",
      "Processing batch 969/11884 - Loss: 29.5933\n",
      "Processing batch 970/11884 - Loss: 29.6946\n",
      "Processing batch 971/11884 - Loss: 31.0116\n",
      "Processing batch 972/11884 - Loss: 28.3293\n",
      "Processing batch 973/11884 - Loss: 29.6955\n",
      "Processing batch 974/11884 - Loss: 30.3320\n",
      "Processing batch 975/11884 - Loss: 29.0664\n",
      "Processing batch 976/11884 - Loss: 29.6491\n",
      "Processing batch 977/11884 - Loss: 30.8607\n",
      "Processing batch 978/11884 - Loss: 27.8211\n",
      "Processing batch 979/11884 - Loss: 29.4699\n",
      "Processing batch 980/11884 - Loss: 30.5418\n",
      "Processing batch 981/11884 - Loss: 29.7539\n",
      "Processing batch 982/11884 - Loss: 29.8334\n",
      "Processing batch 983/11884 - Loss: 31.3910\n",
      "Processing batch 984/11884 - Loss: 29.8932\n",
      "Processing batch 985/11884 - Loss: 29.7147\n",
      "Processing batch 986/11884 - Loss: 29.9462\n",
      "Processing batch 987/11884 - Loss: 30.0475\n",
      "Processing batch 988/11884 - Loss: 28.1031\n",
      "Processing batch 989/11884 - Loss: 30.0529\n",
      "Processing batch 990/11884 - Loss: 30.3146\n",
      "Processing batch 991/11884 - Loss: 29.7182\n",
      "Processing batch 992/11884 - Loss: 30.7193\n",
      "Processing batch 993/11884 - Loss: 31.1842\n",
      "Processing batch 994/11884 - Loss: 27.7453\n",
      "Processing batch 995/11884 - Loss: 29.2847\n",
      "Processing batch 996/11884 - Loss: 29.6063\n",
      "Processing batch 997/11884 - Loss: 29.2922\n",
      "Processing batch 998/11884 - Loss: 29.1487\n",
      "Processing batch 999/11884 - Loss: 30.4819\n",
      "Processing batch 1000/11884 - Loss: 29.8210\n",
      "Processing batch 1001/11884 - Loss: 29.8349\n",
      "Processing batch 1002/11884 - Loss: 28.8567\n",
      "Processing batch 1003/11884 - Loss: 30.0189\n",
      "Processing batch 1004/11884 - Loss: 29.0340\n",
      "Processing batch 1005/11884 - Loss: 30.2670\n",
      "Processing batch 1006/11884 - Loss: 29.8071\n",
      "Processing batch 1007/11884 - Loss: 30.2570\n",
      "Processing batch 1008/11884 - Loss: 29.5729\n",
      "Processing batch 1009/11884 - Loss: 29.1324\n",
      "Processing batch 1010/11884 - Loss: 28.9705\n",
      "Processing batch 1011/11884 - Loss: 29.4724\n",
      "Processing batch 1012/11884 - Loss: 29.0537\n",
      "Processing batch 1013/11884 - Loss: 30.2913\n",
      "Processing batch 1014/11884 - Loss: 29.7802\n",
      "Processing batch 1015/11884 - Loss: 29.3078\n",
      "Processing batch 1016/11884 - Loss: 30.2068\n",
      "Processing batch 1017/11884 - Loss: 30.0301\n",
      "Processing batch 1018/11884 - Loss: 31.3041\n",
      "Processing batch 1019/11884 - Loss: 29.7731\n",
      "Processing batch 1020/11884 - Loss: 29.2926\n",
      "Processing batch 1021/11884 - Loss: 30.7178\n",
      "Processing batch 1022/11884 - Loss: 30.7228\n",
      "Processing batch 1023/11884 - Loss: 29.4208\n",
      "Processing batch 1024/11884 - Loss: 28.9064\n",
      "Processing batch 1025/11884 - Loss: 30.6667\n",
      "Processing batch 1026/11884 - Loss: 29.1775\n",
      "Processing batch 1027/11884 - Loss: 29.0940\n",
      "Processing batch 1028/11884 - Loss: 29.2167\n",
      "Processing batch 1029/11884 - Loss: 28.3885\n",
      "Processing batch 1030/11884 - Loss: 30.8238\n",
      "Processing batch 1031/11884 - Loss: 30.8059\n",
      "Processing batch 1032/11884 - Loss: 29.3191\n",
      "Processing batch 1033/11884 - Loss: 31.2830\n",
      "Processing batch 1034/11884 - Loss: 30.5470\n",
      "Processing batch 1035/11884 - Loss: 30.7545\n",
      "Processing batch 1036/11884 - Loss: 28.7331\n",
      "Processing batch 1037/11884 - Loss: 30.8880\n",
      "Processing batch 1038/11884 - Loss: 30.1984\n",
      "Processing batch 1039/11884 - Loss: 29.1222\n",
      "Processing batch 1040/11884 - Loss: 30.0882\n",
      "Processing batch 1041/11884 - Loss: 30.3924\n",
      "Processing batch 1042/11884 - Loss: 29.9979\n",
      "Processing batch 1043/11884 - Loss: 29.3245\n",
      "Processing batch 1044/11884 - Loss: 28.9055\n",
      "Processing batch 1045/11884 - Loss: 29.1545\n",
      "Processing batch 1046/11884 - Loss: 29.7092\n",
      "Processing batch 1047/11884 - Loss: 31.6807\n",
      "Processing batch 1048/11884 - Loss: 31.6438\n",
      "Processing batch 1049/11884 - Loss: 29.9218\n",
      "Processing batch 1050/11884 - Loss: 29.6269\n",
      "Processing batch 1051/11884 - Loss: 28.9473\n",
      "Processing batch 1052/11884 - Loss: 29.8630\n",
      "Processing batch 1053/11884 - Loss: 28.6108\n",
      "Processing batch 1054/11884 - Loss: 29.9705\n",
      "Processing batch 1055/11884 - Loss: 30.4595\n",
      "Processing batch 1056/11884 - Loss: 29.1736\n",
      "Processing batch 1057/11884 - Loss: 30.1631\n",
      "Processing batch 1058/11884 - Loss: 28.8806\n",
      "Processing batch 1059/11884 - Loss: 30.4355\n",
      "Processing batch 1060/11884 - Loss: 31.1388\n",
      "Processing batch 1061/11884 - Loss: 30.3335\n",
      "Processing batch 1062/11884 - Loss: 31.3058\n",
      "Processing batch 1063/11884 - Loss: 30.7136\n",
      "Processing batch 1064/11884 - Loss: 29.0715\n",
      "Processing batch 1065/11884 - Loss: 29.0165\n",
      "Processing batch 1066/11884 - Loss: 28.1890\n",
      "Processing batch 1067/11884 - Loss: 30.1177\n",
      "Processing batch 1068/11884 - Loss: 29.2114\n",
      "Processing batch 1069/11884 - Loss: 29.2912\n",
      "Processing batch 1070/11884 - Loss: 30.3348\n",
      "Processing batch 1071/11884 - Loss: 30.1579\n",
      "Processing batch 1072/11884 - Loss: 29.1405\n",
      "Processing batch 1073/11884 - Loss: 31.0565\n",
      "Processing batch 1074/11884 - Loss: 30.8555\n",
      "Processing batch 1075/11884 - Loss: 30.9946\n",
      "Processing batch 1076/11884 - Loss: 28.6438\n",
      "Processing batch 1077/11884 - Loss: 30.7259\n",
      "Processing batch 1078/11884 - Loss: 30.1285\n",
      "Processing batch 1079/11884 - Loss: 30.5674\n",
      "Processing batch 1080/11884 - Loss: 29.3651\n",
      "Processing batch 1081/11884 - Loss: 28.6800\n",
      "Processing batch 1082/11884 - Loss: 29.7563\n",
      "Processing batch 1083/11884 - Loss: 30.0368\n",
      "Processing batch 1084/11884 - Loss: 29.9299\n",
      "Processing batch 1085/11884 - Loss: 28.6406\n",
      "Processing batch 1086/11884 - Loss: 30.3653\n",
      "Processing batch 1087/11884 - Loss: 29.3702\n",
      "Processing batch 1088/11884 - Loss: 30.0732\n",
      "Processing batch 1089/11884 - Loss: 31.6324\n",
      "Processing batch 1090/11884 - Loss: 28.2045\n",
      "Processing batch 1091/11884 - Loss: 29.0847\n",
      "Processing batch 1092/11884 - Loss: 30.1929\n",
      "Processing batch 1093/11884 - Loss: 30.2231\n",
      "Processing batch 1094/11884 - Loss: 29.1940\n",
      "Processing batch 1095/11884 - Loss: 29.0455\n",
      "Processing batch 1096/11884 - Loss: 29.4969\n",
      "Processing batch 1097/11884 - Loss: 30.8427\n",
      "Processing batch 1098/11884 - Loss: 30.3752\n",
      "Processing batch 1099/11884 - Loss: 29.5444\n",
      "Processing batch 1100/11884 - Loss: 30.1346\n",
      "Processing batch 1101/11884 - Loss: 30.6030\n",
      "Processing batch 1102/11884 - Loss: 29.4946\n",
      "Processing batch 1103/11884 - Loss: 29.0839\n",
      "Processing batch 1104/11884 - Loss: 30.3576\n",
      "Processing batch 1105/11884 - Loss: 29.8945\n",
      "Processing batch 1106/11884 - Loss: 30.2199\n",
      "Processing batch 1107/11884 - Loss: 30.1261\n",
      "Processing batch 1108/11884 - Loss: 28.8811\n",
      "Processing batch 1109/11884 - Loss: 30.3481\n",
      "Processing batch 1110/11884 - Loss: 29.5708\n",
      "Processing batch 1111/11884 - Loss: 30.4456\n",
      "Processing batch 1112/11884 - Loss: 29.2944\n",
      "Processing batch 1113/11884 - Loss: 29.8072\n",
      "Processing batch 1114/11884 - Loss: 30.0642\n",
      "Processing batch 1115/11884 - Loss: 30.5975\n",
      "Processing batch 1116/11884 - Loss: 29.6352\n",
      "Processing batch 1117/11884 - Loss: 30.1317\n",
      "Processing batch 1118/11884 - Loss: 29.7352\n",
      "Processing batch 1119/11884 - Loss: 29.8809\n",
      "Processing batch 1120/11884 - Loss: 29.8255\n",
      "Processing batch 1121/11884 - Loss: 29.6247\n",
      "Processing batch 1122/11884 - Loss: 28.9844\n",
      "Processing batch 1123/11884 - Loss: 29.7175\n",
      "Processing batch 1124/11884 - Loss: 30.9365\n",
      "Processing batch 1125/11884 - Loss: 29.8269\n",
      "Processing batch 1126/11884 - Loss: 29.4237\n",
      "Processing batch 1127/11884 - Loss: 28.8093\n",
      "Processing batch 1128/11884 - Loss: 29.8674\n",
      "Processing batch 1129/11884 - Loss: 30.2201\n",
      "Processing batch 1130/11884 - Loss: 30.1663\n",
      "Processing batch 1131/11884 - Loss: 29.8546\n",
      "Processing batch 1132/11884 - Loss: 29.3558\n",
      "Processing batch 1133/11884 - Loss: 30.4886\n",
      "Processing batch 1134/11884 - Loss: 30.5605\n",
      "Processing batch 1135/11884 - Loss: 31.2775\n",
      "Processing batch 1136/11884 - Loss: 30.3058\n",
      "Processing batch 1137/11884 - Loss: 30.5016\n",
      "Processing batch 1138/11884 - Loss: 29.8162\n",
      "Processing batch 1139/11884 - Loss: 29.6700\n",
      "Processing batch 1140/11884 - Loss: 29.8247\n",
      "Processing batch 1141/11884 - Loss: 30.1532\n",
      "Processing batch 1142/11884 - Loss: 30.8509\n",
      "Processing batch 1143/11884 - Loss: 29.4524\n",
      "Processing batch 1144/11884 - Loss: 31.6243\n",
      "Processing batch 1145/11884 - Loss: 30.0943\n",
      "Processing batch 1146/11884 - Loss: 30.1182\n",
      "Processing batch 1147/11884 - Loss: 29.4012\n",
      "Processing batch 1148/11884 - Loss: 29.6790\n",
      "Processing batch 1149/11884 - Loss: 29.3520\n",
      "Processing batch 1150/11884 - Loss: 30.2600\n",
      "Processing batch 1151/11884 - Loss: 31.6957\n",
      "Processing batch 1152/11884 - Loss: 30.2798\n",
      "Processing batch 1153/11884 - Loss: 30.0249\n",
      "Processing batch 1154/11884 - Loss: 29.7197\n",
      "Processing batch 1155/11884 - Loss: 29.3595\n",
      "Processing batch 1156/11884 - Loss: 30.8448\n",
      "Processing batch 1157/11884 - Loss: 31.4799\n",
      "Processing batch 1158/11884 - Loss: 30.1449\n",
      "Processing batch 1159/11884 - Loss: 29.9355\n",
      "Processing batch 1160/11884 - Loss: 29.6377\n",
      "Processing batch 1161/11884 - Loss: 30.6883\n",
      "Processing batch 1162/11884 - Loss: 28.7954\n",
      "Processing batch 1163/11884 - Loss: 30.0318\n",
      "Processing batch 1164/11884 - Loss: 28.8023\n",
      "Processing batch 1165/11884 - Loss: 29.7508\n",
      "Processing batch 1166/11884 - Loss: 29.8919\n",
      "Processing batch 1167/11884 - Loss: 29.6152\n",
      "Processing batch 1168/11884 - Loss: 28.1310\n",
      "Processing batch 1169/11884 - Loss: 29.8215\n",
      "Processing batch 1170/11884 - Loss: 31.3602\n",
      "Processing batch 1171/11884 - Loss: 30.4129\n",
      "Processing batch 1172/11884 - Loss: 30.1248\n",
      "Processing batch 1173/11884 - Loss: 29.2720\n",
      "Processing batch 1174/11884 - Loss: 29.1907\n",
      "Processing batch 1175/11884 - Loss: 29.8966\n",
      "Processing batch 1176/11884 - Loss: 29.6381\n",
      "Processing batch 1177/11884 - Loss: 29.5485\n",
      "Processing batch 1178/11884 - Loss: 29.6594\n",
      "Processing batch 1179/11884 - Loss: 30.1321\n",
      "Processing batch 1180/11884 - Loss: 30.5426\n",
      "Processing batch 1181/11884 - Loss: 30.2175\n",
      "Processing batch 1182/11884 - Loss: 29.9143\n",
      "Processing batch 1183/11884 - Loss: 27.6910\n",
      "Processing batch 1184/11884 - Loss: 28.4109\n",
      "Processing batch 1185/11884 - Loss: 27.4255\n",
      "Processing batch 1186/11884 - Loss: 31.5485\n",
      "Processing batch 1187/11884 - Loss: 29.0477\n",
      "Processing batch 1188/11884 - Loss: 29.1802\n",
      "Processing batch 1189/11884 - Loss: 29.3814\n",
      "Processing batch 1190/11884 - Loss: 30.3719\n",
      "Processing batch 1191/11884 - Loss: 29.3171\n",
      "Processing batch 1192/11884 - Loss: 28.3499\n",
      "Processing batch 1193/11884 - Loss: 31.0033\n",
      "Processing batch 1194/11884 - Loss: 28.0466\n",
      "Processing batch 1195/11884 - Loss: 30.0256\n",
      "Processing batch 1196/11884 - Loss: 29.7758\n",
      "Processing batch 1197/11884 - Loss: 31.0252\n",
      "Processing batch 1198/11884 - Loss: 29.0840\n",
      "Processing batch 1199/11884 - Loss: 29.2868\n",
      "Processing batch 1200/11884 - Loss: 30.7641\n",
      "Processing batch 1201/11884 - Loss: 30.1484\n",
      "Processing batch 1202/11884 - Loss: 28.3804\n",
      "Processing batch 1203/11884 - Loss: 29.7706\n",
      "Processing batch 1204/11884 - Loss: 29.7176\n",
      "Processing batch 1205/11884 - Loss: 30.5204\n",
      "Processing batch 1206/11884 - Loss: 29.4801\n",
      "Processing batch 1207/11884 - Loss: 30.9629\n",
      "Processing batch 1208/11884 - Loss: 29.0658\n",
      "Processing batch 1209/11884 - Loss: 29.7749\n",
      "Processing batch 1210/11884 - Loss: 29.4326\n",
      "Processing batch 1211/11884 - Loss: 32.0854\n",
      "Processing batch 1212/11884 - Loss: 28.7420\n",
      "Processing batch 1213/11884 - Loss: 28.7943\n",
      "Processing batch 1214/11884 - Loss: 30.8658\n",
      "Processing batch 1215/11884 - Loss: 30.3714\n",
      "Processing batch 1216/11884 - Loss: 30.5340\n",
      "Processing batch 1217/11884 - Loss: 29.5422\n",
      "Processing batch 1218/11884 - Loss: 30.4421\n",
      "Processing batch 1219/11884 - Loss: 30.5066\n",
      "Processing batch 1220/11884 - Loss: 29.7019\n",
      "Processing batch 1221/11884 - Loss: 29.2110\n",
      "Processing batch 1222/11884 - Loss: 30.3054\n",
      "Processing batch 1223/11884 - Loss: 29.3742\n",
      "Processing batch 1224/11884 - Loss: 29.8817\n",
      "Processing batch 1225/11884 - Loss: 30.5936\n",
      "Processing batch 1226/11884 - Loss: 30.5782\n",
      "Processing batch 1227/11884 - Loss: 30.7201\n",
      "Processing batch 1228/11884 - Loss: 28.7960\n",
      "Processing batch 1229/11884 - Loss: 29.6508\n",
      "Processing batch 1230/11884 - Loss: 29.1564\n",
      "Processing batch 1231/11884 - Loss: 32.7940\n",
      "Processing batch 1232/11884 - Loss: 29.8450\n",
      "Processing batch 1233/11884 - Loss: 29.7796\n",
      "Processing batch 1234/11884 - Loss: 29.8264\n",
      "Processing batch 1235/11884 - Loss: 30.0771\n",
      "Processing batch 1236/11884 - Loss: 29.8599\n",
      "Processing batch 1237/11884 - Loss: 31.2964\n",
      "Processing batch 1238/11884 - Loss: 30.7376\n",
      "Processing batch 1239/11884 - Loss: 29.1987\n",
      "Processing batch 1240/11884 - Loss: 29.7781\n",
      "Processing batch 1241/11884 - Loss: 30.4513\n",
      "Processing batch 1242/11884 - Loss: 29.5917\n",
      "Processing batch 1243/11884 - Loss: 30.5648\n",
      "Processing batch 1244/11884 - Loss: 29.9188\n",
      "Processing batch 1245/11884 - Loss: 30.9346\n",
      "Processing batch 1246/11884 - Loss: 31.2100\n",
      "Processing batch 1247/11884 - Loss: 30.3700\n",
      "Processing batch 1248/11884 - Loss: 30.6963\n",
      "Processing batch 1249/11884 - Loss: 30.0128\n",
      "Processing batch 1250/11884 - Loss: 29.5738\n",
      "Processing batch 1251/11884 - Loss: 29.5617\n",
      "Processing batch 1252/11884 - Loss: 31.1722\n",
      "Processing batch 1253/11884 - Loss: 29.5140\n",
      "Processing batch 1254/11884 - Loss: 29.7225\n",
      "Processing batch 1255/11884 - Loss: 29.3745\n",
      "Processing batch 1256/11884 - Loss: 30.0354\n",
      "Processing batch 1257/11884 - Loss: 29.5729\n",
      "Processing batch 1258/11884 - Loss: 30.0916\n",
      "Processing batch 1259/11884 - Loss: 30.4597\n",
      "Processing batch 1260/11884 - Loss: 29.7830\n",
      "Processing batch 1261/11884 - Loss: 29.3829\n",
      "Processing batch 1262/11884 - Loss: 29.5235\n",
      "Processing batch 1263/11884 - Loss: 29.1476\n",
      "Processing batch 1264/11884 - Loss: 28.5628\n",
      "Processing batch 1265/11884 - Loss: 29.3406\n",
      "Processing batch 1266/11884 - Loss: 29.6259\n",
      "Processing batch 1267/11884 - Loss: 30.1443\n",
      "Processing batch 1268/11884 - Loss: 30.0008\n",
      "Processing batch 1269/11884 - Loss: 29.7429\n",
      "Processing batch 1270/11884 - Loss: 30.4504\n",
      "Processing batch 1271/11884 - Loss: 29.3269\n",
      "Processing batch 1272/11884 - Loss: 29.8738\n",
      "Processing batch 1273/11884 - Loss: 30.9735\n",
      "Processing batch 1274/11884 - Loss: 29.8554\n",
      "Processing batch 1275/11884 - Loss: 29.8180\n",
      "Processing batch 1276/11884 - Loss: 30.8339\n",
      "Processing batch 1277/11884 - Loss: 28.1729\n",
      "Processing batch 1278/11884 - Loss: 29.3059\n",
      "Processing batch 1279/11884 - Loss: 29.8252\n",
      "Processing batch 1280/11884 - Loss: 30.4008\n",
      "Processing batch 1281/11884 - Loss: 30.6489\n",
      "Processing batch 1282/11884 - Loss: 29.5929\n",
      "Processing batch 1283/11884 - Loss: 29.5313\n",
      "Processing batch 1284/11884 - Loss: 30.0821\n",
      "Processing batch 1285/11884 - Loss: 29.3369\n",
      "Processing batch 1286/11884 - Loss: 30.6698\n",
      "Processing batch 1287/11884 - Loss: 30.4890\n",
      "Processing batch 1288/11884 - Loss: 29.4774\n",
      "Processing batch 1289/11884 - Loss: 30.7353\n",
      "Processing batch 1290/11884 - Loss: 29.4977\n",
      "Processing batch 1291/11884 - Loss: 30.4347\n",
      "Processing batch 1292/11884 - Loss: 29.5725\n",
      "Processing batch 1293/11884 - Loss: 29.6224\n",
      "Processing batch 1294/11884 - Loss: 29.7021\n",
      "Processing batch 1295/11884 - Loss: 28.7609\n",
      "Processing batch 1296/11884 - Loss: 28.6315\n",
      "Processing batch 1297/11884 - Loss: 30.3691\n",
      "Processing batch 1298/11884 - Loss: 29.5933\n",
      "Processing batch 1299/11884 - Loss: 31.3381\n",
      "Processing batch 1300/11884 - Loss: 30.3918\n",
      "Processing batch 1301/11884 - Loss: 30.3701\n",
      "Processing batch 1302/11884 - Loss: 28.0958\n",
      "Processing batch 1303/11884 - Loss: 29.2535\n",
      "Processing batch 1304/11884 - Loss: 29.9776\n",
      "Processing batch 1305/11884 - Loss: 30.7729\n",
      "Processing batch 1306/11884 - Loss: 29.8285\n",
      "Processing batch 1307/11884 - Loss: 28.1722\n",
      "Processing batch 1308/11884 - Loss: 30.0229\n",
      "Processing batch 1309/11884 - Loss: 29.7765\n",
      "Processing batch 1310/11884 - Loss: 30.2054\n",
      "Processing batch 1311/11884 - Loss: 28.7275\n",
      "Processing batch 1312/11884 - Loss: 29.0648\n",
      "Processing batch 1313/11884 - Loss: 30.1413\n",
      "Processing batch 1314/11884 - Loss: 29.5377\n",
      "Processing batch 1315/11884 - Loss: 29.4303\n",
      "Processing batch 1316/11884 - Loss: 28.9416\n",
      "Processing batch 1317/11884 - Loss: 29.0206\n",
      "Processing batch 1318/11884 - Loss: 29.1119\n",
      "Processing batch 1319/11884 - Loss: 28.7364\n",
      "Processing batch 1320/11884 - Loss: 28.8318\n",
      "Processing batch 1321/11884 - Loss: 28.4027\n",
      "Processing batch 1322/11884 - Loss: 29.1756\n",
      "Processing batch 1323/11884 - Loss: 28.8979\n",
      "Processing batch 1324/11884 - Loss: 29.2272\n",
      "Processing batch 1325/11884 - Loss: 30.2811\n",
      "Processing batch 1326/11884 - Loss: 30.0146\n",
      "Processing batch 1327/11884 - Loss: 29.6844\n",
      "Processing batch 1328/11884 - Loss: 29.8730\n",
      "Processing batch 1329/11884 - Loss: 29.8159\n",
      "Processing batch 1330/11884 - Loss: 29.5093\n",
      "Processing batch 1331/11884 - Loss: 30.4811\n",
      "Processing batch 1332/11884 - Loss: 31.1738\n",
      "Processing batch 1333/11884 - Loss: 31.0432\n",
      "Processing batch 1334/11884 - Loss: 29.7737\n",
      "Processing batch 1335/11884 - Loss: 29.3579\n",
      "Processing batch 1336/11884 - Loss: 30.2527\n",
      "Processing batch 1337/11884 - Loss: 30.4592\n",
      "Processing batch 1338/11884 - Loss: 29.7727\n",
      "Processing batch 1339/11884 - Loss: 30.0353\n",
      "Processing batch 1340/11884 - Loss: 29.7680\n",
      "Processing batch 1341/11884 - Loss: 29.4279\n",
      "Processing batch 1342/11884 - Loss: 29.7667\n",
      "Processing batch 1343/11884 - Loss: 30.5591\n",
      "Processing batch 1344/11884 - Loss: 30.3575\n",
      "Processing batch 1345/11884 - Loss: 31.0463\n",
      "Processing batch 1346/11884 - Loss: 31.0476\n",
      "Processing batch 1347/11884 - Loss: 29.1773\n",
      "Processing batch 1348/11884 - Loss: 30.8366\n",
      "Processing batch 1349/11884 - Loss: 30.1075\n",
      "Processing batch 1350/11884 - Loss: 28.6334\n",
      "Processing batch 1351/11884 - Loss: 29.8806\n",
      "Processing batch 1352/11884 - Loss: 29.8745\n",
      "Processing batch 1353/11884 - Loss: 30.1585\n",
      "Processing batch 1354/11884 - Loss: 29.8554\n",
      "Processing batch 1355/11884 - Loss: 30.5798\n",
      "Processing batch 1356/11884 - Loss: 28.5210\n",
      "Processing batch 1357/11884 - Loss: 30.0878\n",
      "Processing batch 1358/11884 - Loss: 30.0905\n",
      "Processing batch 1359/11884 - Loss: 29.1444\n",
      "Processing batch 1360/11884 - Loss: 30.5849\n",
      "Processing batch 1361/11884 - Loss: 31.1474\n",
      "Processing batch 1362/11884 - Loss: 30.2472\n",
      "Processing batch 1363/11884 - Loss: 30.1649\n",
      "Processing batch 1364/11884 - Loss: 29.7447\n",
      "Processing batch 1365/11884 - Loss: 31.1355\n",
      "Processing batch 1366/11884 - Loss: 30.3511\n",
      "Processing batch 1367/11884 - Loss: 30.0755\n",
      "Processing batch 1368/11884 - Loss: 30.2234\n",
      "Processing batch 1369/11884 - Loss: 30.6982\n",
      "Processing batch 1370/11884 - Loss: 31.0779\n",
      "Processing batch 1371/11884 - Loss: 29.3234\n",
      "Processing batch 1372/11884 - Loss: 29.3515\n",
      "Processing batch 1373/11884 - Loss: 29.7130\n",
      "Processing batch 1374/11884 - Loss: 29.7841\n",
      "Processing batch 1375/11884 - Loss: 29.6785\n",
      "Processing batch 1376/11884 - Loss: 29.8233\n",
      "Processing batch 1377/11884 - Loss: 28.6763\n",
      "Processing batch 1378/11884 - Loss: 30.6925\n",
      "Processing batch 1379/11884 - Loss: 31.1202\n",
      "Processing batch 1380/11884 - Loss: 30.5465\n",
      "Processing batch 1381/11884 - Loss: 30.3725\n",
      "Processing batch 1382/11884 - Loss: 29.9539\n",
      "Processing batch 1383/11884 - Loss: 29.1269\n",
      "Processing batch 1384/11884 - Loss: 30.5299\n",
      "Processing batch 1385/11884 - Loss: 29.4025\n",
      "Processing batch 1386/11884 - Loss: 29.8777\n",
      "Processing batch 1387/11884 - Loss: 28.8957\n",
      "Processing batch 1388/11884 - Loss: 28.7789\n",
      "Processing batch 1389/11884 - Loss: 30.1093\n",
      "Processing batch 1390/11884 - Loss: 29.9455\n",
      "Processing batch 1391/11884 - Loss: 30.4519\n",
      "Processing batch 1392/11884 - Loss: 29.7573\n",
      "Processing batch 1393/11884 - Loss: 30.8007\n",
      "Processing batch 1394/11884 - Loss: 31.8991\n",
      "Processing batch 1395/11884 - Loss: 30.2532\n",
      "Processing batch 1396/11884 - Loss: 29.6113\n",
      "Processing batch 1397/11884 - Loss: 28.7465\n",
      "Processing batch 1398/11884 - Loss: 29.8660\n",
      "Processing batch 1399/11884 - Loss: 29.8137\n",
      "Processing batch 1400/11884 - Loss: 29.3667\n",
      "Processing batch 1401/11884 - Loss: 30.8846\n",
      "Processing batch 1402/11884 - Loss: 28.6336\n",
      "Processing batch 1403/11884 - Loss: 30.8234\n",
      "Processing batch 1404/11884 - Loss: 30.3487\n",
      "Processing batch 1405/11884 - Loss: 30.4156\n",
      "Processing batch 1406/11884 - Loss: 28.8644\n",
      "Processing batch 1407/11884 - Loss: 30.3314\n",
      "Processing batch 1408/11884 - Loss: 28.8132\n",
      "Processing batch 1409/11884 - Loss: 30.5300\n",
      "Processing batch 1410/11884 - Loss: 29.0683\n",
      "Processing batch 1411/11884 - Loss: 29.6898\n",
      "Processing batch 1412/11884 - Loss: 30.0580\n",
      "Processing batch 1413/11884 - Loss: 28.6758\n",
      "Processing batch 1414/11884 - Loss: 28.2753\n",
      "Processing batch 1415/11884 - Loss: 29.7771\n",
      "Processing batch 1416/11884 - Loss: 29.2983\n",
      "Processing batch 1417/11884 - Loss: 29.0045\n",
      "Processing batch 1418/11884 - Loss: 29.0816\n",
      "Processing batch 1419/11884 - Loss: 30.1967\n",
      "Processing batch 1420/11884 - Loss: 29.4466\n",
      "Processing batch 1421/11884 - Loss: 30.4710\n",
      "Processing batch 1422/11884 - Loss: 29.4199\n",
      "Processing batch 1423/11884 - Loss: 29.0445\n",
      "Processing batch 1424/11884 - Loss: 28.8607\n",
      "Processing batch 1425/11884 - Loss: 30.0171\n",
      "Processing batch 1426/11884 - Loss: 29.0817\n",
      "Processing batch 1427/11884 - Loss: 30.2971\n",
      "Processing batch 1428/11884 - Loss: 30.8459\n",
      "Processing batch 1429/11884 - Loss: 29.7195\n",
      "Processing batch 1430/11884 - Loss: 29.8558\n",
      "Processing batch 1431/11884 - Loss: 29.6440\n",
      "Processing batch 1432/11884 - Loss: 31.1810\n",
      "Processing batch 1433/11884 - Loss: 31.6381\n",
      "Processing batch 1434/11884 - Loss: 29.2458\n",
      "Processing batch 1435/11884 - Loss: 29.8360\n",
      "Processing batch 1436/11884 - Loss: 28.8739\n",
      "Processing batch 1437/11884 - Loss: 29.6087\n",
      "Processing batch 1438/11884 - Loss: 29.1674\n",
      "Processing batch 1439/11884 - Loss: 30.7951\n",
      "Processing batch 1440/11884 - Loss: 30.0615\n",
      "Processing batch 1441/11884 - Loss: 29.9557\n",
      "Processing batch 1442/11884 - Loss: 29.4098\n",
      "Processing batch 1443/11884 - Loss: 30.4198\n",
      "Processing batch 1444/11884 - Loss: 29.3633\n",
      "Processing batch 1445/11884 - Loss: 30.0537\n",
      "Processing batch 1446/11884 - Loss: 30.2176\n",
      "Processing batch 1447/11884 - Loss: 29.0549\n",
      "Processing batch 1448/11884 - Loss: 29.0511\n",
      "Processing batch 1449/11884 - Loss: 29.9476\n",
      "Processing batch 1450/11884 - Loss: 29.8565\n",
      "Processing batch 1451/11884 - Loss: 29.7991\n",
      "Processing batch 1452/11884 - Loss: 29.4511\n",
      "Processing batch 1453/11884 - Loss: 30.1918\n",
      "Processing batch 1454/11884 - Loss: 28.0010\n",
      "Processing batch 1455/11884 - Loss: 30.2742\n",
      "Processing batch 1456/11884 - Loss: 30.5175\n",
      "Processing batch 1457/11884 - Loss: 30.5194\n",
      "Processing batch 1458/11884 - Loss: 29.5121\n",
      "Processing batch 1459/11884 - Loss: 29.3020\n",
      "Processing batch 1460/11884 - Loss: 29.1625\n",
      "Processing batch 1461/11884 - Loss: 29.6352\n",
      "Processing batch 1462/11884 - Loss: 29.9588\n",
      "Processing batch 1463/11884 - Loss: 29.2064\n",
      "Processing batch 1464/11884 - Loss: 30.2534\n",
      "Processing batch 1465/11884 - Loss: 30.2902\n",
      "Processing batch 1466/11884 - Loss: 30.1707\n",
      "Processing batch 1467/11884 - Loss: 30.9345\n",
      "Processing batch 1468/11884 - Loss: 29.8357\n",
      "Processing batch 1469/11884 - Loss: 30.7945\n",
      "Processing batch 1470/11884 - Loss: 29.4668\n",
      "Processing batch 1471/11884 - Loss: 30.4282\n",
      "Processing batch 1472/11884 - Loss: 30.0459\n",
      "Processing batch 1473/11884 - Loss: 29.8695\n",
      "Processing batch 1474/11884 - Loss: 30.3190\n",
      "Processing batch 1475/11884 - Loss: 29.3767\n",
      "Processing batch 1476/11884 - Loss: 29.6699\n",
      "Processing batch 1477/11884 - Loss: 29.5699\n",
      "Processing batch 1478/11884 - Loss: 29.2895\n",
      "Processing batch 1479/11884 - Loss: 30.9547\n",
      "Processing batch 1480/11884 - Loss: 29.6155\n",
      "Processing batch 1481/11884 - Loss: 29.5521\n",
      "Processing batch 1482/11884 - Loss: 30.2439\n",
      "Processing batch 1483/11884 - Loss: 28.6688\n",
      "Processing batch 1484/11884 - Loss: 30.1993\n",
      "Processing batch 1485/11884 - Loss: 29.0750\n",
      "Processing batch 1486/11884 - Loss: 29.6885\n",
      "Processing batch 1487/11884 - Loss: 30.3067\n",
      "Processing batch 1488/11884 - Loss: 29.4130\n",
      "Processing batch 1489/11884 - Loss: 29.4497\n",
      "Processing batch 1490/11884 - Loss: 30.0438\n",
      "Processing batch 1491/11884 - Loss: 30.0323\n",
      "Processing batch 1492/11884 - Loss: 31.0755\n",
      "Processing batch 1493/11884 - Loss: 29.4100\n",
      "Processing batch 1494/11884 - Loss: 30.8362\n",
      "Processing batch 1495/11884 - Loss: 29.6533\n",
      "Processing batch 1496/11884 - Loss: 29.1176\n",
      "Processing batch 1497/11884 - Loss: 30.7449\n",
      "Processing batch 1498/11884 - Loss: 29.9100\n",
      "Processing batch 1499/11884 - Loss: 29.9754\n",
      "Processing batch 1500/11884 - Loss: 30.7999\n",
      "Processing batch 1501/11884 - Loss: 29.9868\n",
      "Processing batch 1502/11884 - Loss: 28.2683\n",
      "Processing batch 1503/11884 - Loss: 31.4888\n",
      "Processing batch 1504/11884 - Loss: 29.0538\n",
      "Processing batch 1505/11884 - Loss: 31.5897\n",
      "Processing batch 1506/11884 - Loss: 30.3077\n",
      "Processing batch 1507/11884 - Loss: 30.3611\n",
      "Processing batch 1508/11884 - Loss: 29.1163\n",
      "Processing batch 1509/11884 - Loss: 29.4058\n",
      "Processing batch 1510/11884 - Loss: 29.5768\n",
      "Processing batch 1511/11884 - Loss: 29.4534\n",
      "Processing batch 1512/11884 - Loss: 30.2102\n",
      "Processing batch 1513/11884 - Loss: 29.6818\n",
      "Processing batch 1514/11884 - Loss: 29.4897\n",
      "Processing batch 1515/11884 - Loss: 29.3562\n",
      "Processing batch 1516/11884 - Loss: 30.1496\n",
      "Processing batch 1517/11884 - Loss: 30.7451\n",
      "Processing batch 1518/11884 - Loss: 28.7706\n",
      "Processing batch 1519/11884 - Loss: 29.9282\n",
      "Processing batch 1520/11884 - Loss: 29.7885\n",
      "Processing batch 1521/11884 - Loss: 30.1431\n",
      "Processing batch 1522/11884 - Loss: 29.8790\n",
      "Processing batch 1523/11884 - Loss: 30.5888\n",
      "Processing batch 1524/11884 - Loss: 29.6273\n",
      "Processing batch 1525/11884 - Loss: 30.1552\n",
      "Processing batch 1526/11884 - Loss: 29.3973\n",
      "Processing batch 1527/11884 - Loss: 29.7319\n",
      "Processing batch 1528/11884 - Loss: 30.3209\n",
      "Processing batch 1529/11884 - Loss: 28.4776\n",
      "Processing batch 1530/11884 - Loss: 30.7181\n",
      "Processing batch 1531/11884 - Loss: 28.6385\n",
      "Processing batch 1532/11884 - Loss: 30.9459\n",
      "Processing batch 1533/11884 - Loss: 30.5150\n",
      "Processing batch 1534/11884 - Loss: 28.9099\n",
      "Processing batch 1535/11884 - Loss: 30.3402\n",
      "Processing batch 1536/11884 - Loss: 30.9340\n",
      "Processing batch 1537/11884 - Loss: 29.1139\n",
      "Processing batch 1538/11884 - Loss: 29.5704\n",
      "Processing batch 1539/11884 - Loss: 28.4575\n",
      "Processing batch 1540/11884 - Loss: 29.9140\n",
      "Processing batch 1541/11884 - Loss: 29.9884\n",
      "Processing batch 1542/11884 - Loss: 29.1840\n",
      "Processing batch 1543/11884 - Loss: 28.1119\n",
      "Processing batch 1544/11884 - Loss: 29.8203\n",
      "Processing batch 1545/11884 - Loss: 29.6904\n",
      "Processing batch 1546/11884 - Loss: 30.0932\n",
      "Processing batch 1547/11884 - Loss: 30.1696\n",
      "Processing batch 1548/11884 - Loss: 30.2618\n",
      "Processing batch 1549/11884 - Loss: 29.5979\n",
      "Processing batch 1550/11884 - Loss: 30.0202\n",
      "Processing batch 1551/11884 - Loss: 30.0819\n",
      "Processing batch 1552/11884 - Loss: 29.7483\n",
      "Processing batch 1553/11884 - Loss: 30.1116\n",
      "Processing batch 1554/11884 - Loss: 31.0226\n",
      "Processing batch 1555/11884 - Loss: 30.3163\n",
      "Processing batch 1556/11884 - Loss: 31.4785\n",
      "Processing batch 1557/11884 - Loss: 30.8021\n",
      "Processing batch 1558/11884 - Loss: 30.6487\n",
      "Processing batch 1559/11884 - Loss: 30.2724\n",
      "Processing batch 1560/11884 - Loss: 29.5887\n",
      "Processing batch 1561/11884 - Loss: 30.2947\n",
      "Processing batch 1562/11884 - Loss: 29.6735\n",
      "Processing batch 1563/11884 - Loss: 30.6926\n",
      "Processing batch 1564/11884 - Loss: 29.2450\n",
      "Processing batch 1565/11884 - Loss: 29.4363\n",
      "Processing batch 1566/11884 - Loss: 29.3873\n",
      "Processing batch 1567/11884 - Loss: 29.4326\n",
      "Processing batch 1568/11884 - Loss: 29.6585\n",
      "Processing batch 1569/11884 - Loss: 29.6697\n",
      "Processing batch 1570/11884 - Loss: 31.1241\n",
      "Processing batch 1571/11884 - Loss: 30.6917\n",
      "Processing batch 1572/11884 - Loss: 29.1715\n",
      "Processing batch 1573/11884 - Loss: 29.9915\n",
      "Processing batch 1574/11884 - Loss: 28.3017\n",
      "Processing batch 1575/11884 - Loss: 28.9996\n",
      "Processing batch 1576/11884 - Loss: 30.7617\n",
      "Processing batch 1577/11884 - Loss: 29.6203\n",
      "Processing batch 1578/11884 - Loss: 28.8464\n",
      "Processing batch 1579/11884 - Loss: 29.4622\n",
      "Processing batch 1580/11884 - Loss: 28.7034\n",
      "Processing batch 1581/11884 - Loss: 30.8709\n",
      "Processing batch 1582/11884 - Loss: 30.2270\n",
      "Processing batch 1583/11884 - Loss: 30.7135\n",
      "Processing batch 1584/11884 - Loss: 29.6462\n",
      "Processing batch 1585/11884 - Loss: 30.1965\n",
      "Processing batch 1586/11884 - Loss: 31.0532\n",
      "Processing batch 1587/11884 - Loss: 29.3319\n",
      "Processing batch 1588/11884 - Loss: 29.3374\n",
      "Processing batch 1589/11884 - Loss: 29.5852\n",
      "Processing batch 1590/11884 - Loss: 29.2070\n",
      "Processing batch 1591/11884 - Loss: 30.0640\n",
      "Processing batch 1592/11884 - Loss: 29.3825\n",
      "Processing batch 1593/11884 - Loss: 29.8391\n",
      "Processing batch 1594/11884 - Loss: 29.7609\n",
      "Processing batch 1595/11884 - Loss: 29.6873\n",
      "Processing batch 1596/11884 - Loss: 28.9683\n",
      "Processing batch 1597/11884 - Loss: 27.6043\n",
      "Processing batch 1598/11884 - Loss: 30.2027\n",
      "Processing batch 1599/11884 - Loss: 30.0555\n",
      "Processing batch 1600/11884 - Loss: 30.9139\n",
      "Processing batch 1601/11884 - Loss: 30.5974\n",
      "Processing batch 1602/11884 - Loss: 29.8653\n",
      "Processing batch 1603/11884 - Loss: 27.5820\n",
      "Processing batch 1604/11884 - Loss: 30.1469\n",
      "Processing batch 1605/11884 - Loss: 29.7098\n",
      "Processing batch 1606/11884 - Loss: 28.7001\n",
      "Processing batch 1607/11884 - Loss: 30.0115\n",
      "Processing batch 1608/11884 - Loss: 29.6306\n",
      "Processing batch 1609/11884 - Loss: 29.8166\n",
      "Processing batch 1610/11884 - Loss: 30.1524\n",
      "Processing batch 1611/11884 - Loss: 30.5759\n",
      "Processing batch 1612/11884 - Loss: 29.4826\n",
      "Processing batch 1613/11884 - Loss: 31.5639\n",
      "Processing batch 1614/11884 - Loss: 30.4741\n",
      "Processing batch 1615/11884 - Loss: 28.6990\n",
      "Processing batch 1616/11884 - Loss: 29.4898\n",
      "Processing batch 1617/11884 - Loss: 29.7518\n",
      "Processing batch 1618/11884 - Loss: 29.4485\n",
      "Processing batch 1619/11884 - Loss: 28.5851\n",
      "Processing batch 1620/11884 - Loss: 30.6665\n",
      "Processing batch 1621/11884 - Loss: 30.9092\n",
      "Processing batch 1622/11884 - Loss: 29.1665\n",
      "Processing batch 1623/11884 - Loss: 29.2656\n",
      "Processing batch 1624/11884 - Loss: 30.0435\n",
      "Processing batch 1625/11884 - Loss: 30.4358\n",
      "Processing batch 1626/11884 - Loss: 29.2423\n",
      "Processing batch 1627/11884 - Loss: 30.6840\n",
      "Processing batch 1628/11884 - Loss: 29.6727\n",
      "Processing batch 1629/11884 - Loss: 28.4171\n",
      "Processing batch 1630/11884 - Loss: 29.2754\n",
      "Processing batch 1631/11884 - Loss: 30.1638\n",
      "Processing batch 1632/11884 - Loss: 30.3752\n",
      "Processing batch 1633/11884 - Loss: 29.3119\n",
      "Processing batch 1634/11884 - Loss: 29.4554\n",
      "Processing batch 1635/11884 - Loss: 29.8640\n",
      "Processing batch 1636/11884 - Loss: 29.7022\n",
      "Processing batch 1637/11884 - Loss: 28.9305\n",
      "Processing batch 1638/11884 - Loss: 29.8308\n",
      "Processing batch 1639/11884 - Loss: 29.1739\n",
      "Processing batch 1640/11884 - Loss: 30.0289\n",
      "Processing batch 1641/11884 - Loss: 28.3207\n",
      "Processing batch 1642/11884 - Loss: 28.4384\n",
      "Processing batch 1643/11884 - Loss: 29.8186\n",
      "Processing batch 1644/11884 - Loss: 30.6669\n",
      "Processing batch 1645/11884 - Loss: 30.6468\n",
      "Processing batch 1646/11884 - Loss: 28.2163\n",
      "Processing batch 1647/11884 - Loss: 30.8057\n",
      "Processing batch 1648/11884 - Loss: 30.4551\n",
      "Processing batch 1649/11884 - Loss: 31.0250\n",
      "Processing batch 1650/11884 - Loss: 29.8648\n",
      "Processing batch 1651/11884 - Loss: 29.7620\n",
      "Processing batch 1652/11884 - Loss: 31.3571\n",
      "Processing batch 1653/11884 - Loss: 29.1917\n",
      "Processing batch 1654/11884 - Loss: 30.4914\n",
      "Processing batch 1655/11884 - Loss: 29.1322\n",
      "Processing batch 1656/11884 - Loss: 29.8552\n",
      "Processing batch 1657/11884 - Loss: 29.4478\n",
      "Processing batch 1658/11884 - Loss: 29.4749\n",
      "Processing batch 1659/11884 - Loss: 31.2185\n",
      "Processing batch 1660/11884 - Loss: 29.5787\n",
      "Processing batch 1661/11884 - Loss: 30.1002\n",
      "Processing batch 1662/11884 - Loss: 30.1681\n",
      "Processing batch 1663/11884 - Loss: 30.5067\n",
      "Processing batch 1664/11884 - Loss: 29.6914\n",
      "Processing batch 1665/11884 - Loss: 30.1031\n",
      "Processing batch 1666/11884 - Loss: 29.9456\n",
      "Processing batch 1667/11884 - Loss: 28.7243\n",
      "Processing batch 1668/11884 - Loss: 28.9295\n",
      "Processing batch 1669/11884 - Loss: 30.2264\n",
      "Processing batch 1670/11884 - Loss: 30.6242\n",
      "Processing batch 1671/11884 - Loss: 29.7712\n",
      "Processing batch 1672/11884 - Loss: 31.9244\n",
      "Processing batch 1673/11884 - Loss: 29.8504\n",
      "Processing batch 1674/11884 - Loss: 29.7752\n",
      "Processing batch 1675/11884 - Loss: 29.2915\n",
      "Processing batch 1676/11884 - Loss: 30.2197\n",
      "Processing batch 1677/11884 - Loss: 29.1211\n",
      "Processing batch 1678/11884 - Loss: 30.5339\n",
      "Processing batch 1679/11884 - Loss: 29.9188\n",
      "Processing batch 1680/11884 - Loss: 29.9496\n",
      "Processing batch 1681/11884 - Loss: 29.1722\n",
      "Processing batch 1682/11884 - Loss: 29.5660\n",
      "Processing batch 1683/11884 - Loss: 30.0470\n",
      "Processing batch 1684/11884 - Loss: 30.4807\n",
      "Processing batch 1685/11884 - Loss: 28.9219\n",
      "Processing batch 1686/11884 - Loss: 30.3494\n",
      "Processing batch 1687/11884 - Loss: 30.6997\n",
      "Processing batch 1688/11884 - Loss: 30.0088\n",
      "Processing batch 1689/11884 - Loss: 31.2210\n",
      "Processing batch 1690/11884 - Loss: 30.8805\n",
      "Processing batch 1691/11884 - Loss: 30.0733\n",
      "Processing batch 1692/11884 - Loss: 30.0235\n",
      "Processing batch 1693/11884 - Loss: 30.5585\n",
      "Processing batch 1694/11884 - Loss: 30.0202\n",
      "Processing batch 1695/11884 - Loss: 30.1546\n",
      "Processing batch 1696/11884 - Loss: 29.8528\n",
      "Processing batch 1697/11884 - Loss: 30.0218\n",
      "Processing batch 1698/11884 - Loss: 29.4734\n",
      "Processing batch 1699/11884 - Loss: 30.4386\n",
      "Processing batch 1700/11884 - Loss: 27.8029\n",
      "Processing batch 1701/11884 - Loss: 30.7979\n",
      "Processing batch 1702/11884 - Loss: 29.9545\n",
      "Processing batch 1703/11884 - Loss: 30.1973\n",
      "Processing batch 1704/11884 - Loss: 29.7615\n",
      "Processing batch 1705/11884 - Loss: 29.4122\n",
      "Processing batch 1706/11884 - Loss: 29.8799\n",
      "Processing batch 1707/11884 - Loss: 30.0360\n",
      "Processing batch 1708/11884 - Loss: 30.2581\n",
      "Processing batch 1709/11884 - Loss: 30.0579\n",
      "Processing batch 1710/11884 - Loss: 30.4862\n",
      "Processing batch 1711/11884 - Loss: 28.9604\n",
      "Processing batch 1712/11884 - Loss: 30.2668\n",
      "Processing batch 1713/11884 - Loss: 29.4155\n",
      "Processing batch 1714/11884 - Loss: 28.4346\n",
      "Processing batch 1715/11884 - Loss: 29.7442\n",
      "Processing batch 1716/11884 - Loss: 29.1811\n",
      "Processing batch 1717/11884 - Loss: 30.1963\n",
      "Processing batch 1718/11884 - Loss: 30.3820\n",
      "Processing batch 1719/11884 - Loss: 29.9876\n",
      "Processing batch 1720/11884 - Loss: 29.7075\n",
      "Processing batch 1721/11884 - Loss: 29.3828\n",
      "Processing batch 1722/11884 - Loss: 29.3731\n",
      "Processing batch 1723/11884 - Loss: 30.0765\n",
      "Processing batch 1724/11884 - Loss: 28.9637\n",
      "Processing batch 1725/11884 - Loss: 29.1996\n",
      "Processing batch 1726/11884 - Loss: 29.5165\n",
      "Processing batch 1727/11884 - Loss: 30.1054\n",
      "Processing batch 1728/11884 - Loss: 29.1779\n",
      "Processing batch 1729/11884 - Loss: 30.8062\n",
      "Processing batch 1730/11884 - Loss: 29.4854\n",
      "Processing batch 1731/11884 - Loss: 29.4230\n",
      "Processing batch 1732/11884 - Loss: 30.5285\n",
      "Processing batch 1733/11884 - Loss: 28.9813\n",
      "Processing batch 1734/11884 - Loss: 29.3217\n",
      "Processing batch 1735/11884 - Loss: 29.7537\n",
      "Processing batch 1736/11884 - Loss: 29.9546\n",
      "Processing batch 1737/11884 - Loss: 28.7853\n",
      "Processing batch 1738/11884 - Loss: 29.4585\n",
      "Processing batch 1739/11884 - Loss: 28.8787\n",
      "Processing batch 1740/11884 - Loss: 29.3838\n",
      "Processing batch 1741/11884 - Loss: 28.0566\n",
      "Processing batch 1742/11884 - Loss: 30.0703\n",
      "Processing batch 1743/11884 - Loss: 29.1423\n",
      "Processing batch 1744/11884 - Loss: 30.2831\n",
      "Processing batch 1745/11884 - Loss: 28.7191\n",
      "Processing batch 1746/11884 - Loss: 29.7477\n",
      "Processing batch 1747/11884 - Loss: 29.2085\n",
      "Processing batch 1748/11884 - Loss: 29.3799\n",
      "Processing batch 1749/11884 - Loss: 29.1001\n",
      "Processing batch 1750/11884 - Loss: 29.2912\n",
      "Processing batch 1751/11884 - Loss: 29.5956\n",
      "Processing batch 1752/11884 - Loss: 30.5438\n",
      "Processing batch 1753/11884 - Loss: 30.9642\n",
      "Processing batch 1754/11884 - Loss: 30.2458\n",
      "Processing batch 1755/11884 - Loss: 29.8502\n",
      "Processing batch 1756/11884 - Loss: 29.3459\n",
      "Processing batch 1757/11884 - Loss: 30.2854\n",
      "Processing batch 1758/11884 - Loss: 29.1603\n",
      "Processing batch 1759/11884 - Loss: 30.6922\n",
      "Processing batch 1760/11884 - Loss: 29.9224\n",
      "Processing batch 1761/11884 - Loss: 29.5541\n",
      "Processing batch 1762/11884 - Loss: 28.5071\n",
      "Processing batch 1763/11884 - Loss: 29.6549\n",
      "Processing batch 1764/11884 - Loss: 28.1604\n",
      "Processing batch 1765/11884 - Loss: 28.5373\n",
      "Processing batch 1766/11884 - Loss: 29.9214\n",
      "Processing batch 1767/11884 - Loss: 31.0373\n",
      "Processing batch 1768/11884 - Loss: 31.2498\n",
      "Processing batch 1769/11884 - Loss: 29.7451\n",
      "Processing batch 1770/11884 - Loss: 29.9123\n",
      "Processing batch 1771/11884 - Loss: 28.5412\n",
      "Processing batch 1772/11884 - Loss: 30.0957\n",
      "Processing batch 1773/11884 - Loss: 31.0284\n",
      "Processing batch 1774/11884 - Loss: 28.5258\n",
      "Processing batch 1775/11884 - Loss: 29.1181\n",
      "Processing batch 1776/11884 - Loss: 30.2654\n",
      "Processing batch 1777/11884 - Loss: 29.8422\n",
      "Processing batch 1778/11884 - Loss: 30.5744\n",
      "Processing batch 1779/11884 - Loss: 29.1960\n",
      "Processing batch 1780/11884 - Loss: 30.5868\n",
      "Processing batch 1781/11884 - Loss: 29.4933\n",
      "Processing batch 1782/11884 - Loss: 29.2912\n",
      "Processing batch 1783/11884 - Loss: 30.0821\n",
      "Processing batch 1784/11884 - Loss: 29.3109\n",
      "Processing batch 1785/11884 - Loss: 30.7405\n",
      "Processing batch 1786/11884 - Loss: 30.5713\n",
      "Processing batch 1787/11884 - Loss: 28.3516\n",
      "Processing batch 1788/11884 - Loss: 30.5130\n",
      "Processing batch 1789/11884 - Loss: 29.0500\n",
      "Processing batch 1790/11884 - Loss: 29.5373\n",
      "Processing batch 1791/11884 - Loss: 29.3798\n",
      "Processing batch 1792/11884 - Loss: 30.5097\n",
      "Processing batch 1793/11884 - Loss: 28.3385\n",
      "Processing batch 1794/11884 - Loss: 30.9697\n",
      "Processing batch 1795/11884 - Loss: 28.6902\n",
      "Processing batch 1796/11884 - Loss: 30.2550\n",
      "Processing batch 1797/11884 - Loss: 28.6427\n",
      "Processing batch 1798/11884 - Loss: 28.2965\n",
      "Processing batch 1799/11884 - Loss: 30.3274\n",
      "Processing batch 1800/11884 - Loss: 29.6969\n",
      "Processing batch 1801/11884 - Loss: 28.3359\n",
      "Processing batch 1802/11884 - Loss: 30.4004\n",
      "Processing batch 1803/11884 - Loss: 29.6682\n",
      "Processing batch 1804/11884 - Loss: 29.4247\n",
      "Processing batch 1805/11884 - Loss: 29.7021\n",
      "Processing batch 1806/11884 - Loss: 29.5785\n",
      "Processing batch 1807/11884 - Loss: 30.1350\n",
      "Processing batch 1808/11884 - Loss: 29.8262\n",
      "Processing batch 1809/11884 - Loss: 30.2534\n",
      "Processing batch 1810/11884 - Loss: 30.2720\n",
      "Processing batch 1811/11884 - Loss: 30.3997\n",
      "Processing batch 1812/11884 - Loss: 30.7640\n",
      "Processing batch 1813/11884 - Loss: 29.1544\n",
      "Processing batch 1814/11884 - Loss: 30.0044\n",
      "Processing batch 1815/11884 - Loss: 29.6188\n",
      "Processing batch 1816/11884 - Loss: 29.7259\n",
      "Processing batch 1817/11884 - Loss: 31.0160\n",
      "Processing batch 1818/11884 - Loss: 29.7528\n",
      "Processing batch 1819/11884 - Loss: 31.7402\n",
      "Processing batch 1820/11884 - Loss: 29.1796\n",
      "Processing batch 1821/11884 - Loss: 29.8478\n",
      "Processing batch 1822/11884 - Loss: 30.7770\n",
      "Processing batch 1823/11884 - Loss: 29.6865\n",
      "Processing batch 1824/11884 - Loss: 30.5575\n",
      "Processing batch 1825/11884 - Loss: 30.9987\n",
      "Processing batch 1826/11884 - Loss: 29.5625\n",
      "Processing batch 1827/11884 - Loss: 30.7190\n",
      "Processing batch 1828/11884 - Loss: 29.8208\n",
      "Processing batch 1829/11884 - Loss: 29.6485\n",
      "Processing batch 1830/11884 - Loss: 28.5526\n",
      "Processing batch 1831/11884 - Loss: 30.6695\n",
      "Processing batch 1832/11884 - Loss: 29.5807\n",
      "Processing batch 1833/11884 - Loss: 29.6459\n",
      "Processing batch 1834/11884 - Loss: 31.3063\n",
      "Processing batch 1835/11884 - Loss: 30.4060\n",
      "Processing batch 1836/11884 - Loss: 30.2329\n",
      "Processing batch 1837/11884 - Loss: 32.0629\n",
      "Processing batch 1838/11884 - Loss: 29.7875\n",
      "Processing batch 1839/11884 - Loss: 29.5433\n",
      "Processing batch 1840/11884 - Loss: 30.8678\n",
      "Processing batch 1841/11884 - Loss: 28.6566\n",
      "Processing batch 1842/11884 - Loss: 29.9329\n",
      "Processing batch 1843/11884 - Loss: 30.7346\n",
      "Processing batch 1844/11884 - Loss: 30.1456\n",
      "Processing batch 1845/11884 - Loss: 31.0540\n",
      "Processing batch 1846/11884 - Loss: 30.0997\n",
      "Processing batch 1847/11884 - Loss: 30.4032\n",
      "Processing batch 1848/11884 - Loss: 30.0101\n",
      "Processing batch 1849/11884 - Loss: 30.4730\n",
      "Processing batch 1850/11884 - Loss: 29.5021\n",
      "Processing batch 1851/11884 - Loss: 29.5358\n",
      "Processing batch 1852/11884 - Loss: 30.2783\n",
      "Processing batch 1853/11884 - Loss: 30.7874\n",
      "Processing batch 1854/11884 - Loss: 29.1301\n",
      "Processing batch 1855/11884 - Loss: 29.5800\n",
      "Processing batch 1856/11884 - Loss: 30.0879\n",
      "Processing batch 1857/11884 - Loss: 29.4286\n",
      "Processing batch 1858/11884 - Loss: 30.8879\n",
      "Processing batch 1859/11884 - Loss: 31.6519\n",
      "Processing batch 1860/11884 - Loss: 29.8949\n",
      "Processing batch 1861/11884 - Loss: 29.4957\n",
      "Processing batch 1862/11884 - Loss: 29.1456\n",
      "Processing batch 1863/11884 - Loss: 30.7373\n",
      "Processing batch 1864/11884 - Loss: 28.5606\n",
      "Processing batch 1865/11884 - Loss: 29.1689\n",
      "Processing batch 1866/11884 - Loss: 30.1178\n",
      "Processing batch 1867/11884 - Loss: 30.4773\n",
      "Processing batch 1868/11884 - Loss: 29.5858\n",
      "Processing batch 1869/11884 - Loss: 30.4274\n",
      "Processing batch 1870/11884 - Loss: 30.6583\n",
      "Processing batch 1871/11884 - Loss: 30.6249\n",
      "Processing batch 1872/11884 - Loss: 30.6277\n",
      "Processing batch 1873/11884 - Loss: 30.4364\n",
      "Processing batch 1874/11884 - Loss: 30.0013\n",
      "Processing batch 1875/11884 - Loss: 29.2475\n",
      "Processing batch 1876/11884 - Loss: 28.9285\n",
      "Processing batch 1877/11884 - Loss: 30.3449\n",
      "Processing batch 1878/11884 - Loss: 29.1709\n",
      "Processing batch 1879/11884 - Loss: 29.1183\n",
      "Processing batch 1880/11884 - Loss: 29.4616\n",
      "Processing batch 1881/11884 - Loss: 29.7052\n",
      "Processing batch 1882/11884 - Loss: 29.5233\n",
      "Processing batch 1883/11884 - Loss: 31.0502\n",
      "Processing batch 1884/11884 - Loss: 30.6703\n",
      "Processing batch 1885/11884 - Loss: 31.1690\n",
      "Processing batch 1886/11884 - Loss: 30.0278\n",
      "Processing batch 1887/11884 - Loss: 29.7496\n",
      "Processing batch 1888/11884 - Loss: 30.2837\n",
      "Processing batch 1889/11884 - Loss: 31.2592\n",
      "Processing batch 1890/11884 - Loss: 31.3871\n",
      "Processing batch 1891/11884 - Loss: 29.3824\n",
      "Processing batch 1892/11884 - Loss: 28.9817\n",
      "Processing batch 1893/11884 - Loss: 30.2883\n",
      "Processing batch 1894/11884 - Loss: 30.4170\n",
      "Processing batch 1895/11884 - Loss: 30.8669\n",
      "Processing batch 1896/11884 - Loss: 29.9109\n",
      "Processing batch 1897/11884 - Loss: 30.0091\n",
      "Processing batch 1898/11884 - Loss: 30.1109\n",
      "Processing batch 1899/11884 - Loss: 29.4594\n",
      "Processing batch 1900/11884 - Loss: 28.9653\n",
      "Processing batch 1901/11884 - Loss: 29.0542\n",
      "Processing batch 1902/11884 - Loss: 29.5461\n",
      "Processing batch 1903/11884 - Loss: 30.7942\n",
      "Processing batch 1904/11884 - Loss: 30.4561\n",
      "Processing batch 1905/11884 - Loss: 30.1501\n",
      "Processing batch 1906/11884 - Loss: 30.9383\n",
      "Processing batch 1907/11884 - Loss: 28.9367\n",
      "Processing batch 1908/11884 - Loss: 29.9859\n",
      "Processing batch 1909/11884 - Loss: 30.2487\n",
      "Processing batch 1910/11884 - Loss: 30.6544\n",
      "Processing batch 1911/11884 - Loss: 30.6029\n",
      "Processing batch 1912/11884 - Loss: 31.3872\n",
      "Processing batch 1913/11884 - Loss: 29.6214\n",
      "Processing batch 1914/11884 - Loss: 30.6341\n",
      "Processing batch 1915/11884 - Loss: 28.3633\n",
      "Processing batch 1916/11884 - Loss: 30.7487\n",
      "Processing batch 1917/11884 - Loss: 29.1865\n",
      "Processing batch 1918/11884 - Loss: 30.1111\n",
      "Processing batch 1919/11884 - Loss: 29.4642\n",
      "Processing batch 1920/11884 - Loss: 29.4775\n",
      "Processing batch 1921/11884 - Loss: 29.7270\n",
      "Processing batch 1922/11884 - Loss: 28.4360\n",
      "Processing batch 1923/11884 - Loss: 30.5185\n",
      "Processing batch 1924/11884 - Loss: 28.8905\n",
      "Processing batch 1925/11884 - Loss: 29.4498\n",
      "Processing batch 1926/11884 - Loss: 29.1853\n",
      "Processing batch 1927/11884 - Loss: 28.5696\n",
      "Processing batch 1928/11884 - Loss: 29.7159\n",
      "Processing batch 1929/11884 - Loss: 31.0487\n",
      "Processing batch 1930/11884 - Loss: 30.3930\n",
      "Processing batch 1931/11884 - Loss: 28.6342\n",
      "Processing batch 1932/11884 - Loss: 29.0872\n",
      "Processing batch 1933/11884 - Loss: 27.8611\n",
      "Processing batch 1934/11884 - Loss: 29.8140\n",
      "Processing batch 1935/11884 - Loss: 30.5359\n",
      "Processing batch 1936/11884 - Loss: 29.6092\n",
      "Processing batch 1937/11884 - Loss: 29.4309\n",
      "Processing batch 1938/11884 - Loss: 29.6361\n",
      "Processing batch 1939/11884 - Loss: 28.9747\n",
      "Processing batch 1940/11884 - Loss: 30.2768\n",
      "Processing batch 1941/11884 - Loss: 31.3414\n",
      "Processing batch 1942/11884 - Loss: 29.1036\n",
      "Processing batch 1943/11884 - Loss: 30.8796\n",
      "Processing batch 1944/11884 - Loss: 29.7098\n",
      "Processing batch 1945/11884 - Loss: 30.7057\n",
      "Processing batch 1946/11884 - Loss: 30.1074\n",
      "Processing batch 1947/11884 - Loss: 30.3345\n",
      "Processing batch 1948/11884 - Loss: 29.1684\n",
      "Processing batch 1949/11884 - Loss: 27.9937\n",
      "Processing batch 1950/11884 - Loss: 30.1299\n",
      "Processing batch 1951/11884 - Loss: 29.5150\n",
      "Processing batch 1952/11884 - Loss: 29.8536\n",
      "Processing batch 1953/11884 - Loss: 31.1823\n",
      "Processing batch 1954/11884 - Loss: 29.5412\n",
      "Processing batch 1955/11884 - Loss: 29.0191\n",
      "Processing batch 1956/11884 - Loss: 28.6639\n",
      "Processing batch 1957/11884 - Loss: 30.3494\n",
      "Processing batch 1958/11884 - Loss: 30.4492\n",
      "Processing batch 1959/11884 - Loss: 29.9019\n",
      "Processing batch 1960/11884 - Loss: 29.9289\n",
      "Processing batch 1961/11884 - Loss: 28.5221\n",
      "Processing batch 1962/11884 - Loss: 29.5253\n",
      "Processing batch 1963/11884 - Loss: 29.5352\n",
      "Processing batch 1964/11884 - Loss: 29.5855\n",
      "Processing batch 1965/11884 - Loss: 30.9698\n",
      "Processing batch 1966/11884 - Loss: 30.2102\n",
      "Processing batch 1967/11884 - Loss: 30.2772\n",
      "Processing batch 1968/11884 - Loss: 29.0238\n",
      "Processing batch 1969/11884 - Loss: 30.8684\n",
      "Processing batch 1970/11884 - Loss: 27.8789\n",
      "Processing batch 1971/11884 - Loss: 29.9179\n",
      "Processing batch 1972/11884 - Loss: 29.9457\n",
      "Processing batch 1973/11884 - Loss: 30.7253\n",
      "Processing batch 1974/11884 - Loss: 29.2489\n",
      "Processing batch 1975/11884 - Loss: 29.9532\n",
      "Processing batch 1976/11884 - Loss: 29.5943\n",
      "Processing batch 1977/11884 - Loss: 31.3205\n",
      "Processing batch 1978/11884 - Loss: 29.2853\n",
      "Processing batch 1979/11884 - Loss: 30.3006\n",
      "Processing batch 1980/11884 - Loss: 29.1640\n",
      "Processing batch 1981/11884 - Loss: 30.0005\n",
      "Processing batch 1982/11884 - Loss: 28.7095\n",
      "Processing batch 1983/11884 - Loss: 30.0705\n",
      "Processing batch 1984/11884 - Loss: 30.3425\n",
      "Processing batch 1985/11884 - Loss: 28.9990\n",
      "Processing batch 1986/11884 - Loss: 29.2129\n",
      "Processing batch 1987/11884 - Loss: 29.5343\n",
      "Processing batch 1988/11884 - Loss: 29.6515\n",
      "Processing batch 1989/11884 - Loss: 31.0354\n",
      "Processing batch 1990/11884 - Loss: 29.6602\n",
      "Processing batch 1991/11884 - Loss: 29.5997\n",
      "Processing batch 1992/11884 - Loss: 30.6786\n",
      "Processing batch 1993/11884 - Loss: 30.1908\n",
      "Processing batch 1994/11884 - Loss: 30.8249\n",
      "Processing batch 1995/11884 - Loss: 28.5519\n",
      "Processing batch 1996/11884 - Loss: 30.1786\n",
      "Processing batch 1997/11884 - Loss: 29.1594\n",
      "Processing batch 1998/11884 - Loss: 29.7019\n",
      "Processing batch 1999/11884 - Loss: 30.9106\n",
      "Processing batch 2000/11884 - Loss: 29.1423\n",
      "Processing batch 2001/11884 - Loss: 29.9705\n",
      "Processing batch 2002/11884 - Loss: 29.2860\n",
      "Processing batch 2003/11884 - Loss: 29.6021\n",
      "Processing batch 2004/11884 - Loss: 28.7731\n",
      "Processing batch 2005/11884 - Loss: 29.5134\n",
      "Processing batch 2006/11884 - Loss: 29.5818\n",
      "Processing batch 2007/11884 - Loss: 30.3202\n",
      "Processing batch 2008/11884 - Loss: 30.5261\n",
      "Processing batch 2009/11884 - Loss: 29.1478\n",
      "Processing batch 2010/11884 - Loss: 29.4963\n",
      "Processing batch 2011/11884 - Loss: 29.8069\n",
      "Processing batch 2012/11884 - Loss: 28.6135\n",
      "Processing batch 2013/11884 - Loss: 30.7144\n",
      "Processing batch 2014/11884 - Loss: 31.5061\n",
      "Processing batch 2015/11884 - Loss: 30.5884\n",
      "Processing batch 2016/11884 - Loss: 30.9492\n",
      "Processing batch 2017/11884 - Loss: 30.5046\n",
      "Processing batch 2018/11884 - Loss: 30.0828\n",
      "Processing batch 2019/11884 - Loss: 30.0726\n",
      "Processing batch 2020/11884 - Loss: 29.8296\n",
      "Processing batch 2021/11884 - Loss: 29.9915\n",
      "Processing batch 2022/11884 - Loss: 30.6655\n",
      "Processing batch 2023/11884 - Loss: 29.5863\n",
      "Processing batch 2024/11884 - Loss: 29.6470\n",
      "Processing batch 2025/11884 - Loss: 30.3919\n",
      "Processing batch 2026/11884 - Loss: 30.2396\n",
      "Processing batch 2027/11884 - Loss: 30.0028\n",
      "Processing batch 2028/11884 - Loss: 30.4339\n",
      "Processing batch 2029/11884 - Loss: 30.7122\n",
      "Processing batch 2030/11884 - Loss: 29.8188\n",
      "Processing batch 2031/11884 - Loss: 31.4877\n",
      "Processing batch 2032/11884 - Loss: 30.6480\n",
      "Processing batch 2033/11884 - Loss: 29.7690\n",
      "Processing batch 2034/11884 - Loss: 29.3101\n",
      "Processing batch 2035/11884 - Loss: 28.5678\n",
      "Processing batch 2036/11884 - Loss: 29.5921\n",
      "Processing batch 2037/11884 - Loss: 30.0738\n",
      "Processing batch 2038/11884 - Loss: 31.3522\n",
      "Processing batch 2039/11884 - Loss: 29.1874\n",
      "Processing batch 2040/11884 - Loss: 29.5912\n",
      "Processing batch 2041/11884 - Loss: 30.2056\n",
      "Processing batch 2042/11884 - Loss: 29.6031\n",
      "Processing batch 2043/11884 - Loss: 29.1947\n",
      "Processing batch 2044/11884 - Loss: 28.6276\n",
      "Processing batch 2045/11884 - Loss: 30.1856\n",
      "Processing batch 2046/11884 - Loss: 29.7854\n",
      "Processing batch 2047/11884 - Loss: 30.2330\n",
      "Processing batch 2048/11884 - Loss: 28.9171\n",
      "Processing batch 2049/11884 - Loss: 29.9815\n",
      "Processing batch 2050/11884 - Loss: 29.3110\n",
      "Processing batch 2051/11884 - Loss: 30.3004\n",
      "Processing batch 2052/11884 - Loss: 28.6539\n",
      "Processing batch 2053/11884 - Loss: 30.8367\n",
      "Processing batch 2054/11884 - Loss: 30.8379\n",
      "Processing batch 2055/11884 - Loss: 29.2745\n",
      "Processing batch 2056/11884 - Loss: 29.5772\n",
      "Processing batch 2057/11884 - Loss: 30.3209\n",
      "Processing batch 2058/11884 - Loss: 29.5699\n",
      "Processing batch 2059/11884 - Loss: 29.6293\n",
      "Processing batch 2060/11884 - Loss: 29.7860\n",
      "Processing batch 2061/11884 - Loss: 29.4596\n",
      "Processing batch 2062/11884 - Loss: 29.9735\n",
      "Processing batch 2063/11884 - Loss: 29.2619\n",
      "Processing batch 2064/11884 - Loss: 28.8010\n",
      "Processing batch 2065/11884 - Loss: 31.1755\n",
      "Processing batch 2066/11884 - Loss: 28.8907\n",
      "Processing batch 2067/11884 - Loss: 29.2961\n",
      "Processing batch 2068/11884 - Loss: 30.5097\n",
      "Processing batch 2069/11884 - Loss: 29.7616\n",
      "Processing batch 2070/11884 - Loss: 30.3399\n",
      "Processing batch 2071/11884 - Loss: 28.8914\n",
      "Processing batch 2072/11884 - Loss: 29.6404\n",
      "Processing batch 2073/11884 - Loss: 29.4958\n",
      "Processing batch 2074/11884 - Loss: 30.4969\n",
      "Processing batch 2075/11884 - Loss: 29.7688\n",
      "Processing batch 2076/11884 - Loss: 29.4263\n",
      "Processing batch 2077/11884 - Loss: 30.0938\n",
      "Processing batch 2078/11884 - Loss: 30.3453\n",
      "Processing batch 2079/11884 - Loss: 29.9655\n",
      "Processing batch 2080/11884 - Loss: 30.3778\n",
      "Processing batch 2081/11884 - Loss: 28.6015\n",
      "Processing batch 2082/11884 - Loss: 31.0371\n",
      "Processing batch 2083/11884 - Loss: 30.9225\n",
      "Processing batch 2084/11884 - Loss: 30.9087\n",
      "Processing batch 2085/11884 - Loss: 30.1165\n",
      "Processing batch 2086/11884 - Loss: 29.0764\n",
      "Processing batch 2087/11884 - Loss: 30.0913\n",
      "Processing batch 2088/11884 - Loss: 30.7700\n",
      "Processing batch 2089/11884 - Loss: 29.7829\n",
      "Processing batch 2090/11884 - Loss: 30.1300\n",
      "Processing batch 2091/11884 - Loss: 30.6526\n",
      "Processing batch 2092/11884 - Loss: 29.1983\n",
      "Processing batch 2093/11884 - Loss: 30.4045\n",
      "Processing batch 2094/11884 - Loss: 30.1267\n",
      "Processing batch 2095/11884 - Loss: 30.4778\n",
      "Processing batch 2096/11884 - Loss: 28.6081\n",
      "Processing batch 2097/11884 - Loss: 29.6136\n",
      "Processing batch 2098/11884 - Loss: 29.2661\n",
      "Processing batch 2099/11884 - Loss: 31.2738\n",
      "Processing batch 2100/11884 - Loss: 30.4676\n",
      "Processing batch 2101/11884 - Loss: 30.1721\n",
      "Processing batch 2102/11884 - Loss: 30.3367\n",
      "Processing batch 2103/11884 - Loss: 30.9123\n",
      "Processing batch 2104/11884 - Loss: 29.3275\n",
      "Processing batch 2105/11884 - Loss: 30.6634\n",
      "Processing batch 2106/11884 - Loss: 29.0589\n",
      "Processing batch 2107/11884 - Loss: 29.0887\n",
      "Processing batch 2108/11884 - Loss: 31.2614\n",
      "Processing batch 2109/11884 - Loss: 28.1917\n",
      "Processing batch 2110/11884 - Loss: 29.5225\n",
      "Processing batch 2111/11884 - Loss: 29.2294\n",
      "Processing batch 2112/11884 - Loss: 30.4601\n",
      "Processing batch 2113/11884 - Loss: 29.8191\n",
      "Processing batch 2114/11884 - Loss: 28.9176\n",
      "Processing batch 2115/11884 - Loss: 30.4335\n",
      "Processing batch 2116/11884 - Loss: 29.6691\n",
      "Processing batch 2117/11884 - Loss: 29.7413\n",
      "Processing batch 2118/11884 - Loss: 29.8252\n",
      "Processing batch 2119/11884 - Loss: 29.8638\n",
      "Processing batch 2120/11884 - Loss: 30.4640\n",
      "Processing batch 2121/11884 - Loss: 29.7394\n",
      "Processing batch 2122/11884 - Loss: 30.0499\n",
      "Processing batch 2123/11884 - Loss: 29.6256\n",
      "Processing batch 2124/11884 - Loss: 27.7151\n",
      "Processing batch 2125/11884 - Loss: 30.3007\n",
      "Processing batch 2126/11884 - Loss: 29.5488\n",
      "Processing batch 2127/11884 - Loss: 27.8305\n",
      "Processing batch 2128/11884 - Loss: 30.6609\n",
      "Processing batch 2129/11884 - Loss: 29.1518\n",
      "Processing batch 2130/11884 - Loss: 30.7704\n",
      "Processing batch 2131/11884 - Loss: 29.6626\n",
      "Processing batch 2132/11884 - Loss: 30.0447\n",
      "Processing batch 2133/11884 - Loss: 30.7861\n",
      "Processing batch 2134/11884 - Loss: 29.3071\n",
      "Processing batch 2135/11884 - Loss: 29.5554\n",
      "Processing batch 2136/11884 - Loss: 29.1176\n",
      "Processing batch 2137/11884 - Loss: 30.0452\n",
      "Processing batch 2138/11884 - Loss: 30.6222\n",
      "Processing batch 2139/11884 - Loss: 28.8827\n",
      "Processing batch 2140/11884 - Loss: 29.7320\n",
      "Processing batch 2141/11884 - Loss: 30.8270\n",
      "Processing batch 2142/11884 - Loss: 31.2045\n",
      "Processing batch 2143/11884 - Loss: 29.7148\n",
      "Processing batch 2144/11884 - Loss: 27.7004\n",
      "Processing batch 2145/11884 - Loss: 29.9657\n",
      "Processing batch 2146/11884 - Loss: 29.1322\n",
      "Processing batch 2147/11884 - Loss: 29.8694\n",
      "Processing batch 2148/11884 - Loss: 28.5824\n",
      "Processing batch 2149/11884 - Loss: 29.0140\n",
      "Processing batch 2150/11884 - Loss: 29.6683\n",
      "Processing batch 2151/11884 - Loss: 29.6426\n",
      "Processing batch 2152/11884 - Loss: 29.2201\n",
      "Processing batch 2153/11884 - Loss: 28.2116\n",
      "Processing batch 2154/11884 - Loss: 30.7515\n",
      "Processing batch 2155/11884 - Loss: 29.5775\n",
      "Processing batch 2156/11884 - Loss: 31.2247\n",
      "Processing batch 2157/11884 - Loss: 30.8463\n",
      "Processing batch 2158/11884 - Loss: 29.3874\n",
      "Processing batch 2159/11884 - Loss: 30.6448\n",
      "Processing batch 2160/11884 - Loss: 29.6819\n",
      "Processing batch 2161/11884 - Loss: 31.1429\n",
      "Processing batch 2162/11884 - Loss: 29.2846\n",
      "Processing batch 2163/11884 - Loss: 29.6276\n",
      "Processing batch 2164/11884 - Loss: 29.7172\n",
      "Processing batch 2165/11884 - Loss: 29.7794\n",
      "Processing batch 2166/11884 - Loss: 30.2861\n",
      "Processing batch 2167/11884 - Loss: 29.8176\n",
      "Processing batch 2168/11884 - Loss: 30.5422\n",
      "Processing batch 2169/11884 - Loss: 30.5694\n",
      "Processing batch 2170/11884 - Loss: 29.0261\n",
      "Processing batch 2171/11884 - Loss: 29.4343\n",
      "Processing batch 2172/11884 - Loss: 29.3054\n",
      "Processing batch 2173/11884 - Loss: 31.0521\n",
      "Processing batch 2174/11884 - Loss: 30.7409\n",
      "Processing batch 2175/11884 - Loss: 30.4885\n",
      "Processing batch 2176/11884 - Loss: 30.2286\n",
      "Processing batch 2177/11884 - Loss: 29.6743\n",
      "Processing batch 2178/11884 - Loss: 29.7229\n",
      "Processing batch 2179/11884 - Loss: 28.8848\n",
      "Processing batch 2180/11884 - Loss: 30.5608\n",
      "Processing batch 2181/11884 - Loss: 30.9292\n",
      "Processing batch 2182/11884 - Loss: 29.2743\n",
      "Processing batch 2183/11884 - Loss: 29.2260\n",
      "Processing batch 2184/11884 - Loss: 30.3091\n",
      "Processing batch 2185/11884 - Loss: 30.3904\n",
      "Processing batch 2186/11884 - Loss: 29.7532\n",
      "Processing batch 2187/11884 - Loss: 29.4596\n",
      "Processing batch 2188/11884 - Loss: 29.8208\n",
      "Processing batch 2189/11884 - Loss: 29.3164\n",
      "Processing batch 2190/11884 - Loss: 30.2974\n",
      "Processing batch 2191/11884 - Loss: 30.0954\n",
      "Processing batch 2192/11884 - Loss: 28.3885\n",
      "Processing batch 2193/11884 - Loss: 29.7471\n",
      "Processing batch 2194/11884 - Loss: 30.8844\n",
      "Processing batch 2195/11884 - Loss: 30.1303\n",
      "Processing batch 2196/11884 - Loss: 31.2190\n",
      "Processing batch 2197/11884 - Loss: 30.2886\n",
      "Processing batch 2198/11884 - Loss: 30.2974\n",
      "Processing batch 2199/11884 - Loss: 29.6996\n",
      "Processing batch 2200/11884 - Loss: 31.4474\n",
      "Processing batch 2201/11884 - Loss: 31.1062\n",
      "Processing batch 2202/11884 - Loss: 29.0855\n",
      "Processing batch 2203/11884 - Loss: 29.8782\n",
      "Processing batch 2204/11884 - Loss: 29.7336\n",
      "Processing batch 2205/11884 - Loss: 29.2620\n",
      "Processing batch 2206/11884 - Loss: 29.2051\n",
      "Processing batch 2207/11884 - Loss: 30.0818\n",
      "Processing batch 2208/11884 - Loss: 29.7200\n",
      "Processing batch 2209/11884 - Loss: 27.9436\n",
      "Processing batch 2210/11884 - Loss: 30.2337\n",
      "Processing batch 2211/11884 - Loss: 30.1743\n",
      "Processing batch 2212/11884 - Loss: 31.0377\n",
      "Processing batch 2213/11884 - Loss: 28.9930\n",
      "Processing batch 2214/11884 - Loss: 29.9440\n",
      "Processing batch 2215/11884 - Loss: 29.1659\n",
      "Processing batch 2216/11884 - Loss: 30.7054\n",
      "Processing batch 2217/11884 - Loss: 30.3383\n",
      "Processing batch 2218/11884 - Loss: 29.1017\n",
      "Processing batch 2219/11884 - Loss: 29.4101\n",
      "Processing batch 2220/11884 - Loss: 29.4708\n",
      "Processing batch 2221/11884 - Loss: 29.1957\n",
      "Processing batch 2222/11884 - Loss: 29.5688\n",
      "Processing batch 2223/11884 - Loss: 30.3542\n",
      "Processing batch 2224/11884 - Loss: 28.8253\n",
      "Processing batch 2225/11884 - Loss: 30.1624\n",
      "Processing batch 2226/11884 - Loss: 30.6829\n",
      "Processing batch 2227/11884 - Loss: 31.3855\n",
      "Processing batch 2228/11884 - Loss: 30.0955\n",
      "Processing batch 2229/11884 - Loss: 30.7718\n",
      "Processing batch 2230/11884 - Loss: 29.7629\n",
      "Processing batch 2231/11884 - Loss: 29.0467\n",
      "Processing batch 2232/11884 - Loss: 30.3911\n",
      "Processing batch 2233/11884 - Loss: 29.4800\n",
      "Processing batch 2234/11884 - Loss: 29.8771\n",
      "Processing batch 2235/11884 - Loss: 30.0201\n",
      "Processing batch 2236/11884 - Loss: 29.2674\n",
      "Processing batch 2237/11884 - Loss: 29.3339\n",
      "Processing batch 2238/11884 - Loss: 30.3401\n",
      "Processing batch 2239/11884 - Loss: 30.4278\n",
      "Processing batch 2240/11884 - Loss: 31.1113\n",
      "Processing batch 2241/11884 - Loss: 30.0264\n",
      "Processing batch 2242/11884 - Loss: 30.4987\n",
      "Processing batch 2243/11884 - Loss: 31.0894\n",
      "Processing batch 2244/11884 - Loss: 30.3670\n",
      "Processing batch 2245/11884 - Loss: 29.3722\n",
      "Processing batch 2246/11884 - Loss: 28.0040\n",
      "Processing batch 2247/11884 - Loss: 29.7540\n",
      "Processing batch 2248/11884 - Loss: 29.2561\n",
      "Processing batch 2249/11884 - Loss: 29.7603\n",
      "Processing batch 2250/11884 - Loss: 29.3062\n",
      "Processing batch 2251/11884 - Loss: 30.4083\n",
      "Processing batch 2252/11884 - Loss: 29.0561\n",
      "Processing batch 2253/11884 - Loss: 29.6655\n",
      "Processing batch 2254/11884 - Loss: 28.7727\n",
      "Processing batch 2255/11884 - Loss: 28.6001\n",
      "Processing batch 2256/11884 - Loss: 28.8028\n",
      "Processing batch 2257/11884 - Loss: 29.0884\n",
      "Processing batch 2258/11884 - Loss: 29.8067\n",
      "Processing batch 2259/11884 - Loss: 30.4093\n",
      "Processing batch 2260/11884 - Loss: 30.3885\n",
      "Processing batch 2261/11884 - Loss: 29.0358\n",
      "Processing batch 2262/11884 - Loss: 30.4130\n",
      "Processing batch 2263/11884 - Loss: 31.4294\n",
      "Processing batch 2264/11884 - Loss: 30.4273\n",
      "Processing batch 2265/11884 - Loss: 29.1584\n",
      "Processing batch 2266/11884 - Loss: 29.6485\n",
      "Processing batch 2267/11884 - Loss: 29.7975\n",
      "Processing batch 2268/11884 - Loss: 29.4991\n",
      "Processing batch 2269/11884 - Loss: 29.8608\n",
      "Processing batch 2270/11884 - Loss: 29.4934\n",
      "Processing batch 2271/11884 - Loss: 28.7401\n",
      "Processing batch 2272/11884 - Loss: 30.3646\n",
      "Processing batch 2273/11884 - Loss: 29.3124\n",
      "Processing batch 2274/11884 - Loss: 30.4081\n",
      "Processing batch 2275/11884 - Loss: 28.7825\n",
      "Processing batch 2276/11884 - Loss: 29.8026\n",
      "Processing batch 2277/11884 - Loss: 29.0707\n",
      "Processing batch 2278/11884 - Loss: 29.7120\n",
      "Processing batch 2279/11884 - Loss: 29.8176\n",
      "Processing batch 2280/11884 - Loss: 28.4769\n",
      "Processing batch 2281/11884 - Loss: 29.3987\n",
      "Processing batch 2282/11884 - Loss: 28.8816\n",
      "Processing batch 2283/11884 - Loss: 29.5226\n",
      "Processing batch 2284/11884 - Loss: 29.6349\n",
      "Processing batch 2285/11884 - Loss: 29.9059\n",
      "Processing batch 2286/11884 - Loss: 29.9853\n",
      "Processing batch 2287/11884 - Loss: 30.7260\n",
      "Processing batch 2288/11884 - Loss: 30.1952\n",
      "Processing batch 2289/11884 - Loss: 29.2184\n",
      "Processing batch 2290/11884 - Loss: 29.0899\n",
      "Processing batch 2291/11884 - Loss: 30.2463\n",
      "Processing batch 2292/11884 - Loss: 30.1485\n",
      "Processing batch 2293/11884 - Loss: 30.0598\n",
      "Processing batch 2294/11884 - Loss: 29.3113\n",
      "Processing batch 2295/11884 - Loss: 29.9233\n",
      "Processing batch 2296/11884 - Loss: 28.9374\n",
      "Processing batch 2297/11884 - Loss: 30.7962\n",
      "Processing batch 2298/11884 - Loss: 29.3408\n",
      "Processing batch 2299/11884 - Loss: 29.8750\n",
      "Processing batch 2300/11884 - Loss: 30.8869\n",
      "Processing batch 2301/11884 - Loss: 28.6865\n",
      "Processing batch 2302/11884 - Loss: 29.9521\n",
      "Processing batch 2303/11884 - Loss: 30.0168\n",
      "Processing batch 2304/11884 - Loss: 31.0365\n",
      "Processing batch 2305/11884 - Loss: 29.9497\n",
      "Processing batch 2306/11884 - Loss: 31.1001\n",
      "Processing batch 2307/11884 - Loss: 30.3210\n",
      "Processing batch 2308/11884 - Loss: 29.4245\n",
      "Processing batch 2309/11884 - Loss: 30.5083\n",
      "Processing batch 2310/11884 - Loss: 28.4435\n",
      "Processing batch 2311/11884 - Loss: 30.2219\n",
      "Processing batch 2312/11884 - Loss: 29.8158\n",
      "Processing batch 2313/11884 - Loss: 30.6243\n",
      "Processing batch 2314/11884 - Loss: 30.7969\n",
      "Processing batch 2315/11884 - Loss: 30.0394\n",
      "Processing batch 2316/11884 - Loss: 31.2011\n",
      "Processing batch 2317/11884 - Loss: 29.1941\n",
      "Processing batch 2318/11884 - Loss: 29.7495\n",
      "Processing batch 2319/11884 - Loss: 30.1607\n",
      "Processing batch 2320/11884 - Loss: 30.9735\n",
      "Processing batch 2321/11884 - Loss: 29.6723\n",
      "Processing batch 2322/11884 - Loss: 31.3281\n",
      "Processing batch 2323/11884 - Loss: 28.4587\n",
      "Processing batch 2324/11884 - Loss: 29.6333\n",
      "Processing batch 2325/11884 - Loss: 29.3083\n",
      "Processing batch 2326/11884 - Loss: 28.6689\n",
      "Processing batch 2327/11884 - Loss: 29.8138\n",
      "Processing batch 2328/11884 - Loss: 27.7746\n",
      "Processing batch 2329/11884 - Loss: 29.2949\n",
      "Processing batch 2330/11884 - Loss: 29.2534\n",
      "Processing batch 2331/11884 - Loss: 29.3213\n",
      "Processing batch 2332/11884 - Loss: 30.0071\n",
      "Processing batch 2333/11884 - Loss: 30.5936\n",
      "Processing batch 2334/11884 - Loss: 28.6533\n",
      "Processing batch 2335/11884 - Loss: 29.2446\n",
      "Processing batch 2336/11884 - Loss: 30.6178\n",
      "Processing batch 2337/11884 - Loss: 29.3568\n",
      "Processing batch 2338/11884 - Loss: 29.7050\n",
      "Processing batch 2339/11884 - Loss: 29.8176\n",
      "Processing batch 2340/11884 - Loss: 29.7975\n",
      "Processing batch 2341/11884 - Loss: 29.6657\n",
      "Processing batch 2342/11884 - Loss: 30.0833\n",
      "Processing batch 2343/11884 - Loss: 30.2792\n",
      "Processing batch 2344/11884 - Loss: 28.4160\n",
      "Processing batch 2345/11884 - Loss: 31.2445\n",
      "Processing batch 2346/11884 - Loss: 29.5025\n",
      "Processing batch 2347/11884 - Loss: 29.5883\n",
      "Processing batch 2348/11884 - Loss: 30.5124\n",
      "Processing batch 2349/11884 - Loss: 30.2144\n",
      "Processing batch 2350/11884 - Loss: 31.0401\n",
      "Processing batch 2351/11884 - Loss: 30.0880\n",
      "Processing batch 2352/11884 - Loss: 30.1890\n",
      "Processing batch 2353/11884 - Loss: 29.3992\n",
      "Processing batch 2354/11884 - Loss: 30.8182\n",
      "Processing batch 2355/11884 - Loss: 29.4857\n",
      "Processing batch 2356/11884 - Loss: 30.4957\n",
      "Processing batch 2357/11884 - Loss: 30.0716\n",
      "Processing batch 2358/11884 - Loss: 29.6083\n",
      "Processing batch 2359/11884 - Loss: 30.9497\n",
      "Processing batch 2360/11884 - Loss: 28.3255\n",
      "Processing batch 2361/11884 - Loss: 29.6571\n",
      "Processing batch 2362/11884 - Loss: 29.1926\n",
      "Processing batch 2363/11884 - Loss: 29.2769\n",
      "Processing batch 2364/11884 - Loss: 29.7716\n",
      "Processing batch 2365/11884 - Loss: 30.1744\n",
      "Processing batch 2366/11884 - Loss: 29.6104\n",
      "Processing batch 2367/11884 - Loss: 29.6127\n",
      "Processing batch 2368/11884 - Loss: 30.9568\n",
      "Processing batch 2369/11884 - Loss: 29.2657\n",
      "Processing batch 2370/11884 - Loss: 29.4529\n",
      "Processing batch 2371/11884 - Loss: 29.6850\n",
      "Processing batch 2372/11884 - Loss: 30.3480\n",
      "Processing batch 2373/11884 - Loss: 29.3620\n",
      "Processing batch 2374/11884 - Loss: 29.0747\n",
      "Processing batch 2375/11884 - Loss: 30.6999\n",
      "Processing batch 2376/11884 - Loss: 29.6200\n",
      "Processing batch 2377/11884 - Loss: 29.2176\n",
      "Processing batch 2378/11884 - Loss: 29.8933\n",
      "Processing batch 2379/11884 - Loss: 29.1744\n",
      "Processing batch 2380/11884 - Loss: 27.7867\n",
      "Processing batch 2381/11884 - Loss: 30.0437\n",
      "Processing batch 2382/11884 - Loss: 28.8762\n",
      "Processing batch 2383/11884 - Loss: 28.5913\n",
      "Processing batch 2384/11884 - Loss: 30.9071\n",
      "Processing batch 2385/11884 - Loss: 29.5776\n",
      "Processing batch 2386/11884 - Loss: 29.5581\n",
      "Processing batch 2387/11884 - Loss: 30.0154\n",
      "Processing batch 2388/11884 - Loss: 29.3854\n",
      "Processing batch 2389/11884 - Loss: 28.2938\n",
      "Processing batch 2390/11884 - Loss: 29.9585\n",
      "Processing batch 2391/11884 - Loss: 29.4144\n",
      "Processing batch 2392/11884 - Loss: 30.0445\n",
      "Processing batch 2393/11884 - Loss: 29.4961\n",
      "Processing batch 2394/11884 - Loss: 30.8519\n",
      "Processing batch 2395/11884 - Loss: 30.9419\n",
      "Processing batch 2396/11884 - Loss: 31.4419\n",
      "Processing batch 2397/11884 - Loss: 30.0953\n",
      "Processing batch 2398/11884 - Loss: 29.4986\n",
      "Processing batch 2399/11884 - Loss: 30.2865\n",
      "Processing batch 2400/11884 - Loss: 27.6712\n",
      "Processing batch 2401/11884 - Loss: 30.2740\n",
      "Processing batch 2402/11884 - Loss: 30.4407\n",
      "Processing batch 2403/11884 - Loss: 28.9887\n",
      "Processing batch 2404/11884 - Loss: 29.4594\n",
      "Processing batch 2405/11884 - Loss: 28.7302\n",
      "Processing batch 2406/11884 - Loss: 29.4201\n",
      "Processing batch 2407/11884 - Loss: 29.1595\n",
      "Processing batch 2408/11884 - Loss: 29.4975\n",
      "Processing batch 2409/11884 - Loss: 30.4105\n",
      "Processing batch 2410/11884 - Loss: 31.0433\n",
      "Processing batch 2411/11884 - Loss: 29.4447\n",
      "Processing batch 2412/11884 - Loss: 29.6973\n",
      "Processing batch 2413/11884 - Loss: 30.3214\n",
      "Processing batch 2414/11884 - Loss: 30.1886\n",
      "Processing batch 2415/11884 - Loss: 28.9886\n",
      "Processing batch 2416/11884 - Loss: 28.4072\n",
      "Processing batch 2417/11884 - Loss: 29.9561\n",
      "Processing batch 2418/11884 - Loss: 29.2803\n",
      "Processing batch 2419/11884 - Loss: 29.4458\n",
      "Processing batch 2420/11884 - Loss: 29.7439\n",
      "Processing batch 2421/11884 - Loss: 30.5898\n",
      "Processing batch 2422/11884 - Loss: 29.9240\n",
      "Processing batch 2423/11884 - Loss: 30.6005\n",
      "Processing batch 2424/11884 - Loss: 29.3842\n",
      "Processing batch 2425/11884 - Loss: 29.8197\n",
      "Processing batch 2426/11884 - Loss: 30.6722\n",
      "Processing batch 2427/11884 - Loss: 29.2450\n",
      "Processing batch 2428/11884 - Loss: 29.5436\n",
      "Processing batch 2429/11884 - Loss: 30.2029\n",
      "Processing batch 2430/11884 - Loss: 29.0964\n",
      "Processing batch 2431/11884 - Loss: 29.4565\n",
      "Processing batch 2432/11884 - Loss: 30.1502\n",
      "Processing batch 2433/11884 - Loss: 30.6178\n",
      "Processing batch 2434/11884 - Loss: 31.6586\n",
      "Processing batch 2435/11884 - Loss: 29.3076\n",
      "Processing batch 2436/11884 - Loss: 28.5476\n",
      "Processing batch 2437/11884 - Loss: 31.4736\n",
      "Processing batch 2438/11884 - Loss: 30.5320\n",
      "Processing batch 2439/11884 - Loss: 30.3354\n",
      "Processing batch 2440/11884 - Loss: 30.4944\n",
      "Processing batch 2441/11884 - Loss: 30.1496\n",
      "Processing batch 2442/11884 - Loss: 30.1946\n",
      "Processing batch 2443/11884 - Loss: 31.2908\n",
      "Processing batch 2444/11884 - Loss: 29.8403\n",
      "Processing batch 2445/11884 - Loss: 30.2320\n",
      "Processing batch 2446/11884 - Loss: 29.4738\n",
      "Processing batch 2447/11884 - Loss: 30.5030\n",
      "Processing batch 2448/11884 - Loss: 31.4122\n",
      "Processing batch 2449/11884 - Loss: 28.9066\n",
      "Processing batch 2450/11884 - Loss: 30.2900\n",
      "Processing batch 2451/11884 - Loss: 28.7274\n",
      "Processing batch 2452/11884 - Loss: 29.7044\n",
      "Processing batch 2453/11884 - Loss: 29.8894\n",
      "Processing batch 2454/11884 - Loss: 28.9098\n",
      "Processing batch 2455/11884 - Loss: 29.7424\n",
      "Processing batch 2456/11884 - Loss: 29.3455\n",
      "Processing batch 2457/11884 - Loss: 29.8254\n",
      "Processing batch 2458/11884 - Loss: 29.9516\n",
      "Processing batch 2459/11884 - Loss: 31.6667\n",
      "Processing batch 2460/11884 - Loss: 30.0127\n",
      "Processing batch 2461/11884 - Loss: 30.0659\n",
      "Processing batch 2462/11884 - Loss: 29.6850\n",
      "Processing batch 2463/11884 - Loss: 30.1006\n",
      "Processing batch 2464/11884 - Loss: 29.8695\n",
      "Processing batch 2465/11884 - Loss: 30.9475\n",
      "Processing batch 2466/11884 - Loss: 29.6255\n",
      "Processing batch 2467/11884 - Loss: 29.8984\n",
      "Processing batch 2468/11884 - Loss: 28.4927\n",
      "Processing batch 2469/11884 - Loss: 30.4622\n",
      "Processing batch 2470/11884 - Loss: 29.7020\n",
      "Processing batch 2471/11884 - Loss: 30.0532\n",
      "Processing batch 2472/11884 - Loss: 29.8205\n",
      "Processing batch 2473/11884 - Loss: 29.9045\n",
      "Processing batch 2474/11884 - Loss: 29.9318\n",
      "Processing batch 2475/11884 - Loss: 28.7819\n",
      "Processing batch 2476/11884 - Loss: 29.2675\n",
      "Processing batch 2477/11884 - Loss: 29.6578\n",
      "Processing batch 2478/11884 - Loss: 29.4594\n",
      "Processing batch 2479/11884 - Loss: 30.1563\n",
      "Processing batch 2480/11884 - Loss: 29.7165\n",
      "Processing batch 2481/11884 - Loss: 29.1450\n",
      "Processing batch 2482/11884 - Loss: 29.1898\n",
      "Processing batch 2483/11884 - Loss: 30.6422\n",
      "Processing batch 2484/11884 - Loss: 29.5414\n",
      "Processing batch 2485/11884 - Loss: 28.8983\n",
      "Processing batch 2486/11884 - Loss: 29.7929\n",
      "Processing batch 2487/11884 - Loss: 30.3158\n",
      "Processing batch 2488/11884 - Loss: 30.0587\n",
      "Processing batch 2489/11884 - Loss: 30.5987\n",
      "Processing batch 2490/11884 - Loss: 30.2366\n",
      "Processing batch 2491/11884 - Loss: 30.0149\n",
      "Processing batch 2492/11884 - Loss: 30.3389\n",
      "Processing batch 2493/11884 - Loss: 30.7590\n",
      "Processing batch 2494/11884 - Loss: 28.9471\n",
      "Processing batch 2495/11884 - Loss: 29.9576\n",
      "Processing batch 2496/11884 - Loss: 30.8457\n",
      "Processing batch 2497/11884 - Loss: 30.3667\n",
      "Processing batch 2498/11884 - Loss: 28.8487\n",
      "Processing batch 2499/11884 - Loss: 28.7694\n",
      "Processing batch 2500/11884 - Loss: 29.7284\n",
      "Processing batch 2501/11884 - Loss: 29.7309\n",
      "Processing batch 2502/11884 - Loss: 30.7292\n",
      "Processing batch 2503/11884 - Loss: 29.3509\n",
      "Processing batch 2504/11884 - Loss: 29.8908\n",
      "Processing batch 2505/11884 - Loss: 28.4236\n",
      "Processing batch 2506/11884 - Loss: 30.2699\n",
      "Processing batch 2507/11884 - Loss: 29.6548\n",
      "Processing batch 2508/11884 - Loss: 29.1817\n",
      "Processing batch 2509/11884 - Loss: 29.1951\n",
      "Processing batch 2510/11884 - Loss: 29.8400\n",
      "Processing batch 2511/11884 - Loss: 30.2645\n",
      "Processing batch 2512/11884 - Loss: 30.2147\n",
      "Processing batch 2513/11884 - Loss: 30.4701\n",
      "Processing batch 2514/11884 - Loss: 29.3038\n",
      "Processing batch 2515/11884 - Loss: 29.6630\n",
      "Processing batch 2516/11884 - Loss: 29.9873\n",
      "Processing batch 2517/11884 - Loss: 29.3866\n",
      "Processing batch 2518/11884 - Loss: 29.9346\n",
      "Processing batch 2519/11884 - Loss: 30.3185\n",
      "Processing batch 2520/11884 - Loss: 30.0430\n",
      "Processing batch 2521/11884 - Loss: 29.1037\n",
      "Processing batch 2522/11884 - Loss: 29.9185\n",
      "Processing batch 2523/11884 - Loss: 29.2514\n",
      "Processing batch 2524/11884 - Loss: 30.4180\n",
      "Processing batch 2525/11884 - Loss: 29.1054\n",
      "Processing batch 2526/11884 - Loss: 31.7249\n",
      "Processing batch 2527/11884 - Loss: 28.3296\n",
      "Processing batch 2528/11884 - Loss: 30.0280\n",
      "Processing batch 2529/11884 - Loss: 29.1220\n",
      "Processing batch 2530/11884 - Loss: 30.7363\n",
      "Processing batch 2531/11884 - Loss: 29.8735\n",
      "Processing batch 2532/11884 - Loss: 28.7870\n",
      "Processing batch 2533/11884 - Loss: 29.5083\n",
      "Processing batch 2534/11884 - Loss: 30.5256\n",
      "Processing batch 2535/11884 - Loss: 30.7562\n",
      "Processing batch 2536/11884 - Loss: 27.9351\n",
      "Processing batch 2537/11884 - Loss: 30.4425\n",
      "Processing batch 2538/11884 - Loss: 28.4487\n",
      "Processing batch 2539/11884 - Loss: 29.7679\n",
      "Processing batch 2540/11884 - Loss: 31.1476\n",
      "Processing batch 2541/11884 - Loss: 30.8150\n",
      "Processing batch 2542/11884 - Loss: 30.0880\n",
      "Processing batch 2543/11884 - Loss: 31.2107\n",
      "Processing batch 2544/11884 - Loss: 29.3402\n",
      "Processing batch 2545/11884 - Loss: 29.5411\n",
      "Processing batch 2546/11884 - Loss: 30.0280\n",
      "Processing batch 2547/11884 - Loss: 29.6904\n",
      "Processing batch 2548/11884 - Loss: 29.1837\n",
      "Processing batch 2549/11884 - Loss: 29.0323\n",
      "Processing batch 2550/11884 - Loss: 29.9770\n",
      "Processing batch 2551/11884 - Loss: 28.8379\n",
      "Processing batch 2552/11884 - Loss: 31.2499\n",
      "Processing batch 2553/11884 - Loss: 30.6761\n",
      "Processing batch 2554/11884 - Loss: 28.7890\n",
      "Processing batch 2555/11884 - Loss: 29.6372\n",
      "Processing batch 2556/11884 - Loss: 31.1252\n",
      "Processing batch 2557/11884 - Loss: 28.9697\n",
      "Processing batch 2558/11884 - Loss: 30.1552\n",
      "Processing batch 2559/11884 - Loss: 29.3947\n",
      "Processing batch 2560/11884 - Loss: 30.4100\n",
      "Processing batch 2561/11884 - Loss: 30.9255\n",
      "Processing batch 2562/11884 - Loss: 30.2876\n",
      "Processing batch 2563/11884 - Loss: 29.0545\n",
      "Processing batch 2564/11884 - Loss: 29.9552\n",
      "Processing batch 2565/11884 - Loss: 28.9293\n",
      "Processing batch 2566/11884 - Loss: 29.3437\n",
      "Processing batch 2567/11884 - Loss: 29.7772\n",
      "Processing batch 2568/11884 - Loss: 29.9630\n",
      "Processing batch 2569/11884 - Loss: 29.7520\n",
      "Processing batch 2570/11884 - Loss: 31.8161\n",
      "Processing batch 2571/11884 - Loss: 29.9763\n",
      "Processing batch 2572/11884 - Loss: 30.1174\n",
      "Processing batch 2573/11884 - Loss: 30.0409\n",
      "Processing batch 2574/11884 - Loss: 29.8912\n",
      "Processing batch 2575/11884 - Loss: 30.6522\n",
      "Processing batch 2576/11884 - Loss: 29.4536\n",
      "Processing batch 2577/11884 - Loss: 29.3424\n",
      "Processing batch 2578/11884 - Loss: 30.1163\n",
      "Processing batch 2579/11884 - Loss: 28.4023\n",
      "Processing batch 2580/11884 - Loss: 29.6552\n",
      "Processing batch 2581/11884 - Loss: 28.9616\n",
      "Processing batch 2582/11884 - Loss: 29.9801\n",
      "Processing batch 2583/11884 - Loss: 29.1201\n",
      "Processing batch 2584/11884 - Loss: 28.9755\n",
      "Processing batch 2585/11884 - Loss: 31.0109\n",
      "Processing batch 2586/11884 - Loss: 29.3755\n",
      "Processing batch 2587/11884 - Loss: 30.5211\n",
      "Processing batch 2588/11884 - Loss: 29.8778\n",
      "Processing batch 2589/11884 - Loss: 29.0773\n",
      "Processing batch 2590/11884 - Loss: 30.3936\n",
      "Processing batch 2591/11884 - Loss: 31.2242\n",
      "Processing batch 2592/11884 - Loss: 29.4712\n",
      "Processing batch 2593/11884 - Loss: 29.3244\n",
      "Processing batch 2594/11884 - Loss: 29.1246\n",
      "Processing batch 2595/11884 - Loss: 29.6259\n",
      "Processing batch 2596/11884 - Loss: 30.8996\n",
      "Processing batch 2597/11884 - Loss: 29.1219\n",
      "Processing batch 2598/11884 - Loss: 30.0560\n",
      "Processing batch 2599/11884 - Loss: 28.8396\n",
      "Processing batch 2600/11884 - Loss: 30.3421\n",
      "Processing batch 2601/11884 - Loss: 29.8893\n",
      "Processing batch 2602/11884 - Loss: 29.4962\n",
      "Processing batch 2603/11884 - Loss: 30.5061\n",
      "Processing batch 2604/11884 - Loss: 29.9582\n",
      "Processing batch 2605/11884 - Loss: 30.0773\n",
      "Processing batch 2606/11884 - Loss: 30.1497\n",
      "Processing batch 2607/11884 - Loss: 29.5461\n",
      "Processing batch 2608/11884 - Loss: 30.5078\n",
      "Processing batch 2609/11884 - Loss: 31.0630\n",
      "Processing batch 2610/11884 - Loss: 28.5004\n",
      "Processing batch 2611/11884 - Loss: 30.1601\n",
      "Processing batch 2612/11884 - Loss: 30.6033\n",
      "Processing batch 2613/11884 - Loss: 30.7115\n",
      "Processing batch 2614/11884 - Loss: 31.2957\n",
      "Processing batch 2615/11884 - Loss: 29.7752\n",
      "Processing batch 2616/11884 - Loss: 30.7286\n",
      "Processing batch 2617/11884 - Loss: 29.9640\n",
      "Processing batch 2618/11884 - Loss: 27.7021\n",
      "Processing batch 2619/11884 - Loss: 29.2428\n",
      "Processing batch 2620/11884 - Loss: 30.8024\n",
      "Processing batch 2621/11884 - Loss: 29.6974\n",
      "Processing batch 2622/11884 - Loss: 28.0372\n",
      "Processing batch 2623/11884 - Loss: 28.3428\n",
      "Processing batch 2624/11884 - Loss: 29.4597\n",
      "Processing batch 2625/11884 - Loss: 29.7485\n",
      "Processing batch 2626/11884 - Loss: 30.4336\n",
      "Processing batch 2627/11884 - Loss: 31.0582\n",
      "Processing batch 2628/11884 - Loss: 30.5662\n",
      "Processing batch 2629/11884 - Loss: 30.6162\n",
      "Processing batch 2630/11884 - Loss: 29.6432\n",
      "Processing batch 2631/11884 - Loss: 30.2194\n",
      "Processing batch 2632/11884 - Loss: 29.0881\n",
      "Processing batch 2633/11884 - Loss: 29.1372\n",
      "Processing batch 2634/11884 - Loss: 28.9528\n",
      "Processing batch 2635/11884 - Loss: 29.8876\n",
      "Processing batch 2636/11884 - Loss: 29.9981\n",
      "Processing batch 2637/11884 - Loss: 29.3948\n",
      "Processing batch 2638/11884 - Loss: 28.9556\n",
      "Processing batch 2639/11884 - Loss: 30.2631\n",
      "Processing batch 2640/11884 - Loss: 29.2346\n",
      "Processing batch 2641/11884 - Loss: 29.9518\n",
      "Processing batch 2642/11884 - Loss: 28.3585\n",
      "Processing batch 2643/11884 - Loss: 30.5247\n",
      "Processing batch 2644/11884 - Loss: 30.9329\n",
      "Processing batch 2645/11884 - Loss: 29.3541\n",
      "Processing batch 2646/11884 - Loss: 31.2889\n",
      "Processing batch 2647/11884 - Loss: 29.8147\n",
      "Processing batch 2648/11884 - Loss: 29.4699\n",
      "Processing batch 2649/11884 - Loss: 28.9799\n",
      "Processing batch 2650/11884 - Loss: 29.0509\n",
      "Processing batch 2651/11884 - Loss: 30.2622\n",
      "Processing batch 2652/11884 - Loss: 29.2060\n",
      "Processing batch 2653/11884 - Loss: 31.6400\n",
      "Processing batch 2654/11884 - Loss: 28.8553\n",
      "Processing batch 2655/11884 - Loss: 30.6320\n",
      "Processing batch 2656/11884 - Loss: 28.1216\n",
      "Processing batch 2657/11884 - Loss: 29.1183\n",
      "Processing batch 2658/11884 - Loss: 31.0754\n",
      "Processing batch 2659/11884 - Loss: 30.6928\n",
      "Processing batch 2660/11884 - Loss: 30.6126\n",
      "Processing batch 2661/11884 - Loss: 29.9453\n",
      "Processing batch 2662/11884 - Loss: 30.1629\n",
      "Processing batch 2663/11884 - Loss: 28.9244\n",
      "Processing batch 2664/11884 - Loss: 29.0844\n",
      "Processing batch 2665/11884 - Loss: 30.2109\n",
      "Processing batch 2666/11884 - Loss: 29.7196\n",
      "Processing batch 2667/11884 - Loss: 30.0506\n",
      "Processing batch 2668/11884 - Loss: 29.8499\n",
      "Processing batch 2669/11884 - Loss: 30.2353\n",
      "Processing batch 2670/11884 - Loss: 28.6548\n",
      "Processing batch 2671/11884 - Loss: 29.7755\n",
      "Processing batch 2672/11884 - Loss: 29.6854\n",
      "Processing batch 2673/11884 - Loss: 29.8546\n",
      "Processing batch 2674/11884 - Loss: 29.6972\n",
      "Processing batch 2675/11884 - Loss: 27.8471\n",
      "Processing batch 2676/11884 - Loss: 29.9578\n",
      "Processing batch 2677/11884 - Loss: 30.9389\n",
      "Processing batch 2678/11884 - Loss: 30.8596\n",
      "Processing batch 2679/11884 - Loss: 30.2394\n",
      "Processing batch 2680/11884 - Loss: 29.7363\n",
      "Processing batch 2681/11884 - Loss: 30.1633\n",
      "Processing batch 2682/11884 - Loss: 29.9039\n",
      "Processing batch 2683/11884 - Loss: 29.4229\n",
      "Processing batch 2684/11884 - Loss: 30.8034\n",
      "Processing batch 2685/11884 - Loss: 30.6474\n",
      "Processing batch 2686/11884 - Loss: 31.0608\n",
      "Processing batch 2687/11884 - Loss: 29.7352\n",
      "Processing batch 2688/11884 - Loss: 29.6709\n",
      "Processing batch 2689/11884 - Loss: 30.0673\n",
      "Processing batch 2690/11884 - Loss: 30.5358\n",
      "Processing batch 2691/11884 - Loss: 30.4482\n",
      "Processing batch 2692/11884 - Loss: 29.7417\n",
      "Processing batch 2693/11884 - Loss: 30.9312\n",
      "Processing batch 2694/11884 - Loss: 28.0612\n",
      "Processing batch 2695/11884 - Loss: 28.4902\n",
      "Processing batch 2696/11884 - Loss: 30.9819\n",
      "Processing batch 2697/11884 - Loss: 29.5361\n",
      "Processing batch 2698/11884 - Loss: 30.9016\n",
      "Processing batch 2699/11884 - Loss: 30.4189\n",
      "Processing batch 2700/11884 - Loss: 29.2927\n",
      "Processing batch 2701/11884 - Loss: 29.5812\n",
      "Processing batch 2702/11884 - Loss: 29.1074\n",
      "Processing batch 2703/11884 - Loss: 30.0950\n",
      "Processing batch 2704/11884 - Loss: 30.0437\n",
      "Processing batch 2705/11884 - Loss: 30.5855\n",
      "Processing batch 2706/11884 - Loss: 29.7067\n",
      "Processing batch 2707/11884 - Loss: 31.8694\n",
      "Processing batch 2708/11884 - Loss: 28.6281\n",
      "Processing batch 2709/11884 - Loss: 30.3562\n",
      "Processing batch 2710/11884 - Loss: 30.1764\n",
      "Processing batch 2711/11884 - Loss: 30.2761\n",
      "Processing batch 2712/11884 - Loss: 29.6531\n",
      "Processing batch 2713/11884 - Loss: 30.0419\n",
      "Processing batch 2714/11884 - Loss: 29.2599\n",
      "Processing batch 2715/11884 - Loss: 30.7854\n",
      "Processing batch 2716/11884 - Loss: 29.7306\n",
      "Processing batch 2717/11884 - Loss: 29.6855\n",
      "Processing batch 2718/11884 - Loss: 29.4582\n",
      "Processing batch 2719/11884 - Loss: 29.0622\n",
      "Processing batch 2720/11884 - Loss: 29.6771\n",
      "Processing batch 2721/11884 - Loss: 30.2607\n",
      "Processing batch 2722/11884 - Loss: 30.4387\n",
      "Processing batch 2723/11884 - Loss: 29.4820\n",
      "Processing batch 2724/11884 - Loss: 29.2843\n",
      "Processing batch 2725/11884 - Loss: 30.0543\n",
      "Processing batch 2726/11884 - Loss: 30.5671\n",
      "Processing batch 2727/11884 - Loss: 29.4179\n",
      "Processing batch 2728/11884 - Loss: 30.1378\n",
      "Processing batch 2729/11884 - Loss: 30.0642\n",
      "Processing batch 2730/11884 - Loss: 30.0744\n",
      "Processing batch 2731/11884 - Loss: 29.6081\n",
      "Processing batch 2732/11884 - Loss: 29.5351\n",
      "Processing batch 2733/11884 - Loss: 30.2133\n",
      "Processing batch 2734/11884 - Loss: 30.8372\n",
      "Processing batch 2735/11884 - Loss: 29.9127\n",
      "Processing batch 2736/11884 - Loss: 30.4593\n",
      "Processing batch 2737/11884 - Loss: 31.6960\n",
      "Processing batch 2738/11884 - Loss: 30.4175\n",
      "Processing batch 2739/11884 - Loss: 30.2983\n",
      "Processing batch 2740/11884 - Loss: 28.7595\n",
      "Processing batch 2741/11884 - Loss: 30.2129\n",
      "Processing batch 2742/11884 - Loss: 29.3716\n",
      "Processing batch 2743/11884 - Loss: 29.8521\n",
      "Processing batch 2744/11884 - Loss: 29.8396\n",
      "Processing batch 2745/11884 - Loss: 27.9339\n",
      "Processing batch 2746/11884 - Loss: 29.2682\n",
      "Processing batch 2747/11884 - Loss: 30.3994\n",
      "Processing batch 2748/11884 - Loss: 29.7584\n",
      "Processing batch 2749/11884 - Loss: 30.5443\n",
      "Processing batch 2750/11884 - Loss: 29.5339\n",
      "Processing batch 2751/11884 - Loss: 28.7857\n",
      "Processing batch 2752/11884 - Loss: 29.1612\n",
      "Processing batch 2753/11884 - Loss: 29.5271\n",
      "Processing batch 2754/11884 - Loss: 29.8479\n",
      "Processing batch 2755/11884 - Loss: 29.6165\n",
      "Processing batch 2756/11884 - Loss: 29.0580\n",
      "Processing batch 2757/11884 - Loss: 29.6514\n",
      "Processing batch 2758/11884 - Loss: 30.0392\n",
      "Processing batch 2759/11884 - Loss: 28.7451\n",
      "Processing batch 2760/11884 - Loss: 29.7165\n",
      "Processing batch 2761/11884 - Loss: 28.8738\n",
      "Processing batch 2762/11884 - Loss: 31.1760\n",
      "Processing batch 2763/11884 - Loss: 29.9722\n",
      "Processing batch 2764/11884 - Loss: 29.9571\n",
      "Processing batch 2765/11884 - Loss: 29.0673\n",
      "Processing batch 2766/11884 - Loss: 30.6932\n",
      "Processing batch 2767/11884 - Loss: 30.2811\n",
      "Processing batch 2768/11884 - Loss: 30.1878\n",
      "Processing batch 2769/11884 - Loss: 30.2080\n",
      "Processing batch 2770/11884 - Loss: 30.1111\n",
      "Processing batch 2771/11884 - Loss: 31.9630\n",
      "Processing batch 2772/11884 - Loss: 27.9597\n",
      "Processing batch 2773/11884 - Loss: 29.5353\n",
      "Processing batch 2774/11884 - Loss: 29.7612\n",
      "Processing batch 2775/11884 - Loss: 29.7749\n",
      "Processing batch 2776/11884 - Loss: 29.1943\n",
      "Processing batch 2777/11884 - Loss: 29.7641\n",
      "Processing batch 2778/11884 - Loss: 31.1194\n",
      "Processing batch 2779/11884 - Loss: 29.3156\n",
      "Processing batch 2780/11884 - Loss: 32.0504\n",
      "Processing batch 2781/11884 - Loss: 29.7552\n",
      "Processing batch 2782/11884 - Loss: 31.3197\n",
      "Processing batch 2783/11884 - Loss: 30.1626\n",
      "Processing batch 2784/11884 - Loss: 29.0612\n",
      "Processing batch 2785/11884 - Loss: 29.5593\n",
      "Processing batch 2786/11884 - Loss: 30.8053\n",
      "Processing batch 2787/11884 - Loss: 30.0981\n",
      "Processing batch 2788/11884 - Loss: 29.3808\n",
      "Processing batch 2789/11884 - Loss: 30.5355\n",
      "Processing batch 2790/11884 - Loss: 30.6456\n",
      "Processing batch 2791/11884 - Loss: 30.6477\n",
      "Processing batch 2792/11884 - Loss: 29.6726\n",
      "Processing batch 2793/11884 - Loss: 30.8344\n",
      "Processing batch 2794/11884 - Loss: 29.1031\n",
      "Processing batch 2795/11884 - Loss: 30.3099\n",
      "Processing batch 2796/11884 - Loss: 30.6246\n",
      "Processing batch 2797/11884 - Loss: 29.2132\n",
      "Processing batch 2798/11884 - Loss: 30.4470\n",
      "Processing batch 2799/11884 - Loss: 30.4415\n",
      "Processing batch 2800/11884 - Loss: 30.3015\n",
      "Processing batch 2801/11884 - Loss: 29.6507\n",
      "Processing batch 2802/11884 - Loss: 28.8490\n",
      "Processing batch 2803/11884 - Loss: 28.8147\n",
      "Processing batch 2804/11884 - Loss: 29.2101\n",
      "Processing batch 2805/11884 - Loss: 31.2799\n",
      "Processing batch 2806/11884 - Loss: 30.3728\n",
      "Processing batch 2807/11884 - Loss: 31.8065\n",
      "Processing batch 2808/11884 - Loss: 29.4161\n",
      "Processing batch 2809/11884 - Loss: 29.8398\n",
      "Processing batch 2810/11884 - Loss: 30.6400\n",
      "Processing batch 2811/11884 - Loss: 30.0202\n",
      "Processing batch 2812/11884 - Loss: 28.4956\n",
      "Processing batch 2813/11884 - Loss: 30.5373\n",
      "Processing batch 2814/11884 - Loss: 29.2482\n",
      "Processing batch 2815/11884 - Loss: 29.5558\n",
      "Processing batch 2816/11884 - Loss: 30.0647\n",
      "Processing batch 2817/11884 - Loss: 31.1515\n",
      "Processing batch 2818/11884 - Loss: 29.9421\n",
      "Processing batch 2819/11884 - Loss: 30.7097\n",
      "Processing batch 2820/11884 - Loss: 32.2473\n",
      "Processing batch 2821/11884 - Loss: 29.4770\n",
      "Processing batch 2822/11884 - Loss: 30.4166\n",
      "Processing batch 2823/11884 - Loss: 30.4088\n",
      "Processing batch 2824/11884 - Loss: 30.0766\n",
      "Processing batch 2825/11884 - Loss: 28.5193\n",
      "Processing batch 2826/11884 - Loss: 30.8929\n",
      "Processing batch 2827/11884 - Loss: 30.2480\n",
      "Processing batch 2828/11884 - Loss: 30.9584\n",
      "Processing batch 2829/11884 - Loss: 30.7471\n",
      "Processing batch 2830/11884 - Loss: 30.0790\n",
      "Processing batch 2831/11884 - Loss: 30.6906\n",
      "Processing batch 2832/11884 - Loss: 28.6349\n",
      "Processing batch 2833/11884 - Loss: 28.5851\n",
      "Processing batch 2834/11884 - Loss: 30.8735\n",
      "Processing batch 2835/11884 - Loss: 29.8074\n",
      "Processing batch 2836/11884 - Loss: 29.5960\n",
      "Processing batch 2837/11884 - Loss: 30.6795\n",
      "Processing batch 2838/11884 - Loss: 29.6460\n",
      "Processing batch 2839/11884 - Loss: 28.8649\n",
      "Processing batch 2840/11884 - Loss: 29.4340\n",
      "Processing batch 2841/11884 - Loss: 28.6498\n",
      "Processing batch 2842/11884 - Loss: 31.2160\n",
      "Processing batch 2843/11884 - Loss: 29.4315\n",
      "Processing batch 2844/11884 - Loss: 29.8823\n",
      "Processing batch 2845/11884 - Loss: 30.7168\n",
      "Processing batch 2846/11884 - Loss: 29.4450\n",
      "Processing batch 2847/11884 - Loss: 30.0107\n",
      "Processing batch 2848/11884 - Loss: 29.1567\n",
      "Processing batch 2849/11884 - Loss: 29.5657\n",
      "Processing batch 2850/11884 - Loss: 29.6836\n",
      "Processing batch 2851/11884 - Loss: 30.6732\n",
      "Processing batch 2852/11884 - Loss: 30.3466\n",
      "Processing batch 2853/11884 - Loss: 29.4585\n",
      "Processing batch 2854/11884 - Loss: 29.2938\n",
      "Processing batch 2855/11884 - Loss: 29.9486\n",
      "Processing batch 2856/11884 - Loss: 29.0460\n",
      "Processing batch 2857/11884 - Loss: 29.2126\n",
      "Processing batch 2858/11884 - Loss: 29.9535\n",
      "Processing batch 2859/11884 - Loss: 30.5463\n",
      "Processing batch 2860/11884 - Loss: 28.5426\n",
      "Processing batch 2861/11884 - Loss: 29.6287\n",
      "Processing batch 2862/11884 - Loss: 30.1807\n",
      "Processing batch 2863/11884 - Loss: 29.8209\n",
      "Processing batch 2864/11884 - Loss: 30.5157\n",
      "Processing batch 2865/11884 - Loss: 29.9771\n",
      "Processing batch 2866/11884 - Loss: 30.2062\n",
      "Processing batch 2867/11884 - Loss: 28.7756\n",
      "Processing batch 2868/11884 - Loss: 29.5857\n",
      "Processing batch 2869/11884 - Loss: 30.5361\n",
      "Processing batch 2870/11884 - Loss: 29.3869\n",
      "Processing batch 2871/11884 - Loss: 29.4411\n",
      "Processing batch 2872/11884 - Loss: 29.0144\n",
      "Processing batch 2873/11884 - Loss: 29.8053\n",
      "Processing batch 2874/11884 - Loss: 29.9989\n",
      "Processing batch 2875/11884 - Loss: 29.7751\n",
      "Processing batch 2876/11884 - Loss: 28.8528\n",
      "Processing batch 2877/11884 - Loss: 29.7680\n",
      "Processing batch 2878/11884 - Loss: 29.3415\n",
      "Processing batch 2879/11884 - Loss: 28.3583\n",
      "Processing batch 2880/11884 - Loss: 28.7882\n",
      "Processing batch 2881/11884 - Loss: 30.2621\n",
      "Processing batch 2882/11884 - Loss: 29.3549\n",
      "Processing batch 2883/11884 - Loss: 30.1988\n",
      "Processing batch 2884/11884 - Loss: 28.7460\n",
      "Processing batch 2885/11884 - Loss: 29.1197\n",
      "Processing batch 2886/11884 - Loss: 29.1966\n",
      "Processing batch 2887/11884 - Loss: 29.4077\n",
      "Processing batch 2888/11884 - Loss: 30.1169\n",
      "Processing batch 2889/11884 - Loss: 30.7771\n",
      "Processing batch 2890/11884 - Loss: 29.0823\n",
      "Processing batch 2891/11884 - Loss: 30.6091\n",
      "Processing batch 2892/11884 - Loss: 29.9165\n",
      "Processing batch 2893/11884 - Loss: 29.1863\n",
      "Processing batch 2894/11884 - Loss: 30.5003\n",
      "Processing batch 2895/11884 - Loss: 29.9880\n",
      "Processing batch 2896/11884 - Loss: 29.5804\n",
      "Processing batch 2897/11884 - Loss: 29.1398\n",
      "Processing batch 2898/11884 - Loss: 29.4156\n",
      "Processing batch 2899/11884 - Loss: 28.6453\n",
      "Processing batch 2900/11884 - Loss: 30.0973\n",
      "Processing batch 2901/11884 - Loss: 29.9868\n",
      "Processing batch 2902/11884 - Loss: 28.0597\n",
      "Processing batch 2903/11884 - Loss: 29.0309\n",
      "Processing batch 2904/11884 - Loss: 28.4947\n",
      "Processing batch 2905/11884 - Loss: 29.8460\n",
      "Processing batch 2906/11884 - Loss: 29.3665\n",
      "Processing batch 2907/11884 - Loss: 30.3138\n",
      "Processing batch 2908/11884 - Loss: 29.4082\n",
      "Processing batch 2909/11884 - Loss: 28.9481\n",
      "Processing batch 2910/11884 - Loss: 29.8677\n",
      "Processing batch 2911/11884 - Loss: 29.1763\n",
      "Processing batch 2912/11884 - Loss: 29.0169\n",
      "Processing batch 2913/11884 - Loss: 29.9711\n",
      "Processing batch 2914/11884 - Loss: 29.5432\n",
      "Processing batch 2915/11884 - Loss: 29.5019\n",
      "Processing batch 2916/11884 - Loss: 29.7365\n",
      "Processing batch 2917/11884 - Loss: 29.3699\n",
      "Processing batch 2918/11884 - Loss: 29.5790\n",
      "Processing batch 2919/11884 - Loss: 29.7514\n",
      "Processing batch 2920/11884 - Loss: 29.2982\n",
      "Processing batch 2921/11884 - Loss: 31.6059\n",
      "Processing batch 2922/11884 - Loss: 30.9373\n",
      "Processing batch 2923/11884 - Loss: 30.3505\n",
      "Processing batch 2924/11884 - Loss: 29.7377\n",
      "Processing batch 2925/11884 - Loss: 29.3845\n",
      "Processing batch 2926/11884 - Loss: 30.5638\n",
      "Processing batch 2927/11884 - Loss: 28.9754\n",
      "Processing batch 2928/11884 - Loss: 29.3417\n",
      "Processing batch 2929/11884 - Loss: 30.5445\n",
      "Processing batch 2930/11884 - Loss: 30.2120\n",
      "Processing batch 2931/11884 - Loss: 29.4719\n",
      "Processing batch 2932/11884 - Loss: 30.2147\n",
      "Processing batch 2933/11884 - Loss: 28.9822\n",
      "Processing batch 2934/11884 - Loss: 27.9409\n",
      "Processing batch 2935/11884 - Loss: 29.8545\n",
      "Processing batch 2936/11884 - Loss: 28.8905\n",
      "Processing batch 2937/11884 - Loss: 29.5725\n",
      "Processing batch 2938/11884 - Loss: 29.2377\n",
      "Processing batch 2939/11884 - Loss: 29.4427\n",
      "Processing batch 2940/11884 - Loss: 28.8131\n",
      "Processing batch 2941/11884 - Loss: 28.6167\n",
      "Processing batch 2942/11884 - Loss: 30.0713\n",
      "Processing batch 2943/11884 - Loss: 29.7341\n",
      "Processing batch 2944/11884 - Loss: 30.2959\n",
      "Processing batch 2945/11884 - Loss: 29.4206\n",
      "Processing batch 2946/11884 - Loss: 29.9824\n",
      "Processing batch 2947/11884 - Loss: 28.0215\n",
      "Processing batch 2948/11884 - Loss: 30.8588\n",
      "Processing batch 2949/11884 - Loss: 30.2681\n",
      "Processing batch 2950/11884 - Loss: 29.2198\n",
      "Processing batch 2951/11884 - Loss: 28.7977\n",
      "Processing batch 2952/11884 - Loss: 29.7923\n",
      "Processing batch 2953/11884 - Loss: 29.4062\n",
      "Processing batch 2954/11884 - Loss: 30.0824\n",
      "Processing batch 2955/11884 - Loss: 29.4240\n",
      "Processing batch 2956/11884 - Loss: 30.8444\n",
      "Processing batch 2957/11884 - Loss: 29.6467\n",
      "Processing batch 2958/11884 - Loss: 30.7258\n",
      "Processing batch 2959/11884 - Loss: 30.3725\n",
      "Processing batch 2960/11884 - Loss: 30.0914\n",
      "Processing batch 2961/11884 - Loss: 29.2076\n",
      "Processing batch 2962/11884 - Loss: 29.8006\n",
      "Processing batch 2963/11884 - Loss: 29.8785\n",
      "Processing batch 2964/11884 - Loss: 29.7421\n",
      "Processing batch 2965/11884 - Loss: 30.3349\n",
      "Processing batch 2966/11884 - Loss: 29.0655\n",
      "Processing batch 2967/11884 - Loss: 30.4550\n",
      "Processing batch 2968/11884 - Loss: 29.3790\n",
      "Processing batch 2969/11884 - Loss: 30.2204\n",
      "Processing batch 2970/11884 - Loss: 32.0280\n",
      "Processing batch 2971/11884 - Loss: 30.1898\n",
      "Processing batch 2972/11884 - Loss: 29.6129\n",
      "Processing batch 2973/11884 - Loss: 30.2549\n",
      "Processing batch 2974/11884 - Loss: 29.7055\n",
      "Processing batch 2975/11884 - Loss: 28.7022\n",
      "Processing batch 2976/11884 - Loss: 29.6860\n",
      "Processing batch 2977/11884 - Loss: 29.7906\n",
      "Processing batch 2978/11884 - Loss: 29.4806\n",
      "Processing batch 2979/11884 - Loss: 29.8234\n",
      "Processing batch 2980/11884 - Loss: 30.0268\n",
      "Processing batch 2981/11884 - Loss: 28.4167\n",
      "Processing batch 2982/11884 - Loss: 28.9912\n",
      "Processing batch 2983/11884 - Loss: 30.4962\n",
      "Processing batch 2984/11884 - Loss: 31.0739\n",
      "Processing batch 2985/11884 - Loss: 29.0335\n",
      "Processing batch 2986/11884 - Loss: 29.7715\n",
      "Processing batch 2987/11884 - Loss: 30.5729\n",
      "Processing batch 2988/11884 - Loss: 29.4187\n",
      "Processing batch 2989/11884 - Loss: 30.0189\n",
      "Processing batch 2990/11884 - Loss: 29.7745\n",
      "Processing batch 2991/11884 - Loss: 31.1647\n",
      "Processing batch 2992/11884 - Loss: 29.2703\n",
      "Processing batch 2993/11884 - Loss: 29.2341\n",
      "Processing batch 2994/11884 - Loss: 31.1974\n",
      "Processing batch 2995/11884 - Loss: 29.4575\n",
      "Processing batch 2996/11884 - Loss: 30.3845\n",
      "Processing batch 2997/11884 - Loss: 30.1454\n",
      "Processing batch 2998/11884 - Loss: 30.8643\n",
      "Processing batch 2999/11884 - Loss: 30.9465\n",
      "Processing batch 3000/11884 - Loss: 30.7304\n",
      "Processing batch 3001/11884 - Loss: 30.1309\n",
      "Processing batch 3002/11884 - Loss: 29.6161\n",
      "Processing batch 3003/11884 - Loss: 30.2999\n",
      "Processing batch 3004/11884 - Loss: 29.3230\n",
      "Processing batch 3005/11884 - Loss: 28.9765\n",
      "Processing batch 3006/11884 - Loss: 30.4641\n",
      "Processing batch 3007/11884 - Loss: 31.1066\n",
      "Processing batch 3008/11884 - Loss: 30.4333\n",
      "Processing batch 3009/11884 - Loss: 29.7982\n",
      "Processing batch 3010/11884 - Loss: 31.0610\n",
      "Processing batch 3011/11884 - Loss: 30.2867\n",
      "Processing batch 3012/11884 - Loss: 30.1343\n",
      "Processing batch 3013/11884 - Loss: 29.4981\n",
      "Processing batch 3014/11884 - Loss: 28.2908\n",
      "Processing batch 3015/11884 - Loss: 30.1655\n",
      "Processing batch 3016/11884 - Loss: 29.0388\n",
      "Processing batch 3017/11884 - Loss: 28.5526\n",
      "Processing batch 3018/11884 - Loss: 31.2279\n",
      "Processing batch 3019/11884 - Loss: 29.7312\n",
      "Processing batch 3020/11884 - Loss: 30.4991\n",
      "Processing batch 3021/11884 - Loss: 30.2637\n",
      "Processing batch 3022/11884 - Loss: 28.9903\n",
      "Processing batch 3023/11884 - Loss: 30.3809\n",
      "Processing batch 3024/11884 - Loss: 30.2488\n",
      "Processing batch 3025/11884 - Loss: 30.1211\n",
      "Processing batch 3026/11884 - Loss: 28.4943\n",
      "Processing batch 3027/11884 - Loss: 27.9654\n",
      "Processing batch 3028/11884 - Loss: 30.0181\n",
      "Processing batch 3029/11884 - Loss: 29.2059\n",
      "Processing batch 3030/11884 - Loss: 29.5153\n",
      "Processing batch 3031/11884 - Loss: 29.5061\n",
      "Processing batch 3032/11884 - Loss: 27.9422\n",
      "Processing batch 3033/11884 - Loss: 29.8001\n",
      "Processing batch 3034/11884 - Loss: 30.1385\n",
      "Processing batch 3035/11884 - Loss: 29.9549\n",
      "Processing batch 3036/11884 - Loss: 28.5586\n",
      "Processing batch 3037/11884 - Loss: 28.7485\n",
      "Processing batch 3038/11884 - Loss: 29.3687\n",
      "Processing batch 3039/11884 - Loss: 29.5510\n",
      "Processing batch 3040/11884 - Loss: 29.5080\n",
      "Processing batch 3041/11884 - Loss: 29.4054\n",
      "Processing batch 3042/11884 - Loss: 29.4289\n",
      "Processing batch 3043/11884 - Loss: 30.0169\n",
      "Processing batch 3044/11884 - Loss: 29.6791\n",
      "Processing batch 3045/11884 - Loss: 29.0282\n",
      "Processing batch 3046/11884 - Loss: 29.6732\n",
      "Processing batch 3047/11884 - Loss: 29.4439\n",
      "Processing batch 3048/11884 - Loss: 29.5689\n",
      "Processing batch 3049/11884 - Loss: 30.1284\n",
      "Processing batch 3050/11884 - Loss: 29.8419\n",
      "Processing batch 3051/11884 - Loss: 30.1978\n",
      "Processing batch 3052/11884 - Loss: 29.9695\n",
      "Processing batch 3053/11884 - Loss: 30.0357\n",
      "Processing batch 3054/11884 - Loss: 29.9265\n",
      "Processing batch 3055/11884 - Loss: 28.6133\n",
      "Processing batch 3056/11884 - Loss: 30.2186\n",
      "Processing batch 3057/11884 - Loss: 29.4674\n",
      "Processing batch 3058/11884 - Loss: 29.7620\n",
      "Processing batch 3059/11884 - Loss: 28.9681\n",
      "Processing batch 3060/11884 - Loss: 29.3850\n",
      "Processing batch 3061/11884 - Loss: 30.3719\n",
      "Processing batch 3062/11884 - Loss: 29.7409\n",
      "Processing batch 3063/11884 - Loss: 29.8586\n",
      "Processing batch 3064/11884 - Loss: 29.0597\n",
      "Processing batch 3065/11884 - Loss: 30.5980\n",
      "Processing batch 3066/11884 - Loss: 30.3762\n",
      "Processing batch 3067/11884 - Loss: 28.7234\n",
      "Processing batch 3068/11884 - Loss: 29.3137\n",
      "Processing batch 3069/11884 - Loss: 28.9087\n",
      "Processing batch 3070/11884 - Loss: 29.2150\n",
      "Processing batch 3071/11884 - Loss: 30.4950\n",
      "Processing batch 3072/11884 - Loss: 29.4970\n",
      "Processing batch 3073/11884 - Loss: 30.8010\n",
      "Processing batch 3074/11884 - Loss: 29.7511\n",
      "Processing batch 3075/11884 - Loss: 29.8377\n",
      "Processing batch 3076/11884 - Loss: 29.4691\n",
      "Processing batch 3077/11884 - Loss: 30.5206\n",
      "Processing batch 3078/11884 - Loss: 30.0291\n",
      "Processing batch 3079/11884 - Loss: 29.7392\n",
      "Processing batch 3080/11884 - Loss: 28.9356\n",
      "Processing batch 3081/11884 - Loss: 31.0583\n",
      "Processing batch 3082/11884 - Loss: 30.3159\n",
      "Processing batch 3083/11884 - Loss: 32.2971\n",
      "Processing batch 3084/11884 - Loss: 31.1394\n",
      "Processing batch 3085/11884 - Loss: 29.7339\n",
      "Processing batch 3086/11884 - Loss: 29.0449\n",
      "Processing batch 3087/11884 - Loss: 28.5243\n",
      "Processing batch 3088/11884 - Loss: 29.7997\n",
      "Processing batch 3089/11884 - Loss: 29.6676\n",
      "Processing batch 3090/11884 - Loss: 29.2825\n",
      "Processing batch 3091/11884 - Loss: 30.5032\n",
      "Processing batch 3092/11884 - Loss: 30.9438\n",
      "Processing batch 3093/11884 - Loss: 30.0285\n",
      "Processing batch 3094/11884 - Loss: 30.1082\n",
      "Processing batch 3095/11884 - Loss: 29.1582\n",
      "Processing batch 3096/11884 - Loss: 30.3617\n",
      "Processing batch 3097/11884 - Loss: 29.3149\n",
      "Processing batch 3098/11884 - Loss: 29.9545\n",
      "Processing batch 3099/11884 - Loss: 29.7547\n",
      "Processing batch 3100/11884 - Loss: 29.6696\n",
      "Processing batch 3101/11884 - Loss: 28.9616\n",
      "Processing batch 3102/11884 - Loss: 30.6987\n",
      "Processing batch 3103/11884 - Loss: 31.5479\n",
      "Processing batch 3104/11884 - Loss: 30.3815\n",
      "Processing batch 3105/11884 - Loss: 29.9605\n",
      "Processing batch 3106/11884 - Loss: 29.8772\n",
      "Processing batch 3107/11884 - Loss: 30.2475\n",
      "Processing batch 3108/11884 - Loss: 29.4879\n",
      "Processing batch 3109/11884 - Loss: 30.6313\n",
      "Processing batch 3110/11884 - Loss: 30.1285\n",
      "Processing batch 3111/11884 - Loss: 30.3785\n",
      "Processing batch 3112/11884 - Loss: 30.6471\n",
      "Processing batch 3113/11884 - Loss: 30.5607\n",
      "Processing batch 3114/11884 - Loss: 28.7896\n",
      "Processing batch 3115/11884 - Loss: 30.0030\n",
      "Processing batch 3116/11884 - Loss: 29.2349\n",
      "Processing batch 3117/11884 - Loss: 29.4482\n",
      "Processing batch 3118/11884 - Loss: 30.0604\n",
      "Processing batch 3119/11884 - Loss: 29.4224\n",
      "Processing batch 3120/11884 - Loss: 29.7907\n",
      "Processing batch 3121/11884 - Loss: 29.2301\n",
      "Processing batch 3122/11884 - Loss: 30.3278\n",
      "Processing batch 3123/11884 - Loss: 29.0772\n",
      "Processing batch 3124/11884 - Loss: 29.8659\n",
      "Processing batch 3125/11884 - Loss: 29.3392\n",
      "Processing batch 3126/11884 - Loss: 30.0271\n",
      "Processing batch 3127/11884 - Loss: 29.5602\n",
      "Processing batch 3128/11884 - Loss: 29.4738\n",
      "Processing batch 3129/11884 - Loss: 29.9204\n",
      "Processing batch 3130/11884 - Loss: 30.0456\n",
      "Processing batch 3131/11884 - Loss: 29.3329\n",
      "Processing batch 3132/11884 - Loss: 29.9490\n",
      "Processing batch 3133/11884 - Loss: 28.6007\n",
      "Processing batch 3134/11884 - Loss: 28.0988\n",
      "Processing batch 3135/11884 - Loss: 29.3697\n",
      "Processing batch 3136/11884 - Loss: 30.3432\n",
      "Processing batch 3137/11884 - Loss: 29.6498\n",
      "Processing batch 3138/11884 - Loss: 28.0751\n",
      "Processing batch 3139/11884 - Loss: 29.9158\n",
      "Processing batch 3140/11884 - Loss: 30.8004\n",
      "Processing batch 3141/11884 - Loss: 30.3116\n",
      "Processing batch 3142/11884 - Loss: 29.6167\n",
      "Processing batch 3143/11884 - Loss: 30.6941\n",
      "Processing batch 3144/11884 - Loss: 31.3160\n",
      "Processing batch 3145/11884 - Loss: 29.9792\n",
      "Processing batch 3146/11884 - Loss: 29.7409\n",
      "Processing batch 3147/11884 - Loss: 29.7493\n",
      "Processing batch 3148/11884 - Loss: 30.7710\n",
      "Processing batch 3149/11884 - Loss: 29.7636\n",
      "Processing batch 3150/11884 - Loss: 29.1058\n",
      "Processing batch 3151/11884 - Loss: 28.1515\n",
      "Processing batch 3152/11884 - Loss: 30.1231\n",
      "Processing batch 3153/11884 - Loss: 30.4743\n",
      "Processing batch 3154/11884 - Loss: 29.4615\n",
      "Processing batch 3155/11884 - Loss: 30.0857\n",
      "Processing batch 3156/11884 - Loss: 30.6083\n",
      "Processing batch 3157/11884 - Loss: 29.2504\n",
      "Processing batch 3158/11884 - Loss: 30.4552\n",
      "Processing batch 3159/11884 - Loss: 29.3861\n",
      "Processing batch 3160/11884 - Loss: 29.5586\n",
      "Processing batch 3161/11884 - Loss: 29.2517\n",
      "Processing batch 3162/11884 - Loss: 30.4652\n",
      "Processing batch 3163/11884 - Loss: 28.9450\n",
      "Processing batch 3164/11884 - Loss: 29.5997\n",
      "Processing batch 3165/11884 - Loss: 29.7910\n",
      "Processing batch 3166/11884 - Loss: 29.4280\n",
      "Processing batch 3167/11884 - Loss: 29.5259\n",
      "Processing batch 3168/11884 - Loss: 29.0329\n",
      "Processing batch 3169/11884 - Loss: 30.4171\n",
      "Processing batch 3170/11884 - Loss: 30.9018\n",
      "Processing batch 3171/11884 - Loss: 30.3531\n",
      "Processing batch 3172/11884 - Loss: 30.0529\n",
      "Processing batch 3173/11884 - Loss: 28.8759\n",
      "Processing batch 3174/11884 - Loss: 29.4148\n",
      "Processing batch 3175/11884 - Loss: 30.1285\n",
      "Processing batch 3176/11884 - Loss: 29.1436\n",
      "Processing batch 3177/11884 - Loss: 29.7602\n",
      "Processing batch 3178/11884 - Loss: 30.0079\n",
      "Processing batch 3179/11884 - Loss: 29.9815\n",
      "Processing batch 3180/11884 - Loss: 29.2472\n",
      "Processing batch 3181/11884 - Loss: 30.3354\n",
      "Processing batch 3182/11884 - Loss: 29.5345\n",
      "Processing batch 3183/11884 - Loss: 30.5688\n",
      "Processing batch 3184/11884 - Loss: 29.4473\n",
      "Processing batch 3185/11884 - Loss: 31.1613\n",
      "Processing batch 3186/11884 - Loss: 30.3139\n",
      "Processing batch 3187/11884 - Loss: 29.4502\n",
      "Processing batch 3188/11884 - Loss: 31.4222\n",
      "Processing batch 3189/11884 - Loss: 31.0851\n",
      "Processing batch 3190/11884 - Loss: 29.3685\n",
      "Processing batch 3191/11884 - Loss: 28.9760\n",
      "Processing batch 3192/11884 - Loss: 28.9679\n",
      "Processing batch 3193/11884 - Loss: 30.2803\n",
      "Processing batch 3194/11884 - Loss: 30.7980\n",
      "Processing batch 3195/11884 - Loss: 28.8252\n",
      "Processing batch 3196/11884 - Loss: 31.0055\n",
      "Processing batch 3197/11884 - Loss: 29.7038\n",
      "Processing batch 3198/11884 - Loss: 28.7272\n",
      "Processing batch 3199/11884 - Loss: 28.2359\n",
      "Processing batch 3200/11884 - Loss: 29.9433\n",
      "Processing batch 3201/11884 - Loss: 29.9530\n",
      "Processing batch 3202/11884 - Loss: 30.0315\n",
      "Processing batch 3203/11884 - Loss: 30.1785\n",
      "Processing batch 3204/11884 - Loss: 29.0265\n",
      "Processing batch 3205/11884 - Loss: 29.3884\n",
      "Processing batch 3206/11884 - Loss: 29.0751\n",
      "Processing batch 3207/11884 - Loss: 29.6222\n",
      "Processing batch 3208/11884 - Loss: 30.9351\n",
      "Processing batch 3209/11884 - Loss: 29.3190\n",
      "Processing batch 3210/11884 - Loss: 29.4365\n",
      "Processing batch 3211/11884 - Loss: 28.7732\n",
      "Processing batch 3212/11884 - Loss: 30.0794\n",
      "Processing batch 3213/11884 - Loss: 29.7375\n",
      "Processing batch 3214/11884 - Loss: 29.3105\n",
      "Processing batch 3215/11884 - Loss: 31.3591\n",
      "Processing batch 3216/11884 - Loss: 30.0013\n",
      "Processing batch 3217/11884 - Loss: 28.5442\n",
      "Processing batch 3218/11884 - Loss: 29.0808\n",
      "Processing batch 3219/11884 - Loss: 31.2680\n",
      "Processing batch 3220/11884 - Loss: 30.7197\n",
      "Processing batch 3221/11884 - Loss: 30.1337\n",
      "Processing batch 3222/11884 - Loss: 29.3577\n",
      "Processing batch 3223/11884 - Loss: 29.8348\n",
      "Processing batch 3224/11884 - Loss: 30.7454\n",
      "Processing batch 3225/11884 - Loss: 29.9743\n",
      "Processing batch 3226/11884 - Loss: 28.7573\n",
      "Processing batch 3227/11884 - Loss: 29.6200\n",
      "Processing batch 3228/11884 - Loss: 30.8942\n",
      "Processing batch 3229/11884 - Loss: 29.4421\n",
      "Processing batch 3230/11884 - Loss: 30.9051\n",
      "Processing batch 3231/11884 - Loss: 29.4644\n",
      "Processing batch 3232/11884 - Loss: 28.0844\n",
      "Processing batch 3233/11884 - Loss: 29.6939\n",
      "Processing batch 3234/11884 - Loss: 29.6361\n",
      "Processing batch 3235/11884 - Loss: 28.8412\n",
      "Processing batch 3236/11884 - Loss: 29.9524\n",
      "Processing batch 3237/11884 - Loss: 29.5467\n",
      "Processing batch 3238/11884 - Loss: 31.1858\n",
      "Processing batch 3239/11884 - Loss: 29.8628\n",
      "Processing batch 3240/11884 - Loss: 31.1248\n",
      "Processing batch 3241/11884 - Loss: 28.5424\n",
      "Processing batch 3242/11884 - Loss: 28.8984\n",
      "Processing batch 3243/11884 - Loss: 31.1643\n",
      "Processing batch 3244/11884 - Loss: 29.8218\n",
      "Processing batch 3245/11884 - Loss: 29.8877\n",
      "Processing batch 3246/11884 - Loss: 30.7359\n",
      "Processing batch 3247/11884 - Loss: 30.3446\n",
      "Processing batch 3248/11884 - Loss: 29.7698\n",
      "Processing batch 3249/11884 - Loss: 29.5260\n",
      "Processing batch 3250/11884 - Loss: 29.4777\n",
      "Processing batch 3251/11884 - Loss: 30.2072\n",
      "Processing batch 3252/11884 - Loss: 28.6538\n",
      "Processing batch 3253/11884 - Loss: 30.5417\n",
      "Processing batch 3254/11884 - Loss: 30.4274\n",
      "Processing batch 3255/11884 - Loss: 29.9777\n",
      "Processing batch 3256/11884 - Loss: 29.9860\n",
      "Processing batch 3257/11884 - Loss: 29.6045\n",
      "Processing batch 3258/11884 - Loss: 29.9445\n",
      "Processing batch 3259/11884 - Loss: 29.8407\n",
      "Processing batch 3260/11884 - Loss: 29.0521\n",
      "Processing batch 3261/11884 - Loss: 29.8466\n",
      "Processing batch 3262/11884 - Loss: 30.7656\n",
      "Processing batch 3263/11884 - Loss: 30.0377\n",
      "Processing batch 3264/11884 - Loss: 30.3682\n",
      "Processing batch 3265/11884 - Loss: 30.5081\n",
      "Processing batch 3266/11884 - Loss: 30.9669\n",
      "Processing batch 3267/11884 - Loss: 30.0741\n",
      "Processing batch 3268/11884 - Loss: 30.0401\n",
      "Processing batch 3269/11884 - Loss: 30.8276\n",
      "Processing batch 3270/11884 - Loss: 28.7443\n",
      "Processing batch 3271/11884 - Loss: 28.8839\n",
      "Processing batch 3272/11884 - Loss: 29.1178\n",
      "Processing batch 3273/11884 - Loss: 31.1699\n",
      "Processing batch 3274/11884 - Loss: 30.1904\n",
      "Processing batch 3275/11884 - Loss: 29.2298\n",
      "Processing batch 3276/11884 - Loss: 30.7751\n",
      "Processing batch 3277/11884 - Loss: 29.1290\n",
      "Processing batch 3278/11884 - Loss: 29.6922\n",
      "Processing batch 3279/11884 - Loss: 29.7179\n",
      "Processing batch 3280/11884 - Loss: 30.5281\n",
      "Processing batch 3281/11884 - Loss: 28.8739\n",
      "Processing batch 3282/11884 - Loss: 30.8235\n",
      "Processing batch 3283/11884 - Loss: 30.6946\n",
      "Processing batch 3284/11884 - Loss: 29.7913\n",
      "Processing batch 3285/11884 - Loss: 30.1940\n",
      "Processing batch 3286/11884 - Loss: 28.9615\n",
      "Processing batch 3287/11884 - Loss: 27.8056\n",
      "Processing batch 3288/11884 - Loss: 31.2307\n",
      "Processing batch 3289/11884 - Loss: 29.7300\n",
      "Processing batch 3290/11884 - Loss: 30.1055\n",
      "Processing batch 3291/11884 - Loss: 31.2707\n",
      "Processing batch 3292/11884 - Loss: 29.8559\n",
      "Processing batch 3293/11884 - Loss: 29.7716\n",
      "Processing batch 3294/11884 - Loss: 30.9323\n",
      "Processing batch 3295/11884 - Loss: 30.3495\n",
      "Processing batch 3296/11884 - Loss: 29.5131\n",
      "Processing batch 3297/11884 - Loss: 30.5110\n",
      "Processing batch 3298/11884 - Loss: 31.0564\n",
      "Processing batch 3299/11884 - Loss: 30.0549\n",
      "Processing batch 3300/11884 - Loss: 30.0260\n",
      "Processing batch 3301/11884 - Loss: 30.0029\n",
      "Processing batch 3302/11884 - Loss: 29.3659\n",
      "Processing batch 3303/11884 - Loss: 30.5283\n",
      "Processing batch 3304/11884 - Loss: 29.6668\n",
      "Processing batch 3305/11884 - Loss: 29.3769\n",
      "Processing batch 3306/11884 - Loss: 28.4800\n",
      "Processing batch 3307/11884 - Loss: 29.6068\n",
      "Processing batch 3308/11884 - Loss: 29.6061\n",
      "Processing batch 3309/11884 - Loss: 30.5845\n",
      "Processing batch 3310/11884 - Loss: 29.8810\n",
      "Processing batch 3311/11884 - Loss: 29.2895\n",
      "Processing batch 3312/11884 - Loss: 31.2275\n",
      "Processing batch 3313/11884 - Loss: 29.7643\n",
      "Processing batch 3314/11884 - Loss: 31.1517\n",
      "Processing batch 3315/11884 - Loss: 28.9059\n",
      "Processing batch 3316/11884 - Loss: 30.3889\n",
      "Processing batch 3317/11884 - Loss: 30.3551\n",
      "Processing batch 3318/11884 - Loss: 29.4860\n",
      "Processing batch 3319/11884 - Loss: 29.2636\n",
      "Processing batch 3320/11884 - Loss: 29.7274\n",
      "Processing batch 3321/11884 - Loss: 30.2219\n",
      "Processing batch 3322/11884 - Loss: 29.5993\n",
      "Processing batch 3323/11884 - Loss: 29.7870\n",
      "Processing batch 3324/11884 - Loss: 29.6936\n",
      "Processing batch 3325/11884 - Loss: 30.1531\n",
      "Processing batch 3326/11884 - Loss: 29.7773\n",
      "Processing batch 3327/11884 - Loss: 29.6096\n",
      "Processing batch 3328/11884 - Loss: 30.0380\n",
      "Processing batch 3329/11884 - Loss: 30.0028\n",
      "Processing batch 3330/11884 - Loss: 30.2685\n",
      "Processing batch 3331/11884 - Loss: 30.0167\n",
      "Processing batch 3332/11884 - Loss: 30.0339\n",
      "Processing batch 3333/11884 - Loss: 29.5766\n",
      "Processing batch 3334/11884 - Loss: 30.0333\n",
      "Processing batch 3335/11884 - Loss: 30.0124\n",
      "Processing batch 3336/11884 - Loss: 30.7809\n",
      "Processing batch 3337/11884 - Loss: 30.8517\n",
      "Processing batch 3338/11884 - Loss: 29.3810\n",
      "Processing batch 3339/11884 - Loss: 28.5444\n",
      "Processing batch 3340/11884 - Loss: 28.8123\n",
      "Processing batch 3341/11884 - Loss: 30.0790\n",
      "Processing batch 3342/11884 - Loss: 29.1467\n",
      "Processing batch 3343/11884 - Loss: 30.4185\n",
      "Processing batch 3344/11884 - Loss: 30.6597\n",
      "Processing batch 3345/11884 - Loss: 30.6973\n",
      "Processing batch 3346/11884 - Loss: 29.0596\n",
      "Processing batch 3347/11884 - Loss: 30.1144\n",
      "Processing batch 3348/11884 - Loss: 29.5004\n",
      "Processing batch 3349/11884 - Loss: 29.6052\n",
      "Processing batch 3350/11884 - Loss: 30.7999\n",
      "Processing batch 3351/11884 - Loss: 30.9303\n",
      "Processing batch 3352/11884 - Loss: 30.3184\n",
      "Processing batch 3353/11884 - Loss: 30.4270\n",
      "Processing batch 3354/11884 - Loss: 30.2675\n",
      "Processing batch 3355/11884 - Loss: 30.2287\n",
      "Processing batch 3356/11884 - Loss: 27.1757\n",
      "Processing batch 3357/11884 - Loss: 29.8653\n",
      "Processing batch 3358/11884 - Loss: 30.0545\n",
      "Processing batch 3359/11884 - Loss: 28.1403\n",
      "Processing batch 3360/11884 - Loss: 29.0856\n",
      "Processing batch 3361/11884 - Loss: 31.1727\n",
      "Processing batch 3362/11884 - Loss: 30.7044\n",
      "Processing batch 3363/11884 - Loss: 29.8563\n",
      "Processing batch 3364/11884 - Loss: 29.9197\n",
      "Processing batch 3365/11884 - Loss: 30.7549\n",
      "Processing batch 3366/11884 - Loss: 30.5522\n",
      "Processing batch 3367/11884 - Loss: 29.0929\n",
      "Processing batch 3368/11884 - Loss: 28.9553\n",
      "Processing batch 3369/11884 - Loss: 30.9222\n",
      "Processing batch 3370/11884 - Loss: 30.3414\n",
      "Processing batch 3371/11884 - Loss: 28.3772\n",
      "Processing batch 3372/11884 - Loss: 30.2720\n",
      "Processing batch 3373/11884 - Loss: 31.0743\n",
      "Processing batch 3374/11884 - Loss: 31.3848\n",
      "Processing batch 3375/11884 - Loss: 28.4925\n",
      "Processing batch 3376/11884 - Loss: 29.8216\n",
      "Processing batch 3377/11884 - Loss: 29.1178\n",
      "Processing batch 3378/11884 - Loss: 30.1147\n",
      "Processing batch 3379/11884 - Loss: 30.4840\n",
      "Processing batch 3380/11884 - Loss: 28.9959\n",
      "Processing batch 3381/11884 - Loss: 30.2388\n",
      "Processing batch 3382/11884 - Loss: 28.3798\n",
      "Processing batch 3383/11884 - Loss: 30.1145\n",
      "Processing batch 3384/11884 - Loss: 28.5088\n",
      "Processing batch 3385/11884 - Loss: 29.3123\n",
      "Processing batch 3386/11884 - Loss: 30.2073\n",
      "Processing batch 3387/11884 - Loss: 29.8774\n",
      "Processing batch 3388/11884 - Loss: 29.6125\n",
      "Processing batch 3389/11884 - Loss: 30.2977\n",
      "Processing batch 3390/11884 - Loss: 30.0482\n",
      "Processing batch 3391/11884 - Loss: 30.1535\n",
      "Processing batch 3392/11884 - Loss: 31.1688\n",
      "Processing batch 3393/11884 - Loss: 30.5314\n",
      "Processing batch 3394/11884 - Loss: 29.0894\n",
      "Processing batch 3395/11884 - Loss: 28.3983\n",
      "Processing batch 3396/11884 - Loss: 30.1854\n",
      "Processing batch 3397/11884 - Loss: 29.9222\n",
      "Processing batch 3398/11884 - Loss: 30.3754\n",
      "Processing batch 3399/11884 - Loss: 29.8986\n",
      "Processing batch 3400/11884 - Loss: 29.3573\n",
      "Processing batch 3401/11884 - Loss: 31.5259\n",
      "Processing batch 3402/11884 - Loss: 29.5511\n",
      "Processing batch 3403/11884 - Loss: 30.3295\n",
      "Processing batch 3404/11884 - Loss: 29.5294\n",
      "Processing batch 3405/11884 - Loss: 29.3816\n",
      "Processing batch 3406/11884 - Loss: 29.0497\n",
      "Processing batch 3407/11884 - Loss: 29.7344\n",
      "Processing batch 3408/11884 - Loss: 29.1407\n",
      "Processing batch 3409/11884 - Loss: 29.9437\n",
      "Processing batch 3410/11884 - Loss: 30.3520\n",
      "Processing batch 3411/11884 - Loss: 29.7763\n",
      "Processing batch 3412/11884 - Loss: 29.3560\n",
      "Processing batch 3413/11884 - Loss: 30.2311\n",
      "Processing batch 3414/11884 - Loss: 30.2570\n",
      "Processing batch 3415/11884 - Loss: 30.3327\n",
      "Processing batch 3416/11884 - Loss: 30.5468\n",
      "Processing batch 3417/11884 - Loss: 30.5163\n",
      "Processing batch 3418/11884 - Loss: 28.8797\n",
      "Processing batch 3419/11884 - Loss: 29.0440\n",
      "Processing batch 3420/11884 - Loss: 30.2815\n",
      "Processing batch 3421/11884 - Loss: 30.2826\n",
      "Processing batch 3422/11884 - Loss: 31.4429\n",
      "Processing batch 3423/11884 - Loss: 28.3005\n",
      "Processing batch 3424/11884 - Loss: 30.2673\n",
      "Processing batch 3425/11884 - Loss: 29.2776\n",
      "Processing batch 3426/11884 - Loss: 29.5163\n",
      "Processing batch 3427/11884 - Loss: 28.3018\n",
      "Processing batch 3428/11884 - Loss: 30.0510\n",
      "Processing batch 3429/11884 - Loss: 29.9292\n",
      "Processing batch 3430/11884 - Loss: 30.9379\n",
      "Processing batch 3431/11884 - Loss: 28.4850\n",
      "Processing batch 3432/11884 - Loss: 29.5703\n",
      "Processing batch 3433/11884 - Loss: 30.4719\n",
      "Processing batch 3434/11884 - Loss: 29.1006\n",
      "Processing batch 3435/11884 - Loss: 29.4798\n",
      "Processing batch 3436/11884 - Loss: 29.1037\n",
      "Processing batch 3437/11884 - Loss: 30.5255\n",
      "Processing batch 3438/11884 - Loss: 29.1418\n",
      "Processing batch 3439/11884 - Loss: 28.0289\n",
      "Processing batch 3440/11884 - Loss: 28.6760\n",
      "Processing batch 3441/11884 - Loss: 30.3263\n",
      "Processing batch 3442/11884 - Loss: 29.1763\n",
      "Processing batch 3443/11884 - Loss: 29.0106\n",
      "Processing batch 3444/11884 - Loss: 30.3102\n",
      "Processing batch 3445/11884 - Loss: 28.5059\n",
      "Processing batch 3446/11884 - Loss: 30.2080\n",
      "Processing batch 3447/11884 - Loss: 31.0551\n",
      "Processing batch 3448/11884 - Loss: 29.0332\n",
      "Processing batch 3449/11884 - Loss: 29.8204\n",
      "Processing batch 3450/11884 - Loss: 29.5672\n",
      "Processing batch 3451/11884 - Loss: 28.6387\n",
      "Processing batch 3452/11884 - Loss: 30.5460\n",
      "Processing batch 3453/11884 - Loss: 30.0639\n",
      "Processing batch 3454/11884 - Loss: 29.8003\n",
      "Processing batch 3455/11884 - Loss: 29.2087\n",
      "Processing batch 3456/11884 - Loss: 30.1336\n",
      "Processing batch 3457/11884 - Loss: 29.3831\n",
      "Processing batch 3458/11884 - Loss: 28.6532\n",
      "Processing batch 3459/11884 - Loss: 28.8162\n",
      "Processing batch 3460/11884 - Loss: 29.8481\n",
      "Processing batch 3461/11884 - Loss: 29.7077\n",
      "Processing batch 3462/11884 - Loss: 29.3657\n",
      "Processing batch 3463/11884 - Loss: 29.2032\n",
      "Processing batch 3464/11884 - Loss: 29.6846\n",
      "Processing batch 3465/11884 - Loss: 29.8967\n",
      "Processing batch 3466/11884 - Loss: 30.1179\n",
      "Processing batch 3467/11884 - Loss: 29.0679\n",
      "Processing batch 3468/11884 - Loss: 29.9938\n",
      "Processing batch 3469/11884 - Loss: 30.0055\n",
      "Processing batch 3470/11884 - Loss: 29.4221\n",
      "Processing batch 3471/11884 - Loss: 29.4299\n",
      "Processing batch 3472/11884 - Loss: 29.6876\n",
      "Processing batch 3473/11884 - Loss: 31.2657\n",
      "Processing batch 3474/11884 - Loss: 30.1190\n",
      "Processing batch 3475/11884 - Loss: 30.1082\n",
      "Processing batch 3476/11884 - Loss: 29.0829\n",
      "Processing batch 3477/11884 - Loss: 28.7654\n",
      "Processing batch 3478/11884 - Loss: 28.9808\n",
      "Processing batch 3479/11884 - Loss: 30.3524\n",
      "Processing batch 3480/11884 - Loss: 28.4136\n",
      "Processing batch 3481/11884 - Loss: 30.4520\n",
      "Processing batch 3482/11884 - Loss: 28.6828\n",
      "Processing batch 3483/11884 - Loss: 29.8612\n",
      "Processing batch 3484/11884 - Loss: 30.0281\n",
      "Processing batch 3485/11884 - Loss: 30.0873\n",
      "Processing batch 3486/11884 - Loss: 30.7418\n",
      "Processing batch 3487/11884 - Loss: 30.1810\n",
      "Processing batch 3488/11884 - Loss: 29.1860\n",
      "Processing batch 3489/11884 - Loss: 31.4620\n",
      "Processing batch 3490/11884 - Loss: 29.0058\n",
      "Processing batch 3491/11884 - Loss: 29.2647\n",
      "Processing batch 3492/11884 - Loss: 30.0372\n",
      "Processing batch 3493/11884 - Loss: 30.5920\n",
      "Processing batch 3494/11884 - Loss: 29.1505\n",
      "Processing batch 3495/11884 - Loss: 28.8888\n",
      "Processing batch 3496/11884 - Loss: 30.4364\n",
      "Processing batch 3497/11884 - Loss: 29.9093\n",
      "Processing batch 3498/11884 - Loss: 30.0279\n",
      "Processing batch 3499/11884 - Loss: 29.0482\n",
      "Processing batch 3500/11884 - Loss: 29.7406\n",
      "Processing batch 3501/11884 - Loss: 29.8216\n",
      "Processing batch 3502/11884 - Loss: 29.3886\n",
      "Processing batch 3503/11884 - Loss: 30.6739\n",
      "Processing batch 3504/11884 - Loss: 28.9814\n",
      "Processing batch 3505/11884 - Loss: 28.0515\n",
      "Processing batch 3506/11884 - Loss: 31.2804\n",
      "Processing batch 3507/11884 - Loss: 29.7225\n",
      "Processing batch 3508/11884 - Loss: 29.5825\n",
      "Processing batch 3509/11884 - Loss: 31.0025\n",
      "Processing batch 3510/11884 - Loss: 30.2451\n",
      "Processing batch 3511/11884 - Loss: 29.6630\n",
      "Processing batch 3512/11884 - Loss: 28.7075\n",
      "Processing batch 3513/11884 - Loss: 30.1134\n",
      "Processing batch 3514/11884 - Loss: 31.1795\n",
      "Processing batch 3515/11884 - Loss: 29.4422\n",
      "Processing batch 3516/11884 - Loss: 30.0997\n",
      "Processing batch 3517/11884 - Loss: 29.2424\n",
      "Processing batch 3518/11884 - Loss: 29.8840\n",
      "Processing batch 3519/11884 - Loss: 28.8588\n",
      "Processing batch 3520/11884 - Loss: 31.7345\n",
      "Processing batch 3521/11884 - Loss: 31.0367\n",
      "Processing batch 3522/11884 - Loss: 29.7198\n",
      "Processing batch 3523/11884 - Loss: 29.1036\n",
      "Processing batch 3524/11884 - Loss: 29.2974\n",
      "Processing batch 3525/11884 - Loss: 28.5177\n",
      "Processing batch 3526/11884 - Loss: 30.2986\n",
      "Processing batch 3527/11884 - Loss: 29.7135\n",
      "Processing batch 3528/11884 - Loss: 31.0566\n",
      "Processing batch 3529/11884 - Loss: 29.4428\n",
      "Processing batch 3530/11884 - Loss: 28.8928\n",
      "Processing batch 3531/11884 - Loss: 28.7331\n",
      "Processing batch 3532/11884 - Loss: 29.5513\n",
      "Processing batch 3533/11884 - Loss: 30.8096\n",
      "Processing batch 3534/11884 - Loss: 29.3630\n",
      "Processing batch 3535/11884 - Loss: 31.3604\n",
      "Processing batch 3536/11884 - Loss: 29.4441\n",
      "Processing batch 3537/11884 - Loss: 28.8140\n",
      "Processing batch 3538/11884 - Loss: 30.4368\n",
      "Processing batch 3539/11884 - Loss: 29.6992\n",
      "Processing batch 3540/11884 - Loss: 28.8916\n",
      "Processing batch 3541/11884 - Loss: 29.8259\n",
      "Processing batch 3542/11884 - Loss: 31.3158\n",
      "Processing batch 3543/11884 - Loss: 30.8380\n",
      "Processing batch 3544/11884 - Loss: 29.2692\n",
      "Processing batch 3545/11884 - Loss: 29.6081\n",
      "Processing batch 3546/11884 - Loss: 29.4412\n",
      "Processing batch 3547/11884 - Loss: 30.0945\n",
      "Processing batch 3548/11884 - Loss: 29.5500\n",
      "Processing batch 3549/11884 - Loss: 28.5262\n",
      "Processing batch 3550/11884 - Loss: 28.8131\n",
      "Processing batch 3551/11884 - Loss: 31.2488\n",
      "Processing batch 3552/11884 - Loss: 30.0143\n",
      "Processing batch 3553/11884 - Loss: 29.3733\n",
      "Processing batch 3554/11884 - Loss: 30.9455\n",
      "Processing batch 3555/11884 - Loss: 30.6437\n",
      "Processing batch 3556/11884 - Loss: 28.3285\n",
      "Processing batch 3557/11884 - Loss: 29.0795\n",
      "Processing batch 3558/11884 - Loss: 30.0431\n",
      "Processing batch 3559/11884 - Loss: 30.0146\n",
      "Processing batch 3560/11884 - Loss: 28.4261\n",
      "Processing batch 3561/11884 - Loss: 29.9319\n",
      "Processing batch 3562/11884 - Loss: 29.6572\n",
      "Processing batch 3563/11884 - Loss: 30.2933\n",
      "Processing batch 3564/11884 - Loss: 30.5655\n",
      "Processing batch 3565/11884 - Loss: 30.2534\n",
      "Processing batch 3566/11884 - Loss: 29.3826\n",
      "Processing batch 3567/11884 - Loss: 30.1139\n",
      "Processing batch 3568/11884 - Loss: 29.2164\n",
      "Processing batch 3569/11884 - Loss: 28.7716\n",
      "Processing batch 3570/11884 - Loss: 30.1527\n",
      "Processing batch 3571/11884 - Loss: 29.1974\n",
      "Processing batch 3572/11884 - Loss: 30.4622\n",
      "Processing batch 3573/11884 - Loss: 31.1533\n",
      "Processing batch 3574/11884 - Loss: 30.0023\n",
      "Processing batch 3575/11884 - Loss: 29.5706\n",
      "Processing batch 3576/11884 - Loss: 29.6704\n",
      "Processing batch 3577/11884 - Loss: 31.1025\n",
      "Processing batch 3578/11884 - Loss: 30.0052\n",
      "Processing batch 3579/11884 - Loss: 30.2322\n",
      "Processing batch 3580/11884 - Loss: 29.3104\n",
      "Processing batch 3581/11884 - Loss: 29.2081\n",
      "Processing batch 3582/11884 - Loss: 30.8135\n",
      "Processing batch 3583/11884 - Loss: 31.2728\n",
      "Processing batch 3584/11884 - Loss: 29.4895\n",
      "Processing batch 3585/11884 - Loss: 29.2606\n",
      "Processing batch 3586/11884 - Loss: 28.9012\n",
      "Processing batch 3587/11884 - Loss: 29.2099\n",
      "Processing batch 3588/11884 - Loss: 29.4894\n",
      "Processing batch 3589/11884 - Loss: 30.7393\n",
      "Processing batch 3590/11884 - Loss: 28.1594\n",
      "Processing batch 3591/11884 - Loss: 29.6844\n",
      "Processing batch 3592/11884 - Loss: 29.3281\n",
      "Processing batch 3593/11884 - Loss: 29.6333\n",
      "Processing batch 3594/11884 - Loss: 29.5349\n",
      "Processing batch 3595/11884 - Loss: 30.0194\n",
      "Processing batch 3596/11884 - Loss: 28.6516\n",
      "Processing batch 3597/11884 - Loss: 30.5202\n",
      "Processing batch 3598/11884 - Loss: 28.6239\n",
      "Processing batch 3599/11884 - Loss: 30.6410\n",
      "Processing batch 3600/11884 - Loss: 29.6601\n",
      "Processing batch 3601/11884 - Loss: 30.4222\n",
      "Processing batch 3602/11884 - Loss: 28.7643\n",
      "Processing batch 3603/11884 - Loss: 30.1744\n",
      "Processing batch 3604/11884 - Loss: 29.9984\n",
      "Processing batch 3605/11884 - Loss: 30.9885\n",
      "Processing batch 3606/11884 - Loss: 31.1361\n",
      "Processing batch 3607/11884 - Loss: 30.0319\n",
      "Processing batch 3608/11884 - Loss: 29.8976\n",
      "Processing batch 3609/11884 - Loss: 28.6502\n",
      "Processing batch 3610/11884 - Loss: 31.0932\n",
      "Processing batch 3611/11884 - Loss: 30.5273\n",
      "Processing batch 3612/11884 - Loss: 30.1632\n",
      "Processing batch 3613/11884 - Loss: 29.2440\n",
      "Processing batch 3614/11884 - Loss: 29.4642\n",
      "Processing batch 3615/11884 - Loss: 30.6004\n",
      "Processing batch 3616/11884 - Loss: 29.0556\n",
      "Processing batch 3617/11884 - Loss: 28.0620\n",
      "Processing batch 3618/11884 - Loss: 30.2565\n",
      "Processing batch 3619/11884 - Loss: 29.2411\n",
      "Processing batch 3620/11884 - Loss: 29.2288\n",
      "Processing batch 3621/11884 - Loss: 29.5229\n",
      "Processing batch 3622/11884 - Loss: 29.0954\n",
      "Processing batch 3623/11884 - Loss: 29.4040\n",
      "Processing batch 3624/11884 - Loss: 30.4838\n",
      "Processing batch 3625/11884 - Loss: 29.0649\n",
      "Processing batch 3626/11884 - Loss: 30.9004\n",
      "Processing batch 3627/11884 - Loss: 29.3748\n",
      "Processing batch 3628/11884 - Loss: 30.1774\n",
      "Processing batch 3629/11884 - Loss: 30.5465\n",
      "Processing batch 3630/11884 - Loss: 27.8584\n",
      "Processing batch 3631/11884 - Loss: 29.7371\n",
      "Processing batch 3632/11884 - Loss: 30.2327\n",
      "Processing batch 3633/11884 - Loss: 29.5200\n",
      "Processing batch 3634/11884 - Loss: 29.9626\n",
      "Processing batch 3635/11884 - Loss: 29.8993\n",
      "Processing batch 3636/11884 - Loss: 30.3617\n",
      "Processing batch 3637/11884 - Loss: 29.7355\n",
      "Processing batch 3638/11884 - Loss: 29.6690\n",
      "Processing batch 3639/11884 - Loss: 29.7448\n",
      "Processing batch 3640/11884 - Loss: 29.2483\n",
      "Processing batch 3641/11884 - Loss: 29.2706\n",
      "Processing batch 3642/11884 - Loss: 30.7028\n",
      "Processing batch 3643/11884 - Loss: 28.7944\n",
      "Processing batch 3644/11884 - Loss: 28.8016\n",
      "Processing batch 3645/11884 - Loss: 30.0393\n",
      "Processing batch 3646/11884 - Loss: 29.3019\n",
      "Processing batch 3647/11884 - Loss: 29.1489\n",
      "Processing batch 3648/11884 - Loss: 28.1758\n",
      "Processing batch 3649/11884 - Loss: 30.1419\n",
      "Processing batch 3650/11884 - Loss: 29.9415\n",
      "Processing batch 3651/11884 - Loss: 29.0032\n",
      "Processing batch 3652/11884 - Loss: 30.1096\n",
      "Processing batch 3653/11884 - Loss: 30.6406\n",
      "Processing batch 3654/11884 - Loss: 28.4682\n",
      "Processing batch 3655/11884 - Loss: 29.1019\n",
      "Processing batch 3656/11884 - Loss: 30.3547\n",
      "Processing batch 3657/11884 - Loss: 30.3890\n",
      "Processing batch 3658/11884 - Loss: 29.6431\n",
      "Processing batch 3659/11884 - Loss: 28.1100\n",
      "Processing batch 3660/11884 - Loss: 30.5052\n",
      "Processing batch 3661/11884 - Loss: 30.5473\n",
      "Processing batch 3662/11884 - Loss: 29.8189\n",
      "Processing batch 3663/11884 - Loss: 29.3053\n",
      "Processing batch 3664/11884 - Loss: 29.6436\n",
      "Processing batch 3665/11884 - Loss: 28.4063\n",
      "Processing batch 3666/11884 - Loss: 31.1337\n",
      "Processing batch 3667/11884 - Loss: 29.2236\n",
      "Processing batch 3668/11884 - Loss: 30.2963\n",
      "Processing batch 3669/11884 - Loss: 30.2557\n",
      "Processing batch 3670/11884 - Loss: 30.2101\n",
      "Processing batch 3671/11884 - Loss: 29.4028\n",
      "Processing batch 3672/11884 - Loss: 31.1812\n",
      "Processing batch 3673/11884 - Loss: 30.2360\n",
      "Processing batch 3674/11884 - Loss: 29.0423\n",
      "Processing batch 3675/11884 - Loss: 29.7199\n",
      "Processing batch 3676/11884 - Loss: 30.0678\n",
      "Processing batch 3677/11884 - Loss: 30.5606\n",
      "Processing batch 3678/11884 - Loss: 29.0145\n",
      "Processing batch 3679/11884 - Loss: 28.6841\n",
      "Processing batch 3680/11884 - Loss: 28.8976\n",
      "Processing batch 3681/11884 - Loss: 29.8594\n",
      "Processing batch 3682/11884 - Loss: 32.0391\n",
      "Processing batch 3683/11884 - Loss: 29.7822\n",
      "Processing batch 3684/11884 - Loss: 30.7072\n",
      "Processing batch 3685/11884 - Loss: 30.7224\n",
      "Processing batch 3686/11884 - Loss: 30.0899\n",
      "Processing batch 3687/11884 - Loss: 26.9518\n",
      "Processing batch 3688/11884 - Loss: 28.4807\n",
      "Processing batch 3689/11884 - Loss: 29.8434\n",
      "Processing batch 3690/11884 - Loss: 29.8773\n",
      "Processing batch 3691/11884 - Loss: 30.2975\n",
      "Processing batch 3692/11884 - Loss: 29.9906\n",
      "Processing batch 3693/11884 - Loss: 29.0040\n",
      "Processing batch 3694/11884 - Loss: 30.1327\n",
      "Processing batch 3695/11884 - Loss: 29.8722\n",
      "Processing batch 3696/11884 - Loss: 31.3681\n",
      "Processing batch 3697/11884 - Loss: 29.4340\n",
      "Processing batch 3698/11884 - Loss: 29.4269\n",
      "Processing batch 3699/11884 - Loss: 28.2809\n",
      "Processing batch 3700/11884 - Loss: 28.7392\n",
      "Processing batch 3701/11884 - Loss: 30.1280\n",
      "Processing batch 3702/11884 - Loss: 28.6602\n",
      "Processing batch 3703/11884 - Loss: 30.4918\n",
      "Processing batch 3704/11884 - Loss: 29.6559\n",
      "Processing batch 3705/11884 - Loss: 27.8807\n",
      "Processing batch 3706/11884 - Loss: 28.4812\n",
      "Processing batch 3707/11884 - Loss: 29.0303\n",
      "Processing batch 3708/11884 - Loss: 31.3413\n",
      "Processing batch 3709/11884 - Loss: 29.9765\n",
      "Processing batch 3710/11884 - Loss: 28.8937\n",
      "Processing batch 3711/11884 - Loss: 31.2008\n",
      "Processing batch 3712/11884 - Loss: 29.9415\n",
      "Processing batch 3713/11884 - Loss: 28.8068\n",
      "Processing batch 3714/11884 - Loss: 28.8681\n",
      "Processing batch 3715/11884 - Loss: 29.6925\n",
      "Processing batch 3716/11884 - Loss: 29.9649\n",
      "Processing batch 3717/11884 - Loss: 29.5004\n",
      "Processing batch 3718/11884 - Loss: 30.0550\n",
      "Processing batch 3719/11884 - Loss: 29.9396\n",
      "Processing batch 3720/11884 - Loss: 30.3571\n",
      "Processing batch 3721/11884 - Loss: 30.0985\n",
      "Processing batch 3722/11884 - Loss: 30.6102\n",
      "Processing batch 3723/11884 - Loss: 29.7092\n",
      "Processing batch 3724/11884 - Loss: 28.6966\n",
      "Processing batch 3725/11884 - Loss: 30.0503\n",
      "Processing batch 3726/11884 - Loss: 30.4473\n",
      "Processing batch 3727/11884 - Loss: 30.2058\n",
      "Processing batch 3728/11884 - Loss: 30.7611\n",
      "Processing batch 3729/11884 - Loss: 29.3349\n",
      "Processing batch 3730/11884 - Loss: 30.5560\n",
      "Processing batch 3731/11884 - Loss: 30.6347\n",
      "Processing batch 3732/11884 - Loss: 28.7702\n",
      "Processing batch 3733/11884 - Loss: 29.3557\n",
      "Processing batch 3734/11884 - Loss: 30.9179\n",
      "Processing batch 3735/11884 - Loss: 29.8994\n",
      "Processing batch 3736/11884 - Loss: 30.6841\n",
      "Processing batch 3737/11884 - Loss: 28.3792\n",
      "Processing batch 3738/11884 - Loss: 28.8008\n",
      "Processing batch 3739/11884 - Loss: 29.5391\n",
      "Processing batch 3740/11884 - Loss: 27.9376\n",
      "Processing batch 3741/11884 - Loss: 29.1260\n",
      "Processing batch 3742/11884 - Loss: 30.7814\n",
      "Processing batch 3743/11884 - Loss: 28.7179\n",
      "Processing batch 3744/11884 - Loss: 30.0626\n",
      "Processing batch 3745/11884 - Loss: 30.5589\n",
      "Processing batch 3746/11884 - Loss: 29.9026\n",
      "Processing batch 3747/11884 - Loss: 30.5975\n",
      "Processing batch 3748/11884 - Loss: 29.8867\n",
      "Processing batch 3749/11884 - Loss: 29.1216\n",
      "Processing batch 3750/11884 - Loss: 30.3259\n",
      "Processing batch 3751/11884 - Loss: 30.2547\n",
      "Processing batch 3752/11884 - Loss: 30.6713\n",
      "Processing batch 3753/11884 - Loss: 30.1655\n",
      "Processing batch 3754/11884 - Loss: 29.6831\n",
      "Processing batch 3755/11884 - Loss: 30.1027\n",
      "Processing batch 3756/11884 - Loss: 30.2055\n",
      "Processing batch 3757/11884 - Loss: 30.3480\n",
      "Processing batch 3758/11884 - Loss: 28.9654\n",
      "Processing batch 3759/11884 - Loss: 29.7685\n",
      "Processing batch 3760/11884 - Loss: 28.7237\n",
      "Processing batch 3761/11884 - Loss: 30.3165\n",
      "Processing batch 3762/11884 - Loss: 29.9816\n",
      "Processing batch 3763/11884 - Loss: 29.2183\n",
      "Processing batch 3764/11884 - Loss: 29.5815\n",
      "Processing batch 3765/11884 - Loss: 29.6659\n",
      "Processing batch 3766/11884 - Loss: 29.6294\n",
      "Processing batch 3767/11884 - Loss: 29.3852\n",
      "Processing batch 3768/11884 - Loss: 31.3128\n",
      "Processing batch 3769/11884 - Loss: 29.6676\n",
      "Processing batch 3770/11884 - Loss: 29.6313\n",
      "Processing batch 3771/11884 - Loss: 30.4977\n",
      "Processing batch 3772/11884 - Loss: 29.3155\n",
      "Processing batch 3773/11884 - Loss: 30.5258\n",
      "Processing batch 3774/11884 - Loss: 29.6229\n",
      "Processing batch 3775/11884 - Loss: 30.6150\n",
      "Processing batch 3776/11884 - Loss: 29.2886\n",
      "Processing batch 3777/11884 - Loss: 30.0879\n",
      "Processing batch 3778/11884 - Loss: 29.5067\n",
      "Processing batch 3779/11884 - Loss: 30.1977\n",
      "Processing batch 3780/11884 - Loss: 29.2065\n",
      "Processing batch 3781/11884 - Loss: 29.8946\n",
      "Processing batch 3782/11884 - Loss: 29.9816\n",
      "Processing batch 3783/11884 - Loss: 28.9744\n",
      "Processing batch 3784/11884 - Loss: 29.5828\n",
      "Processing batch 3785/11884 - Loss: 30.0848\n",
      "Processing batch 3786/11884 - Loss: 30.6198\n",
      "Processing batch 3787/11884 - Loss: 29.6538\n",
      "Processing batch 3788/11884 - Loss: 29.5264\n",
      "Processing batch 3789/11884 - Loss: 29.9563\n",
      "Processing batch 3790/11884 - Loss: 30.0221\n",
      "Processing batch 3791/11884 - Loss: 29.1621\n",
      "Processing batch 3792/11884 - Loss: 30.0765\n",
      "Processing batch 3793/11884 - Loss: 28.5549\n",
      "Processing batch 3794/11884 - Loss: 29.8028\n",
      "Processing batch 3795/11884 - Loss: 29.2619\n",
      "Processing batch 3796/11884 - Loss: 29.3866\n",
      "Processing batch 3797/11884 - Loss: 30.1926\n",
      "Processing batch 3798/11884 - Loss: 29.6480\n",
      "Processing batch 3799/11884 - Loss: 28.6042\n",
      "Processing batch 3800/11884 - Loss: 30.3890\n",
      "Processing batch 3801/11884 - Loss: 30.1918\n",
      "Processing batch 3802/11884 - Loss: 30.4403\n",
      "Processing batch 3803/11884 - Loss: 30.3160\n",
      "Processing batch 3804/11884 - Loss: 30.4460\n",
      "Processing batch 3805/11884 - Loss: 29.6585\n",
      "Processing batch 3806/11884 - Loss: 29.4976\n",
      "Processing batch 3807/11884 - Loss: 29.5341\n",
      "Processing batch 3808/11884 - Loss: 29.1065\n",
      "Processing batch 3809/11884 - Loss: 30.1385\n",
      "Processing batch 3810/11884 - Loss: 29.0966\n",
      "Processing batch 3811/11884 - Loss: 28.8721\n",
      "Processing batch 3812/11884 - Loss: 30.3883\n",
      "Processing batch 3813/11884 - Loss: 30.5260\n",
      "Processing batch 3814/11884 - Loss: 29.2555\n",
      "Processing batch 3815/11884 - Loss: 29.8704\n",
      "Processing batch 3816/11884 - Loss: 28.9983\n",
      "Processing batch 3817/11884 - Loss: 28.8891\n",
      "Processing batch 3818/11884 - Loss: 29.8254\n",
      "Processing batch 3819/11884 - Loss: 29.4090\n",
      "Processing batch 3820/11884 - Loss: 29.4044\n",
      "Processing batch 3821/11884 - Loss: 31.2130\n",
      "Processing batch 3822/11884 - Loss: 30.0022\n",
      "Processing batch 3823/11884 - Loss: 31.0565\n",
      "Processing batch 3824/11884 - Loss: 30.9907\n",
      "Processing batch 3825/11884 - Loss: 29.4056\n",
      "Processing batch 3826/11884 - Loss: 30.2275\n",
      "Processing batch 3827/11884 - Loss: 30.6567\n",
      "Processing batch 3828/11884 - Loss: 30.7016\n",
      "Processing batch 3829/11884 - Loss: 30.4224\n",
      "Processing batch 3830/11884 - Loss: 29.4840\n",
      "Processing batch 3831/11884 - Loss: 30.5906\n",
      "Processing batch 3832/11884 - Loss: 27.1735\n",
      "Processing batch 3833/11884 - Loss: 30.6885\n",
      "Processing batch 3834/11884 - Loss: 30.6148\n",
      "Processing batch 3835/11884 - Loss: 31.3667\n",
      "Processing batch 3836/11884 - Loss: 27.9375\n",
      "Processing batch 3837/11884 - Loss: 29.2063\n",
      "Processing batch 3838/11884 - Loss: 29.5076\n",
      "Processing batch 3839/11884 - Loss: 30.9100\n",
      "Processing batch 3840/11884 - Loss: 28.8391\n",
      "Processing batch 3841/11884 - Loss: 28.4511\n",
      "Processing batch 3842/11884 - Loss: 30.3122\n",
      "Processing batch 3843/11884 - Loss: 29.6729\n",
      "Processing batch 3844/11884 - Loss: 29.3223\n",
      "Processing batch 3845/11884 - Loss: 30.4012\n",
      "Processing batch 3846/11884 - Loss: 30.6364\n",
      "Processing batch 3847/11884 - Loss: 29.0194\n",
      "Processing batch 3848/11884 - Loss: 28.6267\n",
      "Processing batch 3849/11884 - Loss: 28.6730\n",
      "Processing batch 3850/11884 - Loss: 30.3149\n",
      "Processing batch 3851/11884 - Loss: 29.6257\n",
      "Processing batch 3852/11884 - Loss: 29.2784\n",
      "Processing batch 3853/11884 - Loss: 30.4010\n",
      "Processing batch 3854/11884 - Loss: 28.4116\n",
      "Processing batch 3855/11884 - Loss: 31.0505\n",
      "Processing batch 3856/11884 - Loss: 29.3787\n",
      "Processing batch 3857/11884 - Loss: 30.3391\n",
      "Processing batch 3858/11884 - Loss: 30.5193\n",
      "Processing batch 3859/11884 - Loss: 28.1257\n",
      "Processing batch 3860/11884 - Loss: 29.2119\n",
      "Processing batch 3861/11884 - Loss: 29.6858\n",
      "Processing batch 3862/11884 - Loss: 30.1708\n",
      "Processing batch 3863/11884 - Loss: 29.9549\n",
      "Processing batch 3864/11884 - Loss: 29.3737\n",
      "Processing batch 3865/11884 - Loss: 29.9162\n",
      "Processing batch 3866/11884 - Loss: 29.8049\n",
      "Processing batch 3867/11884 - Loss: 28.9323\n",
      "Processing batch 3868/11884 - Loss: 31.2190\n",
      "Processing batch 3869/11884 - Loss: 29.4029\n",
      "Processing batch 3870/11884 - Loss: 30.1776\n",
      "Processing batch 3871/11884 - Loss: 29.6753\n",
      "Processing batch 3872/11884 - Loss: 30.0992\n",
      "Processing batch 3873/11884 - Loss: 29.9850\n",
      "Processing batch 3874/11884 - Loss: 29.9158\n",
      "Processing batch 3875/11884 - Loss: 30.5365\n",
      "Processing batch 3876/11884 - Loss: 29.8932\n",
      "Processing batch 3877/11884 - Loss: 29.4917\n",
      "Processing batch 3878/11884 - Loss: 29.8004\n",
      "Processing batch 3879/11884 - Loss: 30.1908\n",
      "Processing batch 3880/11884 - Loss: 29.1973\n",
      "Processing batch 3881/11884 - Loss: 28.7827\n",
      "Processing batch 3882/11884 - Loss: 28.5326\n",
      "Processing batch 3883/11884 - Loss: 30.0360\n",
      "Processing batch 3884/11884 - Loss: 29.3706\n",
      "Processing batch 3885/11884 - Loss: 29.4324\n",
      "Processing batch 3886/11884 - Loss: 30.2468\n",
      "Processing batch 3887/11884 - Loss: 30.9346\n",
      "Processing batch 3888/11884 - Loss: 30.1461\n",
      "Processing batch 3889/11884 - Loss: 30.8129\n",
      "Processing batch 3890/11884 - Loss: 29.9724\n",
      "Processing batch 3891/11884 - Loss: 30.3074\n",
      "Processing batch 3892/11884 - Loss: 29.6845\n",
      "Processing batch 3893/11884 - Loss: 29.1378\n",
      "Processing batch 3894/11884 - Loss: 29.0255\n",
      "Processing batch 3895/11884 - Loss: 30.6275\n",
      "Processing batch 3896/11884 - Loss: 29.5626\n",
      "Processing batch 3897/11884 - Loss: 30.1383\n",
      "Processing batch 3898/11884 - Loss: 29.7111\n",
      "Processing batch 3899/11884 - Loss: 28.6794\n",
      "Processing batch 3900/11884 - Loss: 29.9437\n",
      "Processing batch 3901/11884 - Loss: 29.1294\n",
      "Processing batch 3902/11884 - Loss: 31.0462\n",
      "Processing batch 3903/11884 - Loss: 29.2666\n",
      "Processing batch 3904/11884 - Loss: 28.8620\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.00001\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[31], line 10\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, val_loader, optimizer, epochs, device, scheduler)\u001B[0m\n\u001B[1;32m      8\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m---> 10\u001B[0m     train_loss, val_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m scheduler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     12\u001B[0m         scheduler\u001B[38;5;241m.\u001B[39mstep()\n",
      "Cell \u001B[0;32mIn[30], line 15\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(model, optimizer, train_loader, val_loader, device)\u001B[0m\n\u001B[1;32m     13\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     14\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 15\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     17\u001B[0m val_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:21:00.698490Z",
     "start_time": "2025-08-06T22:21:00.682509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "games_review_count = train_reviews_df.group_by(\"game_index\").len().filter(pl.col(\"len\") > 100).sort(\"len\", descending=True)\n",
    "games_review_count"
   ],
   "id": "1bdb28dd00c4500a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (2_256, 2)\n",
       "┌────────────┬───────┐\n",
       "│ game_index ┆ len   │\n",
       "│ ---        ┆ ---   │\n",
       "│ i64        ┆ u32   │\n",
       "╞════════════╪═══════╡\n",
       "│ 79         ┆ 99574 │\n",
       "│ 1952       ┆ 66802 │\n",
       "│ 1087       ┆ 66699 │\n",
       "│ 1593       ┆ 59376 │\n",
       "│ 1933       ┆ 48873 │\n",
       "│ …          ┆ …     │\n",
       "│ 2660       ┆ 101   │\n",
       "│ 6099       ┆ 101   │\n",
       "│ 1654       ┆ 101   │\n",
       "│ 1881       ┆ 101   │\n",
       "│ 3897       ┆ 101   │\n",
       "└────────────┴───────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2_256, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>game_index</th><th>len</th></tr><tr><td>i64</td><td>u32</td></tr></thead><tbody><tr><td>79</td><td>99574</td></tr><tr><td>1952</td><td>66802</td></tr><tr><td>1087</td><td>66699</td></tr><tr><td>1593</td><td>59376</td></tr><tr><td>1933</td><td>48873</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2660</td><td>101</td></tr><tr><td>6099</td><td>101</td></tr><tr><td>1654</td><td>101</td></tr><tr><td>1881</td><td>101</td></tr><tr><td>3897</td><td>101</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:27:59.449736Z",
     "start_time": "2025-08-06T22:27:59.388374Z"
    }
   },
   "cell_type": "code",
   "source": "train_rev_filtered = train_reviews_df.join(games_review_count, on=\"game_index\", how=\"inner\")",
   "id": "52a816e124b3617f",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:28:06.940179Z",
     "start_time": "2025-08-06T22:28:06.936678Z"
    }
   },
   "cell_type": "code",
   "source": "train_rev_filtered",
   "id": "8455aad2371e75a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (3_423_046, 12)\n",
       "┌───────────┬────────────┬────────────┬────────────┬───┬───────────┬───────────┬───────────┬───────┐\n",
       "│ review_id ┆ user_index ┆ game_index ┆ review     ┆ … ┆ comment_c ┆ timestamp ┆ scrape_da ┆ len   │\n",
       "│ ---       ┆ ---        ┆ ---        ┆ ---        ┆   ┆ ount      ┆ _created  ┆ te        ┆ ---   │\n",
       "│ i64       ┆ i64        ┆ i64        ┆ str        ┆   ┆ ---       ┆ ---       ┆ ---       ┆ u32   │\n",
       "│           ┆            ┆            ┆            ┆   ┆ i64       ┆ datetime[ ┆ date      ┆       │\n",
       "│           ┆            ┆            ┆            ┆   ┆           ┆ μs, Ameri ┆           ┆       │\n",
       "│           ┆            ┆            ┆            ┆   ┆           ┆ ca/Lima]  ┆           ┆       │\n",
       "╞═══════════╪════════════╪════════════╪════════════╪═══╪═══════════╪═══════════╪═══════════╪═══════╡\n",
       "│ 8387507   ┆ 245760     ┆ 1593       ┆ fun game   ┆ … ┆ 2         ┆ 2013-12-2 ┆ 2025-06-1 ┆ 59376 │\n",
       "│           ┆            ┆            ┆            ┆   ┆           ┆ 9         ┆ 2         ┆       │\n",
       "│           ┆            ┆            ┆            ┆   ┆           ┆ 00:19:25  ┆           ┆       │\n",
       "│           ┆            ┆            ┆            ┆   ┆           ┆ -05       ┆           ┆       │\n",
       "│ 12995168  ┆ 245762     ┆ 1567       ┆ Created by ┆ … ┆ 0         ┆ 2014-11-1 ┆ 2025-06-1 ┆ 374   │\n",
       "│           ┆            ┆            ┆ Daedalic   ┆   ┆           ┆ 8         ┆ 2         ┆       │\n",
       "│           ┆            ┆            ┆ Entertainm ┆   ┆           ┆ 12:03:31  ┆           ┆       │\n",
       "│           ┆            ┆            ┆ …          ┆   ┆           ┆ -05       ┆           ┆       │\n",
       "│ 8387526   ┆ 245763     ┆ 1413       ┆ You will   ┆ … ┆ 0         ┆ 2013-12-2 ┆ 2025-06-1 ┆ 13682 │\n",
       "│           ┆            ┆            ┆ never know ┆   ┆           ┆ 9         ┆ 2         ┆       │\n",
       "│           ┆            ┆            ┆ the joy    ┆   ┆           ┆ 00:20:24  ┆           ┆       │\n",
       "│           ┆            ┆            ┆ of…        ┆   ┆           ┆ -05       ┆           ┆       │\n",
       "│ 8387531   ┆ 245764     ┆ 79         ┆ This is an ┆ … ┆ 0         ┆ 2013-12-2 ┆ 2025-06-1 ┆ 99574 │\n",
       "│           ┆            ┆            ┆ awesome    ┆   ┆           ┆ 9         ┆ 2         ┆       │\n",
       "│           ┆            ┆            ┆ game to    ┆   ┆           ┆ 00:20:41  ┆           ┆       │\n",
       "│           ┆            ┆            ┆ get…       ┆   ┆           ┆ -05       ┆           ┆       │\n",
       "│ 8387544   ┆ 245767     ┆ 402        ┆ Just       ┆ … ┆ 0         ┆ 2013-12-2 ┆ 2025-06-1 ┆ 3579  │\n",
       "│           ┆            ┆            ┆ fantastic  ┆   ┆           ┆ 9         ┆ 2         ┆       │\n",
       "│           ┆            ┆            ┆ of a game  ┆   ┆           ┆ 00:21:07  ┆           ┆       │\n",
       "│           ┆            ┆            ┆ one o…     ┆   ┆           ┆ -05       ┆           ┆       │\n",
       "│ …         ┆ …          ┆ …          ┆ …          ┆ … ┆ …         ┆ …         ┆ …         ┆ …     │\n",
       "│ 18782168  ┆ 207738     ┆ 882        ┆ Great      ┆ … ┆ 0         ┆ 2015-10-3 ┆ 2025-06-1 ┆ 1952  │\n",
       "│           ┆            ┆            ┆ Game,      ┆   ┆           ┆ 0         ┆ 2         ┆       │\n",
       "│           ┆            ┆            ┆ campaign   ┆   ┆           ┆ 21:32:00  ┆           ┆       │\n",
       "│           ┆            ┆            ┆ needed be… ┆   ┆           ┆ -05       ┆           ┆       │\n",
       "│ 16945878  ┆ 207738     ┆ 1087       ┆ Had a lot  ┆ … ┆ 0         ┆ 2015-07-0 ┆ 2025-06-1 ┆ 66699 │\n",
       "│           ┆            ┆            ┆ of fun. :) ┆   ┆           ┆ 8         ┆ 2         ┆       │\n",
       "│           ┆            ┆            ┆            ┆   ┆           ┆ 12:52:31  ┆           ┆       │\n",
       "│           ┆            ┆            ┆            ┆   ┆           ┆ -05       ┆           ┆       │\n",
       "│ 20592790  ┆ 207738     ┆ 2918       ┆ Yeah Its   ┆ … ┆ 0         ┆ 2016-01-1 ┆ 2025-06-1 ┆ 347   │\n",
       "│           ┆            ┆            ┆ pretty     ┆   ┆           ┆ 7         ┆ 2         ┆       │\n",
       "│           ┆            ┆            ┆ good.      ┆   ┆           ┆ 15:46:49  ┆           ┆       │\n",
       "│           ┆            ┆            ┆            ┆   ┆           ┆ -05       ┆           ┆       │\n",
       "│ 19924464  ┆ 207738     ┆ 311        ┆ Lots of    ┆ … ┆ 0         ┆ 2015-12-2 ┆ 2025-06-1 ┆ 4159  │\n",
       "│           ┆            ┆            ┆ fun,       ┆   ┆           ┆ 4         ┆ 2         ┆       │\n",
       "│           ┆            ┆            ┆ Tribal age ┆   ┆           ┆ 20:59:51  ┆           ┆       │\n",
       "│           ┆            ┆            ┆ was th…    ┆   ┆           ┆ -05       ┆           ┆       │\n",
       "│ 19637744  ┆ 207738     ┆ 1739       ┆ A smashing ┆ … ┆ 0         ┆ 2015-12-1 ┆ 2025-06-1 ┆ 8623  │\n",
       "│           ┆            ┆            ┆ Great      ┆   ┆           ┆ 1         ┆ 2         ┆       │\n",
       "│           ┆            ┆            ┆ read, I    ┆   ┆           ┆ 15:40:19  ┆           ┆       │\n",
       "│           ┆            ┆            ┆ actua…     ┆   ┆           ┆ -05       ┆           ┆       │\n",
       "└───────────┴────────────┴────────────┴────────────┴───┴───────────┴───────────┴───────────┴───────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3_423_046, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>review_id</th><th>user_index</th><th>game_index</th><th>review</th><th>written_during_early_access</th><th>voted_up</th><th>weighted_vote_score</th><th>votes_up</th><th>comment_count</th><th>timestamp_created</th><th>scrape_date</th><th>len</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>bool</td><td>bool</td><td>f64</td><td>i64</td><td>i64</td><td>datetime[μs, America/Lima]</td><td>date</td><td>u32</td></tr></thead><tbody><tr><td>8387507</td><td>245760</td><td>1593</td><td>&quot;fun game&quot;</td><td>false</td><td>true</td><td>0.5</td><td>0</td><td>2</td><td>2013-12-29 00:19:25 -05</td><td>2025-06-12</td><td>59376</td></tr><tr><td>12995168</td><td>245762</td><td>1567</td><td>&quot;Created by Daedalic Entertainm…</td><td>false</td><td>true</td><td>0.612418</td><td>11</td><td>0</td><td>2014-11-18 12:03:31 -05</td><td>2025-06-12</td><td>374</td></tr><tr><td>8387526</td><td>245763</td><td>1413</td><td>&quot;You will never know the joy of…</td><td>false</td><td>true</td><td>0.521739</td><td>1</td><td>0</td><td>2013-12-29 00:20:24 -05</td><td>2025-06-12</td><td>13682</td></tr><tr><td>8387531</td><td>245764</td><td>79</td><td>&quot;This is an awesome game to get…</td><td>false</td><td>true</td><td>0.522059</td><td>1</td><td>0</td><td>2013-12-29 00:20:41 -05</td><td>2025-06-12</td><td>99574</td></tr><tr><td>8387544</td><td>245767</td><td>402</td><td>&quot;Just fantastic of a game one o…</td><td>false</td><td>true</td><td>0.5</td><td>0</td><td>0</td><td>2013-12-29 00:21:07 -05</td><td>2025-06-12</td><td>3579</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>18782168</td><td>207738</td><td>882</td><td>&quot;Great Game, campaign needed be…</td><td>false</td><td>true</td><td>0.472441</td><td>0</td><td>0</td><td>2015-10-30 21:32:00 -05</td><td>2025-06-12</td><td>1952</td></tr><tr><td>16945878</td><td>207738</td><td>1087</td><td>&quot;Had a lot of fun. :)&quot;</td><td>false</td><td>true</td><td>0.5</td><td>0</td><td>0</td><td>2015-07-08 12:52:31 -05</td><td>2025-06-12</td><td>66699</td></tr><tr><td>20592790</td><td>207738</td><td>2918</td><td>&quot;Yeah Its pretty good.&quot;</td><td>false</td><td>true</td><td>0.472441</td><td>0</td><td>0</td><td>2016-01-17 15:46:49 -05</td><td>2025-06-12</td><td>347</td></tr><tr><td>19924464</td><td>207738</td><td>311</td><td>&quot;Lots of fun, Tribal age was th…</td><td>false</td><td>true</td><td>0.472441</td><td>0</td><td>0</td><td>2015-12-24 20:59:51 -05</td><td>2025-06-12</td><td>4159</td></tr><tr><td>19637744</td><td>207738</td><td>1739</td><td>&quot;A smashing Great read, I actua…</td><td>false</td><td>true</td><td>0.472441</td><td>0</td><td>0</td><td>2015-12-11 15:40:19 -05</td><td>2025-06-12</td><td>8623</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:21:34.930685Z",
     "start_time": "2025-08-06T22:21:34.853684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "users_reviews_count = train_reviews_df.group_by(\"user_index\").len().filter(pl.col(\"len\") > 10).sort(\"len\", descending=True)\n",
    "users_reviews_count"
   ],
   "id": "4daabd63edb2edde",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (26_868, 2)\n",
       "┌────────────┬─────┐\n",
       "│ user_index ┆ len │\n",
       "│ ---        ┆ --- │\n",
       "│ i64        ┆ u32 │\n",
       "╞════════════╪═════╡\n",
       "│ 14466      ┆ 401 │\n",
       "│ 638491     ┆ 338 │\n",
       "│ 297975     ┆ 307 │\n",
       "│ 1414       ┆ 286 │\n",
       "│ 29917      ┆ 282 │\n",
       "│ …          ┆ …   │\n",
       "│ 379586     ┆ 11  │\n",
       "│ 61082      ┆ 11  │\n",
       "│ 882670     ┆ 11  │\n",
       "│ 439495     ┆ 11  │\n",
       "│ 579752     ┆ 11  │\n",
       "└────────────┴─────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (26_868, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_index</th><th>len</th></tr><tr><td>i64</td><td>u32</td></tr></thead><tbody><tr><td>14466</td><td>401</td></tr><tr><td>638491</td><td>338</td></tr><tr><td>297975</td><td>307</td></tr><tr><td>1414</td><td>286</td></tr><tr><td>29917</td><td>282</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>379586</td><td>11</td></tr><tr><td>61082</td><td>11</td></tr><tr><td>882670</td><td>11</td></tr><tr><td>439495</td><td>11</td></tr><tr><td>579752</td><td>11</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:35:38.269489Z",
     "start_time": "2025-08-06T22:35:38.245713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_rev_filtered2 = train_rev_filtered.join(users_reviews_count, on=\"user_index\", how=\"inner\")\n",
    "train_rev_filtered2"
   ],
   "id": "cc7c3c41541f5604",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (476_913, 13)\n",
       "┌───────────┬────────────┬────────────┬────────────┬───┬───────────┬───────────┬───────┬───────────┐\n",
       "│ review_id ┆ user_index ┆ game_index ┆ review     ┆ … ┆ timestamp ┆ scrape_da ┆ len   ┆ len_right │\n",
       "│ ---       ┆ ---        ┆ ---        ┆ ---        ┆   ┆ _created  ┆ te        ┆ ---   ┆ ---       │\n",
       "│ i64       ┆ i64        ┆ i64        ┆ str        ┆   ┆ ---       ┆ ---       ┆ u32   ┆ u32       │\n",
       "│           ┆            ┆            ┆            ┆   ┆ datetime[ ┆ date      ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ μs, Ameri ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ ca/Lima]  ┆           ┆       ┆           │\n",
       "╞═══════════╪════════════╪════════════╪════════════╪═══╪═══════════╪═══════════╪═══════╪═══════════╡\n",
       "│ 9060036   ┆ 245795     ┆ 649        ┆ snorefest  ┆ … ┆ 2014-02-1 ┆ 2025-06-1 ┆ 10994 ┆ 31        │\n",
       "│           ┆            ┆            ┆ twink      ┆   ┆ 1         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ trainer    ┆   ┆ 21:25:38  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 8614221   ┆ 245918     ┆ 176        ┆ was my     ┆ … ┆ 2014-01-0 ┆ 2025-06-1 ┆ 4357  ┆ 11        │\n",
       "│           ┆            ┆            ┆ first      ┆   ┆ 7         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ purchase   ┆   ┆ 21:43:10  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆ on steam…  ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 11951089  ┆ 245996     ┆ 130        ┆ amazing    ┆ … ┆ 2014-09-0 ┆ 2025-06-1 ┆ 121   ┆ 13        │\n",
       "│           ┆            ┆            ┆ game,      ┆   ┆ 1         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ 10/10      ┆   ┆ 15:31:44  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 23877431  ┆ 246073     ┆ 5952       ┆ I am sure  ┆ … ┆ 2016-06-2 ┆ 2025-06-1 ┆ 379   ┆ 64        │\n",
       "│           ┆            ┆            ┆ it is fine ┆   ┆ 7         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ for some … ┆   ┆ 18:43:35  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 14714223  ┆ 246094     ┆ 1522       ┆ Inspired   ┆ … ┆ 2015-03-0 ┆ 2025-06-1 ┆ 5429  ┆ 20        │\n",
       "│           ┆            ┆            ┆ me to      ┆   ┆ 1         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ become a   ┆   ┆ 14:04:13  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆ surgeo…    ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ …         ┆ …          ┆ …          ┆ …          ┆ … ┆ …         ┆ …         ┆ …     ┆ …         │\n",
       "│ 18782168  ┆ 207738     ┆ 882        ┆ Great      ┆ … ┆ 2015-10-3 ┆ 2025-06-1 ┆ 1952  ┆ 51        │\n",
       "│           ┆            ┆            ┆ Game,      ┆   ┆ 0         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ campaign   ┆   ┆ 21:32:00  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆ needed be… ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 16945878  ┆ 207738     ┆ 1087       ┆ Had a lot  ┆ … ┆ 2015-07-0 ┆ 2025-06-1 ┆ 66699 ┆ 51        │\n",
       "│           ┆            ┆            ┆ of fun. :) ┆   ┆ 8         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ 12:52:31  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 20592790  ┆ 207738     ┆ 2918       ┆ Yeah Its   ┆ … ┆ 2016-01-1 ┆ 2025-06-1 ┆ 347   ┆ 51        │\n",
       "│           ┆            ┆            ┆ pretty     ┆   ┆ 7         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ good.      ┆   ┆ 15:46:49  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 19924464  ┆ 207738     ┆ 311        ┆ Lots of    ┆ … ┆ 2015-12-2 ┆ 2025-06-1 ┆ 4159  ┆ 51        │\n",
       "│           ┆            ┆            ┆ fun,       ┆   ┆ 4         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ Tribal age ┆   ┆ 20:59:51  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆ was th…    ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 19637744  ┆ 207738     ┆ 1739       ┆ A smashing ┆ … ┆ 2015-12-1 ┆ 2025-06-1 ┆ 8623  ┆ 51        │\n",
       "│           ┆            ┆            ┆ Great      ┆   ┆ 1         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ read, I    ┆   ┆ 15:40:19  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆ actua…     ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "└───────────┴────────────┴────────────┴────────────┴───┴───────────┴───────────┴───────┴───────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (476_913, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>review_id</th><th>user_index</th><th>game_index</th><th>review</th><th>written_during_early_access</th><th>voted_up</th><th>weighted_vote_score</th><th>votes_up</th><th>comment_count</th><th>timestamp_created</th><th>scrape_date</th><th>len</th><th>len_right</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>bool</td><td>bool</td><td>f64</td><td>i64</td><td>i64</td><td>datetime[μs, America/Lima]</td><td>date</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>9060036</td><td>245795</td><td>649</td><td>&quot;snorefest twink trainer&quot;</td><td>false</td><td>false</td><td>0.426997</td><td>3</td><td>0</td><td>2014-02-11 21:25:38 -05</td><td>2025-06-12</td><td>10994</td><td>31</td></tr><tr><td>8614221</td><td>245918</td><td>176</td><td>&quot;was my first purchase on steam…</td><td>false</td><td>true</td><td>0.5</td><td>0</td><td>0</td><td>2014-01-07 21:43:10 -05</td><td>2025-06-12</td><td>4357</td><td>11</td></tr><tr><td>11951089</td><td>245996</td><td>130</td><td>&quot;amazing game, 10/10&quot;</td><td>false</td><td>true</td><td>0.483912</td><td>0</td><td>0</td><td>2014-09-01 15:31:44 -05</td><td>2025-06-12</td><td>121</td><td>13</td></tr><tr><td>23877431</td><td>246073</td><td>5952</td><td>&quot;I am sure it is fine for some …</td><td>false</td><td>false</td><td>0.479514</td><td>4</td><td>1</td><td>2016-06-27 18:43:35 -05</td><td>2025-06-12</td><td>379</td><td>64</td></tr><tr><td>14714223</td><td>246094</td><td>1522</td><td>&quot;Inspired me to become a surgeo…</td><td>false</td><td>true</td><td>0.54205</td><td>10</td><td>0</td><td>2015-03-01 14:04:13 -05</td><td>2025-06-12</td><td>5429</td><td>20</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>18782168</td><td>207738</td><td>882</td><td>&quot;Great Game, campaign needed be…</td><td>false</td><td>true</td><td>0.472441</td><td>0</td><td>0</td><td>2015-10-30 21:32:00 -05</td><td>2025-06-12</td><td>1952</td><td>51</td></tr><tr><td>16945878</td><td>207738</td><td>1087</td><td>&quot;Had a lot of fun. :)&quot;</td><td>false</td><td>true</td><td>0.5</td><td>0</td><td>0</td><td>2015-07-08 12:52:31 -05</td><td>2025-06-12</td><td>66699</td><td>51</td></tr><tr><td>20592790</td><td>207738</td><td>2918</td><td>&quot;Yeah Its pretty good.&quot;</td><td>false</td><td>true</td><td>0.472441</td><td>0</td><td>0</td><td>2016-01-17 15:46:49 -05</td><td>2025-06-12</td><td>347</td><td>51</td></tr><tr><td>19924464</td><td>207738</td><td>311</td><td>&quot;Lots of fun, Tribal age was th…</td><td>false</td><td>true</td><td>0.472441</td><td>0</td><td>0</td><td>2015-12-24 20:59:51 -05</td><td>2025-06-12</td><td>4159</td><td>51</td></tr><tr><td>19637744</td><td>207738</td><td>1739</td><td>&quot;A smashing Great read, I actua…</td><td>false</td><td>true</td><td>0.472441</td><td>0</td><td>0</td><td>2015-12-11 15:40:19 -05</td><td>2025-06-12</td><td>8623</td><td>51</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:40:40.571646Z",
     "start_time": "2025-08-06T22:40:40.560449Z"
    }
   },
   "cell_type": "code",
   "source": "val_rev_filtered = val_reviews_df.join(games_review_count, on=\"game_index\", how=\"inner\").join(users_reviews_count, on=\"user_index\", how=\"inner\")",
   "id": "4d1a16056b32304a",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:40:46.261132Z",
     "start_time": "2025-08-06T22:40:46.257364Z"
    }
   },
   "cell_type": "code",
   "source": "val_rev_filtered",
   "id": "ad0c2306c76e273b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape: (52_029, 13)\n",
       "┌───────────┬────────────┬────────────┬────────────┬───┬───────────┬───────────┬───────┬───────────┐\n",
       "│ review_id ┆ user_index ┆ game_index ┆ review     ┆ … ┆ timestamp ┆ scrape_da ┆ len   ┆ len_right │\n",
       "│ ---       ┆ ---        ┆ ---        ┆ ---        ┆   ┆ _created  ┆ te        ┆ ---   ┆ ---       │\n",
       "│ i64       ┆ i64        ┆ i64        ┆ str        ┆   ┆ ---       ┆ ---       ┆ u32   ┆ u32       │\n",
       "│           ┆            ┆            ┆            ┆   ┆ datetime[ ┆ date      ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ μs, Ameri ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ ca/Lima]  ┆           ┆       ┆           │\n",
       "╞═══════════╪════════════╪════════════╪════════════╪═══╪═══════════╪═══════════╪═══════╪═══════════╡\n",
       "│ 30499009  ┆ 247547     ┆ 1253       ┆ I don't    ┆ … ┆ 2017-03-1 ┆ 2025-06-1 ┆ 412   ┆ 16        │\n",
       "│           ┆            ┆            ┆ like hack' ┆   ┆ 3         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ n'slash    ┆   ┆ 18:31:48  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆ game…      ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 34397007  ┆ 247574     ┆ 5133       ┆ Love the   ┆ … ┆ 2017-08-2 ┆ 2025-06-1 ┆ 212   ┆ 16        │\n",
       "│           ┆            ┆            ┆ games by   ┆   ┆ 1         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ this       ┆   ┆ 10:12:20  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆ develop…   ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 29404151  ┆ 245984     ┆ 2163       ┆ 10/10      ┆ … ┆ 2017-01-2 ┆ 2025-06-1 ┆ 1801  ┆ 11        │\n",
       "│           ┆            ┆            ┆ Great      ┆   ┆ 1         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ Co-Op expe ┆   ┆ 13:23:17  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆ rience.    ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 29125057  ┆ 246094     ┆ 7912       ┆ Animated   ┆ … ┆ 2017-01-0 ┆ 2025-06-1 ┆ 365   ┆ 20        │\n",
       "│           ┆            ┆            ┆ Wallpapers ┆   ┆ 8         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ are cool,  ┆   ┆ 14:44:54  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆ …          ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 35625841  ┆ 246895     ┆ 1087       ┆ This       ┆ … ┆ 2017-10-1 ┆ 2025-06-1 ┆ 66699 ┆ 16        │\n",
       "│           ┆            ┆            ┆ review was ┆   ┆ 0         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ formerly a ┆   ┆ 15:50:04  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆ thu…       ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ …         ┆ …          ┆ …          ┆ …          ┆ … ┆ …         ┆ …         ┆ …     ┆ …         │\n",
       "│ 32937157  ┆ 170404     ┆ 235        ┆ 5/10 still ┆ … ┆ 2017-06-2 ┆ 2025-06-1 ┆ 325   ┆ 70        │\n",
       "│           ┆            ┆            ┆ waiting on ┆   ┆ 8         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ Katamari…  ┆   ┆ 09:10:50  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 38547436  ┆ 207738     ┆ 710        ┆ Pretty fun ┆ … ┆ 2017-12-2 ┆ 2025-06-1 ┆ 167   ┆ 51        │\n",
       "│           ┆            ┆            ┆ game,      ┆   ┆ 2         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ however    ┆   ┆ 14:49:42  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆ the G…     ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 38750584  ┆ 207738     ┆ 1758       ┆ An amazing ┆ … ┆ 2017-12-2 ┆ 2025-06-1 ┆ 16319 ┆ 51        │\n",
       "│           ┆            ┆            ┆ experience ┆   ┆ 7         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ from sta…  ┆   ┆ 14:33:37  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 38062243  ┆ 207738     ┆ 1512       ┆ Its just   ┆ … ┆ 2017-12-0 ┆ 2025-06-1 ┆ 833   ┆ 51        │\n",
       "│           ┆            ┆            ┆ so dull.   ┆   ┆ 4         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ 13:06:53  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆            ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "│ 34195510  ┆ 207738     ┆ 1429       ┆ I had a    ┆ … ┆ 2017-08-1 ┆ 2025-06-1 ┆ 392   ┆ 51        │\n",
       "│           ┆            ┆            ┆ lot of fun ┆   ┆ 2         ┆ 2         ┆       ┆           │\n",
       "│           ┆            ┆            ┆ with this  ┆   ┆ 12:00:04  ┆           ┆       ┆           │\n",
       "│           ┆            ┆            ┆ g…         ┆   ┆ -05       ┆           ┆       ┆           │\n",
       "└───────────┴────────────┴────────────┴────────────┴───┴───────────┴───────────┴───────┴───────────┘"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (52_029, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>review_id</th><th>user_index</th><th>game_index</th><th>review</th><th>written_during_early_access</th><th>voted_up</th><th>weighted_vote_score</th><th>votes_up</th><th>comment_count</th><th>timestamp_created</th><th>scrape_date</th><th>len</th><th>len_right</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>bool</td><td>bool</td><td>f64</td><td>i64</td><td>i64</td><td>datetime[μs, America/Lima]</td><td>date</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>30499009</td><td>247547</td><td>1253</td><td>&quot;I don&#x27;t like hack&#x27;n&#x27;slash game…</td><td>false</td><td>true</td><td>0.502488</td><td>1</td><td>0</td><td>2017-03-13 18:31:48 -05</td><td>2025-06-12</td><td>412</td><td>16</td></tr><tr><td>34397007</td><td>247574</td><td>5133</td><td>&quot;Love the games by this develop…</td><td>false</td><td>true</td><td>0.5</td><td>0</td><td>0</td><td>2017-08-21 10:12:20 -05</td><td>2025-06-12</td><td>212</td><td>16</td></tr><tr><td>29404151</td><td>245984</td><td>2163</td><td>&quot;10/10 Great Co-Op experience.&quot;</td><td>false</td><td>true</td><td>0.445056</td><td>0</td><td>0</td><td>2017-01-21 13:23:17 -05</td><td>2025-06-12</td><td>1801</td><td>11</td></tr><tr><td>29125057</td><td>246094</td><td>7912</td><td>&quot;Animated Wallpapers are cool, …</td><td>true</td><td>true</td><td>0.533582</td><td>2</td><td>2</td><td>2017-01-08 14:44:54 -05</td><td>2025-06-12</td><td>365</td><td>20</td></tr><tr><td>35625841</td><td>246895</td><td>1087</td><td>&quot;This review was formerly a thu…</td><td>false</td><td>false</td><td>0.498881</td><td>9</td><td>2</td><td>2017-10-10 15:50:04 -05</td><td>2025-06-12</td><td>66699</td><td>16</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>32937157</td><td>170404</td><td>235</td><td>&quot;5/10 still waiting on Katamari…</td><td>false</td><td>true</td><td>0.289843</td><td>2</td><td>0</td><td>2017-06-28 09:10:50 -05</td><td>2025-06-12</td><td>325</td><td>70</td></tr><tr><td>38547436</td><td>207738</td><td>710</td><td>&quot;Pretty fun game, however the G…</td><td>false</td><td>false</td><td>0.605005</td><td>12</td><td>0</td><td>2017-12-22 14:49:42 -05</td><td>2025-06-12</td><td>167</td><td>51</td></tr><tr><td>38750584</td><td>207738</td><td>1758</td><td>&quot;An amazing experience from sta…</td><td>false</td><td>true</td><td>0.5</td><td>2</td><td>0</td><td>2017-12-27 14:33:37 -05</td><td>2025-06-12</td><td>16319</td><td>51</td></tr><tr><td>38062243</td><td>207738</td><td>1512</td><td>&quot;Its just so dull.&quot;</td><td>false</td><td>false</td><td>0.475921</td><td>4</td><td>0</td><td>2017-12-04 13:06:53 -05</td><td>2025-06-12</td><td>833</td><td>51</td></tr><tr><td>34195510</td><td>207738</td><td>1429</td><td>&quot;I had a lot of fun with this g…</td><td>false</td><td>true</td><td>0.5</td><td>0</td><td>0</td><td>2017-08-12 12:00:04 -05</td><td>2025-06-12</td><td>392</td><td>51</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:42:01.718663Z",
     "start_time": "2025-08-06T22:42:01.716489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filtered_train = ReviewsDataset(train_rev_filtered2)\n",
    "filtered_val = ReviewsDataset(val_rev_filtered)"
   ],
   "id": "4e64a42d7911687a",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T22:43:21.376818Z",
     "start_time": "2025-08-06T22:43:21.374182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filtered_train_loader = torch.utils.data.DataLoader(filtered_train, batch_size=256, shuffle=True, pin_memory=True)\n",
    "filtered_val_loader = torch.utils.data.DataLoader(filtered_val, batch_size=256, shuffle=True, pin_memory=True)"
   ],
   "id": "588c39e5fd9da491",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T23:44:20.091103Z",
     "start_time": "2025-08-06T23:44:19.509856Z"
    }
   },
   "cell_type": "code",
   "source": "model = SimpleRetrievalModel(embedding_dim=64).to(device)",
   "id": "72a03c67ab9e9c76",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T23:45:22.837749Z",
     "start_time": "2025-08-06T23:44:20.652594Z"
    }
   },
   "cell_type": "code",
   "source": "train_loss = train(model, filtered_train_loader, filtered_val_loader, torch.optim.Adam(model.parameters(), lr=0.001), epochs=20, device=torch.device(\"cuda\"))",
   "id": "595d579777c7a57a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/1863 - Loss: 22.8077\n",
      "Processing batch 2/1863 - Loss: 22.0271\n",
      "Processing batch 3/1863 - Loss: 22.6050\n",
      "Processing batch 4/1863 - Loss: 22.4268\n",
      "Processing batch 5/1863 - Loss: 23.0112\n",
      "Processing batch 6/1863 - Loss: 21.9440\n",
      "Processing batch 7/1863 - Loss: 21.9081\n",
      "Processing batch 8/1863 - Loss: 22.0075\n",
      "Processing batch 9/1863 - Loss: 22.2608\n",
      "Processing batch 10/1863 - Loss: 22.2519\n",
      "Processing batch 11/1863 - Loss: 22.5610\n",
      "Processing batch 12/1863 - Loss: 21.6746\n",
      "Processing batch 13/1863 - Loss: 22.6561\n",
      "Processing batch 14/1863 - Loss: 22.9144\n",
      "Processing batch 15/1863 - Loss: 22.2682\n",
      "Processing batch 16/1863 - Loss: 21.3811\n",
      "Processing batch 17/1863 - Loss: 23.1097\n",
      "Processing batch 18/1863 - Loss: 22.1073\n",
      "Processing batch 19/1863 - Loss: 21.6227\n",
      "Processing batch 20/1863 - Loss: 21.8905\n",
      "Processing batch 21/1863 - Loss: 22.7935\n",
      "Processing batch 22/1863 - Loss: 22.3241\n",
      "Processing batch 23/1863 - Loss: 22.0870\n",
      "Processing batch 24/1863 - Loss: 22.6202\n",
      "Processing batch 25/1863 - Loss: 21.9030\n",
      "Processing batch 26/1863 - Loss: 22.1084\n",
      "Processing batch 27/1863 - Loss: 21.7660\n",
      "Processing batch 28/1863 - Loss: 22.7229\n",
      "Processing batch 29/1863 - Loss: 21.7094\n",
      "Processing batch 30/1863 - Loss: 21.7022\n",
      "Processing batch 31/1863 - Loss: 23.4799\n",
      "Processing batch 32/1863 - Loss: 20.7526\n",
      "Processing batch 33/1863 - Loss: 21.8952\n",
      "Processing batch 34/1863 - Loss: 21.3482\n",
      "Processing batch 35/1863 - Loss: 22.1441\n",
      "Processing batch 36/1863 - Loss: 22.5446\n",
      "Processing batch 37/1863 - Loss: 21.8967\n",
      "Processing batch 38/1863 - Loss: 22.3832\n",
      "Processing batch 39/1863 - Loss: 21.3614\n",
      "Processing batch 40/1863 - Loss: 21.7634\n",
      "Processing batch 41/1863 - Loss: 21.9866\n",
      "Processing batch 42/1863 - Loss: 22.1720\n",
      "Processing batch 43/1863 - Loss: 22.3739\n",
      "Processing batch 44/1863 - Loss: 22.4380\n",
      "Processing batch 45/1863 - Loss: 22.1655\n",
      "Processing batch 46/1863 - Loss: 22.8259\n",
      "Processing batch 47/1863 - Loss: 22.7040\n",
      "Processing batch 48/1863 - Loss: 23.4397\n",
      "Processing batch 49/1863 - Loss: 21.7653\n",
      "Processing batch 50/1863 - Loss: 22.9107\n",
      "Processing batch 51/1863 - Loss: 22.1926\n",
      "Processing batch 52/1863 - Loss: 22.6244\n",
      "Processing batch 53/1863 - Loss: 22.7596\n",
      "Processing batch 54/1863 - Loss: 22.8621\n",
      "Processing batch 55/1863 - Loss: 22.1329\n",
      "Processing batch 56/1863 - Loss: 23.4791\n",
      "Processing batch 57/1863 - Loss: 21.8918\n",
      "Processing batch 58/1863 - Loss: 22.4992\n",
      "Processing batch 59/1863 - Loss: 21.8443\n",
      "Processing batch 60/1863 - Loss: 21.5730\n",
      "Processing batch 61/1863 - Loss: 21.7247\n",
      "Processing batch 62/1863 - Loss: 21.9227\n",
      "Processing batch 63/1863 - Loss: 21.8403\n",
      "Processing batch 64/1863 - Loss: 22.5601\n",
      "Processing batch 65/1863 - Loss: 22.6910\n",
      "Processing batch 66/1863 - Loss: 22.1865\n",
      "Processing batch 67/1863 - Loss: 22.1922\n",
      "Processing batch 68/1863 - Loss: 22.1148\n",
      "Processing batch 69/1863 - Loss: 22.7290\n",
      "Processing batch 70/1863 - Loss: 22.7031\n",
      "Processing batch 71/1863 - Loss: 21.9651\n",
      "Processing batch 72/1863 - Loss: 21.4793\n",
      "Processing batch 73/1863 - Loss: 22.0187\n",
      "Processing batch 74/1863 - Loss: 21.7606\n",
      "Processing batch 75/1863 - Loss: 22.7032\n",
      "Processing batch 76/1863 - Loss: 22.1209\n",
      "Processing batch 77/1863 - Loss: 21.7976\n",
      "Processing batch 78/1863 - Loss: 22.2302\n",
      "Processing batch 79/1863 - Loss: 22.1832\n",
      "Processing batch 80/1863 - Loss: 22.1405\n",
      "Processing batch 81/1863 - Loss: 21.7756\n",
      "Processing batch 82/1863 - Loss: 21.2473\n",
      "Processing batch 83/1863 - Loss: 22.0486\n",
      "Processing batch 84/1863 - Loss: 21.3107\n",
      "Processing batch 85/1863 - Loss: 21.7247\n",
      "Processing batch 86/1863 - Loss: 21.7699\n",
      "Processing batch 87/1863 - Loss: 21.0060\n",
      "Processing batch 88/1863 - Loss: 21.5665\n",
      "Processing batch 89/1863 - Loss: 21.9057\n",
      "Processing batch 90/1863 - Loss: 21.8170\n",
      "Processing batch 91/1863 - Loss: 22.3341\n",
      "Processing batch 92/1863 - Loss: 22.1749\n",
      "Processing batch 93/1863 - Loss: 22.8794\n",
      "Processing batch 94/1863 - Loss: 22.8784\n",
      "Processing batch 95/1863 - Loss: 21.9402\n",
      "Processing batch 96/1863 - Loss: 21.4688\n",
      "Processing batch 97/1863 - Loss: 22.2541\n",
      "Processing batch 98/1863 - Loss: 22.0581\n",
      "Processing batch 99/1863 - Loss: 22.8793\n",
      "Processing batch 100/1863 - Loss: 22.8911\n",
      "Processing batch 101/1863 - Loss: 21.5509\n",
      "Processing batch 102/1863 - Loss: 21.9605\n",
      "Processing batch 103/1863 - Loss: 21.6925\n",
      "Processing batch 104/1863 - Loss: 20.8999\n",
      "Processing batch 105/1863 - Loss: 21.1920\n",
      "Processing batch 106/1863 - Loss: 22.5443\n",
      "Processing batch 107/1863 - Loss: 19.8811\n",
      "Processing batch 108/1863 - Loss: 22.5002\n",
      "Processing batch 109/1863 - Loss: 21.4980\n",
      "Processing batch 110/1863 - Loss: 21.3391\n",
      "Processing batch 111/1863 - Loss: 21.5543\n",
      "Processing batch 112/1863 - Loss: 21.3026\n",
      "Processing batch 113/1863 - Loss: 22.3903\n",
      "Processing batch 114/1863 - Loss: 21.7454\n",
      "Processing batch 115/1863 - Loss: 21.3889\n",
      "Processing batch 116/1863 - Loss: 22.2148\n",
      "Processing batch 117/1863 - Loss: 22.5909\n",
      "Processing batch 118/1863 - Loss: 22.0992\n",
      "Processing batch 119/1863 - Loss: 22.7545\n",
      "Processing batch 120/1863 - Loss: 21.7580\n",
      "Processing batch 121/1863 - Loss: 21.9798\n",
      "Processing batch 122/1863 - Loss: 21.9754\n",
      "Processing batch 123/1863 - Loss: 22.0577\n",
      "Processing batch 124/1863 - Loss: 22.0948\n",
      "Processing batch 125/1863 - Loss: 22.1763\n",
      "Processing batch 126/1863 - Loss: 22.4489\n",
      "Processing batch 127/1863 - Loss: 21.0749\n",
      "Processing batch 128/1863 - Loss: 22.2073\n",
      "Processing batch 129/1863 - Loss: 22.9121\n",
      "Processing batch 130/1863 - Loss: 22.3139\n",
      "Processing batch 131/1863 - Loss: 21.9185\n",
      "Processing batch 132/1863 - Loss: 22.3442\n",
      "Processing batch 133/1863 - Loss: 21.9781\n",
      "Processing batch 134/1863 - Loss: 21.4601\n",
      "Processing batch 135/1863 - Loss: 22.5314\n",
      "Processing batch 136/1863 - Loss: 21.7647\n",
      "Processing batch 137/1863 - Loss: 21.4744\n",
      "Processing batch 138/1863 - Loss: 21.0990\n",
      "Processing batch 139/1863 - Loss: 21.9199\n",
      "Processing batch 140/1863 - Loss: 20.4861\n",
      "Processing batch 141/1863 - Loss: 22.6022\n",
      "Processing batch 142/1863 - Loss: 22.4342\n",
      "Processing batch 143/1863 - Loss: 22.3770\n",
      "Processing batch 144/1863 - Loss: 22.7302\n",
      "Processing batch 145/1863 - Loss: 21.9621\n",
      "Processing batch 146/1863 - Loss: 21.7838\n",
      "Processing batch 147/1863 - Loss: 22.3426\n",
      "Processing batch 148/1863 - Loss: 22.7157\n",
      "Processing batch 149/1863 - Loss: 21.8312\n",
      "Processing batch 150/1863 - Loss: 22.2540\n",
      "Processing batch 151/1863 - Loss: 22.7096\n",
      "Processing batch 152/1863 - Loss: 22.2999\n",
      "Processing batch 153/1863 - Loss: 21.8950\n",
      "Processing batch 154/1863 - Loss: 22.2425\n",
      "Processing batch 155/1863 - Loss: 22.4232\n",
      "Processing batch 156/1863 - Loss: 20.6762\n",
      "Processing batch 157/1863 - Loss: 21.9709\n",
      "Processing batch 158/1863 - Loss: 22.5652\n",
      "Processing batch 159/1863 - Loss: 21.6958\n",
      "Processing batch 160/1863 - Loss: 21.4586\n",
      "Processing batch 161/1863 - Loss: 22.0055\n",
      "Processing batch 162/1863 - Loss: 22.1995\n",
      "Processing batch 163/1863 - Loss: 21.1235\n",
      "Processing batch 164/1863 - Loss: 21.0678\n",
      "Processing batch 165/1863 - Loss: 21.7701\n",
      "Processing batch 166/1863 - Loss: 22.3143\n",
      "Processing batch 167/1863 - Loss: 21.5750\n",
      "Processing batch 168/1863 - Loss: 21.2562\n",
      "Processing batch 169/1863 - Loss: 22.0435\n",
      "Processing batch 170/1863 - Loss: 21.8473\n",
      "Processing batch 171/1863 - Loss: 21.8870\n",
      "Processing batch 172/1863 - Loss: 21.4777\n",
      "Processing batch 173/1863 - Loss: 22.4055\n",
      "Processing batch 174/1863 - Loss: 22.4801\n",
      "Processing batch 175/1863 - Loss: 22.5491\n",
      "Processing batch 176/1863 - Loss: 21.6512\n",
      "Processing batch 177/1863 - Loss: 21.3446\n",
      "Processing batch 178/1863 - Loss: 22.2586\n",
      "Processing batch 179/1863 - Loss: 22.9736\n",
      "Processing batch 180/1863 - Loss: 21.7926\n",
      "Processing batch 181/1863 - Loss: 21.5007\n",
      "Processing batch 182/1863 - Loss: 22.2567\n",
      "Processing batch 183/1863 - Loss: 22.0633\n",
      "Processing batch 184/1863 - Loss: 20.9326\n",
      "Processing batch 185/1863 - Loss: 21.1989\n",
      "Processing batch 186/1863 - Loss: 21.9526\n",
      "Processing batch 187/1863 - Loss: 21.7246\n",
      "Processing batch 188/1863 - Loss: 20.6619\n",
      "Processing batch 189/1863 - Loss: 21.7608\n",
      "Processing batch 190/1863 - Loss: 21.5720\n",
      "Processing batch 191/1863 - Loss: 22.5529\n",
      "Processing batch 192/1863 - Loss: 22.2514\n",
      "Processing batch 193/1863 - Loss: 21.2227\n",
      "Processing batch 194/1863 - Loss: 22.4262\n",
      "Processing batch 195/1863 - Loss: 23.1668\n",
      "Processing batch 196/1863 - Loss: 21.1252\n",
      "Processing batch 197/1863 - Loss: 21.4941\n",
      "Processing batch 198/1863 - Loss: 21.1257\n",
      "Processing batch 199/1863 - Loss: 22.4480\n",
      "Processing batch 200/1863 - Loss: 21.4346\n",
      "Processing batch 201/1863 - Loss: 21.7073\n",
      "Processing batch 202/1863 - Loss: 21.8345\n",
      "Processing batch 203/1863 - Loss: 20.9416\n",
      "Processing batch 204/1863 - Loss: 21.3949\n",
      "Processing batch 205/1863 - Loss: 21.6635\n",
      "Processing batch 206/1863 - Loss: 21.4664\n",
      "Processing batch 207/1863 - Loss: 21.4039\n",
      "Processing batch 208/1863 - Loss: 22.4991\n",
      "Processing batch 209/1863 - Loss: 21.9513\n",
      "Processing batch 210/1863 - Loss: 22.1568\n",
      "Processing batch 211/1863 - Loss: 21.9817\n",
      "Processing batch 212/1863 - Loss: 21.5496\n",
      "Processing batch 213/1863 - Loss: 21.5092\n",
      "Processing batch 214/1863 - Loss: 22.0825\n",
      "Processing batch 215/1863 - Loss: 22.3632\n",
      "Processing batch 216/1863 - Loss: 22.0099\n",
      "Processing batch 217/1863 - Loss: 21.8072\n",
      "Processing batch 218/1863 - Loss: 21.8329\n",
      "Processing batch 219/1863 - Loss: 22.1324\n",
      "Processing batch 220/1863 - Loss: 21.1768\n",
      "Processing batch 221/1863 - Loss: 21.6950\n",
      "Processing batch 222/1863 - Loss: 22.4094\n",
      "Processing batch 223/1863 - Loss: 21.5913\n",
      "Processing batch 224/1863 - Loss: 21.9179\n",
      "Processing batch 225/1863 - Loss: 22.0183\n",
      "Processing batch 226/1863 - Loss: 21.9126\n",
      "Processing batch 227/1863 - Loss: 21.6533\n",
      "Processing batch 228/1863 - Loss: 20.7603\n",
      "Processing batch 229/1863 - Loss: 21.3800\n",
      "Processing batch 230/1863 - Loss: 22.1307\n",
      "Processing batch 231/1863 - Loss: 21.6672\n",
      "Processing batch 232/1863 - Loss: 21.3457\n",
      "Processing batch 233/1863 - Loss: 21.2219\n",
      "Processing batch 234/1863 - Loss: 21.6973\n",
      "Processing batch 235/1863 - Loss: 22.3407\n",
      "Processing batch 236/1863 - Loss: 21.6285\n",
      "Processing batch 237/1863 - Loss: 22.5284\n",
      "Processing batch 238/1863 - Loss: 21.7697\n",
      "Processing batch 239/1863 - Loss: 21.5527\n",
      "Processing batch 240/1863 - Loss: 21.3378\n",
      "Processing batch 241/1863 - Loss: 21.4358\n",
      "Processing batch 242/1863 - Loss: 20.4977\n",
      "Processing batch 243/1863 - Loss: 21.0123\n",
      "Processing batch 244/1863 - Loss: 21.5683\n",
      "Processing batch 245/1863 - Loss: 21.2570\n",
      "Processing batch 246/1863 - Loss: 21.4219\n",
      "Processing batch 247/1863 - Loss: 21.6978\n",
      "Processing batch 248/1863 - Loss: 20.9077\n",
      "Processing batch 249/1863 - Loss: 21.3739\n",
      "Processing batch 250/1863 - Loss: 21.9168\n",
      "Processing batch 251/1863 - Loss: 22.1142\n",
      "Processing batch 252/1863 - Loss: 20.4665\n",
      "Processing batch 253/1863 - Loss: 22.2244\n",
      "Processing batch 254/1863 - Loss: 21.0150\n",
      "Processing batch 255/1863 - Loss: 21.8094\n",
      "Processing batch 256/1863 - Loss: 20.9225\n",
      "Processing batch 257/1863 - Loss: 21.5366\n",
      "Processing batch 258/1863 - Loss: 21.8143\n",
      "Processing batch 259/1863 - Loss: 21.1843\n",
      "Processing batch 260/1863 - Loss: 22.2581\n",
      "Processing batch 261/1863 - Loss: 21.6390\n",
      "Processing batch 262/1863 - Loss: 20.7348\n",
      "Processing batch 263/1863 - Loss: 21.4905\n",
      "Processing batch 264/1863 - Loss: 21.7233\n",
      "Processing batch 265/1863 - Loss: 22.4910\n",
      "Processing batch 266/1863 - Loss: 21.7987\n",
      "Processing batch 267/1863 - Loss: 22.0835\n",
      "Processing batch 268/1863 - Loss: 20.5094\n",
      "Processing batch 269/1863 - Loss: 20.8512\n",
      "Processing batch 270/1863 - Loss: 21.1120\n",
      "Processing batch 271/1863 - Loss: 21.0448\n",
      "Processing batch 272/1863 - Loss: 21.0757\n",
      "Processing batch 273/1863 - Loss: 21.8282\n",
      "Processing batch 274/1863 - Loss: 21.4018\n",
      "Processing batch 275/1863 - Loss: 21.9070\n",
      "Processing batch 276/1863 - Loss: 20.8296\n",
      "Processing batch 277/1863 - Loss: 21.3339\n",
      "Processing batch 278/1863 - Loss: 21.8385\n",
      "Processing batch 279/1863 - Loss: 21.6551\n",
      "Processing batch 280/1863 - Loss: 20.9898\n",
      "Processing batch 281/1863 - Loss: 21.8170\n",
      "Processing batch 282/1863 - Loss: 22.0500\n",
      "Processing batch 283/1863 - Loss: 22.1975\n",
      "Processing batch 284/1863 - Loss: 21.2578\n",
      "Processing batch 285/1863 - Loss: 21.8158\n",
      "Processing batch 286/1863 - Loss: 20.6833\n",
      "Processing batch 287/1863 - Loss: 21.1657\n",
      "Processing batch 288/1863 - Loss: 21.0628\n",
      "Processing batch 289/1863 - Loss: 21.3974\n",
      "Processing batch 290/1863 - Loss: 21.4340\n",
      "Processing batch 291/1863 - Loss: 22.1943\n",
      "Processing batch 292/1863 - Loss: 20.3131\n",
      "Processing batch 293/1863 - Loss: 21.9150\n",
      "Processing batch 294/1863 - Loss: 22.1628\n",
      "Processing batch 295/1863 - Loss: 22.1221\n",
      "Processing batch 296/1863 - Loss: 21.1397\n",
      "Processing batch 297/1863 - Loss: 21.1120\n",
      "Processing batch 298/1863 - Loss: 22.3598\n",
      "Processing batch 299/1863 - Loss: 22.9448\n",
      "Processing batch 300/1863 - Loss: 20.6007\n",
      "Processing batch 301/1863 - Loss: 20.2295\n",
      "Processing batch 302/1863 - Loss: 20.4365\n",
      "Processing batch 303/1863 - Loss: 21.6842\n",
      "Processing batch 304/1863 - Loss: 21.0993\n",
      "Processing batch 305/1863 - Loss: 22.0956\n",
      "Processing batch 306/1863 - Loss: 20.7035\n",
      "Processing batch 307/1863 - Loss: 21.4491\n",
      "Processing batch 308/1863 - Loss: 20.9482\n",
      "Processing batch 309/1863 - Loss: 21.8079\n",
      "Processing batch 310/1863 - Loss: 20.5254\n",
      "Processing batch 311/1863 - Loss: 20.8370\n",
      "Processing batch 312/1863 - Loss: 20.9503\n",
      "Processing batch 313/1863 - Loss: 21.5872\n",
      "Processing batch 314/1863 - Loss: 22.1247\n",
      "Processing batch 315/1863 - Loss: 21.8922\n",
      "Processing batch 316/1863 - Loss: 21.5446\n",
      "Processing batch 317/1863 - Loss: 21.3137\n",
      "Processing batch 318/1863 - Loss: 20.9171\n",
      "Processing batch 319/1863 - Loss: 21.1298\n",
      "Processing batch 320/1863 - Loss: 21.5626\n",
      "Processing batch 321/1863 - Loss: 21.2510\n",
      "Processing batch 322/1863 - Loss: 21.7819\n",
      "Processing batch 323/1863 - Loss: 21.5066\n",
      "Processing batch 324/1863 - Loss: 21.7556\n",
      "Processing batch 325/1863 - Loss: 21.5360\n",
      "Processing batch 326/1863 - Loss: 21.3208\n",
      "Processing batch 327/1863 - Loss: 21.9024\n",
      "Processing batch 328/1863 - Loss: 21.0131\n",
      "Processing batch 329/1863 - Loss: 20.6051\n",
      "Processing batch 330/1863 - Loss: 20.8305\n",
      "Processing batch 331/1863 - Loss: 21.1111\n",
      "Processing batch 332/1863 - Loss: 21.1479\n",
      "Processing batch 333/1863 - Loss: 21.4620\n",
      "Processing batch 334/1863 - Loss: 20.5134\n",
      "Processing batch 335/1863 - Loss: 21.3684\n",
      "Processing batch 336/1863 - Loss: 21.1898\n",
      "Processing batch 337/1863 - Loss: 21.6049\n",
      "Processing batch 338/1863 - Loss: 20.6933\n",
      "Processing batch 339/1863 - Loss: 20.9985\n",
      "Processing batch 340/1863 - Loss: 21.0376\n",
      "Processing batch 341/1863 - Loss: 21.7876\n",
      "Processing batch 342/1863 - Loss: 21.2881\n",
      "Processing batch 343/1863 - Loss: 20.7317\n",
      "Processing batch 344/1863 - Loss: 21.4804\n",
      "Processing batch 345/1863 - Loss: 20.4730\n",
      "Processing batch 346/1863 - Loss: 21.1680\n",
      "Processing batch 347/1863 - Loss: 20.3621\n",
      "Processing batch 348/1863 - Loss: 21.6786\n",
      "Processing batch 349/1863 - Loss: 21.7337\n",
      "Processing batch 350/1863 - Loss: 20.4885\n",
      "Processing batch 351/1863 - Loss: 21.7798\n",
      "Processing batch 352/1863 - Loss: 20.9128\n",
      "Processing batch 353/1863 - Loss: 21.0796\n",
      "Processing batch 354/1863 - Loss: 21.3082\n",
      "Processing batch 355/1863 - Loss: 20.9468\n",
      "Processing batch 356/1863 - Loss: 21.3089\n",
      "Processing batch 357/1863 - Loss: 20.8842\n",
      "Processing batch 358/1863 - Loss: 21.2645\n",
      "Processing batch 359/1863 - Loss: 21.1442\n",
      "Processing batch 360/1863 - Loss: 22.4617\n",
      "Processing batch 361/1863 - Loss: 21.1893\n",
      "Processing batch 362/1863 - Loss: 21.6783\n",
      "Processing batch 363/1863 - Loss: 20.8752\n",
      "Processing batch 364/1863 - Loss: 21.4370\n",
      "Processing batch 365/1863 - Loss: 21.7364\n",
      "Processing batch 366/1863 - Loss: 21.1446\n",
      "Processing batch 367/1863 - Loss: 20.7076\n",
      "Processing batch 368/1863 - Loss: 21.8549\n",
      "Processing batch 369/1863 - Loss: 21.2793\n",
      "Processing batch 370/1863 - Loss: 22.0442\n",
      "Processing batch 371/1863 - Loss: 21.0037\n",
      "Processing batch 372/1863 - Loss: 20.6239\n",
      "Processing batch 373/1863 - Loss: 20.9979\n",
      "Processing batch 374/1863 - Loss: 21.5655\n",
      "Processing batch 375/1863 - Loss: 20.6468\n",
      "Processing batch 376/1863 - Loss: 20.9133\n",
      "Processing batch 377/1863 - Loss: 21.4624\n",
      "Processing batch 378/1863 - Loss: 20.9654\n",
      "Processing batch 379/1863 - Loss: 22.1706\n",
      "Processing batch 380/1863 - Loss: 22.0065\n",
      "Processing batch 381/1863 - Loss: 21.8421\n",
      "Processing batch 382/1863 - Loss: 20.7518\n",
      "Processing batch 383/1863 - Loss: 21.1334\n",
      "Processing batch 384/1863 - Loss: 22.0604\n",
      "Processing batch 385/1863 - Loss: 21.1564\n",
      "Processing batch 386/1863 - Loss: 22.4472\n",
      "Processing batch 387/1863 - Loss: 21.2572\n",
      "Processing batch 388/1863 - Loss: 21.1569\n",
      "Processing batch 389/1863 - Loss: 21.1372\n",
      "Processing batch 390/1863 - Loss: 20.8651\n",
      "Processing batch 391/1863 - Loss: 21.6834\n",
      "Processing batch 392/1863 - Loss: 20.9815\n",
      "Processing batch 393/1863 - Loss: 21.3593\n",
      "Processing batch 394/1863 - Loss: 21.7389\n",
      "Processing batch 395/1863 - Loss: 20.6838\n",
      "Processing batch 396/1863 - Loss: 21.9275\n",
      "Processing batch 397/1863 - Loss: 21.0498\n",
      "Processing batch 398/1863 - Loss: 20.7301\n",
      "Processing batch 399/1863 - Loss: 21.5597\n",
      "Processing batch 400/1863 - Loss: 21.3139\n",
      "Processing batch 401/1863 - Loss: 20.7594\n",
      "Processing batch 402/1863 - Loss: 20.6366\n",
      "Processing batch 403/1863 - Loss: 20.8704\n",
      "Processing batch 404/1863 - Loss: 21.9702\n",
      "Processing batch 405/1863 - Loss: 21.4210\n",
      "Processing batch 406/1863 - Loss: 21.7536\n",
      "Processing batch 407/1863 - Loss: 21.1071\n",
      "Processing batch 408/1863 - Loss: 21.2205\n",
      "Processing batch 409/1863 - Loss: 21.7368\n",
      "Processing batch 410/1863 - Loss: 21.4100\n",
      "Processing batch 411/1863 - Loss: 20.2935\n",
      "Processing batch 412/1863 - Loss: 20.9579\n",
      "Processing batch 413/1863 - Loss: 21.8183\n",
      "Processing batch 414/1863 - Loss: 21.0325\n",
      "Processing batch 415/1863 - Loss: 21.2731\n",
      "Processing batch 416/1863 - Loss: 20.3999\n",
      "Processing batch 417/1863 - Loss: 21.8905\n",
      "Processing batch 418/1863 - Loss: 20.8525\n",
      "Processing batch 419/1863 - Loss: 20.9851\n",
      "Processing batch 420/1863 - Loss: 21.1086\n",
      "Processing batch 421/1863 - Loss: 21.4363\n",
      "Processing batch 422/1863 - Loss: 22.4135\n",
      "Processing batch 423/1863 - Loss: 20.4189\n",
      "Processing batch 424/1863 - Loss: 21.8740\n",
      "Processing batch 425/1863 - Loss: 20.9079\n",
      "Processing batch 426/1863 - Loss: 20.8543\n",
      "Processing batch 427/1863 - Loss: 21.8399\n",
      "Processing batch 428/1863 - Loss: 20.3760\n",
      "Processing batch 429/1863 - Loss: 21.5668\n",
      "Processing batch 430/1863 - Loss: 20.4314\n",
      "Processing batch 431/1863 - Loss: 20.5991\n",
      "Processing batch 432/1863 - Loss: 20.7794\n",
      "Processing batch 433/1863 - Loss: 21.2979\n",
      "Processing batch 434/1863 - Loss: 21.1056\n",
      "Processing batch 435/1863 - Loss: 20.8779\n",
      "Processing batch 436/1863 - Loss: 20.8744\n",
      "Processing batch 437/1863 - Loss: 21.1154\n",
      "Processing batch 438/1863 - Loss: 20.9802\n",
      "Processing batch 439/1863 - Loss: 20.6128\n",
      "Processing batch 440/1863 - Loss: 21.2216\n",
      "Processing batch 441/1863 - Loss: 21.2287\n",
      "Processing batch 442/1863 - Loss: 21.2579\n",
      "Processing batch 443/1863 - Loss: 20.3371\n",
      "Processing batch 444/1863 - Loss: 20.6385\n",
      "Processing batch 445/1863 - Loss: 21.5345\n",
      "Processing batch 446/1863 - Loss: 21.3496\n",
      "Processing batch 447/1863 - Loss: 21.9097\n",
      "Processing batch 448/1863 - Loss: 21.7320\n",
      "Processing batch 449/1863 - Loss: 21.6684\n",
      "Processing batch 450/1863 - Loss: 21.4959\n",
      "Processing batch 451/1863 - Loss: 20.9376\n",
      "Processing batch 452/1863 - Loss: 21.5674\n",
      "Processing batch 453/1863 - Loss: 21.1261\n",
      "Processing batch 454/1863 - Loss: 20.8113\n",
      "Processing batch 455/1863 - Loss: 21.3386\n",
      "Processing batch 456/1863 - Loss: 21.1000\n",
      "Processing batch 457/1863 - Loss: 21.7321\n",
      "Processing batch 458/1863 - Loss: 20.9064\n",
      "Processing batch 459/1863 - Loss: 20.9298\n",
      "Processing batch 460/1863 - Loss: 19.8957\n",
      "Processing batch 461/1863 - Loss: 20.2962\n",
      "Processing batch 462/1863 - Loss: 20.5445\n",
      "Processing batch 463/1863 - Loss: 21.9464\n",
      "Processing batch 464/1863 - Loss: 20.2753\n",
      "Processing batch 465/1863 - Loss: 20.2989\n",
      "Processing batch 466/1863 - Loss: 20.5442\n",
      "Processing batch 467/1863 - Loss: 21.1560\n",
      "Processing batch 468/1863 - Loss: 21.7690\n",
      "Processing batch 469/1863 - Loss: 20.4092\n",
      "Processing batch 470/1863 - Loss: 21.1076\n",
      "Processing batch 471/1863 - Loss: 21.7234\n",
      "Processing batch 472/1863 - Loss: 21.3865\n",
      "Processing batch 473/1863 - Loss: 20.7438\n",
      "Processing batch 474/1863 - Loss: 20.7748\n",
      "Processing batch 475/1863 - Loss: 20.8475\n",
      "Processing batch 476/1863 - Loss: 20.8436\n",
      "Processing batch 477/1863 - Loss: 21.1112\n",
      "Processing batch 478/1863 - Loss: 20.2328\n",
      "Processing batch 479/1863 - Loss: 20.9280\n",
      "Processing batch 480/1863 - Loss: 21.3438\n",
      "Processing batch 481/1863 - Loss: 21.1681\n",
      "Processing batch 482/1863 - Loss: 20.9117\n",
      "Processing batch 483/1863 - Loss: 20.6188\n",
      "Processing batch 484/1863 - Loss: 20.4455\n",
      "Processing batch 485/1863 - Loss: 21.4626\n",
      "Processing batch 486/1863 - Loss: 20.2780\n",
      "Processing batch 487/1863 - Loss: 20.9306\n",
      "Processing batch 488/1863 - Loss: 19.7635\n",
      "Processing batch 489/1863 - Loss: 20.7729\n",
      "Processing batch 490/1863 - Loss: 21.0340\n",
      "Processing batch 491/1863 - Loss: 20.2740\n",
      "Processing batch 492/1863 - Loss: 21.8077\n",
      "Processing batch 493/1863 - Loss: 21.6449\n",
      "Processing batch 494/1863 - Loss: 21.7794\n",
      "Processing batch 495/1863 - Loss: 21.4070\n",
      "Processing batch 496/1863 - Loss: 20.6873\n",
      "Processing batch 497/1863 - Loss: 20.8595\n",
      "Processing batch 498/1863 - Loss: 20.1515\n",
      "Processing batch 499/1863 - Loss: 19.5805\n",
      "Processing batch 500/1863 - Loss: 20.3058\n",
      "Processing batch 501/1863 - Loss: 20.3789\n",
      "Processing batch 502/1863 - Loss: 21.0011\n",
      "Processing batch 503/1863 - Loss: 19.7364\n",
      "Processing batch 504/1863 - Loss: 20.6638\n",
      "Processing batch 505/1863 - Loss: 20.7105\n",
      "Processing batch 506/1863 - Loss: 20.8151\n",
      "Processing batch 507/1863 - Loss: 21.1884\n",
      "Processing batch 508/1863 - Loss: 20.7454\n",
      "Processing batch 509/1863 - Loss: 20.3337\n",
      "Processing batch 510/1863 - Loss: 20.7708\n",
      "Processing batch 511/1863 - Loss: 21.0772\n",
      "Processing batch 512/1863 - Loss: 21.3768\n",
      "Processing batch 513/1863 - Loss: 20.7579\n",
      "Processing batch 514/1863 - Loss: 19.4833\n",
      "Processing batch 515/1863 - Loss: 20.4336\n",
      "Processing batch 516/1863 - Loss: 19.7873\n",
      "Processing batch 517/1863 - Loss: 21.2829\n",
      "Processing batch 518/1863 - Loss: 20.5820\n",
      "Processing batch 519/1863 - Loss: 20.7600\n",
      "Processing batch 520/1863 - Loss: 19.7872\n",
      "Processing batch 521/1863 - Loss: 19.6562\n",
      "Processing batch 522/1863 - Loss: 20.3404\n",
      "Processing batch 523/1863 - Loss: 20.5352\n",
      "Processing batch 524/1863 - Loss: 21.1152\n",
      "Processing batch 525/1863 - Loss: 22.8111\n",
      "Processing batch 526/1863 - Loss: 20.3894\n",
      "Processing batch 527/1863 - Loss: 20.8145\n",
      "Processing batch 528/1863 - Loss: 20.3034\n",
      "Processing batch 529/1863 - Loss: 19.6280\n",
      "Processing batch 530/1863 - Loss: 21.1254\n",
      "Processing batch 531/1863 - Loss: 21.2559\n",
      "Processing batch 532/1863 - Loss: 21.1536\n",
      "Processing batch 533/1863 - Loss: 20.5849\n",
      "Processing batch 534/1863 - Loss: 20.9994\n",
      "Processing batch 535/1863 - Loss: 20.7570\n",
      "Processing batch 536/1863 - Loss: 20.1538\n",
      "Processing batch 537/1863 - Loss: 20.4634\n",
      "Processing batch 538/1863 - Loss: 20.2324\n",
      "Processing batch 539/1863 - Loss: 20.2377\n",
      "Processing batch 540/1863 - Loss: 20.5635\n",
      "Processing batch 541/1863 - Loss: 20.6359\n",
      "Processing batch 542/1863 - Loss: 20.4438\n",
      "Processing batch 543/1863 - Loss: 21.1027\n",
      "Processing batch 544/1863 - Loss: 20.8263\n",
      "Processing batch 545/1863 - Loss: 21.1191\n",
      "Processing batch 546/1863 - Loss: 20.6253\n",
      "Processing batch 547/1863 - Loss: 19.7454\n",
      "Processing batch 548/1863 - Loss: 20.6109\n",
      "Processing batch 549/1863 - Loss: 21.4541\n",
      "Processing batch 550/1863 - Loss: 21.1648\n",
      "Processing batch 551/1863 - Loss: 20.6286\n",
      "Processing batch 552/1863 - Loss: 20.8793\n",
      "Processing batch 553/1863 - Loss: 21.5241\n",
      "Processing batch 554/1863 - Loss: 20.3283\n",
      "Processing batch 555/1863 - Loss: 20.6984\n",
      "Processing batch 556/1863 - Loss: 21.7594\n",
      "Processing batch 557/1863 - Loss: 20.8430\n",
      "Processing batch 558/1863 - Loss: 20.0493\n",
      "Processing batch 559/1863 - Loss: 21.6058\n",
      "Processing batch 560/1863 - Loss: 20.1199\n",
      "Processing batch 561/1863 - Loss: 20.1131\n",
      "Processing batch 562/1863 - Loss: 20.9663\n",
      "Processing batch 563/1863 - Loss: 21.1526\n",
      "Processing batch 564/1863 - Loss: 20.4393\n",
      "Processing batch 565/1863 - Loss: 20.5844\n",
      "Processing batch 566/1863 - Loss: 20.5294\n",
      "Processing batch 567/1863 - Loss: 20.4567\n",
      "Processing batch 568/1863 - Loss: 21.7566\n",
      "Processing batch 569/1863 - Loss: 20.7490\n",
      "Processing batch 570/1863 - Loss: 20.4812\n",
      "Processing batch 571/1863 - Loss: 20.5753\n",
      "Processing batch 572/1863 - Loss: 20.6932\n",
      "Processing batch 573/1863 - Loss: 20.6088\n",
      "Processing batch 574/1863 - Loss: 19.9454\n",
      "Processing batch 575/1863 - Loss: 21.0142\n",
      "Processing batch 576/1863 - Loss: 20.1724\n",
      "Processing batch 577/1863 - Loss: 20.3383\n",
      "Processing batch 578/1863 - Loss: 20.2686\n",
      "Processing batch 579/1863 - Loss: 20.5731\n",
      "Processing batch 580/1863 - Loss: 20.1427\n",
      "Processing batch 581/1863 - Loss: 20.0393\n",
      "Processing batch 582/1863 - Loss: 20.8658\n",
      "Processing batch 583/1863 - Loss: 21.2948\n",
      "Processing batch 584/1863 - Loss: 20.2013\n",
      "Processing batch 585/1863 - Loss: 20.2706\n",
      "Processing batch 586/1863 - Loss: 20.6039\n",
      "Processing batch 587/1863 - Loss: 20.6915\n",
      "Processing batch 588/1863 - Loss: 19.5926\n",
      "Processing batch 589/1863 - Loss: 20.6073\n",
      "Processing batch 590/1863 - Loss: 20.3198\n",
      "Processing batch 591/1863 - Loss: 21.3066\n",
      "Processing batch 592/1863 - Loss: 21.6969\n",
      "Processing batch 593/1863 - Loss: 20.3204\n",
      "Processing batch 594/1863 - Loss: 20.6884\n",
      "Processing batch 595/1863 - Loss: 20.9541\n",
      "Processing batch 596/1863 - Loss: 20.2378\n",
      "Processing batch 597/1863 - Loss: 19.8049\n",
      "Processing batch 598/1863 - Loss: 20.3604\n",
      "Processing batch 599/1863 - Loss: 21.9600\n",
      "Processing batch 600/1863 - Loss: 20.8901\n",
      "Processing batch 601/1863 - Loss: 21.2543\n",
      "Processing batch 602/1863 - Loss: 20.0493\n",
      "Processing batch 603/1863 - Loss: 20.3673\n",
      "Processing batch 604/1863 - Loss: 20.3125\n",
      "Processing batch 605/1863 - Loss: 20.2618\n",
      "Processing batch 606/1863 - Loss: 20.8676\n",
      "Processing batch 607/1863 - Loss: 19.6839\n",
      "Processing batch 608/1863 - Loss: 20.5637\n",
      "Processing batch 609/1863 - Loss: 20.5881\n",
      "Processing batch 610/1863 - Loss: 21.1580\n",
      "Processing batch 611/1863 - Loss: 20.1652\n",
      "Processing batch 612/1863 - Loss: 20.1121\n",
      "Processing batch 613/1863 - Loss: 20.2121\n",
      "Processing batch 614/1863 - Loss: 20.6089\n",
      "Processing batch 615/1863 - Loss: 20.4324\n",
      "Processing batch 616/1863 - Loss: 20.4538\n",
      "Processing batch 617/1863 - Loss: 20.3457\n",
      "Processing batch 618/1863 - Loss: 20.5729\n",
      "Processing batch 619/1863 - Loss: 20.2059\n",
      "Processing batch 620/1863 - Loss: 19.7301\n",
      "Processing batch 621/1863 - Loss: 19.6690\n",
      "Processing batch 622/1863 - Loss: 20.8795\n",
      "Processing batch 623/1863 - Loss: 21.1374\n",
      "Processing batch 624/1863 - Loss: 20.5706\n",
      "Processing batch 625/1863 - Loss: 20.7487\n",
      "Processing batch 626/1863 - Loss: 21.0565\n",
      "Processing batch 627/1863 - Loss: 19.7364\n",
      "Processing batch 628/1863 - Loss: 20.0483\n",
      "Processing batch 629/1863 - Loss: 19.1313\n",
      "Processing batch 630/1863 - Loss: 21.2922\n",
      "Processing batch 631/1863 - Loss: 20.4903\n",
      "Processing batch 632/1863 - Loss: 21.2673\n",
      "Processing batch 633/1863 - Loss: 21.0001\n",
      "Processing batch 634/1863 - Loss: 20.4944\n",
      "Processing batch 635/1863 - Loss: 20.7251\n",
      "Processing batch 636/1863 - Loss: 20.2412\n",
      "Processing batch 637/1863 - Loss: 20.1240\n",
      "Processing batch 638/1863 - Loss: 21.3715\n",
      "Processing batch 639/1863 - Loss: 20.1140\n",
      "Processing batch 640/1863 - Loss: 20.2232\n",
      "Processing batch 641/1863 - Loss: 20.6644\n",
      "Processing batch 642/1863 - Loss: 21.2411\n",
      "Processing batch 643/1863 - Loss: 21.0750\n",
      "Processing batch 644/1863 - Loss: 20.3610\n",
      "Processing batch 645/1863 - Loss: 20.7693\n",
      "Processing batch 646/1863 - Loss: 19.8873\n",
      "Processing batch 647/1863 - Loss: 20.5533\n",
      "Processing batch 648/1863 - Loss: 20.6458\n",
      "Processing batch 649/1863 - Loss: 20.0184\n",
      "Processing batch 650/1863 - Loss: 21.2778\n",
      "Processing batch 651/1863 - Loss: 20.5368\n",
      "Processing batch 652/1863 - Loss: 19.6159\n",
      "Processing batch 653/1863 - Loss: 19.8335\n",
      "Processing batch 654/1863 - Loss: 19.3972\n",
      "Processing batch 655/1863 - Loss: 20.5180\n",
      "Processing batch 656/1863 - Loss: 19.5556\n",
      "Processing batch 657/1863 - Loss: 20.5307\n",
      "Processing batch 658/1863 - Loss: 20.6764\n",
      "Processing batch 659/1863 - Loss: 20.2883\n",
      "Processing batch 660/1863 - Loss: 19.2279\n",
      "Processing batch 661/1863 - Loss: 20.8924\n",
      "Processing batch 662/1863 - Loss: 20.1466\n",
      "Processing batch 663/1863 - Loss: 21.0132\n",
      "Processing batch 664/1863 - Loss: 20.2416\n",
      "Processing batch 665/1863 - Loss: 21.0675\n",
      "Processing batch 666/1863 - Loss: 19.9150\n",
      "Processing batch 667/1863 - Loss: 20.5370\n",
      "Processing batch 668/1863 - Loss: 19.8674\n",
      "Processing batch 669/1863 - Loss: 19.2758\n",
      "Processing batch 670/1863 - Loss: 21.1472\n",
      "Processing batch 671/1863 - Loss: 20.1466\n",
      "Processing batch 672/1863 - Loss: 20.6453\n",
      "Processing batch 673/1863 - Loss: 19.8139\n",
      "Processing batch 674/1863 - Loss: 19.6122\n",
      "Processing batch 675/1863 - Loss: 20.3374\n",
      "Processing batch 676/1863 - Loss: 20.3722\n",
      "Processing batch 677/1863 - Loss: 20.5463\n",
      "Processing batch 678/1863 - Loss: 20.9256\n",
      "Processing batch 679/1863 - Loss: 19.4131\n",
      "Processing batch 680/1863 - Loss: 19.6796\n",
      "Processing batch 681/1863 - Loss: 19.8861\n",
      "Processing batch 682/1863 - Loss: 19.4719\n",
      "Processing batch 683/1863 - Loss: 19.9792\n",
      "Processing batch 684/1863 - Loss: 20.1018\n",
      "Processing batch 685/1863 - Loss: 20.3146\n",
      "Processing batch 686/1863 - Loss: 19.5505\n",
      "Processing batch 687/1863 - Loss: 19.7219\n",
      "Processing batch 688/1863 - Loss: 20.2630\n",
      "Processing batch 689/1863 - Loss: 20.1447\n",
      "Processing batch 690/1863 - Loss: 20.5853\n",
      "Processing batch 691/1863 - Loss: 19.7926\n",
      "Processing batch 692/1863 - Loss: 20.5870\n",
      "Processing batch 693/1863 - Loss: 20.1808\n",
      "Processing batch 694/1863 - Loss: 20.6033\n",
      "Processing batch 695/1863 - Loss: 20.5522\n",
      "Processing batch 696/1863 - Loss: 20.7884\n",
      "Processing batch 697/1863 - Loss: 19.9632\n",
      "Processing batch 698/1863 - Loss: 20.6439\n",
      "Processing batch 699/1863 - Loss: 19.5033\n",
      "Processing batch 700/1863 - Loss: 20.8783\n",
      "Processing batch 701/1863 - Loss: 20.9101\n",
      "Processing batch 702/1863 - Loss: 20.1492\n",
      "Processing batch 703/1863 - Loss: 20.4852\n",
      "Processing batch 704/1863 - Loss: 20.6276\n",
      "Processing batch 705/1863 - Loss: 20.5807\n",
      "Processing batch 706/1863 - Loss: 19.8252\n",
      "Processing batch 707/1863 - Loss: 19.9440\n",
      "Processing batch 708/1863 - Loss: 20.2184\n",
      "Processing batch 709/1863 - Loss: 19.9654\n",
      "Processing batch 710/1863 - Loss: 20.5282\n",
      "Processing batch 711/1863 - Loss: 19.8646\n",
      "Processing batch 712/1863 - Loss: 20.0025\n",
      "Processing batch 713/1863 - Loss: 20.0664\n",
      "Processing batch 714/1863 - Loss: 19.2713\n",
      "Processing batch 715/1863 - Loss: 20.7500\n",
      "Processing batch 716/1863 - Loss: 21.0325\n",
      "Processing batch 717/1863 - Loss: 19.8790\n",
      "Processing batch 718/1863 - Loss: 19.7532\n",
      "Processing batch 719/1863 - Loss: 19.3111\n",
      "Processing batch 720/1863 - Loss: 19.5278\n",
      "Processing batch 721/1863 - Loss: 19.6646\n",
      "Processing batch 722/1863 - Loss: 19.8031\n",
      "Processing batch 723/1863 - Loss: 19.7474\n",
      "Processing batch 724/1863 - Loss: 20.6828\n",
      "Processing batch 725/1863 - Loss: 19.8947\n",
      "Processing batch 726/1863 - Loss: 19.9961\n",
      "Processing batch 727/1863 - Loss: 20.5155\n",
      "Processing batch 728/1863 - Loss: 19.7119\n",
      "Processing batch 729/1863 - Loss: 19.7118\n",
      "Processing batch 730/1863 - Loss: 20.7179\n",
      "Processing batch 731/1863 - Loss: 19.5667\n",
      "Processing batch 732/1863 - Loss: 20.2033\n",
      "Processing batch 733/1863 - Loss: 20.0667\n",
      "Processing batch 734/1863 - Loss: 19.2066\n",
      "Processing batch 735/1863 - Loss: 20.5543\n",
      "Processing batch 736/1863 - Loss: 21.0541\n",
      "Processing batch 737/1863 - Loss: 19.9499\n",
      "Processing batch 738/1863 - Loss: 20.0113\n",
      "Processing batch 739/1863 - Loss: 20.0420\n",
      "Processing batch 740/1863 - Loss: 20.1168\n",
      "Processing batch 741/1863 - Loss: 20.5328\n",
      "Processing batch 742/1863 - Loss: 19.5252\n",
      "Processing batch 743/1863 - Loss: 20.0205\n",
      "Processing batch 744/1863 - Loss: 21.0314\n",
      "Processing batch 745/1863 - Loss: 19.0417\n",
      "Processing batch 746/1863 - Loss: 19.9899\n",
      "Processing batch 747/1863 - Loss: 18.9700\n",
      "Processing batch 748/1863 - Loss: 19.6829\n",
      "Processing batch 749/1863 - Loss: 20.2378\n",
      "Processing batch 750/1863 - Loss: 19.7954\n",
      "Processing batch 751/1863 - Loss: 19.6885\n",
      "Processing batch 752/1863 - Loss: 20.1213\n",
      "Processing batch 753/1863 - Loss: 19.8313\n",
      "Processing batch 754/1863 - Loss: 19.5411\n",
      "Processing batch 755/1863 - Loss: 19.8445\n",
      "Processing batch 756/1863 - Loss: 19.8907\n",
      "Processing batch 757/1863 - Loss: 20.9250\n",
      "Processing batch 758/1863 - Loss: 19.7993\n",
      "Processing batch 759/1863 - Loss: 19.5790\n",
      "Processing batch 760/1863 - Loss: 20.6863\n",
      "Processing batch 761/1863 - Loss: 19.4374\n",
      "Processing batch 762/1863 - Loss: 19.5616\n",
      "Processing batch 763/1863 - Loss: 20.1766\n",
      "Processing batch 764/1863 - Loss: 20.9019\n",
      "Processing batch 765/1863 - Loss: 20.1174\n",
      "Processing batch 766/1863 - Loss: 20.1379\n",
      "Processing batch 767/1863 - Loss: 19.5332\n",
      "Processing batch 768/1863 - Loss: 20.9701\n",
      "Processing batch 769/1863 - Loss: 19.7482\n",
      "Processing batch 770/1863 - Loss: 19.8612\n",
      "Processing batch 771/1863 - Loss: 20.4076\n",
      "Processing batch 772/1863 - Loss: 19.6820\n",
      "Processing batch 773/1863 - Loss: 20.2088\n",
      "Processing batch 774/1863 - Loss: 20.4667\n",
      "Processing batch 775/1863 - Loss: 20.2697\n",
      "Processing batch 776/1863 - Loss: 21.2672\n",
      "Processing batch 777/1863 - Loss: 19.1968\n",
      "Processing batch 778/1863 - Loss: 19.6910\n",
      "Processing batch 779/1863 - Loss: 19.9876\n",
      "Processing batch 780/1863 - Loss: 19.7507\n",
      "Processing batch 781/1863 - Loss: 21.4659\n",
      "Processing batch 782/1863 - Loss: 20.7155\n",
      "Processing batch 783/1863 - Loss: 19.4346\n",
      "Processing batch 784/1863 - Loss: 20.5100\n",
      "Processing batch 785/1863 - Loss: 20.3060\n",
      "Processing batch 786/1863 - Loss: 19.4915\n",
      "Processing batch 787/1863 - Loss: 21.5582\n",
      "Processing batch 788/1863 - Loss: 19.7216\n",
      "Processing batch 789/1863 - Loss: 20.7832\n",
      "Processing batch 790/1863 - Loss: 20.5827\n",
      "Processing batch 791/1863 - Loss: 19.8237\n",
      "Processing batch 792/1863 - Loss: 19.8265\n",
      "Processing batch 793/1863 - Loss: 20.0138\n",
      "Processing batch 794/1863 - Loss: 20.3066\n",
      "Processing batch 795/1863 - Loss: 19.7223\n",
      "Processing batch 796/1863 - Loss: 19.4285\n",
      "Processing batch 797/1863 - Loss: 20.2386\n",
      "Processing batch 798/1863 - Loss: 19.8631\n",
      "Processing batch 799/1863 - Loss: 19.8680\n",
      "Processing batch 800/1863 - Loss: 19.8723\n",
      "Processing batch 801/1863 - Loss: 19.9784\n",
      "Processing batch 802/1863 - Loss: 19.3570\n",
      "Processing batch 803/1863 - Loss: 20.1948\n",
      "Processing batch 804/1863 - Loss: 19.4873\n",
      "Processing batch 805/1863 - Loss: 19.9699\n",
      "Processing batch 806/1863 - Loss: 20.2391\n",
      "Processing batch 807/1863 - Loss: 20.6324\n",
      "Processing batch 808/1863 - Loss: 20.2035\n",
      "Processing batch 809/1863 - Loss: 19.2995\n",
      "Processing batch 810/1863 - Loss: 19.6674\n",
      "Processing batch 811/1863 - Loss: 20.2296\n",
      "Processing batch 812/1863 - Loss: 19.8055\n",
      "Processing batch 813/1863 - Loss: 20.5270\n",
      "Processing batch 814/1863 - Loss: 19.8179\n",
      "Processing batch 815/1863 - Loss: 20.0939\n",
      "Processing batch 816/1863 - Loss: 20.5410\n",
      "Processing batch 817/1863 - Loss: 19.9980\n",
      "Processing batch 818/1863 - Loss: 20.0747\n",
      "Processing batch 819/1863 - Loss: 20.5491\n",
      "Processing batch 820/1863 - Loss: 19.3512\n",
      "Processing batch 821/1863 - Loss: 19.3059\n",
      "Processing batch 822/1863 - Loss: 19.2294\n",
      "Processing batch 823/1863 - Loss: 19.9826\n",
      "Processing batch 824/1863 - Loss: 19.6839\n",
      "Processing batch 825/1863 - Loss: 20.3606\n",
      "Processing batch 826/1863 - Loss: 20.1781\n",
      "Processing batch 827/1863 - Loss: 19.8372\n",
      "Processing batch 828/1863 - Loss: 20.0282\n",
      "Processing batch 829/1863 - Loss: 20.3345\n",
      "Processing batch 830/1863 - Loss: 19.0952\n",
      "Processing batch 831/1863 - Loss: 20.3821\n",
      "Processing batch 832/1863 - Loss: 19.9404\n",
      "Processing batch 833/1863 - Loss: 19.5556\n",
      "Processing batch 834/1863 - Loss: 19.5041\n",
      "Processing batch 835/1863 - Loss: 19.5796\n",
      "Processing batch 836/1863 - Loss: 19.9596\n",
      "Processing batch 837/1863 - Loss: 19.9447\n",
      "Processing batch 838/1863 - Loss: 20.1786\n",
      "Processing batch 839/1863 - Loss: 20.3403\n",
      "Processing batch 840/1863 - Loss: 19.1799\n",
      "Processing batch 841/1863 - Loss: 19.4671\n",
      "Processing batch 842/1863 - Loss: 19.8187\n",
      "Processing batch 843/1863 - Loss: 18.9814\n",
      "Processing batch 844/1863 - Loss: 19.3549\n",
      "Processing batch 845/1863 - Loss: 20.4696\n",
      "Processing batch 846/1863 - Loss: 19.6908\n",
      "Processing batch 847/1863 - Loss: 18.9034\n",
      "Processing batch 848/1863 - Loss: 20.0756\n",
      "Processing batch 849/1863 - Loss: 19.9021\n",
      "Processing batch 850/1863 - Loss: 18.6213\n",
      "Processing batch 851/1863 - Loss: 20.0933\n",
      "Processing batch 852/1863 - Loss: 19.0626\n",
      "Processing batch 853/1863 - Loss: 19.6487\n",
      "Processing batch 854/1863 - Loss: 19.5429\n",
      "Processing batch 855/1863 - Loss: 19.1351\n",
      "Processing batch 856/1863 - Loss: 20.2400\n",
      "Processing batch 857/1863 - Loss: 19.4045\n",
      "Processing batch 858/1863 - Loss: 19.8325\n",
      "Processing batch 859/1863 - Loss: 19.3166\n",
      "Processing batch 860/1863 - Loss: 18.8866\n",
      "Processing batch 861/1863 - Loss: 20.0570\n",
      "Processing batch 862/1863 - Loss: 19.8892\n",
      "Processing batch 863/1863 - Loss: 20.6422\n",
      "Processing batch 864/1863 - Loss: 19.1955\n",
      "Processing batch 865/1863 - Loss: 20.2096\n",
      "Processing batch 866/1863 - Loss: 20.4833\n",
      "Processing batch 867/1863 - Loss: 20.2326\n",
      "Processing batch 868/1863 - Loss: 19.5248\n",
      "Processing batch 869/1863 - Loss: 19.3661\n",
      "Processing batch 870/1863 - Loss: 19.3461\n",
      "Processing batch 871/1863 - Loss: 20.2208\n",
      "Processing batch 872/1863 - Loss: 19.1777\n",
      "Processing batch 873/1863 - Loss: 20.0980\n",
      "Processing batch 874/1863 - Loss: 19.6867\n",
      "Processing batch 875/1863 - Loss: 20.5777\n",
      "Processing batch 876/1863 - Loss: 19.6713\n",
      "Processing batch 877/1863 - Loss: 18.4461\n",
      "Processing batch 878/1863 - Loss: 19.7955\n",
      "Processing batch 879/1863 - Loss: 19.7828\n",
      "Processing batch 880/1863 - Loss: 20.2716\n",
      "Processing batch 881/1863 - Loss: 20.0396\n",
      "Processing batch 882/1863 - Loss: 19.6824\n",
      "Processing batch 883/1863 - Loss: 19.1256\n",
      "Processing batch 884/1863 - Loss: 19.2837\n",
      "Processing batch 885/1863 - Loss: 19.3825\n",
      "Processing batch 886/1863 - Loss: 19.8950\n",
      "Processing batch 887/1863 - Loss: 19.6392\n",
      "Processing batch 888/1863 - Loss: 19.5408\n",
      "Processing batch 889/1863 - Loss: 19.2403\n",
      "Processing batch 890/1863 - Loss: 19.6405\n",
      "Processing batch 891/1863 - Loss: 19.4793\n",
      "Processing batch 892/1863 - Loss: 20.0894\n",
      "Processing batch 893/1863 - Loss: 20.0310\n",
      "Processing batch 894/1863 - Loss: 19.5838\n",
      "Processing batch 895/1863 - Loss: 19.2284\n",
      "Processing batch 896/1863 - Loss: 19.7772\n",
      "Processing batch 897/1863 - Loss: 20.4110\n",
      "Processing batch 898/1863 - Loss: 19.4257\n",
      "Processing batch 899/1863 - Loss: 20.3555\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfiltered_train_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfiltered_val_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[36], line 10\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, val_loader, optimizer, epochs, device, scheduler)\u001B[0m\n\u001B[1;32m      8\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m---> 10\u001B[0m     train_loss, val_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m scheduler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     12\u001B[0m         scheduler\u001B[38;5;241m.\u001B[39mstep()\n",
      "Cell \u001B[0;32mIn[35], line 15\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(model, optimizer, train_loader, val_loader, device)\u001B[0m\n\u001B[1;32m     13\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     14\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 15\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     17\u001B[0m val_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a085d6de53e836c2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
